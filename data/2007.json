{"abstract":{"entropy":6.7791246505773595,"topics":["genetic algorithm, evolutionary algorithm, problem, optimization problem, recent years, genetic programming, particle swarm, optimization algorithm, natural language, neural networks, algorithm problem, traveling salesman, consider problem, search heuristic, solving problem, artificial intelligence, search algorithm, search space, common longest, solve problem","division multiplexing, novel approach, orthogonal division, frequency division, present novel, orthogonal frequency, orthogonal multiplexing, frequency multiplexing, present algorithm, paper, present approach, paper algorithm, novel algorithm, estimation algorithm, paper based, hmm-based speech, orthogonal ofdm, division ofdm, paper novel, multiplexing ofdm","plays role, web page, multi-agent systems, real world, description logic, constraint satisfaction, markov decision, markov processes, agents, xcs fault, decision processes, autonomous agents, observable pomdps, partially observable, modal logic, spatial reasoning, logic programming, partially pomdps, satisfaction problem, decision making","machine learning, data, reinforcement learning, support vector, learning, data integration, learning classifier, data mining, time series, data management, mutation crossover, classification semi-supervised, information extraction, improve accuracy, vector machine, present data, gradient xcs, learning systems, construction zero-correlation, statistical relational","genetic algorithm, evolutionary algorithm, genetic programming, genetic problem, optimization algorithm, algorithm, evolutionary optimization, solving problem, search algorithm, algorithm problem, algorithm applied, evolutionary problem, problem programming, multi-objective optimization, multiobjective optimization, local search, use algorithm, multi-objective evolutionary, study problem, multi-objective algorithm","recent years, consider problem, recent research, real-world problem, association rules, recent, problem task, sensor networks, genetic approaches, networks, learning networks, recent interest, recent shown, applications problem, applied successfully, applications, problem networks, years research, become popular, task","algorithm reduce, letter scheme, bayesian inference, speaker identification, letter proposes, present simple, scheme based, paper scheme, paper modified, efficient scheme, paper nonlinear, paper networks, present implementation, encryption scheme, reconstruction image, present algorithm, control systems, modified algorithm, scheme, security scheme","division multiplexing, orthogonal division, frequency division, orthogonal frequency, orthogonal multiplexing, frequency multiplexing, orthogonal ofdm, division ofdm, multiplexing ofdm, multiplexing systems, frequency ofdm, low power, proposes systems, proposes frequency, division systems, proposes, frequency systems, power, systems, order","description logic, autonomous agents, logic programming, planning heuristic, artificial intelligence, search planning, knowledge base, practical reasoning, agents goal, techniques planning, case-based reasoning, framework argumentation, logic reasoning, knowledge task, agents, framework allows, planners plans, logic, agents team, logic knowledge","plays role, multi-agent systems, constraint satisfaction, xcs fault, satisfaction problem, general framework, constraint csp, allocation agents, satisfaction csp, ontologies semantic, systems information, problem csp, computing environment, constraint problem, resource agents, framework constraint, systems, reasoning important, resource allocation, present framework","improve accuracy, mutation crossover, scheme watermark, image, watermarking image, wavelet image, present, watermarking scheme, present framework, present algorithm, present image, present objects, present systems, feature, present feature, transform image, present transform, scheme image, coding, present improve","query data, data stream, queries data, type data, top-k data, frequently data, sql query, given data, query, query scoring, information source, obtain data, useful data, data structure, processing data, data source, linear, function, source, query function"],"ranking":[["58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|√?ngel M. P√©rez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ort√≠z-Garc√≠a,Antonio Portilla-Figueras","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Su√°rez,Manuel Valenzuela-Rend√≥n,Hugo Terashima-Mar√≠n,Eduardo Uresti-Charre","58009|GECCO|2007|A gestalt genetic algorithm less details for better search|The basic idea to defend in this paper is that an adequate perception of the search space, sacrificing most of the precision, can paradoxically accelerate the discovery of the most promising solution zones. While any search space can be observed at any scale according to the level of details, there is nothing inherent to the classical metaheuristics to naturally account for this multi-scaling. Nevertheless, the wider the search space the longer the time needed by any metaheuristic to discover and exploit the \"promising\" zones. Any possibility to compress this time is welcome. Abstracting the search space during the search is one such possibility. For instance, a common Ordering Genetic Algorithm (o-GA) is not well suited to efficiently resolve very large instances of the Traveling Salesman Problem (TSP). The mechanism presented here (reminiscent of Gestalt psychology) aims at abstracting the search space by substituting the variables of the problems with macro-versions of them. This substitution allows any given metaheuristic to tackle the problem at various scales or through different multi-resolution lenses. In the TSP problem to be treated here, the towns will simply be aggregated into regions and the metaheuristics will apply on this new one-level-up search space. The whole problem becomes now how to discover the most appropriate regions and to merge this discovery with the running of the o-GA at the new level.|Christophe Philemotte,Hugues Bersini","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","58068|GECCO|2007|Solving the artificial ant on the Santa Fe trail problem in   fitness evaluations|In this paper, we provide an algorithm that systematically considers all small trees in the search space of genetic programming. These small trees are used to generate useful subroutines for genetic programming. This algorithm is tested on the Artificial Ant on the Santa Fe Trail problem, a venerable problem for genetic programming systems. When four levels of iteration are used, the algorithm presented here generates better results than any known published result by a factor of .|Steffen Christensen,Franz Oppacher","58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","44272|IEICE Transations|2007|Particle Swarms for Feature Extraction of Hyperspectral Data|This paper presents a novel feature extraction algorithm based on particle swarms for processing hyperspectral imagery data. Particle swarm optimization, originally developed for global optimization over continuous spaces, is extended to deal with the problem of feature extraction. A formulation utilizing two swarms of particles was developed to optimize simultaneously a desired performance criterion and the number of selected features. Candidate feature sets were evaluated on a regression problem. Artificial neural networks were trained to construct linear and nonlinear models of chemical concentration of glucose in soybean crops. Experimental results utilizing real-world hyperspectral datasets demonstrate the viability of the method. The particle swarms-based approach presented superior performance in comparison with conventional feature extraction methods, on both linear and nonlinear models.|Sildomar Takahashi Monteiro,Yukio Kosugi","58228|GECCO|2007|Adjacency list matchings an ideal genotype for cycle covers|We propose and analyze a novel genotype to represent walk and cycle covers in graphs, namely matchings in the adjacency lists. This representation admits the natural mutation operator of adding a random match and possibly also matching the former partners. To demonstrate the strength of this set-up, we use it to build a simple (+) evolutionary algorithm for the problem of finding an Eulerian cycle in a graph. We analyze several natural variants that stem from different ways to randomly choose the new match. Among other insight, we exhibit a (+) evolutionary algorithm that computes an Euler tour in a graph with $m$ edges in expected optimization time (m log m). This significantly improves the previous best evolutionary solution having expected optimization time (m log m) in the worst-case, but also compares nicely with the runtime of an optimal classical algorithm which is of order (m). A simple coupon collector argument indicates that our optimization time is asymptotically optimal for any randomized search heuristic.|Benjamin Doerr,Daniel Johannsen"],["43950|IEICE Transations|2007|Double Window Cancellation and Combining for OFDM in Time-Invariant Large Delay Spread Channel|In a time-invariant wireless channel, the multipath that exceeds the cyclic prefix (CP) or the guard interval (GI) causes orthogonal frequency division multiplexing (OFDM) systems to hardly achieve high data rate transmission due to the inter-symbol interference (ISI) and the inter-carrier interference (ICI). In this paper the new canceller scheme, named as Double Window Cancellation and Combining (DWCC) is proposed. It includes the entire symbol interval, delayed by multipath as a signal processing window and intends to improve the performance by combining the double windows that can be formed by the pre-and post-ISI cancellation and reconstruction to the received OFDM symbol interfered by the multipath exceeding the guard interval. The proposed scheme has two algorithm structures of the DWCC-I and -II which are distinguished by the operational sequence (Symbol-wise or Group-wise) to the OFDM symbols of the received packet and by the selection of the processing window in the iterative decision feedback processing. Since the performance of the canceller is dependant on the equalization, particularly on the initial equalization, the proposed schemes operate with the time and frequency domain equalizer in the initial and the iterative symbol detection, respectively. For the verification of the proposed schemes, each scheme is evaluated in the turbo coded OFDM for low (QPSK) and high level modulation systems (QAM, QAM), and compared with the conventional canceller with respect to the performance and computational complexity. As a result, the proposed schemes do not have an error floor even for QAM in a severe frequency selective channel.|JunHwan Lee,Yoshihisa Kishiyama,Tomoaki Ohtsuki,Masao Nakagawa","43816|IEICE Transations|2007|MLSE Detection with Blind Linear Prediction and Subcarriers Interpolation for DSTBC-OFDM Systems|This paper proposes low-complexity blind detection for orthogonal frequency division multiplexing (OFDM) systems with the differential space-time block code (DSTBC) under time-varying frequency-selective Rayleigh fading. The detector employs the maximum likelihood sequence estimation (MLSE) in cooperation with the blind linear prediction (BLP), of which prediction coefficients are determined by the method of Lagrange multipliers. Interpolation of channel frequency responses is also applied to the detector in order to reduce the complexity. A complexity analysis and computer simulations demonstrate that the proposed detector can reduce the complexity to about a half, and that the complexity reduction causes only a loss of  dB in average EbN at BER of - when the prediction order and the degree of polynomial approximation are  and , respectively.|Seree Wanichpakdeedecha,Kazuhiko Fukawa,Hiroshi Suzuki,Satoshi Suyama","43877|IEICE Transations|2007|Finite Parameter Model for Doubly-Selective Channel Estimation in OFDM|To describe joint time-and frequency-selective (doubly-selective) channels in mobile broadband wireless communications, we propose to use the finite parameter model based on the same Bessel functions for each tap (Bessel model). An expression of channel estimation mean squared error (MSE) based on the finite parameter models in Orthogonal Frequency Division Multiplexing (OFDM) systems is derived. Then, our Bessel model is compared with commonly used finite parameter models in terms of the channel estimation MSE. Even if the channel taps have different channel correlations and some of the taps do not coincide with the Bessel function, the channel estimation MSE of the Bessel model is shown to be comparable or outperform existing models as validated by Monte-Carlo simulations over an ensemble of channels in typical urban and suburban environments.|Kok Ann Donny Teo,Shuichi Ohno","43953|IEICE Transations|2007|Peak Reduction Improvement in Iterative Clipping and Filtering with a Graded Band-Limiting Filter for OFDM Transmission|The large PAPR of orthogonal frequency division multiplexing (OFDM) transmission is one of the serious problems for mobile communications that require severe power saving. Iterative clipping and filtering is an effective method for the PAPR reduction of OFDM signals. This paper evaluates PAPR reduction effect with a graded band-limiting filter in the iterative clipping and filtering method. The evaluation result by computer simulation shows that the excellent peak reduction effect can be obtained in the fewer iteration numbers by using a roll-off filter instead of the conventional rectangular filter, and the iteration number with the roll-off filter achieving the same PAPR is fewer by twice. The result confirms that the clipping and filtering method by using a graded band-limiting filter can achieve low peak OFDM transmission with less computational complexity.|Toshiyuki Matsuda,Shigeru Tomisato,Masaharu Hata,Hiromasa Fujii,Junichiro Hagiwara","44073|IEICE Transations|2007|A Tree-Structured Blind Algorithm for Joint Parametric Channel Estimation and Non-coherent Data Detection in an OFDM-CDMA Wireless System with Antenna Arrays|A blind joint parametric channel estimation and non-coherent data detection algorithm is proposed for the downlink of an orthogonal-frequency-division-multiplexing code-division-multiple-access (OFDM-CDMA) system with multiple-input-multiple-output (MIMO) antenna arrays. To reduce the computational complexity, we first develop a tree-structured algorithm to estimate high dimensional parameters predominantly describing the involved multipath channels by employing several stages of low dimensional parameter estimation algorithms. In the tree structure, to exploit the space-time distribution of the receive multipath signals, spatial beamformers and spectral filters are adopted for clustered-multipath grouping and path isolation. In conjunction with the multiple access interference (MAI) suppression techniques, the proposed tree architecture algorithm jointly estimates the direction of arrivals, propagation delays, carrier frequency offsets and fading amplitudes of the downlink wireless channels in a MIMO OFDM-CDMA system. With the outputs of the tree architecture, the signals of interest can then be naturally detected with a path-wise maximum ratio combining scheme.|Yung-Yi Wang,Shih-Jen Yang,Jiunn-Tsair Chen","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","43986|IEICE Transations|2007|Analysis of Symmetric Cancellation Coding for OFDM over a Multi-Path Rayleigh Fading Channel|Orthogonal frequency division multiplexing (OFDM) systems for mobile applications suffer from inter-carrier-interference (ICI) due to frequency offset and to time-variation of the channels and from high peak-to-average-power ratio (PAPR). In this paper, we revisit symmetric cancellation coding (SCC) proposed by Sathananthan et al. and compare the effectiveness of SCC with a fixed subtraction combining and the well-known polynomial cancellation coding (PCC) over Rayleigh fading channels with Doppler spread in terms of the signal-to-interference plus noise power ratio (SINR) and bit-error-rate (BER). We also compare SCC with subtraction combining and SCC of Sathananthan et al. with maximum ratio combining (MRC). Our results show that SCC-OFDM with subtraction combining gives higher SINR than PCC-OFDM over the flat Rayleigh fading channel and that this superiority is not maintained under multi-path induced frequency-selective fading unless diversity combining is used. A simulation result shows, however, that SCC-OFDM with subtraction combining may perform better than PCC-OFDM for a certain range of Doppler spread when differential modulation is employed. Finally, we also demonstrate that the SCC-OFDM signal has less PAPR compared to the normal OFDM and PCC-OFDM and hence may be more practical.|Abdullah S. Alaraimi,Takeshi Hashimoto","44046|IEICE Transations|2007|Reduced-Complexity Detection for DPC-OFTDMA System Enhanced by Multi-Layer MIMO-OFDM in Wireless Multimedia Communications|During these years, we have been focusing on developing ultra high-data-rate wireless access systems for future wireless multimedia communications. One of such kind of systems is called DPC-OFTDMA (dynamic parameter controlled orthogonal frequency and time division multiple access) which targets at beyond  Mbps data rate. In order to support higher data rates, e.g., several hundreds of Mbps or even Gbps for future wireless multimedia applications (e.g., streaming video and file transfer), it is necessary to enhance DPC-OFTDMA system based on MIMO-OFDM (multiple-input multiple-output orthogonal frequency division multiplexing) platform. In this paper, we propose an enhanced DPC-OFTDMA system based on Multi-Layer MIMO-OFDM scheme which combines both diversity and multiplexing in order to exploit potentials of both techniques. The performance investigation shows the proposed scheme has better performance than its counterpart based on full-multiplexing MIMO-OFDM scheme. In addition to the Exhaustive Detection (EXD) scheme which applies the same detection algorithm on each subcarrier independently, we propose the Reduced-Complexity Detection (RCD) scheme. The complexity reduction is achieved by exploiting the suboptimal Layer Detection Order and subcarrier correlation. The simulation results show that huge complexity can be reduced with very small performance loss, by using the proposed detection scheme. For example, .% complexity can be cut off with only . dB performance loss for the    enhanced DPC-OFTDMA system.|Ming Lei,Hiroshi Harada","43752|IEICE Transations|2007|Collision Recovery for OFDM System over Wireless Channel|We present an effective method of collision recovery for orthogonal frequency division multiplexing (OFDM)-based communications. For the OFDM system, the modulated message data can be demodulated using the partial time-domain OFDM signal. Therefore, the partial time-domain signal can be adopted to reconstruct the whole OFDM time-domain signal with estimated channel information. This property can be utilized to recover packets from the collisions. Since most collisions are cases in which a long packet collides with a short packet, the collided part is assumed to be short. The simulated results show that the method can recover the two collided packets with a certain probability and can be developed to solve the problem of hidden terminals. This method will dramatically benefit the protocol design of wireless networks, including ad hoc and sensor networks.|Yafei Hou,Masanori Hamamura","44036|IEICE Transations|2007|A Novel Modulation with Parallel Combinatory and High Compaction Multi-Carrier Modulation|In this paper, we propose a new modulation named parallel combinatoryhigh compaction multi-carrier modulation (PCHC-MCM) using the techniques of parallel combinatory orthogonal frequency division multiplexing (PC-OFDM) and high compaction multi-carrier modulation (HC-MCM). Two types of PCHC-MCM systems, which are named as modulated PCHC-MCM system and (unmodulated) PCHC-MCM system, can be designed. The modulated PCHC-MCM system achieves better bit-error rate (BER) performance than that of HC-MCM system with equal bandwidth efficiency (BWE). The PCHC-MCM system can obtain the better peak-to-average power ratio (PAPR) characteristics by selecting appropriate constellation for each subcarrier. On the other hand, since PCHC-MCM can divide the PC-OFDM symbol duration into multiple time-slots, the advantages of frequency hopping (FH) can be applied in the PCHC-MCM system. Therefore, we also combine the PCHC-MCM and frequency hopping multiple access (FHMA) to propose a novel multiple access (MA) system. It can simultaneously transmit multiple users' data within one symbol duration of PC-OFDM.|Yafei Hou,Masanori Hamamura"],["16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","16658|IJCAI|2007|AEMS An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs|Solving large Partially Observable Markov Decision Processes (POMDPs) is a complex task which is often intractable. A lot of effort has been made to develop approximate offline algorithms to solve ever larger POMDPs. However, even state-of-the-art approaches fail to solve large POMDPs in reasonable time. Recent developments in online POMDP search suggest that combining offline computations with online computations is often more efficient and can also considerably reduce the error made by approximate policies computed offline. In the same vein, we propose a new anytime online search algorithm which seeks to minimize, as efficiently as possible, the error made by an approximate value function computed offline. In addition, we show how previous online computations can be reused in following time steps in order to prevent redundant computations. Our preliminary results indicate that our approach is able to tackle large state space and observation space efficiently and under real-time constraints.|St√©phane Ross,Brahim Chaib-draa","66123|AAAI|2007|Continuous State POMDPs for Object Manipulation Tasks|My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. ) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.|Emma Brunskill","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","66182|AAAI|2007|Situated Conversational Agents|A Situated Conversational Agent (SCA) is an agent that engages in dialog about the context within which it is embedded. Situated dialog is characterized by its deep connection to the embedding context, and the precise cross-timing of linguistic and non-linguistic actions. This paper describes initial research into the construction of an SCA that engages in dialog about collaborative physical tasks, in which agents engage in dialog with the joint goal of manipulating the physical context in some manner. Constructing an SCA that can interact naturally in such tasks requires an agent with the ability to interleave planning, action, and observation while operating in a partially observable environment. Consequently, I propose to model an SCA as a Partially Observable Markov Decision Process (POMDP).|William Thompson","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein","16429|IJCAI|2007|First Order Decision Diagrams for Relational MDPs|Markov decision processes capture sequential decision making under uncertainty, where an agent must choose actions so as to optimize long term reward. The paper studies efficient reasoning mechanisms for Relational Markov Decision Processes (RMDP) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them. Two contributions are presented. First, the paper develops First Order Decision Diagrams (FODD), a new compact representation for functions over relational structures, together with a set of operators to combine FODDs, and novel reduction techniques to keep the representation small. Second, the paper shows how FODDs can be used to develop solutions for RMDPs, where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size (number of objects) or instantiation. In particular, a variant of the value iteration algorithm is developed by using special operations over FODDs, and the algorithm is shown to converge to the optimal policy.|Chenggang Wang,Saket Joshi,Roni Khardon"],["57927|GECCO|2007|Improving the human readability of features constructed by genetic programming|The use of machine learning techniques to automatically analyse data for information is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to pre-process data before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract more information about the domain.|Matthew Smith,Larry Bull","80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","44120|IEICE Transations|2007|A Machine Learning Approach for an Indonesian-English Cross Language Question Answering System|We have built a CLQA (Cross Language Question Answering) system for a source language with limited data resources (e.g. Indonesian) using a machine learning approach. The CLQA system consists of four modules question analyzer, keyword translator, passage retriever and answer finder. We used machine learning in two modules, the question classifier (part of the question analyzer) and the answer finder. In the question classifier, we classify the EAT (Expected Answer Type) of a question by using SVM (Support Vector Machine) method. Features for the classification module are basically the output of our shallow question parsing module. To improve the classification score, we use statistical information extracted from our Indonesian corpus. In the answer finder module, using an approach different from the common approach in which answer is located by matching the named entity of the word corpus with the EAT of question, we locate the answer by text chunking the word corpus. The features for the SVM based text chunking process consist of question features, word corpus features and similarity scores between the word corpus and the question keyword. In this way, we eliminate the named entity tagging process for the target document. As for the keyword translator module, we use an Indonesian-English dictionary to translate Indonesian keywords into English. We also use some simple patterns to transform some borrowed English words. The keywords are then combined in boolean queries in order to retrieve relevant passages using IDF scores. We first conducted an experiment using , questions (about % are used as the test data) obtained from  Indonesian college students. We next conducted a similar experiment using the NTCIR (NII Test Collection for IR Systems)  CLQA task by translating the English questions into Indonesian. Compared to the Japanese-English and Chinese-English CLQA results in the NTCIR , we found that our system is superior to others except for one system that uses a high data resource employing  dictionaries. Further, a rough comparison with two other Indonesian-English CLQA systems revealed that our system achieved higher accuracy score.|Ayu Purwarianti,Masatoshi Tsuchiya,Seiichi Nakagawa","58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","58011|GECCO|2007|Introducing fault tolerance to XCS|In this paper, we introduce fault tolerance to XCS and propose a new XCS framework called XCS with Fault Tolerance (XCSFT). As an important branch of learning classifier systems, XCS has been proven capable of evolving maximally accurate, maximally general problem solutions. However, in practice, it oftentimes generates a lot of rules, which lower the readability of the evolved classification model, and thus, people may not be able to get the desired knowledge or useful information out of the model. Inspired by the fault tolerance mechanism proposed in field of data mining, we devise a new XCS framework by integrating the concept and mechanism of fault tolerance into XCS in order to reduce the number of classification rules and therefore to improve the readability of the generated prediction model. A series of $N$-multiplexer experiments, including -bit, -bit, -bit, and -bit multiplexers, are conducted to examine whether XCSFT can accomplish its goal of design. According to the experimental results, XCSFT can offer the same level of prediction accuracy on the test problems as XCS can, while the prediction model evolved by XCSFT consists of significantly fewer classification rules.|Hong-Wei Chen,Ying-Ping Chen","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","66197|AAAI|2007|Refining Rules Incorporated into Knowledge-Based Support Vector Learners Via Successive Linear Programming|Knowledge-based classification and regression methods are especially powerful forms of learning. They allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm, combining that knowledge with data to produce accurate models. A limitation of the use of prior knowledge occurs when the provided knowledge is incorrect. Such knowledge likely still contains useful information, but knowledge-based learners might not be able to fully exploit such information. In fact, incorrect knowledge can lead to poorer models than result from knowledge-free learners. We present a support-vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge, but also suggests changes to the provided knowledge. Our approach is built on the knowledge-based classification and regression methods presented by Fung, Mangasarian, & Shavlik ( ) and by Mangasarian, Shavlik, & Wild (). Experiments on artificial data sets with known properties, as well as on a real-world data set, demonstrate that our method learns more accurate models while also adjusting the provided rules in intuitive ways. Our new algorithm provides an appealing extension to knowledge-based, support-vector learning that is not only able to combine knowledge from rules with data, but is also able to use the data to modify and change those rules to better fit the data.|Richard Maclin,Edward W. Wild,Jude W. Shavlik,Lisa Torrey,Trevor Walker","66054|AAAI|2007|Recognizing Textual Entailment Using a Subsequence Kernel Method|We present a novel approach to recognizing Textual Entailment. Structural features are constructed from abstract tree descriptions, which are automatically extracted from syntactic dependency trees. These features are then applied in a subsequence-kernel-based classifier to learn whether an entailment relation holds between two texts. Our method makes use of machine learning techniques using a limited data set, no external knowledge bases (e.g. WordNet), and no handcrafted inference rules. We achieve an accuracy of .% for text pairs in the Information Extraction and Question Answering task, .% for the RTE- test data, and .% for the RET- test data.|Rui Wang 0005,G√ºnter Neumann","16796|IJCAI|2007|Parametric Kernels for Sequence Data Analysis|A key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. Many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. However, finding an effective transformation scheme for sequence (e.g. time series) data is a difficult task. In this paper, we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwritten character recognition and object recognition from sensor readings. Ordering information is represented by values of a parameter associated with each input data element. A similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a Mercer kernel suitable for use in methods such as support vector machine (SVM). This scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. We apply our method to object and handwritten character recognition tasks and compare against current approaches. The results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. Our contribution is the introduction of a general technique for designing SVM kernels tailored for the classification of sequence data.|Young-In Shin,Donald S. Fussell","16723|IJCAI|2007|Generalized Additive Bayesian Network Classifiers|Bayesian network classifiers (BNC) have received considerable attention in machine learning field. Some special structure BNCs have been proposed and demonstrate promise performance. However, recent researches show that structure learning in BNs may lead to a non-negligible posterior problem, i.e, there might be many structures have similar posterior scores. In this paper, we propose a generalized additive Bayesian network classifiers, which transfers the structure learning problem to a generalized additive models (GAM) learning problem. We first generate a series of very simple BNs, and put them in the framework of GAM, then adopt a gradient-based algorithm to learn the combining parameters, and thus construct a more powerful classifier. On a large suite of benchmark data sets, the proposed approach outperforms many traditional BNCs, such as naive Bayes, TAN, etc, and achieves comparable or better performance in comparison to boosted Bayesian network classifiers.|Jianguo Li,Changshui Zhang,Tao Wang,Yimin Zhang"],["58180|GECCO|2007|Alternative techniques to solve hard multi-objective optimization problems|In this paper, we propose the combination of different optimization techniques in order to solve \"hard\" two- and three-objective optimization problems at a relatively low computational cost. First, we use the -constraint method in order to obtain a few points over (or very near of) the true Pareto front, and then we use an approach based on rough sets to spread these solutions, so that the entire Pareto front can be covered. The constrained single-objective optimizer required by the -constraint method, is the cultured differential evolution, which is an efficient approach for approximating the global optimum of a problem with a low number of fitness function evaluations. The proposed approach is validated using several difficult multi-objective test problems, and our results are compared with respect to a multi-objective evolutionary algorithm representative of the state-of-the-art in the area the NSGA-II.|Ricardo Landa Becerra,Carlos A. Coello Coello,Alfredo Garc√≠a Hern√°ndez-D√≠az,Rafael Caballero,Juli√°n Molina Luque","58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|√?ngel M. P√©rez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ort√≠z-Garc√≠a,Antonio Portilla-Figueras","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","58190|GECCO|2007|Multi-objective hybrid PSO using -fuzzy dominance|This paper describes a PSO-Nelder Mead Simplex hybrid multi-objective optimization algorithm based on a numerical metric called  -fuzzy dominance. Within each iteration of this approach, in addition to the position and velocity update of each particle using PSO, the k-means algorithm is applied to divide the population into smaller sized clusters. The Nelder-Mead simplex algorithm is used separately within each cluster for added local search. The proposed algorithm is shown to perform better than MOPSO on several test problems as well as for the optimization of a genetic model for flowering time control in Arabidopsis. Adding the local search achieves faster convergence, an important feature in computationally intensive optimization of gene networks.|Praveen Koduru,Sanjoy Das,Stephen Welch","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","57919|GECCO|2007|Parallel skeleton for multi-objective optimization|Many real-world problems are based on the optimization of more than one objective function. This work presents a tool for the resolution of multi-objective optimization problems based on the cooperation of a set of algorithms. The invested time in the resolution is decreased by means of a parallel implementation of an evolutionary team algorithm. This model keeps the advantages of heterogeneous island models but also allows to assign more computational resources to the algorithms with better expectations. The elitist scheme applied aims to improve the results obtained with single executions of independent evolutionary algorithms. The user solves the problem without the need of knowing the internal operation details of the used evolutionary algorithms. The computational results obtained on a cluster of PCs for some tests available in the literature are presented.|Coromoto Le√≥n,Gara Miranda,Carlos Segura","58240|GECCO|2007|Symbiotic tabu search|Recombination in the Genetic Algorithm (GA) is supposed to extract the component characteristics from two parents and reassemble them in different combinations hopefully producing an offspring that has the good characteristics of both parents. Symbiotic Combination is formerly introduced as an alternative for sexual recombination operator to overcome the need of explicit design of recombination operators in GA all. This paper presents an optimization algorithm based on using this operator in Tabu Search. The algorithm is benchmarked on two problem sets and is compared with standard genetic algorithm and symbiotic evolutionary adaptation model, showing success rates higher than both cited algorithms.|Ramin Halavati,Saeed Bagheri Shouraki,Bahareh Jafari Jashmi,Mojdeh Jalali Heravi","58186|GECCO|2007|A multi-objective imaging scheduling approach for earth observing satellites|EOSs (Earth Observing Satellites) circle the earth to take shotswhich are requested by customers. To make replete use of resourcesof EOSs, it is required to deal with the problem of united imagingscheduling of EOSs in a given scheduling horizon, which is acomplicated multi-objective combinatorial optimization problem. Inthis paper, we construct a mathematical model for the problem byabstracting imaging constraints of different EOSs. Then we propose anovel multi-objective EOSs imaging scheduling method, which is basedon the Strength Pareto Evolutionary Algorithm . The specialencoding technique and imaging constraint control are applied toguarantee feasibility of solutions. The approach is tested upon fourreal application problems of CBERS EOSs series. From the results, itis confirmed that the proposed approach is effective in solvingmulti-objective EOSs imaging scheduling problems.|Jun Wang,Ning Jing,Jun Li,Zhong Hui Chen","57970|GECCO|2007|A multi-objective approach for the prediction of loan defaults|Credit institutions are seldom faced with problems dealing with single objectives. Often, decisions involving optimizing two or more competing goals simultaneously need to be made, and conventional optimization routines and models are incapable of handling the problems. This study applies Fuzzy dominance based Simplex Genetic Algorithm (a multi-objective evolutionary optimization algorithm) in generating decision rules for predicting loan default in a typical credit institution.|Oluwarotimi Odeh,Praveen Koduru,Sanjoy Das,Allen M. Featherstone,Stephen Welch","58195|GECCO|2007|ICSPEA evolutionary five-axis milling path optimisation|ICSPEA is a novel multi-objective evolutionary algorithm which integrates aspects from the powerful variation operators of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and the well proven multi-objective Strength Pareto Evaluation Scheme of the SPEA . The CMA-ES has already shown excellent performance on various kinds of complex single-objective problems. The evaluation scheme of the SPEA  selects individuals with respect to their current position in the objective space using a scalar index in order to form proper Pareto front approximations. These indices can be used by the CMA-part of ICSPEA for learning and guiding the search towards better Pareto front approximations. The ICSPEA is applied to complex benchmark functions such as an extended n-dimensional Schaffer's function or Quagliarella's problem. The results show that the CMA operator allows ICSPEA to find the Pareto set of the generalised Schaffer's function faster than SPEA . Furthermore, this concept is tested on the complex real-world application of the multi-objective optimization of five-axis milling NC-paths. An application of ICSPEA to the milling-path optimisation problem yielded efficient sets of five-axis NC-paths.|J√∂rn Mehnen,Rajkumar Roy,Petra Kersting,Tobias Wagner"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","16529|IJCAI|2007|Inferring Long-term User Properties Based on Users Location History|Recent development of location technologies enables us to obtain the location history of users. This paper proposes a new method to infer users' longterm properties from their respective location histories. Counting the instances of sensor detection for every user, we can obtain a sensor-user matrix. After generating features from the matrix, a machine learning approach is taken to automatically classify users into different categories for each user property. Inspired by information retrieval research, the problem to infer user properties is reduced to a text categorization problem. We compare weightings of several features and also propose sensor weighting. Our algorithms are evaluated using experimental location data in an office environment.|Yutaka Matsuo,Naoaki Okazaki,Kiyoshi Izumi,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideyuki Nakashima","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,J√ºrgen Branke,Hartmut Schmeck","16478|IJCAI|2007|Probabilistic Mobile Manipulation in Dynamic Environments with Application to Opening Doors|In recent years, probabilistic approaches have found many successful applications to mobile robot localization, and to object state estimation for manipulation. In this paper, we propose a unified approach to these two problems that dynamically models the objects to be manipulated and localizes the robot at the same time. Our approach applies in the common setting where only a lowresolution (cm) grid-map of a building is available, but we also have a high-resolution (.cm) model of the object to be manipulated. Our method is based on defining a unifying probabilistic model over these two representations. The resulting algorithm works in real-time, and estimates the position of objects with sufficient precision for manipulation tasks. We apply our approach to the task of navigating from one office to another (including manipulating doors). Our approach, successfully tested on multiple doors, allows the robot to navigate through a hallway to an office door, grasp and turn the door handle, and continuously manipulate the door as it moves into the office.|Anna Petrovskaya,Andrew Y. Ng","66051|AAAI|2007|Understanding Performance Tradeoffs in Algorithms for Solving Oversubscribed Scheduling|In recent years, planning and scheduling research has paid increasing attention to problems that involve resource oversubscription, where cumulative demand for resources outstrips their availability and some subset of goals or tasks must be excluded. Two basic classes of techniques to solve oversubscribed scheduling problems have emerged searching directly in the space of possible schedules and searching in an alternative space of task permutations (by relying on a schedule builder to provide a mapping to schedule space). In some problem contexts, permutation-based search methods have been shown to outperform schedule-space search methods, while in others the opposite has been shown to be the case. We consider two techniques for which this behavior has been observed TaskSwap (TS), a schedule-space repair search procedure, and Squeaky Wheel Optimization (SWO), a permutation-space scheduling procedure. We analyze the circumstances under which one can be expected to dominate the other. Starting from a real-world scheduling problem where SWO has been shown to outperform TS, we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem, where TS has been found to outperform SWO. Experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate.|Laurence A. Kramer,Laura Barbulescu,Stephen F. Smith","66169|AAAI|2007|Temporal Difference and Policy Search Methods for Reinforcement Learning An Empirical Comparison|Reinforcement learning (RL) methods have become popular in recent years because of their ability to solve complex tasks with minimal feedback. Both genetic algorithms (GAs) and temporal difference (TD) methods have proven effective at solving difficult RL problems, but few rigorous comparisons have been conducted. Thus, no general guidelines describing the methods' relative strengths and weaknesses are available. This paper summarizes a detailed empirical comparison between a GA and a TD method in Keepaway, a standard RL benchmark domain based on robot soccer. The results from this study help isolate the factors critical to the performance of each learning method and yield insights into their general strengths and weaknesses.|Matthew E. Taylor,Shimon Whiteson,Peter Stone","44102|IEICE Transations|2007|A Network Analysis of Genetic Algorithms|In recent years, network analysis has revealed that some real networks have the properties of small-world andor scale-free networks. In this study, a simple Genetic Algorithm (GA) is regarded as a network where each node and each edge respectively represent a population and the possibility of the transition between two nodes. The characteristic path length (CPL), which is one of the most popular criteria in small-world networks, is derived analytically and shows how much the crossover operation affects the path length between two populations. As a result, the crossover operation is not so useful for shortening the CPL.|Hiroyuki Funaya,Kazushi Ikeda"],["43838|IEICE Transations|2007|A Game Theoretic Framework for Fair-Efficient Threshold Parameters Selection in Call Admission Control for CDMA Mobile Multimedia Systems|While efficient use of network resources is an important control objective of call admission control (CAC), the issue of fairness among services should also be taken into account. Game theory provides a suitable framework for formulating such fair and efficient CAC problem. Thus, in this paper, a game theoretic framework for selecting fair-efficient threshold parameters of CAC for the asymmetrical traffic case in CDMA mobile multimedia systems is proposed. For the cooperative game, the arbitration schemes for the interpersonal comparisons of utility and the bargaining problem, including the Nash, Raiffa, and modified Thomson solutions, are investigated. Furthermore, since CAC should be simple and flexible to provide a fast response to diverse QoS call requests during a connection setup, this paper also applies the concept of load factor to the previous Jeon and Jeong's CAC scheme and proposes an approximation approach to reduce the computational complexity (proposed throughput-based CAC scheme). From the numerical results, the proposed throughput-based CAC scheme shows a comparable performance to the previous Jeon and Jeong's CAC scheme while achieving lower computational complexity. All the solutions attain the fairness by satisfying their different fairness senses and efficiency by the Pareto optimality.|Jenjoab Virapanicharoen,Watit Benjapolakul,Kiyomichi Araki","44187|IEICE Transations|2007|Texture Analysis Using Modified Discrete Radon Transform|In this paper, we address the problem of the rotation-invariant texture analysis. For this purpose, we first present a modified version of the discrete Radon transform whose performance, including accuracy and processing time, is significantly better than the conventional transform in direction estimation and categorization of textural images. We then utilize this transform with a rotated version of Gabor filters to propose a new scheme for texture classification. Experimental results on a set of images from the Brodatz album indicate that the proposed scheme outperforms previous works.|Mahmoud R. Hejazi,Yo-Sung Ho","43867|IEICE Transations|2007|Experimental Studies on a Decision-Feedback Channel Tracking Scheme Implemented in FPGA for MIMO-OFDM Systems|This paper describes the experimental evaluation of a testbed with a simple decision-feedback channel tracking scheme for MIMO-OFDM systems. The channel tracking scheme periodically estimates the channel state matrix for each subcarrier from received signals and replicas of the transmitted signal. The estimated channel state matrices, which are obtained at mutually different timings, are combined based on maximum ratio combining and used for MIMO signal detection. The testbed was implemented on field programmable gate arrays (FPGAs) of  scale, which confirms the implementation feasibility of the channel tracking scheme. The packet error rate (PER) and mobility performance of the testbed were measured. The testbed employed a    MIMO channel, zero-forcing algorithm for MIMO signal detection, QAM for the subcarrier modulation scheme, and coding rate of . The proposed scheme suppressed the increase in the required SNR for PER of - to less than  dB when the relative velocity between the transmitter and the receiver was less than  kmh assuming  GHz band operation. In addition, the proposed scheme offers .% better throughput than the conventional scheme. The experimental results demonstrate that the channel tracking scheme implemented in the testbed effectively tracks the fluctuation of a MIMO channel.|Yusuke Asai,Wenjie Jiang,Takeshi Onizawa","43793|IEICE Transations|2007|Asymmetric Traffic Accommodation Using Adaptive Cell Sizing Technique for CDMAFDD Cellular Packet Communications|The traffic with asymmetry between uplink and downlink has recently been getting remarkable on mobile communication systems providing multimedia communication services. In the future mobile communications, the accommodation of asymmetric traffic is essential to realize efficient multimedia mobile communication systems. This paper discusses asymmetric traffic accommodation in CDMAFDD cellular packet communication systems and proposes its efficient scheme using an adaptive cell sizing technique. In the proposed scheme, each base station autonomously controls its coverage area so that almost the same communication quality can be achieved across the service area under the asymmetric traffic conditions. We present some numerical examples to demonstrate the effectiveness of the proposed scheme by using computer simulation. The simulation results show that, under asymmetric traffic conditions, the proposed scheme can provide fair communication quality across the service area in both links and can improve total transmission capacity in the uplink.|Kazuo Mori,Katsuhiro Naito,Hideo Kobayashi,Hamid Aghvami","44134|IEICE Transations|2007|A More Robust Subsampling-Based Image Watermarking|In this letter, we propose a novel subsampling based image watermark sequentially embedding scheme to reduce the risk of common permutation attack. The image is still perceptual after watermarking, and experimental results also show its effectiveness and robustness.|Chih-Cheng Lo,Pao-Tung Wang,Jeng-Shyang Pan,Bin-Yih Liao","43954|IEICE Transations|2007|A New Scheme to Realize the Optimum Watermark Detection for the Additive Embedding Scheme with the Spatial Domain|A typical watermarking scheme consists of an embedding scheme and a detection scheme. In detecting a watermark, there are two kinds of detection errors, a false positive error (FPE) and a false negative error (FNE). A detection scheme is said to be optimum if the FNE probability is minimized for a given FPE probability. In this paper, we present an optimum watermark detection scheme for an additive embedding scheme with a spatial domain. The key idea of the proposed scheme is to use the differences between two brightnesses for detecting a watermark. We prove that under the same FPE probability the FNE probability of the proposed optimum detection scheme is no more than that of the previous optimum detection scheme for the additive embedding scheme with the spatial domain. Then, it is confirmed that for an actual image, the FNE probability of the proposed optimum detection scheme is much lower than that of the previous optimum detection scheme. Moreover, it is confirmed experimentally that the proposed optimum detection scheme can control the FPE probability strictly so that the FPE probability is close to a given probability.|Takaaki Fujita,Maki Yoshida,Toru Fujiwara","43971|IEICE Transations|2007|A Fast Computational Optimization Method Univariate Dynamic Encoding Algorithm for Searches uDEAS|This paper proposes a new computational optimization method modified from the dynamic encoding algorithm for searches (DEAS). Despite the successful optimization performance of DEAS for both benchmark functions and parameter identification, the problem of exponential computation time becomes serious as problem dimension increases. The proposed optimization method named univariate DEAS (uDEAS) is especially implemented to reduce the computation time using a univariate local search scheme. To verify the algorithmic feasibility for global optimization, several test functions are optimized as benchmark. Despite the simpler structure and shorter code length, function optimization performance show that uDEAS is capable of fast and reliable global search for even high dimensional problems.|Jong-Wook Kim,Sang Woo Kim","43975|IEICE Transations|2007|Comments on the Security Proofs of Some Signature Schemes Based on Factorization|We study on the security proof of the improved efficient-Rabin (ERabin) scheme and the F-FDHS scheme. First, we show that the security theorem of the improved ERabin scheme is not correct, and then provide a correct theorem for it. Second, we show that the security theorem of the F-FDHS scheme lacks an assumption. Finally, we present a way to modify the improved ERabin scheme and the F-FDHS scheme.|Wakaha Ogata,Naoya Matsumoto","44250|IEICE Transations|2007|A Security Enhanced Timestamp-Based Password Authentication Scheme Using Smart Cards|The intent of this letter is to propose an efficient time-stamp based password authentication scheme using smart cards. We show various types of forgery attacks against Shen et al.'s timestamp-based password authentication scheme and improve their scheme to ensure robust security for the remote authentication process, keeping all the advantages of their scheme. Our scheme successfully defends the attacks that could be launched against other related previous schemes.|Al-Sakib Khan Pathan,Choong Seon Hong","43991|IEICE Transations|2007|Fair Exchange of Signatures with Multiple Signers|Chen et al. introduced a new notion of a concurrent signature scheme for a fair exchange of signatures with two parties. Chen et al. also proposed a concrete scheme and proved its security under the assumption of discrete logarithm problem. Recently, Hiwatari and Tanaka extended the concept of concurrent signature to many-to-one setting. Hiwatari and Tanaka also proposed a concrete scheme however, it requires some strong assumption to achieve the fair exchange and it is not efficient. This paper gives another construction of concurrent signature for many-to-one setting with multisignature scheme. Hereafter, we call it (n,) concurrent signature scheme. The proposed scheme is more efficient than the scheme of Hiwatari and Tanaka in computation complexity and signature size, and achieves the fair exchange without the assumption required for the scheme of Hiwatari and Tanaka. This paper also gives a construction for the fair exchange of signatures in many-to-many setting, called (n,m) concurrent signature scheme, in appendix.|Yuichi Komano"],["43950|IEICE Transations|2007|Double Window Cancellation and Combining for OFDM in Time-Invariant Large Delay Spread Channel|In a time-invariant wireless channel, the multipath that exceeds the cyclic prefix (CP) or the guard interval (GI) causes orthogonal frequency division multiplexing (OFDM) systems to hardly achieve high data rate transmission due to the inter-symbol interference (ISI) and the inter-carrier interference (ICI). In this paper the new canceller scheme, named as Double Window Cancellation and Combining (DWCC) is proposed. It includes the entire symbol interval, delayed by multipath as a signal processing window and intends to improve the performance by combining the double windows that can be formed by the pre-and post-ISI cancellation and reconstruction to the received OFDM symbol interfered by the multipath exceeding the guard interval. The proposed scheme has two algorithm structures of the DWCC-I and -II which are distinguished by the operational sequence (Symbol-wise or Group-wise) to the OFDM symbols of the received packet and by the selection of the processing window in the iterative decision feedback processing. Since the performance of the canceller is dependant on the equalization, particularly on the initial equalization, the proposed schemes operate with the time and frequency domain equalizer in the initial and the iterative symbol detection, respectively. For the verification of the proposed schemes, each scheme is evaluated in the turbo coded OFDM for low (QPSK) and high level modulation systems (QAM, QAM), and compared with the conventional canceller with respect to the performance and computational complexity. As a result, the proposed schemes do not have an error floor even for QAM in a severe frequency selective channel.|JunHwan Lee,Yoshihisa Kishiyama,Tomoaki Ohtsuki,Masao Nakagawa","43816|IEICE Transations|2007|MLSE Detection with Blind Linear Prediction and Subcarriers Interpolation for DSTBC-OFDM Systems|This paper proposes low-complexity blind detection for orthogonal frequency division multiplexing (OFDM) systems with the differential space-time block code (DSTBC) under time-varying frequency-selective Rayleigh fading. The detector employs the maximum likelihood sequence estimation (MLSE) in cooperation with the blind linear prediction (BLP), of which prediction coefficients are determined by the method of Lagrange multipliers. Interpolation of channel frequency responses is also applied to the detector in order to reduce the complexity. A complexity analysis and computer simulations demonstrate that the proposed detector can reduce the complexity to about a half, and that the complexity reduction causes only a loss of  dB in average EbN at BER of - when the prediction order and the degree of polynomial approximation are  and , respectively.|Seree Wanichpakdeedecha,Kazuhiko Fukawa,Hiroshi Suzuki,Satoshi Suyama","43877|IEICE Transations|2007|Finite Parameter Model for Doubly-Selective Channel Estimation in OFDM|To describe joint time-and frequency-selective (doubly-selective) channels in mobile broadband wireless communications, we propose to use the finite parameter model based on the same Bessel functions for each tap (Bessel model). An expression of channel estimation mean squared error (MSE) based on the finite parameter models in Orthogonal Frequency Division Multiplexing (OFDM) systems is derived. Then, our Bessel model is compared with commonly used finite parameter models in terms of the channel estimation MSE. Even if the channel taps have different channel correlations and some of the taps do not coincide with the Bessel function, the channel estimation MSE of the Bessel model is shown to be comparable or outperform existing models as validated by Monte-Carlo simulations over an ensemble of channels in typical urban and suburban environments.|Kok Ann Donny Teo,Shuichi Ohno","43953|IEICE Transations|2007|Peak Reduction Improvement in Iterative Clipping and Filtering with a Graded Band-Limiting Filter for OFDM Transmission|The large PAPR of orthogonal frequency division multiplexing (OFDM) transmission is one of the serious problems for mobile communications that require severe power saving. Iterative clipping and filtering is an effective method for the PAPR reduction of OFDM signals. This paper evaluates PAPR reduction effect with a graded band-limiting filter in the iterative clipping and filtering method. The evaluation result by computer simulation shows that the excellent peak reduction effect can be obtained in the fewer iteration numbers by using a roll-off filter instead of the conventional rectangular filter, and the iteration number with the roll-off filter achieving the same PAPR is fewer by twice. The result confirms that the clipping and filtering method by using a graded band-limiting filter can achieve low peak OFDM transmission with less computational complexity.|Toshiyuki Matsuda,Shigeru Tomisato,Masaharu Hata,Hiromasa Fujii,Junichiro Hagiwara","43837|IEICE Transations|2007|New Simultaneous Timing and Frequency Synchronization Utilizing Matched Filters for OFDM Systems|Orthogonal frequency division multiplexing (OFDM) is an attractive technique to accomplish wired or wireless broadband communications. Since it has been adopted as the terrestrial digital-video-broadcasting standard in Europe, it has also subsequently been embedded into many broadband communication standards. Many techniques for frame timing and frequency synchronization of OFDM systems have been studied as a result of its increasing importance. We propose a new technique of simultaneously synchronizing frame timing and frequency utilizing matched filters. First, a new short preamble consisting of short sequences multiplied by a DBPSK coded sequence is proposed. Second, we show that the new short preamble results in a new structure for matched filters consisting of a first matched filter, a DBPSK decoder, and a second matched filter. We can avoid the adverse effects of carrier frequency offset (CFO) when frame timing is synchronized because a DBPSK decoder has been deployed between the first and second matched filters. In addition, we show that the CFO can be directly estimated from the peak value of matched filter output. Finally, our simulation results demonstrate that the proposed scheme outperforms the conventional schemes.|Shigenori Kinjo,Hiroshi Ochi","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","43986|IEICE Transations|2007|Analysis of Symmetric Cancellation Coding for OFDM over a Multi-Path Rayleigh Fading Channel|Orthogonal frequency division multiplexing (OFDM) systems for mobile applications suffer from inter-carrier-interference (ICI) due to frequency offset and to time-variation of the channels and from high peak-to-average-power ratio (PAPR). In this paper, we revisit symmetric cancellation coding (SCC) proposed by Sathananthan et al. and compare the effectiveness of SCC with a fixed subtraction combining and the well-known polynomial cancellation coding (PCC) over Rayleigh fading channels with Doppler spread in terms of the signal-to-interference plus noise power ratio (SINR) and bit-error-rate (BER). We also compare SCC with subtraction combining and SCC of Sathananthan et al. with maximum ratio combining (MRC). Our results show that SCC-OFDM with subtraction combining gives higher SINR than PCC-OFDM over the flat Rayleigh fading channel and that this superiority is not maintained under multi-path induced frequency-selective fading unless diversity combining is used. A simulation result shows, however, that SCC-OFDM with subtraction combining may perform better than PCC-OFDM for a certain range of Doppler spread when differential modulation is employed. Finally, we also demonstrate that the SCC-OFDM signal has less PAPR compared to the normal OFDM and PCC-OFDM and hence may be more practical.|Abdullah S. Alaraimi,Takeshi Hashimoto","43791|IEICE Transations|2007|An Effective SLM-PRSC Hybrid Scheme for OFDM PAPR Reduction|In order to improve OFDM (Orthogonal Frequency Division Multiplexing) PAPR (Peak-to-Average Power Ratio) reduction performance of the conventional SLM (SeLective Mapping), we propose an effective SLM-PRSC (PAPR Reduction Sub-Carrier) hybrid scheme. In the proposed scheme, after performing the SLM for the frequency domain OFDM symbol excluding pre-determined PRSC positions, the SLM-PRSC hybrid sequence with the lowest PAPR, which is generated by adding the time domain PRSC sequence to the results of the SLM, is selected as the transmitted OFDM signal. Since the identical PRSC sequences generated a priori are repeatedly used for every OFDM symbol, excessive IFFT (Inverse Fast Fourier Transform) calculation is avoided. Simulation results reveal that the proposed scheme significantly improves the PAPR reduction performance of the conventional SLM, while avoiding excessive increase of IFFT and PAPR calculation.|Seungwoo Han,Suckchel Yang,Yoan Shin","44036|IEICE Transations|2007|A Novel Modulation with Parallel Combinatory and High Compaction Multi-Carrier Modulation|In this paper, we propose a new modulation named parallel combinatoryhigh compaction multi-carrier modulation (PCHC-MCM) using the techniques of parallel combinatory orthogonal frequency division multiplexing (PC-OFDM) and high compaction multi-carrier modulation (HC-MCM). Two types of PCHC-MCM systems, which are named as modulated PCHC-MCM system and (unmodulated) PCHC-MCM system, can be designed. The modulated PCHC-MCM system achieves better bit-error rate (BER) performance than that of HC-MCM system with equal bandwidth efficiency (BWE). The PCHC-MCM system can obtain the better peak-to-average power ratio (PAPR) characteristics by selecting appropriate constellation for each subcarrier. On the other hand, since PCHC-MCM can divide the PC-OFDM symbol duration into multiple time-slots, the advantages of frequency hopping (FH) can be applied in the PCHC-MCM system. Therefore, we also combine the PCHC-MCM and frequency hopping multiple access (FHMA) to propose a novel multiple access (MA) system. It can simultaneously transmit multiple users' data within one symbol duration of PC-OFDM.|Yafei Hou,Masanori Hamamura","44046|IEICE Transations|2007|Reduced-Complexity Detection for DPC-OFTDMA System Enhanced by Multi-Layer MIMO-OFDM in Wireless Multimedia Communications|During these years, we have been focusing on developing ultra high-data-rate wireless access systems for future wireless multimedia communications. One of such kind of systems is called DPC-OFTDMA (dynamic parameter controlled orthogonal frequency and time division multiple access) which targets at beyond  Mbps data rate. In order to support higher data rates, e.g., several hundreds of Mbps or even Gbps for future wireless multimedia applications (e.g., streaming video and file transfer), it is necessary to enhance DPC-OFTDMA system based on MIMO-OFDM (multiple-input multiple-output orthogonal frequency division multiplexing) platform. In this paper, we propose an enhanced DPC-OFTDMA system based on Multi-Layer MIMO-OFDM scheme which combines both diversity and multiplexing in order to exploit potentials of both techniques. The performance investigation shows the proposed scheme has better performance than its counterpart based on full-multiplexing MIMO-OFDM scheme. In addition to the Exhaustive Detection (EXD) scheme which applies the same detection algorithm on each subcarrier independently, we propose the Reduced-Complexity Detection (RCD) scheme. The complexity reduction is achieved by exploiting the suboptimal Layer Detection Order and subcarrier correlation. The simulation results show that huge complexity can be reduced with very small performance loss, by using the proposed detection scheme. For example, .% complexity can be cut off with only . dB performance loss for the    enhanced DPC-OFTDMA system.|Ming Lei,Hiroshi Harada"],["16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","66161|AAAI|2007|Forgetting Actions in Domain Descriptions|Forgetting irrelevantproblematic actions in a domain description can be useful in solving reasoning problems, such as query answering, planning, conftict resolution, prediction, postdiction, etc.. Motivated by such applications, we study what forgetting is, how forgetting can be done, and for which applications forgetting can be useful and how, in the context of reasoning about actions. We study these questions in the action language C (a formalism based on causal explanations), and relate it to forgetting in classical logic and logic programming.|Esra Erdem,Paolo Ferraris","66236|AAAI|2007|Optimal Regression for Reasoning about Knowledge and Actions|We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.|Hans P. van Ditmarsch,Andreas Herzig,Tiago De Lima","16595|IJCAI|2007|Argumentation Based Contract Monitoring in Uncertain Domains|Few existing argumentation frameworks are designed to deal with probabilistic knowledge, and none are designed to represent possibilistic knowledge, making them unsuitable for many real world domains. In this paper we present a subjective logic based framework for argumentation which overcomes this limitation. Reasoning about the state of a literal in this framework can be done in polynomial time. A dialogue game making use of the framework and a utility based heuristic for playing the dialogue game are also presented. We then show how these components can be applied to contract monitoring. The dialogues that emerge bear some similarity to the dialogues that occur when humans argue about contracts, and our approach is highly suited to complex, partially observable domains with fallible sensors where determining environment state cannot be done for free.|Nir Oren,Timothy J. Norman,Alun D. Preece","16787|IJCAI|2007|A Faithful Integration of Description Logics with Logic Programming|Integrating description logics (DL) and logic programming (LP) would produce a very powerful and useful formalism. However, DLs and LP are based on quite different principles, so achieving a seamless integration is not trivial. In this paper, we introduce hybrid MKNF knowledge bases that faithfully integrate DLs with LP using the logic of Minimal Knowledge and Negation as Failure (MKNF) Lifschitz, . We also give reasoning algorithms and tight data complexity bounds for several interesting fragments of our logic.|Boris Motik,Riccardo Rosati","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","16574|IJCAI|2007|Planning for Temporally Extended Goals as Propositional Satisfiability|Planning for temporally extended goals (TEGs) expressed as formulae of Linear-time Temporal Logic (LTL) is a proper generalization of classical planning, not only allowing to specify properties of a goal state but of the whole plan execution. Additionally, LTL formulae can be used to represent domain-specific control knowledge to speed up planning. In this paper we extend SATbased planning for LTL goals (akin to bounded LTL model-checking in verification) to partially ordered plans, thus significantly increasing planning efficiency compared to purely sequential SAT planning. We consider a very relaxed notion of partial ordering and show how planning for LTL goals (without the next-time operator) can be translated into a SAT problem and solved very efficiently. The results extend the practical applicability of SATbased planning to a wider class of planning problems. In addition, they could be applied to solving problems in bounded LTL model-checking more efficiently.|Robert Mattm√ºller,Jussi Rintanen","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang"],["16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","16615|IJCAI|2007|New Constraint Programming Approaches for the Computation of Leximin-Optimal Solutions in Constraint Networks|We study the problem of computing a leximin-optimal solution of a constraint network. This problem is highly motivated by fairness and efficiency requirements in many real-world applications implying human agents. We compare several generic algorithms which solve this problem in a constraint programming framework. The first one is entirely original, and the other ones are partially based on existing works adapted to fit with this problem.|Sylvain Bouveret,Michel Lema√Ætre","66075|AAAI|2007|Allocating Goods on a Graph to Eliminate Envy|We introduce a distributed negotiation framework for multi-agent resource allocation where interactions between agents are limited by a graph defining a negotiation topology. A group of agents may only contract a deal if that group is fully connected according to the negotiation topology. An important criterion for assessing the quality of an allocation of resources, in terms of fairness, is envy-freeness an agent is said to envy another agent if it would prefer to swap places with that other agent. We analyse under what circumstances a sequence of deals respecting the negotiation topology may be expected to converge to a state where no agent envies any of the agents it is directly connected to. We also analyse the computational complexity of a related decision problem, namely the problem of checking whether a given negotiation state admits any deal that would both be beneficial to every agent involved and reduce envy in the agent society.|Yann Chevaleyre,Ulrich Endriss,Nicolas Maudet","44092|IEICE Transations|2007|Agent-Based Speculative Constraint Processing|In this paper, we extend our framework of speculative computation in multi-agent systems by introducing default constraints. In research on multi-agent systems, handling incomplete information due to communication failure or due to other agents' delay in communication is a very important issue. For a solution to this problem, we previously proposed speculative computation based on abduction in the context of master-slave multi-agent systems and gave a procedure in abductive logic programming. In our previous proposal, a master agent prepares a default value for a yesno question in advance, and it performs speculative computation using the default without waiting for a reply to the question. This computation is effective unless the contradictory reply to the default is returned. In this paper, we formalize speculative constraint processing, and propose a correct operational model for such computation so that we can handle not only yesno questions, but also more general types of questions.|Hiroshi Hosobe,Ken Satoh,Philippe Codognet","16505|IJCAI|2007|Market Based Resource Allocation with Incomplete Information|Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).|Bo An,Chunyan Miao,Zhiqi Shen","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66125|AAAI|2007|Transposition Tables for Constraint Satisfaction|In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.|Christophe Lecoutre,Lakhdar Sais,S√©bastien Tabary,Vincent Vidal","16352|IJCAI|2007|Conflict-Driven Answer Set Solving|We introduce a new approach to computing answer sets of logic programs, based on concepts from constraint processing (CSP) and satisfiability checking (SAT). The idea is to view inferences in answer set programming (ASP) as unit propagation on no-goods. This provides us with a uniform constraint-based framework for the different kinds of inferences in ASP. It also allows us to apply advanced techniques from the areas of CSP and SAT. We have implemented our approach in the new ASP solver clasp. Our experiments show that the approach is competitive with state-of-the-art ASP solvers.|Martin Gebser,Benjamin Kaufmann,Andr√© Neumann,Torsten Schaub","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith","66156|AAAI|2007|Counting CSP Solutions Using Generalized XOR Constraints|We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.|Carla P. Gomes,Willem Jan van Hoeve,Ashish Sabharwal,Bart Selman"],["43827|IEICE Transations|2007|Visible Watermarking for Halftone Images|This letter proposes a visible watermarking scheme for halftone images. It exploits HVS filtering to transform the image in binary domain into continuous-tone domain for watermark embedding. Then a codeword search operation converts the watermarked continuous-tone image into binary domain. The scheme is flexible for two weighting factors are involved to adjust the watermark embedding strength and the average intensity of the watermarked image. Moreover, it can be used in some applications where original continuous-tone images are not available and the halftoning method is unknown.|Jeng-Shyang Pan,Hao Luo,Zhe-Ming Lu","16432|IJCAI|2007|Fast Image Alignment Using Anytime Algorithms|Image alignment refers to finding the best transformation from a fixed reference image to a new image of a scene. This process is often guided by similarity measures between images, computed based on the image data. However, in time-critical applications state-of-the-art methods for computing similarity are too slow. Instead of using all the image data to compute similarity, one can use a subset of pixels to improve the speed, but often this comes at the cost of reduced accuracy. This makes the problem of image alignment a natural application domain for deliberation control using anytime algorithms. However, almost no research has been done in this direction. In this paper, we present anytime versions for the computation of two common image similarity measures mean squared difference and mutual information. Off-line, we learn a performance profile specific to each measure, which is then used on-line to select the appropriate amount of pixels to process at each optimization step. When tested against existing techniques, our method achieves comparable quality and robustness with significantly less computation.|Rupert Brooks,Tal Arbel,Doina Precup","43844|IEICE Transations|2007|Attacking Phase Shift Keying Based Watermarking|The letter describes a phase perturbation attack to the Discrete Fourier Transform (DFT) and Phase Shift Keying (PSK) based watermarking scheme which is proposed in . In that paper the watermark information is embedded in the phase of the DFT coefficients. But this kind of PSK based watermarking scheme is very vulnerable to the phase perturbation attack, when some noise is added on the phase of the DFT coefficients, the watermark can't be correctly extracted anymore, while the quality degradation of the attacked watermarked image is visually acceptable.|Jeng-Shyang Pan,Chuang Lin","43981|IEICE Transations|2007|Quadruple Watermarking against Geometrical Attacks Based on Searching for Vertexes|A new quadruple watermarking scheme of digital images against geometrical attacks is proposed in this letter. We treat the center and the four vertexes of the original image as the reference points and embed the same quadruple watermarks by means of polar coordinates, which is geometrically invariant. The center of an image is assumed to not to be removed after rotating, scaling and local distortions according to the general practical image processing. In the watermark extraction process, the vertexes of the image are found by a searching method. Thus watermark synchronization is obtained. Experimental results show that the scheme is robust to the geometrical distortions including rotation, scaling, cropping and local distortions.|Hai-Yan Zhao,Hong-Xia Wang","43775|IEICE Transations|2007|A Semi-Fragile Watermarking Scheme Using Weighted Vote with Sieve and Emphasis for Image Authentication|This paper describes a semi-fragile watermarking scheme for image authentication and tamper-proofing. Each watermark bit is duplicated and randomly embedded in the original image in the discrete wavelet domain by modifying the corresponding image coefficients through quantization. The modifications are made so that they have little effect on the image and that the watermarking is robust against tampering. The watermark image for authentication is reconstructed by taking a weighted vote on the extracted bits. The bits that lose the vote are treated as having been tampered with, and the locations of the lost bits as indicating tampered positions. Thus, authentication and tamper-proofing can be done by observing the images of watermarks that win and lose votes. Sieving, emphasis, and weighted vote were found to be effectively make the authentication and tamper detection more accurate. The proposed scheme is robust against JPEG compression or acceptable modifications, but sensitive to malicious attacks such as cutting and pasting.|Nozomi Ishihara,K√¥ki Abe","66091|AAAI|2007|Detection of Multiple Deformable Objects using PCA-SIFT|In this paper, we address the problem of identifying and localizing multiple instances of highly deformable objects in real-time video data. We present an approach which uses PCA-SIFT (Scale Invariant Feature Transform) in combination with a clustered voting scheme to achieve detection and localization of multiple objects while providing robustness against rapid shape deformation, partial occlusion, and perspective changes. We test our approach in two highly deformable robot domains and evaluate Its performance using ROC (Receiver Operating Characteristic) statistics.|Stefan Zickler,Alexei A. Efros","44134|IEICE Transations|2007|A More Robust Subsampling-Based Image Watermarking|In this letter, we propose a novel subsampling based image watermark sequentially embedding scheme to reduce the risk of common permutation attack. The image is still perceptual after watermarking, and experimental results also show its effectiveness and robustness.|Chih-Cheng Lo,Pao-Tung Wang,Jeng-Shyang Pan,Bin-Yih Liao","43954|IEICE Transations|2007|A New Scheme to Realize the Optimum Watermark Detection for the Additive Embedding Scheme with the Spatial Domain|A typical watermarking scheme consists of an embedding scheme and a detection scheme. In detecting a watermark, there are two kinds of detection errors, a false positive error (FPE) and a false negative error (FNE). A detection scheme is said to be optimum if the FNE probability is minimized for a given FPE probability. In this paper, we present an optimum watermark detection scheme for an additive embedding scheme with a spatial domain. The key idea of the proposed scheme is to use the differences between two brightnesses for detecting a watermark. We prove that under the same FPE probability the FNE probability of the proposed optimum detection scheme is no more than that of the previous optimum detection scheme for the additive embedding scheme with the spatial domain. Then, it is confirmed that for an actual image, the FNE probability of the proposed optimum detection scheme is much lower than that of the previous optimum detection scheme. Moreover, it is confirmed experimentally that the proposed optimum detection scheme can control the FPE probability strictly so that the FPE probability is close to a given probability.|Takaaki Fujita,Maki Yoshida,Toru Fujiwara","44340|IEICE Transations|2007|Content Adaptive Visible Watermarking during Ordered Dithering|This letter presents an improved visible watermarking scheme for halftone images. It incorporates watermark embedding into ordered dither halftoning by threshold modulation. The input images include a continuous-tone host image (e.g. an -bit gray level image) and a binary watermark image, and the output is a halftone image with a visible watermark. Our method is content adaptive because it takes local intensity information of the host image into account. Experimental results demonstrate effectiveness of the proposed technique. It can be used in practical applications for halftone images, such as commercial advertisement, content annotation, copyright announcement, etc.|Hao Luo,Jeng-Shyang Pan,Zhe-Ming Lu","44252|IEICE Transations|2007|A Modified Generalized Hough Transform for Image Search|We present the use of a Modified Generalized Hough Transform (MGHT) and deformable contours for image data retrieval where a given contour, gray-scale, or color template image can be detected in the target image, irrespective of its position, size, rotation, and smooth deformation transformations. Potential template positions are found in the target image using our novel modified Generalized Hough Transform method that takes measurements from the template features by extending a line from each edge contour point in its gradient direction to the other end of the object. The gradient difference is used to create a relationship with the orientation and length of this line segment. Potential matching positions in the target image are then searched by also extending a line from each target edge point to another end along the normal, then looking up the measurements data from the template image. Positions with high votes become candidate positions. Each candidate position is used to find a match by allowing the template to undergo a contour transformation. The deformed template contour is matched with the target by measuring the similarity in contour tangent direction and the smoothness of the matching vector. The deformation parameters are then updated via a Bayesian algorithm to find the best match. To avoid getting stuck in a local minimum solution, a novel coarse-and-fine model for contour matching is included. Results are presented for real images of several kinds including bin picking and fingerprint identification.|Preeyakorn Tipwai,Suthep Madarasmi"],["80825|VLDB|2007|Efficiently Answering Top-k Typicality Queries on Large Databases|Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognition science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. To answer questions like \"Who are the top-k most typical NBA players\", the measure of simple typicality is developed. To answer questions like \"Who are the top-k most typical guards distinguishing guards from other players\", the notion of discriminative typicality is proposed. Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations. () The randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers. () The direct local typicality approximation using VP-trees provides an approximation quality guarantee. () A VP-tree can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree data structure. An extensive performance study using two real data sets and a series of synthetic data sets clearly show that top-k typicality queries are meaningful and our methods are practical.|Ming Hua,Jian Pei,Ada Wai-Chee Fu,Xuemin Lin,Ho-fung Leung","80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","80780|VLDB|2007|Approaching the Skyline in Z Order|Given a set of multidimensional data points, skyline query retrieves a set of data points that are not dominated by any other points. This query is useful for multi-preference analysis and decision making. By analyzing the skyline query, we observe a close connection between Z-order curve and skyline processing strategies and propose to use a new index structure called ZBtree, to index and store data points based on Z-order curve. We develop a suite of novel and efficient skyline algorithms, which scale very well to data dimensionality and cardinality, including () ZSearch, which processes skyline queries and supports progressive result delivery () ZUpdate, which facilitates incremental skyline result maintenance and () k-ZSearch, which answers k-dominant skyline query (a skyline variant that retrieves a representative subset of skyline results). Extensive experiments have been conducted to evaluate our proposed algorithms and compare them against the best available algorithms designed for skyline search, skyline result update, and k-dominant skyline search, respectively. The result shows that our algorithms, developed coherently based on the same ideas and concepts, soundly outperforms the state-of-the-art skyline algorithms in their specialized domains.|Ken C. K. Lee,Baihua Zheng,Huajing Li,Wang-Chien Lee","80784|VLDB|2007|Peer-to-Peer Similarity Search in Metric Spaces|This paper addresses the efficient processing of similarity queries in metric spaces, where data is horizontally distributed across a PP network. The proposed approach does not rely on arbitrary data movement, hence each peer joining the network autonomously stores its own data. We present SIMPEER, a novel framework that dynamically clusters peer data, in order to build distributed routing information at super-peer level. SIMPEER allows the evaluation of range and nearest neighbor queries in a distributed manner that reduces communication cost, network latency, bandwidth consumption and computational overhead at each individual peer. SIMPEER utilizes a set of distributed statistics and guarantees that all similar objects to the query are retrieved, without necessarily flooding the network during query processing. The statistics are employed for estimating an adequate query radius for k-nearest neighbor queries, and transform the query to a range query. Our experimental evaluation employs both real-world and synthetic data collections, and our results show that SIMPEER performs efficiently, even in the case of high degree of distribution.|Christos Doulkeridis,Akrivi Vlachou,Yannis Kotidis,Michalis Vazirgiannis","80788|VLDB|2007|CADS Continuous Authentication on Data Streams|We study processing and authentication of long-running queries on outsourced data streams. In this scenario, a data owner (DO) constantly transmits its data to a service provider (SP), together with additional authentication information. Clients register continuous range queries to the SP. Whenever the data change, the SP must update the results of all affected queries and inform the clients accordingly. The clients can verify the correctness of the results using the authentication information provided by the DO. Compared to conventional databases, stream environments pose new challenges such as the need for fast structure updating, support for continuous query processing and authentication, and provision for temporal completeness. Specifically, in addition to the correctness of individual results, the client must be able to verify that there are no missing results in between updates. We face these challenges through several contributions. Since there is no previous work, we first present a technique, called REF, that achieves correctness and temporal completeness but incurs false transmissions, i.e., the SP has to inform clients whenever there is a data update, even if their results are not affected. Then, we propose CADS, which minimizes the processing and transmission overhead through an elaborate indexing scheme and a virtual caching mechanism. Finally, we extend CADS to the case where multiple owners outsource their data to the same SP. The SP integrates all data in a single authentication process, independently of the number of DOs.|Stavros Papadopoulos,Yin Yang,Dimitris Papadias","80809|VLDB|2007|A Bayesian Method for Guessing the Extreme Values in a Data Set|For a large number of data management problems, it would be very useful to be able to obtain a few samples from a data set, and to use the samples to guess the largest (or smallest) value in the entire data set. Minmax online aggregation, top-k query processing, outlier detection, and distance join are just a few possible applications. This paper details a statistically rigorous, Bayesian approach to attacking this problem. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.|Mingxi Wu,Chris Jermaine","80840|VLDB|2007|Data Integration with Uncertainty|This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels, and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, queries to the system may be posed with keywords rather than in a structured form. Third, the data from the sources may be extracted using information extraction techniques and so may yield imprecise data. As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings by-table semantics assumes that there exists a correct mapping but we don't know what it is by-tuple semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of approximate schema mappings, and we describe an algorithm for efficiently computing the top-k answers to queries in such a setting.|Xin Luna Dong,Alon Y. Halevy,Cong Yu","80803|VLDB|2007|Request Window an Approach to Improve Throughput of RDBMS-based Data Integration System by Utilizing Data Sharing Across Concurrent Distributed Queries|This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently issue data requests and fetch data over the network. This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a .x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.|Rubao Lee,Minghong Zhou,Huaming Liao","80790|VLDB|2007|A General Framework for Modeling and Processing Optimization Queries|An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function andor a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that IO optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.|Michael Gibas,Ning Zheng,Hakan Ferhatosmanoglu","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"]]},"title":{"entropy":6.505168577980984,"topics":["model for, model, reinforcement learning, framework for, reasoning about, neural networks, model and, transfer learning, and reasoning, for reasoning, learning for, using model, multi-agent systems, learning and, for agents, data mining, probabilistic and, stock markets, gene expression, semi-supervised learning","for systems, description logic, named entity, for channel, for logic, image using, for ofdm, for image, with and, using and, markov processes, and logic, ldpc code, the logic, decision processes, markov decision, for decision, for using, scheme for, random fields","efficient for, the web, method for, for and, for speech, speech recognition, speech synthesis, for planning, search space, machine learning, support vector, architecture for, hmm-based speech, for recognition, search for, estimation for, local search, technique for, based speech, improving performance","algorithm for, genetic programming, particle swarm, genetic algorithm, for the, the, and its, its application, for problem, genetic for, the problem, evolutionary algorithm, for networks, for optimization, the and, algorithm optimization, using genetic, and application, particle optimization, swarm optimization","for data, data mining, for classification, stock markets, and data, gene expression, classification data, security key, rules for, self-organizing map, for mining, the domain, association rules, for sequential, data, association mining, rules with, rules mining, and reading, and rules","learning for, learning and, reinforcement learning, multi-agent systems, learning model, learning, for action, for interface, semi-supervised learning, and online, feature for, selection for, from observation, for systems, feature classification, feature selection, with action, learning with, for task, online for","for using, using and, named entity, using, random fields, for text, image using, conditional fields, conditional random, the boundary, for sequence, method using, method for, phase for, fields using, for segmentation, for communication, using the, boundary using, and communication","with and, for with, ldpc code, with, web page, error using, regular ldpc, data with, error for, code error, and error, for data, fuzzy with, for code, for delay, blind and, blind for, and data, fuzzy for, algorithm with","for speech, estimation for, speech synthesis, estimation and, speech using, for automatic, based speech, fast for, for based, for circuits, speech and, speech enhancement, estimation with, for sensor, motion estimation, speech the, for with, generation for, method based, automatic with","for planning, architecture for, hmm-based speech, for systems, and planning, technique for, hmm-based synthesis, planning with, for query, for synthesis, indexing and, architecture with, for and, large scale, for retrieval, and retrieval, for hmm-based, technique and, module for, simultaneous and","for the, the, the problem, the and, the effects, the functions, for functions, the with, parameter for, functions and, the curve, the heuristic, area the, heuristic for, the some, the brain, target the, towards the, cellular the, the use","genetic programming, genetic for, for design, using genetic, programming and, design and, the design, programming for, using programming, mechanism design, genetic and, case study, design using, design, design with, genetic the, building block, and diversity, genetic with, time the"],"ranking":[["66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|Lu√≠s C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","66186|AAAI|2007|Learning Graphical Model Structure Using L-Regularization Paths|Sparsity-promoting L-regularization has recently been succesfully used to learn the structure of undirected graphical models. In this paper, we apply this technique to learn the structure of directed graphical models. Specifically, we make three contributions. First, we show how the decomposability of the MDL score, plus the ability to quickly compute entire regularization paths, allows us to efficiently pick the optimal regularization parameter on a per-node basis. Second, we show how to use L variable selection to select the Markov blanket, before a DAG search stage. Finally, we show how L variable selection can be used inside of an order search algorithm. The effectiveness of these L-based approaches are compared to current state of the art methods on  datasets.|Mark W. Schmidt,Alexandru Niculescu-Mizil,Kevin P. Murphy","57974|GECCO|2007|Trading rules on stock markets using genetic network programming with sarsa learning|In this paper, the Genetic Network Programming (GNP) for creating trading rules on stocks is described. GNP is an evolutionary computation, which represents its solutions using graph structures and has some useful features inherently. It has been clarified that GNP works well especially in dynamic environments since GNP can create quite compact programs and has an implicit memory function. In this paper, GNP is applied to creating a stock trading model. There are three important points The first important point is to combine GNP with Sarsa Learning which is one of the reinforcement learning algorithms. Evolution-based methods evolve their programs after task execution because they must calculate fitness values, while reinforcement learning can change programs during task execution, therefore the programs can be created efficiently. The second important point is that GNP uses candlestick chart and selects appropriate technical indices to judge the buying and selling timing of stocks. The third important point is that sub-nodes are used in each node to determine appropriate actions (buyingselling) and to select appropriate stock price information depending on the situation. In the simulations, the trading model is trained using the stock prices of  brands in ,  and . Then the generalization ability is tested using the stock prices in . From the simulation results, it is clarified that the trading rules of the proposed method obtain much higher profits than Buy&Hold method and its effectiveness has been confirmed.|Yan Chen,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","44330|IEICE Transations|2007|A Model-Based Learning Process for Modeling Coarticulation of Human Speech|Machine learning techniques have long been applied in many fields and have gained a lot of success. The purpose of learning processes is generally to obtain a set of parameters based on a given data set by minimizing a certain objective function which can explain the data set in a maximum likelihood or minimum estimation error sense. However, most of the learned parameters are highly data dependent and rarely reflect the true physical mechanism that is involved in the observation data. In order to obtain the inherent knowledge involved in the observed data, it is necessary to combine physical models with learning process rather than only fitting the observations with a black box model. To reveal underlying properties of human speech production, we proposed a learning process based on a physiological articulatory model and a coarticulation model, where both of the models are derived from human mechanisms. A two-layer learning framework was designed to learn the parameters concerned with physiological level using the physiological articulatory model and the parameters in the motor planning level using the coarticulation model. The learning process was carried out on an articulatory database of human speech production. The learned parameters were evaluated by numerical experiments and listening tests. The phonetic targets obtained in the planning stage provided an evidence for understanding the virtual targets of human speech production. As a result, the model based learning process reveals the inherent mechanism of the human speech via the learned parameters with certain physical meaning.|Jianguo Wei,Xugang Lu,Jianwu Dang","16376|IJCAI|2007|Semi-Supervised Learning for Multi-Component Data Classification|This paper presents a method for designing a semisupervised classifier for multi-component data such as web pages consisting of text and link information. The proposed method is based on a hybrid of generative and discriminative approaches to take advantage of both approaches. With our hybrid approach, for each component, we consider an individual generative model trained on labeled samples and a model introduced to reduce the effect of the bias that results when there are few labeled samples. Then, we construct a hybrid classifier by combining all the models based on the maximum entropy principle. In our experimental results using three test collections such as web pages and technical papers, we confirmed that our hybrid approach was effective in improving the generalization performance of multi-component data classification.|Akinori Fujino,Naonori Ueda,Kazumi Saito","16769|IJCAI|2007|A Dual-Pathway Neural Network Model of Control Relinquishment in Motor Skill Learning|Cognitive psychologists have long recognized that the acquisition of a motor skill involves a transition from attention-demanding controlled processing to more fluent automatic processing. Neuroscientific studies suggest that controlled and automatic processing rely on two largely distinct neural pathways. The controlled pathway, which includes the prefrontal cortex, is seen as acquiring declarative representations of skills. In comparison, the automatic pathway is thought to develop procedural representations. Automaticity in motor skill learning involves a reduction in dependence on frontal systems and an increased reliance on the automatic pathway. In this paper, we propose a biologically plausible computational model of motor skill automaticity. This model offers a dual-pathway neurocomputational account of the translation of declarative knowledge into procedural knowledge during motor learning. In support of the model, we review some previously reported human experimental results involving the learning of a sequential key pressing task, and we demonstrate, through simulation, howthe model provides a parsimonious explanation for these results.|Ashish Gupta,David C. Noelle"],["43894|IEICE Transations|2007|Logic Synthesis Method for Dual-Rail RSFQ Digital Circuits Using Root-Shared Binary Decision Diagrams|We propose a new method of logic synthesis for dual-rail RSFQ (rapid single-flux-quantum) digital circuits. RSFQ circuit technology is one of the strongest candidates for the next generation technology of digital circuits. For representing logic functions, we use a root-shared binary decision diagram (RSBDD) which is a directed acyclic graph constructed from binary decision diagrams. In the method, first we construct an RSBDD from given logic functions, and then reduce the number of nodes in the constructed RSBDD by variable re-ordering. Finally, we synthesize a dual-rail RSFQ circuit from the reduced RSBDD. We have implemented the method and have synthesized benchmark circuits. We have synthesized dual-rail circuits that consist of about % fewer logic elements than those synthesized by a Transduction-based method on average.|Koji Obata,Kazuyoshi Takagi,Naofumi Takagi","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","16492|IJCAI|2007|Completing Description Logic Knowledge Bases Using Formal Concept Analysis|We propose an approach for extending both the terminological and the assertional part of a Description Logic knowledge base by using information provided by the knowledge base and by a domain expert. The use of techniques from Formal Concept Analysis ensures that, on the one hand, the interaction with the expert is kept to a minimum, and, on the other hand, we can show that the extended knowledge base is complete in a certain, well-defined sense.|Franz Baader,Bernhard Ganter,Baris Sertkaya,Ulrike Sattler","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","66010|AAAI|2007|A Unification of Extensive-Form Games and Markov Decision Processes|We describe a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization is that it places arbitrary convex optimization problems at decision nodes, in place of the finite action sets typically considered. The possibly-infinite action sets mean we must \"forget\" the exact action taken (feasible solution to the optimization problem), remembering instead only some statistic sufficient for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games.|H. Brendan McMahan,Geoffrey J. Gordon","16367|IJCAI|2007|A Fast Analytical Algorithm for Solving Markov Decision Processes with Real-Valued Resources|Agents often have to construct plans that obey deadlines or, more generally, resource limits for real-valued resources whose consumption can only be characterized by probability distributions, such as execution time or battery power. These planning problems can be modeled with continuous state Markov decision processes (MDPs) but existing solution methods are either inefficient or provide no guarantee on the quality of the resulting policy. We therefore present CPH, a novel solution method that solves the planning problems by first approximating with any desired accuracy the probability distributions over the resource consumptions with phasetype distributions, which use exponential distributions as building blocks. It then uses value iteration to solve the resulting MDPs by exploiting properties of exponential distributions to calculate the necessary convolutions accurately and efficiently while providing strong guarantees on the quality of the resulting policy. Our experimental feasibility study in a Mars rover domain demonstrates a substantial speedup over Lazy Approximation, which is currently the leading algorithm for solving continuous state MDPs with quality guarantees.|Janusz Marecki,Sven Koenig,Milind Tambe","16759|IJCAI|2007|Using Linear Programming for Bayesian Exploration in Markov Decision Processes|A key problem in reinforcement learning is finding a good balance between the need to explore the environment and the need to gain rewards by exploiting existing knowledge. Much research has been devoted to this topic, and many of the proposed methods are aimed simply at ensuring that enough samples are gathered to estimate well the value function. In contrast, Bellman and Kalaba,  proposed constructing a representation in which the states of the original system are paired with knowledge about the current model. Hence, knowledge about the possible Markov models of the environment is represented and maintained explicitly. Unfortunately, this approach is intractable except for bandit problems (where it gives rise to Gittins indices, an optimal exploration method). In this paper, we explore ideas for making this method computationally tractable. We maintain a model of the environment as a Markov Decision Process. We sample finite-length trajectories from the infinite tree using ideas based on sparse sampling. Finding the values of the nodes of this sparse subtree can then be expressed as an optimization problem, which we solve using Linear Programming. We illustrate this approach on a few domains and compare it with other exploration algorithms.|Pablo Samuel Castro,Doina Precup","16703|IJCAI|2007|Image Modeling Using Tree Structured Conditional Random Fields|In this paper we present a discriminative framework based on conditional random fields for stochastic modeling of images in a hierarchical fashion. The main advantage of the proposed framework is its ability to incorporate a rich set of interactions among the image sites. We achieve this by inducing a hierarchy of hidden variables over the given label field. The proposed tree like structure of our model eliminates the need for a huge parameter space and at the same time permits the use of exact and efficient inference procedures based on belief propagation. We demonstrate the generality of our approach by applying it to two important computer vision tasks, namely image labeling and object detection. The model parameters are trained using the contrastive divergence algorithm. We report the performance on real world images and compare it with the existing approaches.|Pranjal Awasthi,Aakanksha Gagrani,Balaraman Ravindran"],["44119|IEICE Transations|2007|State Duration Modeling for HMM-Based Speech Synthesis|This paper describes the explicit modeling of a state duration's probability density function in HMM-based speech synthesis. We redefine, in a statistically correct manner, the probability of staying in a state for a time interval used to obtain the state duration PDF and demonstrate improvements in the duration of synthesized speech.|Heiga Zen,Takashi Masuko,Keiichi Tokuda,Takayoshi Yoshimura,Takao Kobayashi,Tadashi Kitamura","44125|IEICE Transations|2007|Two-Band Excitation for HMM-Based Speech Synthesis|This letter describes a two-band excitation model for HMM-based speech synthesis. The HMM-based speech synthesis system generates speech from the HMM training data of the spectral and excitation parameters. Synthesized speech has a typical quality of \"vocoded sound\" mostly because of the simple excitation model with the voicedunvoiced selection. In this letter, two-band excitation based on the harmonic plus noise speech model is proposed for generating the mixed excitation source. With this model, we can generate the mixed excitation more accurately and reduce the memory for the trained excitation data as well.|Sang-Jin Kim,Minsoo Hahn","44211|IEICE Transations|2007|Details of the Nitech HMM-Based Speech Synthesis System for the Blizzard Challenge |In January , an open evaluation of corpus-based text-to-speech synthesis systems using common speech datasets, named Blizzard Challenge , was conducted. Nitech group participated in this challenge, entering an HMM-based speech synthesis system called Nitech-HTS . This paper describes the technical details, building processes, and performance of our system. We first give an overview of the basic HMM-based speech synthesis system, and then describe new features integrated into Nitech-HTS  such as STRAIGHT-based vocoding, HSMM-based acoustic modeling, and a speech parameter generation algorithm considering GV. Constructed Nitech-HTS  voices can generate speech waveforms at . RT (real-time ratio) on a . GHz Pentium  machine, and footprints of these voices are less than  Mbytes. Subjective listening tests showed that the naturalness and intelligibility of the Nitech-HTS  voices were much better than expected.|Heiga Zen,Tomoki Toda,Masaru Nakamura,Keiichi Tokuda","44100|IEICE Transations|2007|Mel-Wiener Filter for Mel-LPC Based Speech Recognition|This paper proposes a Mel-Wiener filter to enhance Mel-LPC spectra in the presence of additive noise. The transfer function of the proposed filter is defined by using a first-order all-pass filter instead of unit delay. The filter coefficients are estimated based on minimization of the sum of the square error on the linear frequency scale without applying the bilinear transformation and efficiently implemented in the autocorrelation domain. The proposed filter does not require any time-frequency conversion, which saves a large amount of computational load. The performance of the proposed system is comparable to that of ETSI AFE. The optimum filter order is found to be , and thus filtering is computationally inexpensive. The computational cost of the proposed system except VAD is % of ETSI AFE.|Md. Babul Islam,Kazumasa Yamamoto,Hiroshi Matsumoto","44295|IEICE Transations|2007|A Systolic FPGA Architecture of Two-Level Dynamic Programming for Connected Speech Recognition|In this paper, we present an efficient architecture for connected word recognition that can be implemented with field programmable gate array (FPGA). The architecture consists of newly derived two-level dynamic programming (TLDP) that use only bit addition and shift operations. The advantages of this architecture are the spatial efficiency to accommodate more words with limited space and the absence of multiplications to increase computational speed by reducing propagation delays. The architecture is highly regular, consisting of identical and simple processing elements with only nearest-neighbor communication, and external communication occurs with the end processing elements. In order to verify the proposed architecture, we have also designed and implemented it, prototyping with Xilinx FPGAs running at  MHz.|Yong Kim,Hong Jeong","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","44338|IEICE Transations|2007|A Speech Parameter Generation Algorithm Considering Global Variance for HMM-Based Speech Synthesis|This paper describes a novel parameter generation algorithm for an HMM-based speech synthesis technique. The conventional algorithm generates a parameter trajectory of static features that maximizes the likelihood of a given HMM for the parameter sequence consisting of the static and dynamic features under an explicit constraint between those two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed speech parameters usually causes muffled sounds. In order to alleviate the over-smoothing effect, we propose a generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance (GV) of the generated trajectory. The latter likelihood works as a penalty for the over-smoothing, i.e., a reduction of the GV of the generated trajectory. The result of a perceptual evaluation demonstrates that the proposed algorithm causes considerably large improvements in the naturalness of synthetic speech.|Tomoki Toda,Keiichi Tokuda","43996|IEICE Transations|2007|A MFCC-Based CELP Speech Coder for Server-Based Speech Recognition in Network Environments|Existing standard speech coders can provide high quality speech communication. However, they tend to degrade the performance of automatic speech recognition (ASR) systems that use the reconstructed speech. The main cause of the degradation is in that the linear predictive coefficients (LPCs), which are typical spectral envelope parameters in speech coding, are optimized to speech quality rather than to the performance of speech recognition. In this paper, we propose a speech coder using mel-frequency cepstral coefficients (MFCCs) instead of LPCs to improve the performance of a server-based speech recognition system in network environments. To develop the proposed speech coder with a low-bit rate, we first explore the interframe correlation of MFCCs, which results in the predictive quantization of MFCC. Second, a safety-net scheme is proposed to make the MFCC-based speech coder robust to channel errors. As a result, we propose an . kbps MFCC-based CELP coder. It is shown that the proposed speech coder has a comparable speech quality to  kbps G. and the ASR system using the proposed speech coder gives the relative word error rate reduction by .% as compared to the ASR system using G. on a large vocabulary task (AURORA).|Jae Sam Yoon,Gil Ho Lee,Hong Kook Kim","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","44200|IEICE Transations|2007|A Style Control Technique for HMM-Based Expressive Speech Synthesis|This paper describes a technique for controlling the degree of expressivity of a desired emotional expression andor speaking style of synthesized speech in an HMM-based speech synthesis framework. With this technique, multiple emotional expressions and speaking styles of speech are modeled in a single model by using a multiple-regression hidden semi-Markov model (MRHSMM). A set of control parameters, called the style vector, is defined, and each speech synthesis unit is modeled by using the MRHSMM, in which mean parameters of the state output and duration distributions are expressed by multiple-regression of the style vector. In the synthesis stage, the mean parameters of the synthesis units are modified by transforming an arbitrarily given style vector that corresponds to a point in a low-dimensional space, called style space, each of whose coordinates represents a certain specific speaking style or emotion of speech. The results of subjective evaluation tests show that style and its intensity can be controlled by changing the style vector.|Takashi Nose,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi"],["43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58021|GECCO|2007|GARS an improved genetic algorithm with reserve selection for global optimization|This paper investigates how genetic algorithms (GAs) can be improved to solve large-scale and complex problems more efficiently. First of all, we review premature convergence, one of the challenges confronted with when applying GAs to real-world problems. Next, some of the methods now available to prevent premature convergence and their intrinsic defects are discussed. A qualitative analysis is then done on the cause of premature convergence that is the loss of building blocks hosted in less-fit individuals during the course of evolution. Thus, we propose a new improver - GAs with Reserve Selection (GARS), where a reserved area is set up to save potential building blocks and a selection mechanism based on individual uniqueness is employed to activate the potentials. Finally, case studies are done in a few standard problems well known in the literature, where the experimental results demonstrate the effectiveness and robustness of GARS in suppressing premature convergence, and also an enhancement is found in global optimization capacity.|Yang Chen,Jinglu Hu,Kotaro Hirasawa,Songnian Yu","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","44105|IEICE Transations|2007|Binary Self-Organizing Map with Modified Updating Rule and Its Application to Reproduction of Genetic Algorithm|In this paper, we propose a modified reproduction strategy of a Genetic Algorithm (GA) utilizing a Self-Organizing Map (SOM) with a novel updating rule of binary weight vectors based on a significance of elements of inputs. In this rule, an updating order of elements is decided by considering fitness values of individuals in a population. The SOM with the proposed updating rule can realize an effective reproduction.|Ryosuke Kubota,Keiichi Horio,Takeshi Yamakawa"],["44165|IEICE Transations|2007|Incorporating Metadata into Data Mining with Ontology|In this paper, we present a novel method to incorporate metadata into data mining. The method has many advantages. It can be completed automatically and is independent of a specific database. Firstly, we convert metadata into ontology. Then input a rule set to a reasoner, which supports rule-based inference over the ontology model. The outputs of the reasoner describe the prior knowledge in metadata. Finally, incorporate the prior knowledge into data mining.|Guoqi Li,Huanye Sheng,Xun Fan","58060|GECCO|2007|Volatility forecasting using time series data mining and evolutionary computation techniques|Traditional parametric methods have limited success in estimating and forecasting the volatility of financial securities. Recent advance in evolutionary computation has provided additional tools to conduct data mining effectively. The current work applies the genetic programming in a Time Series Data Mining framework to characterize the S&P high frequency data in order to forecast the one step ahead integrated volatility. Results of the experiment have shown to be superior to those derived by the traditional methods.|Irwin Ma,Tony Wong,Thiagas Sankar","16663|IJCAI|2007|QuantMiner A Genetic Algorithm for Mining Quantitative Association Rules|In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers \"good\" intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive data mining tool.|Ansaf Salleb-Aouissi,Christel Vrain,Cyril Nortet","58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","57974|GECCO|2007|Trading rules on stock markets using genetic network programming with sarsa learning|In this paper, the Genetic Network Programming (GNP) for creating trading rules on stocks is described. GNP is an evolutionary computation, which represents its solutions using graph structures and has some useful features inherently. It has been clarified that GNP works well especially in dynamic environments since GNP can create quite compact programs and has an implicit memory function. In this paper, GNP is applied to creating a stock trading model. There are three important points The first important point is to combine GNP with Sarsa Learning which is one of the reinforcement learning algorithms. Evolution-based methods evolve their programs after task execution because they must calculate fitness values, while reinforcement learning can change programs during task execution, therefore the programs can be created efficiently. The second important point is that GNP uses candlestick chart and selects appropriate technical indices to judge the buying and selling timing of stocks. The third important point is that sub-nodes are used in each node to determine appropriate actions (buyingselling) and to select appropriate stock price information depending on the situation. In the simulations, the trading model is trained using the stock prices of  brands in ,  and . Then the generalization ability is tested using the stock prices in . From the simulation results, it is clarified that the trading rules of the proposed method obtain much higher profits than Buy&Hold method and its effectiveness has been confirmed.|Yan Chen,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","58034|GECCO|2007|Clustering gene expression data via mining ensembles of classification rules evolved using moses|A novel approach, model-based clustering, is described foridentifying complex interactions between genes or gene-categories based on static gene expression data. The approach deals with categorical data, which consists of a set of gene expressionprofiles belonging to one category, and a set belonging to anothercategory. An evolutionary algorithm (Meta-Optimizing Semantic Evolutionary Search, or MOSES) is used to learn an ensemble of classification models distinguishing the two categories, based on inputs that are features corresponding to gene expression values. Each feature is associated with a model-based vector, which encodes quantitative information regarding the utilization of the feature across the ensembles of models. Two different ways of constructing these vectors are explored. These model-based vectors are then clustered using a variant of hierarchical clustering called Omniclust. The result is a set of model-based clusters, in which features are gathered together if they are often considered together by classification models -- which may be because they're co-expressed, or may be for subtler reasons involving multi-gene interactions. The method is illustrated by applying it to two datasets regarding human gene expression, one drawn from brain cells and pertinent to the neurogenetics of aging, and the other drawn from blood cells and relating to differentiating between types of lymphoma. We find that, compared to traditional expression-based clustering, the new method often yields clusters that have higher mathematical quality (in the sense of homogeneity and separation) and also yield novel and meaningful insights into the underlying biological processes.|Moshe Looks,Ben Goertzel,L√∫cio de Souza Coelho,Mauricio Mudado,Cassio Pennachin","80822|VLDB|2007|Security in Outsourcing of Association Rule Mining|Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.|Wai Kit Wong,David W. Cheung,Edward Hung,Ben Kao,Nikos Mamoulis","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","16454|IJCAI|2007|On Mining Closed Sets in Multi-Relational Data|We investigate the problem of mining closed sets in multi-relational databases. Previous work introduced different semantics and associated algorithms for mining closed sets in multirelational databases. However, insight into the implications of semantic choices and the relationships among them was still lacking. Our investigation shows that the semantic choices are important because they imply different properties, which in turn affect the range of algorithms that can mine for such sets. Of particular interest is the question whether the seminal LCM algorithm by Uno et al. can be upgraded towards multi-relational problems. LCM is attractive since its run time is linear in the number of closed sets and it does not need to store outputs in order to avoid duplicates. We provide a positive answer to this question for some of the semantic choices, and report on experiments that evaluate the scalability and applicability of the upgraded algorithm on benchmark problems.|Gemma C. Garriga,Roni Khardon,Luc De Raedt","44158|IEICE Transations|2007|Integration of Learning Methods Medical Literature and Expert Inspection in Medical Data Mining|From lessons learned in medical data mining projects we show that integration of advanced computation techniques and human inspection is indispensable in medical data mining. We proposed an integrated approach that merges data mining and text mining methods plus visualization support for expert evaluation. We also appropriately developed temporal abstraction and text mining methods to exploit the collected data. Furthermore, our visual discovery system DMS allowed to actively and effectively working with physicians. Significant findings in hepatitis study were obtained by the integrated approach.|Tu Bao Ho,Saori Kawasaki,Katsuhiko Takabayashi,Canh Hao Nguyen"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","66101|AAAI|2007|Efficient Reinforcement Learning with Relocatable Action Models|Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.|Bethany R. Leffler,Michael L. Littman,Timothy Edmunds","16563|IJCAI|2007|Heuristic Selection of Actions in Multiagent Reinforcement Learning|This work presents a new algorithm, called Heuristically Accelerated Minimax-Q (HAMMQ), that allows the use of heuristics to speed up the well-known Multiagent Reinforcement Learning algorithm Minimax-Q. A heuristic function H that influences the choice of the actions characterises the HAMMQ algorithm. This function is associated with a preference policy that indicates that a certain action must be taken instead of another. A set of empirical evaluations were conducted for the proposed algorithm in a simplified simulator for the robot soccer domain, and experimental results show that even very simple heuristics enhances significantly the performance of the multiagent reinforcement learning algorithm.|Reinaldo A. C. Bianchi,Carlos H. C. Ribeiro,Anna Helena Reali Costa","16802|IJCAI|2007|Representations for Action Selection Learning from Real-Time Observation of Task Experts|The association of perception and action is key to learning by observation in general, and to program-level task imitation in particular. The question is how to structure this information such that learning is tractable for resource-bounded agents. By introducing a combination of symbolic representation with Bayesian reasoning, we demonstrate both theoretical and empirical improvements to a general-purpose imitation system originally based on a model of infant social learning. We also show how prior task knowledge and selective attention can be rigorously incorporated via loss matrices and Automatic Relevance Determination respectively.|Mark A. Wood,Joanna Bryson","66224|AAAI|2007|PLOW A Collaborative Task Learning Agent|To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies deep natural language understanding, knowledge representation and reasoning, dialogue systems, planningagent-based systems and machine learning. A formal evaluation shows the approach has great promise.|James F. Allen,Nathanael Chambers,George Ferguson,Lucian Galescu,Hyuckchul Jung,Mary D. Swift,William Taysom","65975|AAAI|2007|Learning Equilibrium in Resource Selection Games|We consider a resource selection game with incomplete information about the resource-cost functions. All the players know is the set of players, an upper bound on the possible costs, and that the cost functions are positive and nondecreasing. The game is played repeatedly and after every stage each player observes her cost, and the actions of all players. For every    we prove the existence of a learning -equilibrium, which is a profile of algorithms, one for each player such that a unilateral deviation of a player is, up to  not beneficial for her regardless of the actual cost functions. Furthermore, the learning eqUilibrium yields an optimal social cost.|Itai Ashlagi,Dov Monderer,Moshe Tennenholtz","44149|IEICE Transations|2007|Semi-Supervised Learning to Classify Evaluative Expressions from Labeled and Unlabeled Texts|In this paper, we present a method to automatically acquire a large-scale vocabulary of evaluative expressions from a large corpus of blogs. For the purpose, this paper presents a semi-supervised method for classifying evaluative expressions, that is, tuples of subjects, their attributes, and evaluative words, that indicate either favorable or unfavorable opinions towards a specific subject. Due to its characteristics, our semi-supervised method can classify evaluative expressions in a corpus by their polarities, starting from a very small set of seed training examples and using contextual information in the sentences the expressions belong to. Our experimental results with real Weblog data as our corpus show that this bootstrapping approach can improve the accuracy of methods for classifying favorable and unfavorable opinions. We also show that a reasonable amount of evaluative expressions can be really acquired.|Yasuhiro Suzuki,Hiroya Takamura,Manabu Okumura","16376|IJCAI|2007|Semi-Supervised Learning for Multi-Component Data Classification|This paper presents a method for designing a semisupervised classifier for multi-component data such as web pages consisting of text and link information. The proposed method is based on a hybrid of generative and discriminative approaches to take advantage of both approaches. With our hybrid approach, for each component, we consider an individual generative model trained on labeled samples and a model introduced to reduce the effect of the bias that results when there are few labeled samples. Then, we construct a hybrid classifier by combining all the models based on the maximum entropy principle. In our experimental results using three test collections such as web pages and technical papers, we confirmed that our hybrid approach was effective in improving the generalization performance of multi-component data classification.|Akinori Fujino,Naonori Ueda,Kazumi Saito"],["44052|IEICE Transations|2007|Fuzzy Tracker with Self-Tuning PID and Identifier Design Using Conditional-LMI and Improved Random Optimal Algorithm|This study introduces the fuzzy Lyapunov function to the fuzzy PID control systems, modified fuzzy systems, with an optimized robust tracking performance. We propose a compound search strategy called conditional linear matrix inequality (CLMI) approach which was composed of the proposed improved random optimal algorithm (IROA) concatenated with the simplex method to solve the linear matrix inequality (LMI) problem. If solutions of a specific system exist, the scheme finds more than one solutions at a time, and these fixed potential solutions and variable PID gains are ready for tracking performance optimization. The effectiveness of the proposed control scheme is demonstrated by the numerical example of a cart-pole system.|Zhi-Ren Tsai,Jiing-Dong Hwang,Yau-Zen Chang","43920|IEICE Transations|2007|Consideration of the Boundary Condition between Two Media in Acoustic Field Analysis Using the Constrained Interpolation Profile CIP Method|This study examines treatment of a boundary between media to simulate an acoustic field using the CIP method. The handling of spatial derivatives of fields is extremely important for CIP acoustic field analysis. We demonstrate a method of handling this boundary and report results of CIP acoustic field analysis using the present treatment.|Kan Okubo,Sungqwan Oh,Takao Tsuchiya,Nobunao Takeuchi","43972|IEICE Transations|2007|A Digital Image Watermarking Method Using Interval Arithmetic|In this letter, we propose a new digital image watermarking method using interval arithmetic. This is a new application of interval arithmetic. Experimental results show that the proposed method gives a watermarked image of better quality and is robust against some attacks.|Teruya Minamoto,Mitsuaki Yoshihara,Satoshi Fujii","44178|IEICE Transations|2007|High-Performance Training of Conditional Random Fields for Large-Scale Applications of Labeling Sequence Data|Conditional random fields (CRFs) have been successfully applied to various applications of predicting and labeling structured data, such as natural language tagging & parsing, image segmentation & object recognition, and protein secondary structure prediction. The key advantages of CRFs are the ability to encode a variety of overlapping, non-independent features from empirical data as well as the capability of reaching the global normalization and optimization. However, estimating parameters for CRFs is very time-consuming due to an intensive forward-backward computation needed to estimate the likelihood function and its gradient during training. This paper presents a high-performance training of CRFs on massively parallel processing systems that allows us to handle huge datasets with hundreds of thousand data sequences and millions of features. We performed the experiments on an important natural language processing task (text chunking) on large-scale corpora and achieved significant results in terms of both the reduction of computational time and the improvement of prediction accuracy.|Xuan Hieu Phan,Minh Le Nguyen,Yasushi Inoguchi,Susumu Horiguchi","44156|IEICE Transations|2007|Boundary Detection in Echocardiographic Images Using Markovian Level Set Method|Owing to the large amount of speckle noise and ill-defined edges present in echocardiographic images, computer-based boundary detection of the left ventricle has proved to be a challenging problem. In this paper, a Markovian level set method for boundary detection in long-axis echocardiographic images is proposed. It combines Markov random field (MRF) model, which makes use of local statistics with level set method that handles topological changes, to detect a continuous and smooth boundary. Experimental results show that higher accuracy can be achieved with the proposed method compared with two related MRF-based methods.|Jierong Cheng,Say Wei Foo","16383|IJCAI|2007|Keep the Decision Tree and Estimate the Class Probabilities Using its Decision Boundary|This paper proposes a new method to estimate the class membership probability of the cases classified by a Decision Tree. This method provides smooth class probabilities estimate, without any modification of the tree, when the data are numerical. It applies a posteriori and doesn't use additional training cases. It relies on the distance to the decision boundary induced by the decision tree. The distance is computed on the training sample. It is then used as an input for a very simple one-dimension kernel-based density estimator, which provides an estimate of the class membership probability. This geometric method gives good results even with pruned trees, so the intelligibility of the tree is fully preserved.|Isabelle Alvarez,Stephan Bernard,Guillaume Deffuant","16709|IJCAI|2007|Training Conditional Random Fields Using Virtual Evidence Boosting|While conditional random fields (CRFs) have been applied successfully in a variety of domains, their training remains a challenging task. In this paper, we introduce a novel training method for CRFs, called virtual evidence boosting, which simultaneously performs feature selection and parameter estimation. To achieve this, we extend standard boosting to handle virtual evidence, where an observation can be specified as a distribution rather than a single number. This extension allows us to develop a unified framework for learning both local and compatibility features in CRFs. In experiments on synthetic data as well as real activity classification problems, our new training algorithm outperforms other training approaches including maximum likelihood, maximum pseudo-likelihood, and the most recent boosted random fields.|Lin Liao,Tanzeem Choudhury,Dieter Fox,Henry A. Kautz","16795|IJCAI|2007|Document Summarization Using Conditional Random Fields|Many methods, including supervised and unsupervised algorithms, have been developed for extractive document summarization. Most supervised methods consider the summarization task as a two-class classification problem and classify each sentence individually without leveraging the relationship among sentences. The unsupervised methods use heuristic rules to select the most informative sentences into a summary directly, which are hard to generalize. In this paper, we present a Conditional Random Fields (CRF) based framework to keep the merits of the above two kinds of approaches while avoiding their disadvantages. What is more, the proposed framework can take the outcomes of previous methods as features and seamlessly integrate them. The key idea of our approach is to treat the summarization task as a sequence labeling problem. In this view, each document is a sequence of sentences and the summarization procedure labels the sentences by  and . The label of a sentence depends on the assignment of labels of others. We compared our proposed approach with eight existing methods on an open benchmark data set. The results show that our approach can improve the performance by more than .% and .% over the best supervised baseline and unsupervised baseline respectively in terms of two popular metrics F and ROUGE-. Detailed analysis of the improvement is presented as well.|Dou Shen,Jian-Tao Sun,Hua Li,Qiang Yang,Zheng Chen","43932|IEICE Transations|2007|Design Method for Numerical Function Generators Using Recursive Segmentation and EVBDDs|Numerical function generators (NFGs) realize arithmetic functions, such as ex, sin(x), and x, in hardware. They are used in applications where high-speed is essential, such as in digital signal or graphics applications. We introduce the edge-valued binary decision diagram (EVBDD) as a means of reducing the delay and memory requirements in NFGs. We also introduce a recursive segmentation algorithm, which divides the domain of the function to be realized into segments, where the given function is realized as a polynomial. This design reduces the size of the multiplier needed and thus reduces delay. It is also shown that an adder can be replaced by a set of -input AND gates, further reducing delay. We compare our results to NFGs designed with multi-terminal BDDs (MTBDDs). We show that EVBDDs yield a design that has, on the average, only % of the memory and % of the delay of NFGs designed using MTBDDs.|Shinobu Nagayama,Tsutomu Sasao,Jon T. Butler","16703|IJCAI|2007|Image Modeling Using Tree Structured Conditional Random Fields|In this paper we present a discriminative framework based on conditional random fields for stochastic modeling of images in a hierarchical fashion. The main advantage of the proposed framework is its ability to incorporate a rich set of interactions among the image sites. We achieve this by inducing a hierarchy of hidden variables over the given label field. The proposed tree like structure of our model eliminates the need for a huge parameter space and at the same time permits the use of exact and efficient inference procedures based on belief propagation. We demonstrate the generality of our approach by applying it to two important computer vision tasks, namely image labeling and object detection. The model parameters are trained using the contrastive divergence algorithm. We report the performance on real world images and compare it with the existing approaches.|Pranjal Awasthi,Aakanksha Gagrani,Balaraman Ravindran"],["44210|IEICE Transations|2007|Word Error Rate Minimization Using an Integrated Confidence Measure|This paper describes a new criterion for speech recognition using an integrated confidence measure to minimize the word error rate (WER). The conventional criteria for WER minimization obtain the expected WER of a sentence hypothesis merely by comparing it with other hypotheses in an n-best list. The proposed criterion estimates the expected WER by using an integrated confidence measure with word posterior probabilities for a given acoustic input. The integrated confidence measure, which is implemented as a classifier based on maximum entropy (ME) modeling or support vector machines (SVMs), is used to acquire probabilities reflecting whether the word hypotheses are correct. The classifier is comprised of a variety of confidence measures and can deal with a temporal sequence of them to attain a more reliable confidence. Our proposed criterion for minimizing WER achieved a WER of .% and a .% reduction, relative to conventional n-best rescoring methods in transcribing Japanese broadcast news in various environments such as under noisy field and spontaneous speech conditions.|Akio Kobayashi,Kazuo Onoe,Shinichi Homma,Shoei Sato,Toru Imai","43903|IEICE Transations|2007|Sufficient Conditions for a Regular LDPC Code Better than an Irregular LDPC Code|Decoding performance of LDPC (Low-Density Parity-Check) codes is highly dependent on the degree distributions of the Tanner graphs which define the LDPC codes. We compare two LDPC code ensembles, one has a uniform degree distribution and the other a non-uniform one over a BEC (Binary Erasure Channel) and a BSC (Binary Symmetric Channel) thorough DE (Density Evolution). We then derive sufficient conditions on the erasure probability of a BEC and the error probability of a BSC, under which the LDPC code ensembles with uniform degree distributions outperform those with non-uniform degree distributions.|Shinya Miyamoto,Kenta Kasai,Kohichi Sakaniwa","43885|IEICE Transations|2007|Relay Method of Sending Soft Decision Symbol Based on the Result of Error Detecting Code in Cooperative Communication|Transmit diversity gain can be obtained in cooperative communication by cooperating the multiple users with single antenna. In cooperative communication, in the first step, each mobile station (MS) transmits its own data to both the base station (BS) and the other MS. In the second step, each MS's data is transmitted from the other MS to BS. As a result, transmit diversity gain can be obtained without implementing multiple transmit antennas at MS. In the conventional relay method, if error is detected within the received packet by using cyclic redundancy check (CRC) code, MS transmits its own data to BS instead of relaying the other MS's data in the second step. As a result, transmit diversity gain cannot be obtained. In this paper, we propose a novel cooperative method. In the proposed method, if the CRC decoder detects error within the received packet, MS transmits soft decision symbol which is obtained from the decoded data in second step. As a result, the transmit diversity gain always can be obtained. From the computer simulation, we show that the proposed method can achieve the better error rate performance than the conventional one.|Yuki Fukuyama,Osamu Takyu,Koichi Adachi,Masao Nakagawa","44277|IEICE Transations|2007|Real-Time Huffman Encoder with Pipelined CAM-Based Data Path and Code-Word-Table Optimizer|This paper presents a novel optimized real-time Huffman encoder using a pipelined data path based on CAM technology and a parallel code-word-table optimizer. The exploitation of CAM technology enables fast parallel search of the code word table. At the same time, the code word table is optimized according to the frequency of received input symbols and is up-dated in real-time. Since these two functions work in parallel, the proposed architecture realizes fast parallel encoding and keeps a constantly high compression ratio. Evaluation results for the JPEG application show that the proposed architecture can achieve up to % smaller encoded picture sizes than the conventional architectures. The obtained encoding time can be reduced by % in comparison to a conventional SRAM-based architecture, which is suitable even for the latest end-user-devices requiring fast frame-rates. Furthermore, the proposed architecture provides the only encoder that can simultaneously realize small compressed data size and fast processing speed.|Takeshi Kumaki,Yasuto Kuroda,Masakatsu Ishizaki,Tetsushi Koide,Hans J√ºrgen Mattausch,Hideyuki Noda,Katsumi Dosaka,Kazutami Arimoto,Kazunori Saito","44034|IEICE Transations|2007|Construction of Universal Codes Using LDPC Matrices and Their Error Exponents|A universal coding scheme for information from i.i.d., arbitrarily varying sources, or memoryless correlated sources is constructed using LDPC matrices and shown to have an exponential upper bound of decoding error probability. As a corollary, we construct a universal code for the noisy channel model, which is not necessarily BSC. Simulation results show universality of the code with sum-product decoding, and presence of a gap between the error exponent obtained by simulation and that obtained theoretically.|Shigeki Miyake,Mitsuru Maruyama","43764|IEICE Transations|2007|Lyapunov-Based Error Estimations of MIMO Interconnect Reductions by Using the Global Arnoldi Algorithm|We present theoretical foundations about error estimations of the global Krylov subspace techniques for multiple-inputs multiple-outputs (MIMO) Interconnect reductions. Analytical relationships between Lyapunov functions of the original interconnect network and those of the reduced system generated by the global Arnoldi algorithm will be developed. Under this framework, a new moment matching reduced network is proposed. Also, we will show that the reduced system can be expressed as the original network with some additive perturbations.|Chia-Chi Chu,Ming-Hong Lai,Wu-Shiung Feng","44082|IEICE Transations|2007|Average Bit Erasure Probability of Regular LDPC Code Ensembles under MAP Decoding over BEC|The average bit erasure probability of a binary linear code ensemble under maximum a-posteriori probability (MAP) decoding over binary erasure channel (BEC) can be calculated with the average support weight distribution of the ensemble via the EXIT function and the shortened information function. In this paper, we formulate the relationship between the average bit erasure probability under MAP decoding over BEC and the average support weight distribution for a binary linear code ensemble. Then, we formulate the average support weight distribution and the average bit erasure probability under MAP decoding over BEC for regular LDPC code ensembles.|Takayuki Itsui,Kenta Kasai,Ryoji Ikegaya,Tomoharu Shibuya,Kohichi Sakaniwa","66149|AAAI|2007|Reacting to Agreement and Error in Spoken Dialogue Systems Using Degrees of Groundedness|Computational models of grounding are extended to include representations of degrees of groundedness. These representations are then used for decision-making in dialogue management for spoken dialogue systems. Several domains will be explored with this model, and an implementation will be tested and evaluated.|Antonio Roque","16471|IJCAI|2007|A Tighter Error Bound for Decision Tree Learning Using PAC Learnability|Error bounds for decision trees are generally based on depth or breadth of the tree. In this paper, we propose a bound for error rate that depends both on the depth and the breadth of a specific decision tree constructed from the training samples. This bound is derived from sample complexity estimate based on PAC learnability. The proposed bound is compared with other traditional error bounds on several machine learning benchmark data sets as well as on an image data set used in Content Based Image Retrieval (CBIR). Experimental results demonstrate that the proposed bound gives tighter estimation of the empirical error.|Chaithanya Pichuka,Raju S. Bapi,Chakravarthy Bhagvati,Arun K. Pujari,Bulusu Lakshmana Deekshatulu","44337|IEICE Transations|2007|Adaptive Error Compensation for Low Error Fixed-Width Squarers|In this paper, we present a design method for fixed-width squarer that receives an n-bit input and produces an n-bit squared product. To efficiently compensate for the truncation error, modified Booth-folding encoder signals are used for the generation of error compensation bias. The truncated bits are divided into two groups (major and minor) depending upon their effects on the truncation error. Then, different error compensation methods are applied to each group. By simulations, it is shown that the proposed fixed-width squarers have lower error than other fixed-width squarers and are cost-effective.|Kyung-Ju Cho,Jin-Gyun Chung"],["44119|IEICE Transations|2007|State Duration Modeling for HMM-Based Speech Synthesis|This paper describes the explicit modeling of a state duration's probability density function in HMM-based speech synthesis. We redefine, in a statistically correct manner, the probability of staying in a state for a time interval used to obtain the state duration PDF and demonstrate improvements in the duration of synthesized speech.|Heiga Zen,Takashi Masuko,Keiichi Tokuda,Takayoshi Yoshimura,Takao Kobayashi,Tadashi Kitamura","44125|IEICE Transations|2007|Two-Band Excitation for HMM-Based Speech Synthesis|This letter describes a two-band excitation model for HMM-based speech synthesis. The HMM-based speech synthesis system generates speech from the HMM training data of the spectral and excitation parameters. Synthesized speech has a typical quality of \"vocoded sound\" mostly because of the simple excitation model with the voicedunvoiced selection. In this letter, two-band excitation based on the harmonic plus noise speech model is proposed for generating the mixed excitation source. With this model, we can generate the mixed excitation more accurately and reduce the memory for the trained excitation data as well.|Sang-Jin Kim,Minsoo Hahn","44211|IEICE Transations|2007|Details of the Nitech HMM-Based Speech Synthesis System for the Blizzard Challenge |In January , an open evaluation of corpus-based text-to-speech synthesis systems using common speech datasets, named Blizzard Challenge , was conducted. Nitech group participated in this challenge, entering an HMM-based speech synthesis system called Nitech-HTS . This paper describes the technical details, building processes, and performance of our system. We first give an overview of the basic HMM-based speech synthesis system, and then describe new features integrated into Nitech-HTS  such as STRAIGHT-based vocoding, HSMM-based acoustic modeling, and a speech parameter generation algorithm considering GV. Constructed Nitech-HTS  voices can generate speech waveforms at . RT (real-time ratio) on a . GHz Pentium  machine, and footprints of these voices are less than  Mbytes. Subjective listening tests showed that the naturalness and intelligibility of the Nitech-HTS  voices were much better than expected.|Heiga Zen,Tomoki Toda,Masaru Nakamura,Keiichi Tokuda","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","44338|IEICE Transations|2007|A Speech Parameter Generation Algorithm Considering Global Variance for HMM-Based Speech Synthesis|This paper describes a novel parameter generation algorithm for an HMM-based speech synthesis technique. The conventional algorithm generates a parameter trajectory of static features that maximizes the likelihood of a given HMM for the parameter sequence consisting of the static and dynamic features under an explicit constraint between those two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed speech parameters usually causes muffled sounds. In order to alleviate the over-smoothing effect, we propose a generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance (GV) of the generated trajectory. The latter likelihood works as a penalty for the over-smoothing, i.e., a reduction of the GV of the generated trajectory. The result of a perceptual evaluation demonstrates that the proposed algorithm causes considerably large improvements in the naturalness of synthetic speech.|Tomoki Toda,Keiichi Tokuda","43765|IEICE Transations|2007|Robust F Estimation Based on Complex Lpc Analysis for IRS Filtered Noisy Speech|This paper proposes a novel robust fundamental frequency (F) estimation algorithm based on complex-valued speech analysis for an analytic speech signal. Since analytic signal provides spectra only over positive frequencies, spectra can be accurately estimated in low frequencies. Consequently, it is considered that F estimation using the residual signal extracted by complex-valued speech analysis can perform better for F estimation than that for the residual signal extracted by conventional real-valued LPC analysis. In this paper, the autocorrelation function weighted by AMDF is adopted for the F estimation criterion and four signals speech signal, analytic speech signal, LPC residual and complex LPC residual, are evaluated for the F estimation. Speech signals used in the experiments were an IRS filtered speech corrupted by adding white Gaussian noise or Pink noise whose noise levels are , , , -dB. The experimental results demonstrate that the proposed algorithm based on complex LPC residual can perform better than other methods in noisy environment.|Keiichi Funaki,Tatsuhiko Kinjo","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","44333|IEICE Transations|2007|Single Channel Speech Enhancement Based on Perceptual Frequency-Weighting|The present paper describes a quality enhancement of speech corrupted by additive background noise in a single channel system. The proposed approach is based on the introduction of perceptual criteria using a frequency-weighting filter in a subtractive-type enhancement process. This newly developed algorithm allows for an automatic adaptation in the time and frequency of the enhancement system and finds a suitable noise estimate according to the frequency of the corrupted speech. Experimental results show that the proposed approach can efficiently remove additive noise related to various types of noise corruption.|Seiji Hayashi,Masahiro Suguimoto","44161|IEICE Transations|2007|A Hidden Semi-Markov Model-Based Speech Synthesis System|A statistical speech synthesis system based on the hidden Markov model (HMM) was recently proposed. In this system, spectrum, excitation, and duration of speech are modeled simultaneously by context-dependent HMMs, and speech parameter vector sequences are generated from the HMMs themselves. This system defines a speech synthesis problem in a generative model framework and solves it based on the maximum likelihood (ML) criterion. However, there is an inconsistency although state duration probability density functions (PDFs) are explicitly used in the synthesis part of the system, they have not been incorporated into its training part. This inconsistency can make the synthesized speech sound less natural. In this paper, we propose a statistical speech synthesis system based on a hidden semi-Markov model (HSMM), which can be viewed as an HMM with explicit state duration PDFs. The use of HSMMs can solve the above inconsistency because we can incorporate the state duration PDFs explicitly into both the synthesis and the training parts of the system. Subjective listening test results show that use of HSMMs improves the reported naturalness of synthesized speech.|Heiga Zen,Keiichi Tokuda,Takashi Masuko,Takao Kobayashi,Tadashi Kitamura","44200|IEICE Transations|2007|A Style Control Technique for HMM-Based Expressive Speech Synthesis|This paper describes a technique for controlling the degree of expressivity of a desired emotional expression andor speaking style of synthesized speech in an HMM-based speech synthesis framework. With this technique, multiple emotional expressions and speaking styles of speech are modeled in a single model by using a multiple-regression hidden semi-Markov model (MRHSMM). A set of control parameters, called the style vector, is defined, and each speech synthesis unit is modeled by using the MRHSMM, in which mean parameters of the state output and duration distributions are expressed by multiple-regression of the style vector. In the synthesis stage, the mean parameters of the synthesis units are modified by transforming an arbitrarily given style vector that corresponds to a point in a low-dimensional space, called style space, each of whose coordinates represents a certain specific speaking style or emotion of speech. The results of subjective evaluation tests show that style and its intensity can be controlled by changing the style vector.|Takashi Nose,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi"],["44119|IEICE Transations|2007|State Duration Modeling for HMM-Based Speech Synthesis|This paper describes the explicit modeling of a state duration's probability density function in HMM-based speech synthesis. We redefine, in a statistically correct manner, the probability of staying in a state for a time interval used to obtain the state duration PDF and demonstrate improvements in the duration of synthesized speech.|Heiga Zen,Takashi Masuko,Keiichi Tokuda,Takayoshi Yoshimura,Takao Kobayashi,Tadashi Kitamura","44125|IEICE Transations|2007|Two-Band Excitation for HMM-Based Speech Synthesis|This letter describes a two-band excitation model for HMM-based speech synthesis. The HMM-based speech synthesis system generates speech from the HMM training data of the spectral and excitation parameters. Synthesized speech has a typical quality of \"vocoded sound\" mostly because of the simple excitation model with the voicedunvoiced selection. In this letter, two-band excitation based on the harmonic plus noise speech model is proposed for generating the mixed excitation source. With this model, we can generate the mixed excitation more accurately and reduce the memory for the trained excitation data as well.|Sang-Jin Kim,Minsoo Hahn","44211|IEICE Transations|2007|Details of the Nitech HMM-Based Speech Synthesis System for the Blizzard Challenge |In January , an open evaluation of corpus-based text-to-speech synthesis systems using common speech datasets, named Blizzard Challenge , was conducted. Nitech group participated in this challenge, entering an HMM-based speech synthesis system called Nitech-HTS . This paper describes the technical details, building processes, and performance of our system. We first give an overview of the basic HMM-based speech synthesis system, and then describe new features integrated into Nitech-HTS  such as STRAIGHT-based vocoding, HSMM-based acoustic modeling, and a speech parameter generation algorithm considering GV. Constructed Nitech-HTS  voices can generate speech waveforms at . RT (real-time ratio) on a . GHz Pentium  machine, and footprints of these voices are less than  Mbytes. Subjective listening tests showed that the naturalness and intelligibility of the Nitech-HTS  voices were much better than expected.|Heiga Zen,Tomoki Toda,Masaru Nakamura,Keiichi Tokuda","44341|IEICE Transations|2007|Critical Band Subspace-Based Speech Enhancement Using SNR and Auditory Masking Aware Technique|In this paper, a new subspace-based speech enhancement algorithm is presented. First, we construct a perceptual filterbank from psycho-acoustic model and incorporate it in the subspace-based enhancement approach. This filterbank is created through a five-level wavelet packet decomposition. The masking properties of the human auditory system are then derived based on the perceptual filterbank. Finally, the prior SNR and the masking threshold of each critical band are taken to decide the attenuation factor of the optimal linear estimator. Five different types of in-car noises in TAICAR database were used in our evaluation. The experimental results demonstrated that our approach outperformed conventional subspace and spectral subtraction methods.|Jia-Ching Wang,Hsiao Ping Lee,Jhing-Fa Wang,Chung-Hsien Yang","44338|IEICE Transations|2007|A Speech Parameter Generation Algorithm Considering Global Variance for HMM-Based Speech Synthesis|This paper describes a novel parameter generation algorithm for an HMM-based speech synthesis technique. The conventional algorithm generates a parameter trajectory of static features that maximizes the likelihood of a given HMM for the parameter sequence consisting of the static and dynamic features under an explicit constraint between those two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed speech parameters usually causes muffled sounds. In order to alleviate the over-smoothing effect, we propose a generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance (GV) of the generated trajectory. The latter likelihood works as a penalty for the over-smoothing, i.e., a reduction of the GV of the generated trajectory. The result of a perceptual evaluation demonstrates that the proposed algorithm causes considerably large improvements in the naturalness of synthetic speech.|Tomoki Toda,Keiichi Tokuda","44143|IEICE Transations|2007|Indexing Moving Objects for Trajectory Retrieval on Location-Based Services|Due to the continuous growth of wireless communication technology and mobile equipment, the history management of moving object is important in a wide range of location-based applications. To process queries for history data, trajectories, we generally use trajectory-preserving index schemes based on the trajectory preservation property. This property means that a leaf node only contains segments belonging to a particular trajectory, regardless of the spatiotemporal locality of segments. The sacrifice of spatiotemporal locality, however, causes the index to increase the dead space of MBBs of non-leaf nodes and the overlap between the MBBs of nodes. Therefore, an index scheme for trajectories shows good performance with trajectory-based queries, but not with coordinate-based queries, such as range queries. We propose new index schemes that improve the performance of range queries without reducing performance with trajectory based queries.|Duksung Lim,Daesoo Cho,Bonghee Hong","58055|GECCO|2007|Simultaneous optimization of production planning and inspection planning for flexible manufacturing systems|Quality assurance in flexible manufacturing systems (FMSs) has become a matter of great importance in recent years. The possibility for offering high-quality products at lower costs has become an essential for a manufacturer to keep in a competitive edge. In this paper, an approach to the multi-objective optimization of production planning and inspection planning in flexible manufacturing systems is presented. A multi-objective memetic algorithm MOMA is proposed to solve the problems having six objectives minimizing total machining time, machine workload unbalance, greatest machine workload, total tool cost, total inspection time and number of inspections. A schemata-guided local search strategy is proposed for enhancing performances of MOMA. High efficiency of MOMA arises from that multiple objectives can be optimized simultaneously without using heuristics and a set of good non-dominated solutions can be obtained providing additional degrees of freedom for the exploitation of resources of FMSs. Experimental results demonstrate effectiveness of the proposed approach using MOMA for production planning and inspection planning of FMSs.|Jian-Hung Chen","44265|IEICE Transations|2007|Phase Retrieval Based on a Snake for Image Reconstruction|A new phase retrieval method using an active contour model (snake) for image reconstruction is proposed. The proposed method reconstructs a target image by retrieving the phase from the magnitude of its Fourier transform and the measured area of the image. In general, the measured area is different from the true area where the target image exists. Thus a snake, which can extract the shape of the target image, is utilized to renew the measured area. By processing this renewal iteratively, the area obtained by the snake converges to the true area and as a result the proposed method can accurately reconstruct a target image even when the measured area is different from the true area. Experimental results show the effectiveness of the proposed method.|Keiko Kondo,Miki Haseyama,Hideo Kitajima","44161|IEICE Transations|2007|A Hidden Semi-Markov Model-Based Speech Synthesis System|A statistical speech synthesis system based on the hidden Markov model (HMM) was recently proposed. In this system, spectrum, excitation, and duration of speech are modeled simultaneously by context-dependent HMMs, and speech parameter vector sequences are generated from the HMMs themselves. This system defines a speech synthesis problem in a generative model framework and solves it based on the maximum likelihood (ML) criterion. However, there is an inconsistency although state duration probability density functions (PDFs) are explicitly used in the synthesis part of the system, they have not been incorporated into its training part. This inconsistency can make the synthesized speech sound less natural. In this paper, we propose a statistical speech synthesis system based on a hidden semi-Markov model (HSMM), which can be viewed as an HMM with explicit state duration PDFs. The use of HSMMs can solve the above inconsistency because we can incorporate the state duration PDFs explicitly into both the synthesis and the training parts of the system. Subjective listening test results show that use of HSMMs improves the reported naturalness of synthesized speech.|Heiga Zen,Keiichi Tokuda,Takashi Masuko,Takao Kobayashi,Tadashi Kitamura","44200|IEICE Transations|2007|A Style Control Technique for HMM-Based Expressive Speech Synthesis|This paper describes a technique for controlling the degree of expressivity of a desired emotional expression andor speaking style of synthesized speech in an HMM-based speech synthesis framework. With this technique, multiple emotional expressions and speaking styles of speech are modeled in a single model by using a multiple-regression hidden semi-Markov model (MRHSMM). A set of control parameters, called the style vector, is defined, and each speech synthesis unit is modeled by using the MRHSMM, in which mean parameters of the state output and duration distributions are expressed by multiple-regression of the style vector. In the synthesis stage, the mean parameters of the synthesis units are modified by transforming an arbitrarily given style vector that corresponds to a point in a low-dimensional space, called style space, each of whose coordinates represents a certain specific speaking style or emotion of speech. The results of subjective evaluation tests show that style and its intensity can be controlled by changing the style vector.|Takashi Nose,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi"],["66124|AAAI|2007|Heuristic Evaluation Functions for General Game Playing|A general game playing program plays games that it has not previously encountered. A game manager program sends the game playing programs a description of a game's rules and objectives in a well-defined game description language. A central challenge in creating effective general game playing programs is that of constructing heuristic evaluation functions from game descriptions. This paper describes a method for constructing evaluation functions that represent exact values of simplified games. The simplified games are abstract models that incorporate the most essential aspects of the original game, namely payoff, control, and termination. Results of applying this method to a sampling of games suggest that heuristic evaluation functions based on our method are both comprehensible and effective.|James Clune","65973|AAAI|2007|Expressiveness of ADL and Golog Functions Make a Difference|The main focus in the area of action languages, such as GOLOG, was put on expressive power, while the development in the area of action planning was focused on efficient plan generation. An integration of GOLOG and planning languages would provide great advantages. A user could constrain a system's behavior on a high level using GOLOG, while the actual low-level actions are planned by an efficient planning system. First endeavors have been made by Eyerich et al. by identifying a subset of the situation calculus (which is the basis of GOLOG) with the same expressiveness as the ADL fragment of PDDL. However, it was not proven that the identified restrictions define a maximum subset. The most severe restriction appears to be that functions are limited to constants, We will show that this restriction is indeed necessary in most cases.|Gabriele R√∂ger,Bernhard Nebel","66086|AAAI|2007|Near-optimal Observation Selection using Submodular Functions|AI problems such as autonomous robotic exploration, automatic diagnosis and activity recognition have in common the need for choosing among a set of informative but possibly expensive observations. When monitoring spatial phenomena with sensor networks or mobile robots, for example, we need to decide which locations to observe in order to most effectively decrease the uncertainty, at minimum cost. These problems usually are NP-hard. Many observation selection objectives satisfy submodularity, an intuitive diminishing returns property - adding a sensor to a small deployment helps more than adding it to a large deployment. In this paper, we survey recent advances in systematically exploiting this submodularity property to efficiently achieve near-optimal observation selections, under complex constraints. We illustrate the effectiveness of our approaches on problems of monitoring environmental phenomena and water distribution networks.|Andreas Krause,Carlos Guestrin","43897|IEICE Transations|2007|Indifferentiability of Single-Block-Length and Rate- Compression Functions|The security notion of indifferentiability was proposed by Maurer, Renner, and Holenstein in . In , Coron, Dodis, Malinaud, and Puniya discussed the indifferentiability of hash functions. They have shown that the Merkle-Damgrd construction is not secure in the sense of indifferentiability. In this paper, we analyze the security of single-block-length and rate- compression functions in the sense of indifferentiability. We formally show that all single-block-length and rate- compression functions, which include the Davies-Meyer compression function, are insecure. Furthermore, we show how to construct a secure single-block-length and rate- compression function in the sense of indifferentiability. This does not contradict our result above.|Hidenori Kuwakado,Masakatu Morii","57973|GECCO|2007|Defining implicit objective functions for design problems|In many design tasks it is difficult to explicitly define an objective function. This paper uses machine learning to derive an objective in a feature space based on selected examples of previous designs, thus implicitly capturing the features that distinguish that set from others without requiring a predetermined measure of fitness. A genetic algorithm is used to generate new designs, and these are shown to recognisably display the appropriate features. It is demonstrated that the range of relevant features and optimal solutions is easily varied in proportion to the examples selected to define the objective. Methods for improving the function for GA search are discussed.|Sean Hanna","43929|IEICE Transations|2007|Consensus Problem of Multi-Agent Systems with Non-linear Performance Functions|This paper addresses a discrete-time consensus problem with non-linear performance functions over dynamically changing communication topologies. Each agent has a performance value based on its internal information state and exchanges the performance value with other agents to achieve consensus. We derive sufficient conditions for a global consensus using algebraic graph theory.|Naoki Hayashi,Toshimitsu Ushio,Fumiko Harada,Atsuko Ohno","44231|IEICE Transations|2007|A New Curve Control Function for the Detection of the Brain Ventricle Area|This paper proposed a region-based curve control function to detect the brain ventricle area by utilizing a geodesic active contour model. This is based on the average brightness of the brain ventricle area which is brighter in MRI images. Compared numerically by using various types of measurements, the proposed method can detect the brain ventricle area better than the existing methods.|Chul-Ho Won,Dong Hoon Kim,Jyung Hyun Lee,Sang Hyo Woo,Yeon Kwan Moon,Jin-Ho Cho","16770|IJCAI|2007|Some Effects of a Reduced Relational Vocabulary on the Whodunit Problem|A key issue in artificial intelligence lies in finding the amount of input detail needed to do successful learning. Too much detail causes overhead and makes learning prone to over-fitting. Too little detail and it may not be possible to learn anything at all. The issue is particularly relevant when the inputs are relational case descriptions, and a very expressive vocabulary may also lead to inconsistent representations. For example, in the Whodunit Problem, the task is to form hypotheses about the identity of the perpetrator of an event described using relational propositions. The training data consists of arbitrary relational descriptions of many other similar cases. In this paper, we examine the possibility of translating the case descriptions into an alternative vocabulary which has a reduced number of predicates and therefore produces more consistent case descriptions. We compare how the reduced vocabulary affects three different learning algorithms exemplar-based analogy, prototype-based analogy, and association rule learning. We find that it has a positive effect on some algorithms and a negative effect on others, which gives us insight into all three algorithms and indicates when reduced vocabularies might be appropriate.|Daniel T. Halstead,Kenneth D. Forbus","43948|IEICE Transations|2007|On Hash Functions and List Decoding with Side Information|List decoding is a process by which a list of decoded words is output instead of one. This works for a larger noise threshold than the traditional algorithms. Under some circumstances it becomes useful to be able to find out the actual message from the list. List decoding is assumed to be successful, meaning, the sent message features in the decoded list. This problem has been considered by Guruswami. In Guruswami's work, this disambiguation is done by sending supplementary information through a costly, error-free channel. The model is meaningful only if the number of bits of side information required is much less than the message size. But using deterministic schemes one has to essentially send the entire message through the error free channel. Randomized strategies for both sender and receiver reduces the required number of bits of side information drastically. In Guruswami's work, a Reed-Solomon code based hash family is used to construct such randomized schemes. The scheme with probability utmost  reports failure and returns the whole list. The scheme doesn't output a wrong message. Also, in Guruswami's work some theoretical bounds have been proved which lower bound the bits of side information required. Here we examine whether the gap between the theoretical bounds and existing schemes may be narrowed. Particularly, we use the same scheme as in Guruswami's work, but use hash families based on Hermitian curve and function fields of Garcia-Stichtenoth tower and analyze the number of bits of side information required for the scheme.|M. Prem Laxman Das","66165|AAAI|2007|Towards an Integrated Robot with Multiple Cognitive Functions|We present integration mechanisms for combining heterogeneous components in a situated information processing system, illustrated by a cognitive robot able to collaborate with a human and display some understanding of its surroundings. These mechanisms include an architectural schema that encourages parallel and incremental information processing, and a method for binding information from distinct representations that when faced with rapid change in the world can maintain a coherent, though distributed, view of it. Provisional results are demonstrated in a robot combining vision, manipulation, language, planning and reasoning capabilities interacting with a human and manipulable objects.|Nick Hawes,Aaron Sloman,Jeremy Wyatt,Michael Zillich,Henrik Jacobsson,Geert-Jan M. Kruijff,Michael Brenner,Gregor Berginc,Danijel Skocaj"],["58113|GECCO|2007|Best SubTree genetic programming|The result of the program encoded into a Genetic Programming(GP) tree is usually returned by the root of that tree. However, this is not a general strategy. In this paper we present and investigate a new variant where the best subtree is chosen to provide the solution of the problem. The other nodes (not belonging to the best subtree) are deleted. This will reduce the size of the chromosome in those cases where its best subtree is different from the entire tree. We have tested this strategy on a wide range of regression and classification problems. Numerical experiments have shown that the proposed approach can improve both the search speed and the quality of results.|Oana Muntean,Laura Diosan,Mihai Oltean","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57893|GECCO|2007|A data parallel approach to genetic programming using programmable graphics hardware|In recent years the computing power of graphics cards has increased significantly. Indeed, the growth in the computing power of these graphics cards is now several orders of magnitude greater than the growth in the power of computer processor units. Thus these graphics cards are now beginning to be used by the scientific community aslow cost, high performance computing platforms. Traditional genetic programming is a highly computer intensive algorithm but due to its parallel nature it can be distributed over multiple processors to increase the speed of the algorithm considerably. This is not applicable for single processor architectures but graphics cards provide a mechanism for developing a data parallel implementation of genetic programming. In this paper we will describe the technique of general purpose computing using graphics cards and how to extend this technique to genetic programming. We will demonstrate the improvement in the performance of genetic programming on single processor architectures which can be achieved by harnessing the computing power of these next generation graphics cards.|Darren M. Chitty","58106|GECCO|2007|Linear genetic programming of metaheuristics|We suggest a flavour of linear Genetic Programming indomain-specific languages that acts as a hyperheuristic (HH).|Robert E. Keller,Riccardo Poli","58073|GECCO|2007|Using genetic algorithms for naval subsystem damage assessment and design improvements|Some auxiliary systems of next generation naval ships will utilize distributed automatic control. Such distributed control systems will use interconnected sensors, actuators, controllers and networking components to diagnose and reconfigure the auxiliary systems. Testing these systems will be difficult with traditional methods of fault analysis due to the interconnected and automatic nature of these subsystems. We have designed a suite of genetic algorithms to find interesting and hidden damage scenarios in a testbed of a naval subsystem. Given this knowledge, we use a genetic algorithm to improve upon the design of this subsystem.|Christopher McCubbin,David Scheidt,Oliver Bandte,Steven Marshall,Iavor Trifonov","58187|GECCO|2007|Numerical-node building block analysis of genetic programming with simplification|This paper investigates the effects on building blocks of using online simplification in a GP system. Numerical nodes are tracked through individual runs to observe their behaviour. Results show that simplification disrupts building blocks early on, but also creates new building blocks.|Phillip Lee-Ming Wong,Mengjie Zhang","58164|GECCO|2007|Controller design based on genetic programming|Three genetic programming-based approaches are proposed for continuous-time process control design. Two approaches are represented using a network of interconnected continuous-time or discrete-time elementary dynamic building blocs. In the third approach the control algorithm is represented as a recurrent function of discrete-time input variables.all.|Ivan Sekaj,Juraj Perkacz,Tomas Palenik","58135|GECCO|2007|Genetic optimization for yacht design|This paper introduces a procedure for using genetic multi-objective optimization in yacht design. The problem described consists on the optimization of a bulb shape to improve the performance of the yacht. The two objectives considered are the minimization of the drag in calm water together with the minimization of the Vertical Center of Gravity (VCG), all the configurations should satisfy length and volume constraints. Since there is no a single optimum to be found, the MOGA-II was used as multi-objective genetic algorithm. The distributed optimization search exploited the parallelization capabilities of the MOGA-II algorithm which allowed the evaluation of several designs configurations by running concurrent threads of the flow analysis solver. Three bulb shapes of different length are selected between the non-dominated solutions. Using these three solutions, seakeeping tests of a fully appended scale model have been carried out at the towing tank of the University of Trieste. A single hull has been tested for each bulb configurations to check the influence of the bulb shape on the performance of the yacht in waves. The results obtained are very satisfactory, and the procedure described can be applied to even more complex yacht design problems.|Paolo Geremia,Mauro Poian,Silvia Poles","57930|GECCO|2007|Optimal design of ad hoc injection networks by using genetic algorithms|This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.|Gr√©goire Danoy,Enrique Alba,Pascal Bouvry,Matthias R. Brust","16386|IJCAI|2007|Feature Selection and Kernel Design via Linear Programming|The definition of object (e.g., data point) similarity is critical to the performance of many machine learning algorithms, both in terms of accuracy and computational efficiency. However, it is often the case that a similarity function is unknown or chosen by hand. This paper introduces a formulation that given relative similarity comparisons among triples of points of the form object i is more like object j than object k, it constructs a kernel function that preserves the given relationships. Our approach is based on learning a kernel that is a combination of functions taken from a set of base functions (these could be kernels as well). The formulation is based on defining an optimization problem that can be solved using linear programming instead of a semidefinite program usually required for kernel learning. We show how to construct a convex problem from the given set of similarity comparisons and then arrive to a linear programming formulation by employing a subset of the positive definite matrices. We extend this formulation to consider representationevaluation efficiency based on formulating a novel form of feature selection using kernels (that is not much more expensive to solve). Using publicly available data, we experimentally demonstrate how the formulation introduced in this paper shows excellent performance in practice by comparing it with a baseline method and a related state-of-the art approach, in addition of being much more efficient computationally.|Glenn Fung,R√≥mer Rosales,R. Bharat Rao"]]}}