{"abstract":{"entropy":6.3736407721052215,"topics":["data, data base, data system, data management, data model, database, problem data, relational database, large data, data stream, web search, relational data, database system, neural network, web, learning data, search engine, web pages, query, xml documents","artificial intelligence, natural language, system, describes system, expert system, play role, knowledge system, natural processing, describes, language system, knowledge acquisition, present system, control system, real world, language processing, system model, computer program, language generation, management system, word sense","present approach, logic, description logic, logic programming, theorem proving, knowledge representation, present, knowledge base, model, present framework, present model, reasoning, temporal reasoning, approach based, theorem prover, present based, belief revision, situation calculus, model based, model-based diagnosis","machine learning, problem, constraint satisfaction, solving problem, constraint problem, markov decision, learning, consider problem, markov processes, heuristic search, search algorithm, recent years, satisfaction problem, partially observable, algorithm, reinforcement learning, decision processes, present algorithm, solutions problem, planning problem","information retrieval, present novel, information extraction, moving objects, image objects, support vector, objects, nearest neighbor, measure similarity, data statistical, collaborative filtering, discriminant analysis, objects recognition, novel, measuring similarity, vector svm, pattern recognition, machine translation, paper novel, distance metric","web search, search engine, web pages, web, semantic web, web information, database schema, web services, sources information, access data, web engine, schema data, data sources, data web, semantic data, present web, materialized view, web ontology, paper web, web applications","artificial intelligence, multi-agent system, system complex, research intelligence, intelligent system, research artificial, design system, artificial system, research, system need, research system, system require, recent research, intelligent tutoring, intelligence system, mobile robot, physical system, model-based system, research area, system robot","expert system, control system, system use, development system, system user, production system, expert knowledge, system provide, architecture system, use, system support, system generate, provide user, software system, system build, test system, dynamical system, discuss system, control robot, system make","present approach, knowledge base, present based, present novel, approach based, present, model based, knowledge representation, based, present algorithm, present model, present semantic, paper present, paper, novel approach, present knowledge, paper based, problem knowledge, knowledge, present system","logic programming, theorem proving, logic program, theorem prover, programming language, modal logic, first-order logic, model, logic, program, describes approach, probabilistic model, system logic, description logic, present model, present approach, equality elimination, combines logic, expressive power, programming asp","machine learning, problem learning, learning, reinforcement learning, learning algorithm, present learning, mobile robot, addresses problem, learning domains, approach learning, previous work, directed graph, explanation-based learning, learning methods, problem machine, learning examples, problem domains, learning system, research learning, learning task","solving problem, consider problem, search algorithm, heuristic search, solutions problem, search problem, present algorithm, search, algorithm, problem algorithm, address problem, problem, search space, satisfiability sat, local search, algorithm constraint, search constraint, constraint problem, optimal solutions, tree search"],"ranking":[["80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80061|VLDB|1981|DSIS - A Database System with Interrelational Semantics|DSIS is an experimental multi-user database system that supports an entity-oriented data model with relational and interrelational semantics. It maintains a sharp distinction between the user workspace and the shared database. Transactions are executed by a simulated network of stream processors.|Y. Edmund Lien,Jonathan E. Shopiro,Shalom Tsur","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80315|VLDB|2003|XML Schemas in Oracle XML DB|The WC XML Scheme language is becomimg increasingly popular for expressing the data model for XML documents. It is a powerful language that incorporates both strutural and datatype modeling features. There are many benefits to storing XML Schema compliant data in a database system, including better queryability, optimied updates and stronger validation. However, the fidelity of the XML document cannot be sacrificed. Thus, the fundamental problem facing database implementers is how can XML Schemes be mapped to relational (and object-relational) database without losing schema semantics or data-fidelity In this paper, we present the Oracle XML DB solution for a flexible mapping of XML Schemas to object-relational database. It preserves document fidelity, including ordering, namespaces, comments, processing instructions etc., and handles all the XML Schema semantics including cyclic definitions, dervations (extension and restriction), and wildcards. We also discuss various query and update optimiations that involve rewriting XPath operations to directly operate on the underlying relational data.|Ravi Murthy,Sandeepan Banerjee","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos"],["13365|IJCAI|1973|A Gobal View of Automatic Programming|This paper presents a framework for characterizing automatic programming systems In terms of how a task Is communicated to the system, the method and time at which the system acquires the knowledge to perform the task, and the characteristics of the resulting program to perform that task. It describes one approach In which both tasks and knowledge about the task domain are stated in natural language In the terms of that domain. All knowledge of computer science necessary to Implement the task Is Internalized Inside the system.|Robert Balzer","13991|IJCAI|1983|Transportability and Generality in a Natural-Language Interface System|This paper describes the design of a transportable natural language (NL) interface to databases and the constraints that transportability places on each component of such a system. By a transportable NL system, we mean an NL processing system that is constructed so that a domain expert (rather than an AI or linguistics expert) can move the system to a new application domain. After discussing the general problems presented by transportability, this paper describes TEAM (an acronym for Transportable English database Access Medium), a demonstratable prototype of such a system. The discussion of TEAM shows how domain-independent and domain-dependent information can be separated in the different components of a NL interface system, and presents one method of obtaining domain-specific information from a domain expert.|Paul A. Martin,Douglas E. Appelt,Fernando C. N. Pereira","14349|IJCAI|1987|A Microfeature-based Scheme for Modelling Semantics|One fundamental problem of natural language processing is word sense disambiguation. Solving this problem involves the integration of multiple knowledge sources syntactic, semantic, and pragmatic. Recent work has shown how this problem can be modelled as a constraint satisfaction process between competing syntactic and semantic structures. We have defined and implemented a \"locally-distributed\" microfeature based model called MIBS, that uses a distributed short-term memory (STM) composed of microfeatures to represent the underlying sentence semantics. This work represents an improvement over previous work, as it provides a natural language understanding system a means to dynamically determine the current context and adjust its relationship with the sentences that follow. Here, the meaning of a word is represented not as a symbol in some semantic net, but as a collection of smaller features. The values of the microfeatures in STM vary dynamically as the sentence is processed, reflecting the system's \"settling\" in on the sentence's meaning. In addition they represent an automatic context mechanism that helps the system to disambiguate the sentences that follow.|Lawrence A. Bookman","79865|VLDB|1977|Developing a Natural Language Interface to Complex Data|Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed.|Gary G. Hendrix,Earl D. Sacerdoti,Daniel Sagalowicz,Jonathan Slocum","13436|IJCAI|1973|Understanding Without Proofs|The paper describes the analysis part of a running analysis and generation program for natural language. The system is entirely oriented to matching meaningful patterns onto fragmented paragraph length input. Its core Is a choice system based on what I call \"semantic density\". The system is contrasted with () syntax oriented linguistic approaches and () theorem proving approaches to the understanding problem. It is argued by means of examples that the present system is not only more workable, but more intuitively acceptable, at least as an understander for the purpose of translation, than deduction-based systems.|Yorick Wilks","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","14887|IJCAI|1991|High Performance Natural Language Processing on Semantic Network Array Processor|This paper describes a natural language processing system developed for the Semantic Network Array Processor (SNAP). The goal of our work is to develop a scalable and high-performance natural language processing system which utilizes the high degree of parallelism provided by the SNAP machine. We have implemented an experimental machine translation system as a central part of a real-time speech-to-speech dialogue translation system. It is a SNAP version of the  DMDIAIOG speech-to-speech translation system. Memory-based natural language processing and syntactic constraint network model has been incorporated using parallel marker-passing which is directly supported from hardware level. Experimental results demonstrate that the parsing of a sentence is done in the order of milliseconds.|Hiroaki Kitano,Dan I. Moldovan,Seungho Cha","14093|IJCAI|1985|LandScan A Natural Language and Computer Vision System for Analyzing Aerial Images|LandScan (LANguage Driven SCene ANalysis) is presented as an integrated vision system which covers most levels of both vision and natural language processing. Computations are both data-driven and query-driven. In the report we focus on the design of the vision and control modules. Future work will investigate in more detail the design of the natural language interface. The data-driven system employs active control of stereo cameras for image acquisition, and dynamically constructs a surface model from multiple aerial views of an urban scene. The query-driven system allows the user's natural language queries to focus analysis to pertinent regions of the scene. This is different than many image understanding systems which present a symbolic description of the entire scene regardless of what portions of that picture are actually of interest.|Ruzena Bajcsy,Aravind K. Joshi,Eric Krotkov,Amy E. Zwarico","13747|IJCAI|1981|GLP A General Linguistic Processor|GLP is a general linguistic processor for the analysis and generation of natural language. It is part of a speech understanding system currently under development at the Computer Science Department of our university .|G. Goerx","14140|IJCAI|1985|SAPHIR  RESEDA A New Approach to Intelligent Data Base Access|This paper describes a transportable natural language interface to databases, augmented with a knowledge base and inference techniques. The inference mechanism, based on a classical expert system's type of approach, allows, when needed, to automatically convert an Input query into another one which is \"semantically close\". According to RESEDAS theory, \"semantically close\" means that the answer to the transformed query Implies what could have been the answer to the original question. The presented system Integrates natural language processing, expert system and knowledge representation technology to provide a cooperative database access.|Bernard Euzenat,Bernard Normier,Antoine Ogonowski,Gian Piero Zarri"],["16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","65564|AAAI|2005|Diagnosing Terminologies|We present a framework for the debugging of logically contradicting terminologies, which is based on traditional model-based diagnosis. To study the feasibility of this highly general approach we prototypically implemented the hitting set algorithm presented in (Reiter ), and applied it in three different scenarios. First, we use a Description Logic reasoning system as a black-box to determine (necessarily maximal) conflict sets. Then we use our own non-optimized DL reasoning engine to produce small, and a specialized algorithm to determine minimal conflict sets. In a number of expenments we show that the first method already fails for relatively small terminologies. However, based on small, or minimal conflict sets, we can often calculate diagnoses in reasonable time.|Stefan Schlobach","65074|AAAI|1987|Inferring Formal Software Specifications from Episodic Descriptions|The WATSON automatic programming system computes formal behavior specifications for process-control software from informal \"scenarios\" traces of typical system operation. It first generalizes scenarios into stimulus-response rules, then modifies and augments these rules to repair inconsistency and incompleteness. It finally produces a formal specification for the class of computations which implement that scenario and which are also compatible with a set of \"domain axioms\". A particular automaton from that class is constructed as an executable prototype for the specification. WATSON's inference engine combines theorem proving in a very weak temporal logic with faster and stronger, but approximate, model-based reasoning. The use of models and of closed-world reasoning over \"snapshots\" of an evolving knowledge base leads to an interesting special case of non-monotonic reasoning.|Van E. Kelly,Uwe Nonnenmann","16114|IJCAI|2005|Representing Flexible Temporal Behaviors in the Situation Calculus|In this paper we present an approach to representing and managing temporally-flexible behaviors in the Situation Calculus based on a model of time and concurrent situations. We define a new hybrid framework combining temporal constraint reasoning and reasoning about actions. We show that the Constraint Based Interval Planning approach can be imported into the Situation Calculus by defining a temporal and concurrent extension of the basic action theory. Finally, we provide a version of the Golog interpreter suitable for managing flexible plans on multiple timelines.|Alberto Finzi,Fiora Pirri","14889|IJCAI|1991|How to Prove Higher Order Theorems in First Order Logic|In this paper we are interested in using a first order theorem prover to prove theorems that are formulated in some higher order logic. To this end we present translations of higher order logics into first order logic with flat sorts and equality and give a sufficient criterion for the soundness of these translations. In addition translations are introduced that are sound and complete with respect to L. Henkin's general model semantics. Our higher order logics are based on a restricted type structure in the sense of A. Church, they have typed function symbols and predicate symbols, but no sorts.|Manfred Kerber","15438|IJCAI|1999|Programming Resource-Bounded Deliberative Agents|This paper is concerned with providing a common framework for both the logical specification and execution of agents. While numerous high-level agent theories have been proposed in order to model agents, such as theories of intention, these often have little formal connection to practical agentbased systems. On the other hand, many of the agent-based programming languages used for implementing real agents lack firm logical semantics. Our approach is to define a logical framework in which agents can be specified, and then show how such specifications can be directly executed in order to implement the agent's behaviour. We here extend this approach to capture an important aspect of practical agents, namely their resource-bounded nature. We present a logic in which resource-boundedness can be specified, and then consider how specifications within this logic can be directly executed. The mechanism we use to capture finite resources is to replace the standard modal logic previously used to represent an agent's beliefs, with a multi-context representation of belief, thus providing tight control over the agent's reasoning capabilities where necessary. This logical framework provides the basis for the specification and execution of agents comprising dynamic (temporal) activity, deliberation concerning goals, and resource-bounded reasoning.|Michael Fisher,Chiara Ghidini","15202|IJCAI|1995|Default-Reasoning with Models|Reasoning with model-based representations is an intuitive paradigm, which has been shown to be theoretically sound and to possess some computational advantages over reasoning with formula-based representations of knowledge. In this paper we present more evidence to the utility of such representations. In real life situations, one normally completes a lot of missing \"context\" information when answering queries. We model this situation by augmenting the available knowledge about the world with context-specific information we show that reasoning with model-based representations can be done efficiently in the presence of varying context information. We then consider the task of default reasoning. We show that default reasoning is a generalization of reasoning within context, in which the reasoner has many \"context\" rules, which may be conflicting. We characterize the cases in which model-based reasoning supports efficient default reasoning and develop algorithms that handle efficiently fragments of Reiter's default logic. In particular, this includes cases in which performing the default reasoning task with the traditional, formula-based, representation is intractable. Further, we argue that these results support an incremental view of reasoning in a natural way.|Roni Khardon,Dan Roth","66519|AAAI|2008|Terminological Reasoning in SHIQ with Ordered Binary Decision Diagrams|We present a new algorithm for reasoning in the description logic SHIQ, which is the most prominent fragment of the Web Ontology Language OWL. The algorithm is based on ordered binary decision diagrams (OBDDs) as a datastructure for storing and operating on large model representations. We thus draw on the success and the proven scalability of OBDD-based systems. To the best of our knowledge, we present the very first agorithm for using OBDDs for reasoning with general Tboxes.|Sebastian Rudolph,Markus Kr√∂tzsch,Pascal Hitzler","15000|IJCAI|1993|An Abductive Framework for General Logic Programs and other Nonmonotonic Systems|We present an abductive semantics for general propositional logic programs which defines the meaning of a logic program in terms of its extensions. This approach extends the stable model semantics for normal logic programs in a natural way. The new semantics is equivalent to stable semantics for a logic program P whenever P is normal and has a stable model. The abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner. Our approach is based on an idea recently proposed by Konolige for causal reasoning. Instead of maximizing the set of hypotheses alone we maximize the union of the hypotheses, along with possible hypotheses that are excused or refuted by the theory.|Gerhard Brewka,Kurt Konolige","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"],["16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","15875|IJCAI|2003|A Planning Algorithm for Predictive State Representations|We address the problem of optimally controlling stochastic environments that are partially observable. The standard method for tackling such problems is to define and solve a Partially Observable Markov Decision Process (POMDP). However, it is well known that exactly solving POMDPs is very costly computationally. Recently, Littman, Sutton and Singh () have proposed an alternative representation of partially observable environments, called predictive state representations (PSRs). PSRs are grounded in the sequence of actions and observations of the agent, and hence relate the state representation directly to the agent's experience. In this paper, we present a policy iteration algorithm for finding policies using PSRs. In preliminary experiments, our algorithm produced good solutions.|Masoumeh T. Izadi,Doina Precup","66436|AAAI|2008|Symbolic Heuristic Search Value Iteration for Factored POMDPs|We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.|Hyeong Seop Sim,Kee-Eung Kim,Jin Hyung Kim,Du-Seong Chang,Myoung-Wan Koo","65199|AAAI|2004|Stochastic Local Search for POMDP Controllers|The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.|Darius Braziunas,Craig Boutilier","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","15146|IJCAI|1995|Decomposition Techniques for Planning in Stochastic Domains|This paper is concerned with modeling planning problems involving uncertainty as discrete-time, finite-state stochastic automata Solving planning problems is reduced to computing policies for Markov decision processes. Classical methods for solving Markov decision processes cannot cope with the size of the state spaces for typical problems encountered in practice. As an alternative, we investigate methods that decompose global planning problems into a number of local problems solve the local problems separately and then combine the local solutions to generate a global solution. We present algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space. The local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the subsets of the state space defined by the partition. One algorithm relies on constructing and solving an abstract version of the original decision problem. A second algorithm iteratively approximates parameters of the local problems to converge to an optimal solution. We show how properties of a specified partition affect the time and storage required for these algorithms.|Thomas Dean,Shieu-Hong Lin","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","14618|IJCAI|1989|Partial Constraint Satisfaction|A constraint satisfaction problem involves finding values for variables subject to constraints on which combinations of values are allowed. In some cases it may be impossible or impractical to solve these problems completely. We may seek to partially solve the problem in an \"optimal\" or \"sufficient\" sense. A formal model is presented for defining and studying such partial constraint satisfaction problems. The basic components of this model are a constraint satisfaction problem, a problem space, and a metric on that space. Algorithms for solving partial constraint satisfaction problems are discussed. A specific branch and bound algorithm is described. Some initial experimental experience with this algorithm is presented.|Eugene C. Freuder","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell"],["16018|IJCAI|2003|Parametric Distance Metric Learning with Label Information|Distance-based methods in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between patterns in the input space. For many applications, Euclidean distance in the input space is not a good choice and hence more complicated distance metrics have to be used. In this paper, we propose a parametric method for metric learning based on class label information. We first define a dissimilarity measure that can be proved to be metric. It has the favorable property that between-class dissimilarity is always larger than within-class dissimilarity. We then perform parametric learning to find a regression mapping from the input space to a feature space, such that the dissimilarity between patterns in the input space is approximated by the Euclidean distance between points in the feature space. Parametric learning is performed using the iterative majorization algorithm. Experimental results on real-world benchmark data sets show that this approach is promising.|Zhihua Zhang,James T. Kwok,Dit-Yan Yeung","80784|VLDB|2007|Peer-to-Peer Similarity Search in Metric Spaces|This paper addresses the efficient processing of similarity queries in metric spaces, where data is horizontally distributed across a PP network. The proposed approach does not rely on arbitrary data movement, hence each peer joining the network autonomously stores its own data. We present SIMPEER, a novel framework that dynamically clusters peer data, in order to build distributed routing information at super-peer level. SIMPEER allows the evaluation of range and nearest neighbor queries in a distributed manner that reduces communication cost, network latency, bandwidth consumption and computational overhead at each individual peer. SIMPEER utilizes a set of distributed statistics and guarantees that all similar objects to the query are retrieved, without necessarily flooding the network during query processing. The statistics are employed for estimating an adequate query radius for k-nearest neighbor queries, and transform the query to a range query. Our experimental evaluation employs both real-world and synthetic data collections, and our results show that SIMPEER performs efficiently, even in the case of high degree of distribution.|Christos Doulkeridis,Akrivi Vlachou,Yannis Kotidis,Michalis Vazirgiannis","65827|AAAI|2006|Corpus-based and Knowledge-based Measures of Text Semantic Similarity|This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to % error rate reduction with respect to the traditional vector-based similarity metric.|Rada Mihalcea,Courtney Corley,Carlo Strapparava","80162|VLDB|2002|Adaptable Similarity Search using Non-Relevant Information|Many modern database applications require content-based similarity search capability in numeric attribute space. Further, users' notion of similarity varies between search sessions. Therefore online techniques for adaptively refining the similarity metric based on relevance feedback from the user are necessary. Existing methods use retrieved items marked relevant by the user to refine the similarity metric, without taking into account the information about non-relevant (or unsatisfactory) items. Consequently items in database close to non-relevant ones continue to be retrieved in further iterations. In this paper a robust technique is proposed to incorporate non-relevant information to efficiently discover the feasible search region. A decision surface is determined to split the attribute space into relevant and nonrelevant regions. The decision surface is composed of hyperplanes, each of which is normal to the minimum distance vector from a nonrelevant point to the convex hull of the relevant points. A similarity metric, estimated using the relevant objects is used to rank and retrieve database objects in the relevant region. Experiments on simulated and benchmark datasets demonstrate robustness and superior performance of the proposed technique over existing adaptive similarity search techniques.|T. V. Ashwin,Rahul Gupta,Sugata Ghosal","80708|VLDB|2006|Similarity Search A Matching Based Approach|Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks ) it leaves many partial similarities uncovered ) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper.The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved, which is especially useful for information retrieval from multiple systems. We can also apply the proposed algorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that ) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities ) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.|Anthony K. H. Tung,Rui Zhang 0003,Nick Koudas,Beng Chin Ooi","16260|IJCAI|2005|Stepwise Nearest Neighbor Discriminant Analysis|Linear Discriminant Analysis (LDA) is a popular feature extraction technique in statistical pattern recognition. However, it often suffers from the small sample size problem when dealing with the high dimensional data. Moreover, while LDA is guaranteed to find the best directions when each class has a Gaussian density with a common covariance matrix, it can fail if the class densities are more general. In this paper, a new nonparametric feature extraction method, stepwise nearest neighbor discriminant analysis(SNNDA), is proposed from the point of view of the nearest neighbor classification. SNNDA finds the important discriminant directions without assuming the class densities belong to any particular parametric family. It does not depend on the nonsingularity of the within-class scatter matrix either. Our experimental results demonstrate that SNNDA outperforms the existing variant LDA methods and the other state-of-art face recognition approaches on three datasets from ATT and FERET face databases.|Xipeng Qiu,Lide Wu","65555|AAAI|2005|Learning Static Object Segmentation from Motion Segmentation|Dividing an image into its constituent objects can be a useful first step in many visual processing tasks, such as object classification or determining the arrangement of obstacles in an environment. Motion segmentation is a rich source of training data for learning to segment objects by their static image properties. Background subtraction can distinguish between moving objects and their surroundings, and the techniques of statistical machine learning can capture information about objects' shape, size. color, brightness, and texture properties. Presented with a new, static image, the trained model can infer the proper segmentation of the objects present in a scene. The algorithm presented in this work uses the techniques of Markov random field modeling and belief propagation inference, outperforms a standard segmentation algorithm on an object segmentation task, and outperforms a learned boundary detector at determining object boundaries on the test data.|Michael G. Ross,Leslie Pack Kaelbling","16796|IJCAI|2007|Parametric Kernels for Sequence Data Analysis|A key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. Many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. However, finding an effective transformation scheme for sequence (e.g. time series) data is a difficult task. In this paper, we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwritten character recognition and object recognition from sensor readings. Ordering information is represented by values of a parameter associated with each input data element. A similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a Mercer kernel suitable for use in methods such as support vector machine (SVM). This scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. We apply our method to object and handwritten character recognition tasks and compare against current approaches. The results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. Our contribution is the introduction of a general technique for designing SVM kernels tailored for the classification of sequence data.|Young-In Shin,Donald S. Fussell","15588|IJCAI|2001|NLP-driven IR Evaluating Performances over a Text Classification task|Although several attempts have been made to introduce Natural Language Processing (NLP) techniques in Information Retrieval, most ones failed to prove their effectiveness in increasing performances. In this paper Text Classification (TC) has been taken as the IR task and the effect of linguistic capabilities of the underlying system have been studied. A novel model for TC, extending a well know statistical model (i.e. Rocchio's formula Ittner et al., ) and applied to linguistic features has been defined and experimented. The proposed model represents an effective feature selection methodology. All the experiments result in a significant improvement with respect to other purely statistical methods (e.g. Yang, ), thus stressing the relevance of the available linguistic information. Moreover, the derived classifier reachs the performance (about %) of the best known models (i.e. Support Vector Machines (SVM) and K -Nearest Neighbour (KNN)) characterized by an higher computational complexity for training and processing.|Roberto Basili,Alessandro Moschitti,Maria Teresa Pazienza","66796|AAAI|2010|Active Inference for Collective Classification|Distance metric is widely used in similarity estimation. In this paper we find that the most popular Euclidean and Manhattan distance may not be suitable for all data distributions. A general guideline to establish the relation between a distribution model and its corresponding similarity estimation is proposed. Based on maximum likelihood theory, we propose new distance metrics, such as harmonic distance and geometric distance. Because the feature elements may be from heterogeneous sources and usually have different influence on similarity estimation, it is inappropriate to model the distribution as isotropic. We propose a novel boosted distance metric that not only finds the best distance metric that fits the distribution of the underlying elements but also selects the most important feature elements with respect to similarity. The boosted distance metric is tested on fifteen benchmark data sets from the UCI repository and two image retrieval applications. In all the experiments, robust results are obtained based on the proposed methods|Mustafa Bilgic,Lise Getoor"],["80385|VLDB|2004|Simlarity Search for Web Services|Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem locating desired web services. Traditional keyword search is insufficient in this context the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over  web-service operations that shows the high recall and precision of our algorithms.|Xin Dong,Alon Y. Halevy,Jayant Madhavan,Ema Nemes,Jun Zhang","80458|VLDB|2004|Instance-based Schema Matching for Web Databases by Domain-specific Query Probing|In a Web database that dynamically provides information in response to user queries, two distinct schemas, interface schema (the schema users can query) and result schema (the schema users can browse), are presented to users. Each partially reflects the actual schema of the Web database. Most previous work only studied the problem of schema matching across query interfaces of Web databases. In this paper, we propose a novel schema model that distinguishes the interface and the result schema of a Web database in a specific domain. In this model, we address two significant Web database schema-matching problems intra-site and inter-site. The first problem is crucial in automatically extracting data from Web databases, while the second problem plays a significant role in meta-retrieving and integrating data from different Web databases. We also investigate a unified solution to the two problems based on query probing and instance-based schema matching techniques. Using the model, a cross validation technique is also proposed to improve the accuracy of the schema matching. Our experiments on real Web databases demonstrate that the two problems can be solved simultaneously with high precision and recall.|Jiying Wang,Ji-Rong Wen,Frederick H. Lochovsky,Wei-Ying Ma","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80154|VLDB|2001|Self-similarity in the Web|Algorithmic tools for searching and mining the Web are becoming increasingly sophisticated and vital. In this context, algorithms that use and exploit structural information about the Web perform better than generic methods in both efficiency and reliability.We present an extensive characterization of the graph structure of the Web, with a view to enabling high-performance applications that make use of this structure. In particular, we show that the Web emerges as the outcome of a number of essentially independent stochastic processes that evolve at various scales. A striking consequence of this scale invariance is that the structure of the Web is \"fractal\"---cohesive subregions display the same characteristics as the Web at large. An understanding of this underlying fractal nature is therefore applicable to designing data services across multiple domains and scales.We describe potential applications of this line of research to optimized algorithm design for Web-scale data analysis.|Stephen Dill,Ravi Kumar,Kevin S. McCurley,Sridhar Rajagopalan,D. Sivakumar,Andrew Tomkins","16293|IJCAI|2005|Intimate Learning A Novel Approach for Combining Labelled and Unlabelled Data|This paper introduces a new bootstrapping method closely related to co-training and scoped-learning. The method is tested on a Web information extraction task of learning course names from web pages in which we use very few labelled items as seed data ( web pages) and combine with an unlabelled set ( web pages). The overall performance improved the precisionrecall from .%.% for a baseline EM-based method to .%.% for intimate learning.|Zhongmin Shi,Anoop Sarkar","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","16483|IJCAI|2007|Learning Semantic Descriptions of Web Information Sources|The Internet is full of information sources providing various types of data from weather forecasts to travel deals. These sources can be accessed via web-forms, Web Services or RSS feeds. In order to make automated use of these sources, one needs to first model them semantically. Writing semantic descriptions for web sources is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions for web sources, in which we actively invoke sources and compare the data they produce with that of known sources of information. We perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. The paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.|Mark James Carman,Craig A. Knoblock","80437|VLDB|2004|WIC A General-Purpose Algorithm for Monitoring Web Information Sources|The Web is becoming a universal information dissemination medium, due to a number of factors including its support for content dynamicity. A growing number of Web information providers post near real-time updates in domains such as auctions, stock markets, bulletin boards, news, weather, roadway conditions, sports scores, etc. External parties often wish to capture this information for a wide variety of purposes ranging from online data mining to automated synthesis of information from multiple sources. There has been a great deal of work on the design of systems that can process streams of data from Web sources, but little attention has been paid to how to produce these data streams, given that Web pages generally require \"pull-based\" access. In this paper we introduce a new general-purpose algorithm for monitoring Web information sources, effectively converting pull-based sources into push-based ones. Our algorithm can be used in conjunction with continuous query systems that assume information is fed into the query engine in a push-based fashion. Ideally, a Web monitoring algorithm for this purpose should achieve two objectives () timeliness and () completeness of information captured. However, we demonstrate both analytically and empirically using real-world data that these objectives are fundamentally at odds. When resources available for Web monitoring are limited, and the number of sources to monitor is large, it may be necessary to sacrifice some timeliness to achieve better completeness, or vice versa. To take this fact into account, our algorithm is highly parameterized and targets an application-specified balance between timeliness and completeness. In this paper we formalize the problem of optimizing for a flexible combination of timeliness and completeness, and prove that our parameterized algorithm is a - approximation in all cases, and in certain cases is optimal.|Sandeep Pandey,Kedar Dhamdhere,Christopher Olston","66407|AAAI|2008|Predicting Appropriate Semantic Web Terms from Words|The Semantic Web language RDF was designed to unambiguously define and use ontologies to encode data and knowledge on the Web. Many people find it difficult, however, to write complex RDF statements and queries because doing so requires familiarity with the appropriate ontologies and the terms they define. We describe a system that suggests appropriate RDF terms given semantically related English words and general domain and context information. We use the Swoogle Semantic Web search engine to provide RDF term and namespace statistics, the WorldNet lexical ontology to find semantically related words, and a nave Bayes classifier to suggest terms. A customized graph data structure of related namespaces is constructed from Swoogle's database to speed up the classifier model learning and prediction time.|Lushan Han,Tim Finin","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["13781|IJCAI|1981|Time-Oriented Features for Medical Consultation Systems|Medical consultation system is one of the major application of artificial intelligence research, and in this field, it is important to treat time-oriented data. The authors developed a system named MECS-Al (MEdical Consultation System by means of Artificial Intelligence), which is designed as a general purpose tool for constructing systems with capability to treat time-oriented data. The basic idea of this system is to describe the time flow as a chain of discrete events. As a general purpose tool, the system consists of an inference-engine and a knowledge-base editor, so that consultation systems in any field can be easily defined and tested.|T. Koyama,S. Kaihara,T. Minamikawa,T. Kurokawa","66052|AAAI|2007|TeamTalk A Platform for Multi-Human-Robot Dialog Research in Coherent Real and Virtual Spaces|Performing experiments with human-robot interfaces often requires the allocation of expensive and complex hardware and large physical spaces. Those costs constrain development and research to the currently affordable resources, and they retard the testing-and-redevelopment cycle. In order to explore research free from mundane allocation constraints and speed-up our platform development cycle, we have developed a platform for research of multi-human-robot spoken dialog in coherent real and virtual spaces. We describe the system, and speculate on how it will further research in this domain.|Thomas K. Harris,Alexander I. Rudnicky","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","13685|IJCAI|1977|Speech Understanding and AIAI and Speech Understanding|This panel will review research in speech understanding (SU) and in artificial intelligence (AI) from two perspectives the contributions that AI has made to SU -- the resources in AI that have been used in the development of SU systems. the contributions that SU has made to AI - the results of the SU program that have affected or arc likely to affect futuie AI research. Four topics are identified for major consideration Multiple sources of Knowledge which are required how should they be oiganized, System control how to manage the complex inter actions involved. language understanding comparisons of text and speech input. Organization of research -creating complex, multisource, knowledge-based systems.|Donald E. Walker,Lee D. Erman,Allen Newell,Nils J. Nilsson,William H. Paxton,Terry Winograd,William A. Woods","14815|IJCAI|1991|Integration of Neural Networks and Expert Systems for Process Fault Diagnosis|The main thrust of this research is the development of an artificial intelligence (AI) system to be used as an operators' aid in the diagnosis of faults in large-scale chemical process plants. The operator advisory system involves the integration of two fundamentally different AI techniques expert systems and neural networks. A diagnostic strategy based on the hierarchical use of neural networks is used as a first level filter to diagnose faults commonly encountered in chemical process plants. Once the faults are localized within the process by the neural networks, the deep knowledge expert system analyzes the results, and either confirms the diagnosis or offers alternative solutions. The model-based expert system contains information of the plant's structure and function within its object-oriented knowledge base. The diagnostic strategy can handle novel or previously unencountered faults, noisy process sensor measurements, and multiple faults. The operator advisory system is demonstrated using a multi-column distillation plant as a case study.|Warren R. Becraft,Peter L. Lee,Robert B. Newell","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,Fran√ßois Felix Ingrand","14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz","14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue","13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson","13395|IJCAI|1973|Artificial Intelligence and Automatic Programming in CAI|This paper discusses generative computer-assisted instruction (CAI) and its relationship to Artificial Intelligence Research. Systems which have a limited capability for natural language communication are described. In addition, potential areas in which Artificial Intelligence could be applied are outlined. These include individualization of instruction, determining the degree of accuracy of a student response, and problem-solving. A CAI system which is capable of writing computer programs is described in detail. Techniques are given for generating meaningful programming problems. These problems are represented as a sequence of primitive tasks each of which can be coded in several ways. The manner in which the system designs its own solution program and monitors the student solution is also described.|Elliot B. Koffman,Sumner E. Blount"],["14465|IJCAI|1987|DANTES An Expert System for Real-Time Network Troubleshooting|Today's computer networks are large and complex. Their day-to-day operation and maintenance can benefit from the support of an expert system, mainly as an aid in troubleshooting. Network troubleshooting has characteristics, like incomplete data, high rate of events, simultaneous presence of several problems, which raise interesting problems in the development of an expert system. DANTES is an expert system designed to provide real-time assistance to network operators. This paper presents the system and stresses the development issues that are peculiar to network troubleshooting. Of particular importance are performance of inference in real-time, multi-problem handling, and consideration of time in reasoning and revision of belief Dealing with such issues and especially with real-time efficiency is primarily a question of system design. This has implications for the knowledge base organization, reasoning mechanism, and recording of deductions.|Robert Mathonet,Herwig Van Cotthem,Leon Vanryckeghem","14304|IJCAI|1985|Be Brief Be to the Point  Be Seated or Relevant Responses in ManMachine Conversation|In the dialogue part of our system, we have tried to increase the user's possibilities to criticize the machine's results require explanations of them. The system must then provide a clear justification either by furnishing the chain of reasoning or by asking a \"good question\" when it failed. The system must also be able to engage in a real dialogue, with more than one questionone answer. To do that, the system must build and use several kinds of representation the reasoning, the topics and a model of the user, which is used to tailor the system's responses.|Anne Vilnat,G√©rard Sabah","13754|IJCAI|1981|How Expert Should an Expert System Be|A computer system which aids computer engineers in fault diagnosis is described. The system, called CRIB (Computer Retrieval Incidence Bank) is shown to fit into the class of pattern-directed inference systems. Emphasis is placed on the \"before\" and \"after\" phases of system generation and it is shown why, to be called an expert system, these phases are important. The forms of knowledge used in CRIB are shown to be adequate for diagnosis and yet possess little of the structural or functional knowledge of more advanced expert systems. Summaries are given of the three phases of implementation elicitation, implementation of knowledge structures, validation and improvement. The idea of an expert system as a \"model of competence\" is mentioned and the transferrance of the system architecture to software diagnosis, using the same model, is described. There are short discussions of system performance and the nature of expert systems.|Roger T. Hartley","13822|IJCAI|1981|Application Design Issues in Expert System Architecture|We describe an expert system that has been applied to the task of application design. Users supply the system with problem specifications, such as the required output data, and the system produces a graphic representation of the completed application in the form of a flow diagram. The application design task has forced us to consider two important issues in expert system architecture constraint processing and the explicit representation of control flow. The resulting knowledge representation and control logic are discussed.|Harry C. Reinstein,Janice S. Aikins","14167|IJCAI|1985|A Case Study in Structured Knowledge Acquisition|Building an expert system usually comprises an entangled mixture of knowledge acquisition and Implementation efforts. An emerging methodology based on cognitive psychology and software development guides and supports knowledge acquisition while Implementation is deferred. This allows for a more deliberate choice of architecture. This paper presents a case studto test the methodology.|Paul de Greef,Joost Breuker","13310|IJCAI|1971|System Support for the Stanford Hand-Eye System|The Stanford hand-eye system is implemented as several separate tasks, each executing under a timesharing executive. Development of a programming language (SAIL) and augmentation of the timesharing system were required to provide the necessary data sharing and control flow among the tasks. The SAIL language provides facilities for \"associative processing,\" and is extended to serve the data sharing and communication needs of the hand-eye system. Several user facilities are designed to aid running and debugging the system.|Jerome A. Feldman,Robert F. Sproull","14042|IJCAI|1983|On the Requirements of Future Expert Systems|Many artificial intelligence applications require the use of expert systems. As expert systems move into new domains, several significant changes can be expected. Among these are an increase in number of rules in the rule base and an increase in the number of data elements contained in working memory. Also, many new applications require that expert systems move into real-time domains. Here, a system must be able to process large quantities of data which are changing rapidly. Many problem-solving situations will be time critical, and the system must take into account the availability and distribution of scarce system resources. For these reasons, the efficiency of a given expert system design and its ability to perform complex memory management tasks will become Increasingly important. This will require modifications in the traditional production system architectures. In this paper, the design requirements of future expert systems are discussed, and HAPS, a recently implemented production system architecture designed to address these issues, is presented.|Ron Sauers,Rick Walsh","13923|IJCAI|1983|A Distributed Control System for the CMU Rover|This paper describes a distributed software control structure developed for the CMU Rover, an advanced mobile robot equipped with a variety of sensors. Expert modules are used to control the operation of the sensors and actuators, interpret sensory and feedback data, build an internal model of the robot's environment, devise strategies to accomplish proposed tasks and execute these strategies. Each expert module is composed of a master process and a slave process, where the master process controls the scheduling and working of the slave process. Communication among expert modules occurs asynchronously over a blackboard structure. Information specific to the execution of a given task is provided through a control plan. The system is distributed over a network of processors. Real-time operating system kernels local to each processor and an interprocess message communication mechanism ensure transparency of the underlying network structure. The various parts of the system are presented in this paper and future work to be performed is mentioned.|Alberto Elfes,Sarosh Talukdar","14284|IJCAI|1985|A Robot Planning Structure Using Production Rules|Robot plan generation is a field which engendered the development of AI languages and rule-based expert systems. Utilization of these latter concepts permits a flexible formalism for robot planning research. We present a robot plan-generation architecture and its application to a real-world mobile robot system. The system undergoes tests through its utilization in the IIILARE robot project (Ciralt, et al., ). Though the article concentrates on planning, execution monitoring and error recovery are discussed. The system includes models of its synergistic environment as well SR of its sensors and effectors (i.e. operators). Its rules embody both planning specific and domair specific knowledge. The system gains generality and adaptiveness through the use of planning variables which provide constraints to the plan generation system. It is implemented in an efficient compiled Production System language (PSL).|Ralph P. Sobek","14063|IJCAI|1983|ACE An Expert System for Telephone Cable Maintenance|ACE, a system for Automated Cable Expertise, is a Knowledge-Based Expert System designed to provide troubleshooting reports and management analyses for telephone cable maintenance. Design decisions faced during the construction of ACE were guided by recent successes in expert systems technology, most notably RXCN, the Digital Equipment Corporation VAX configuration program. ACE departs from \"standard\" expert system architectures in its use of a conventional data base management system as its primary source of information. Its primary sources of knowledge are the users of the database system and primers on maintenance analysis strategies.|Gregg T. Vesonder,Salvatore J. Stolfo,John E. Zielinski,Frederick D. Miller,David H. Copp"],["65447|AAAI|2005|On Compiling System Models for Faster and More Scalable Diagnosis|Knowledge compilation is one of the more traditional approaches to model-based diagnosis, where a compiled system model is obtained in an off-line phase, and then used to efficiently answer diagnostic queries on-line. The choice of a suitable representation for the compiled model is critical to the success of this approach, and two of the main proposals have been Decomposable Negation Normal Form (DNNF) and Ordered Binary Decision Diagram (OBDD). The contribution of this paper is twofold. First, we show that in the current state of the art, DNNF dominates OBDD in efficiency and scalability for some typical diagnostic tasks. This result is based on a step-by-step comparison of the complexities of diagnostic algorithms for DNNF and OBDD, together with a known succinctness relation between the two representations. Second, we present a tool for model-based diagnosis, which is based on a state-of-the-art DNNF compiler and our implementations of DNNF diagnostic algorithms. We demonstrate the efficiency of this tool against recent results reported on diagnosis using OBDD.|Jinbo Huang,Adnan Darwiche","16245|IJCAI|2005|Sophia A novel approach for Textual Case-based Reasoning|In this paper we present a novel methodology for textual case-based reasoning. This technique is unique in that it automatically discovers case and similarity knowledge, is language independent, is scaleable and facilitates semantic similarity between cases to be carried out inherently without the need for domain knowledge. In addition it provides an insight into the thematical content of the case-base as a whole, which enables users to better structure queries. We present an analysis of the competency of the system by assessing the quality of the similarity knowledge discovered and show how it is ideally suited to case-based retrieval (querying by example).|David W. Patterson,Niall Rooney,Vladimir Dobrynin,Mykola Galushka","17067|IJCAI|2009|Plausible Repairs for Inconsistent Requirements|Knowledge-based recommenders support users in the identification of interesting items from large and potentially complex assortments. In cases where no recommendation could be found for a given set of requirements, such systems propose explanations that indicate minimal sets of faulty requirements. Unfortunately, such explanations are not personalized and do not include repair proposals which triggers a low degree of satisfaction and frequent cancellations of recommendation sessions. In this paper we present a personalized repair approach that integrates the calculation of explanations with collaborative problem solving techniques. In order to demonstrate the applicability of our approach, we present the results of an empirical study that show significant improvements in the accuracy of predictions for interesting repairs.|Alexander Felfernig,Gerhard Friedrich,Monika Schubert,Monika Mandl,Markus Mairitsch,Erich Teppan","16166|IJCAI|2005|Reflection Patterns for Interactive Knowledge Capture|Current knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. In this paper, we present a novel extension to existing knowledge acquisition tools where the system ) captures the episodes of knowledge acquisition and knowledge use through a set of declarative reflection patterns ) performs assessment on how to improve the future knowledge acquisition and knowledge use based on captured episodes, and ) provides assistance to the users by combining the assessment results.|Jihie Kim","16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann","66429|AAAI|2008|Transferring Localization Models across Space|Machine learning approaches to indoor WiFi localization involve an offline phase and an online phase. In the offline phase, data are collected from an environment to build a localization model, which will be applied to new data collected in the online phase for location estimation. However, collecting the labeled data across an entire building would be too time consuming. In this paper, we present a novel approach to transferring the learning model trained on data from one area of a building to another. We learn a mapping function between the signal space and the location space by solving an optimization problem based on manifold learning techniques. A low-dimensional manifold is shared between data collected in different areas in an environment as a bridge to propagate the knowledge across the whole environment. With the help of the transferred knowledge, we can significantly reduce the amount of labeled data which are required for building the localization model. We test the effectiveness of our proposed solution in a real indoor WiFi environment.|Sinno Jialin Pan,Dou Shen,Qiang Yang,James T. Kwok","65932|AAAI|2006|A Unified Knowledge Based Approach for Sense Disambiguation and Semantic Role Labeling|In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our approach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.|Peter Z. Yeh,Bruce W. Porter,Ken Barker","66197|AAAI|2007|Refining Rules Incorporated into Knowledge-Based Support Vector Learners Via Successive Linear Programming|Knowledge-based classification and regression methods are especially powerful forms of learning. They allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm, combining that knowledge with data to produce accurate models. A limitation of the use of prior knowledge occurs when the provided knowledge is incorrect. Such knowledge likely still contains useful information, but knowledge-based learners might not be able to fully exploit such information. In fact, incorrect knowledge can lead to poorer models than result from knowledge-free learners. We present a support-vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge, but also suggests changes to the provided knowledge. Our approach is built on the knowledge-based classification and regression methods presented by Fung, Mangasarian, & Shavlik ( ) and by Mangasarian, Shavlik, & Wild (). Experiments on artificial data sets with known properties, as well as on a real-world data set, demonstrate that our method learns more accurate models while also adjusting the provided rules in intuitive ways. Our new algorithm provides an appealing extension to knowledge-based, support-vector learning that is not only able to combine knowledge from rules with data, but is also able to use the data to modify and change those rules to better fit the data.|Richard Maclin,Edward W. Wild,Jude W. Shavlik,Lisa Torrey,Trevor Walker","65952|AAAI|2007|Enabling Domain-Awareness for a Generic Natural Language Interface|In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.|Yunyao Li,Ishan Chaudhuri,Huahai Yang,Satinder Singh,H. V. Jagadish","14590|IJCAI|1989|The Implementation of Expert Knowledge-Based Systems|We discuss the problem of implementing an expert, knowledge-based system. In particular, we consider which predicates in an expert, knowledge-based system should be actually stored and which should be derived on demand. We present two solutions for unconstrained applications. When realistic constraints are present it is shown that the problem is NP-complete. A sub-optimal algorithm is given which operates in polynomial time when the application is not heavily constrained.|John K. Debenham"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","15116|IJCAI|1995|Model Elimination Logic Programming and Computing Answers|We demonstrate that theorem provers using model elimination (ME) can be used as answer complete interpreters for disjunctive logic programming. More specifically, we introduce a mechanism for computing answers into the restart variant of ME. Building on this, we develop a new calculus called ancestry restart ME. This variant admits a more restrictive regularity restriction than restart ME, and, as a side effect, it is in particular attractive for computing definite answers. The presented calculi can also be used successfully in the context of automated theorem proving. We demonstrate experimentally that it is more difficult to compute (non-trivial)answers to goals, instead of only proving the existence of answers.|Peter Baumgartner,Ulrich Furbach,Frieder Stolzenburg","14243|IJCAI|1985|Judgmental Reasoning for Expert Systems|We present a new system for plausible reasoning in expert systems. It is an extension of the Horn clause subset of first order logic, and is distinguished by its use of non-numeric certainties and the assignment of certainties to logical formula rather than events. Model theoretic and fixpoint semantics are sketched for this language. An implementation in the logic programming language Prolog is described and the advantages of the system are discussed.|Tim Niblett","15034|IJCAI|1993|Oz - A Programming Language for Multi-Agent Systems|Oz is an experimental higher-order concurrent constraint programming system under development at DFKI. It combines ideas from logic and concurrent programming in a simple yet expressive language. From logic programming Oz inherits logic variables and logic data structures, which provide for a programming style where partial information about the values of variables is imposed concurrently and incrementally. A novel feature of Oz is that it accommodates higher-order programming without sacrificing that denotation and equality of variables are captured by first-order logic. Another new feature of Oz is constraint communication, a new form of asynchronous communication exploiting logic variables Constraint communication avoids the problems of stream communication, the conventional communication mechanism employed in concurrent logic programming. Constraint, communication can be seen as providing a minimal form of state fully compatible with logic data structures Based on constraint communication and higher-order programming, Oz readily supports a variety of object-oriented programming styles including multiple inheritance.|Martin Henz,Gert Smolka,J√∂rg W√ºrtz","14889|IJCAI|1991|How to Prove Higher Order Theorems in First Order Logic|In this paper we are interested in using a first order theorem prover to prove theorems that are formulated in some higher order logic. To this end we present translations of higher order logics into first order logic with flat sorts and equality and give a sufficient criterion for the soundness of these translations. In addition translations are introduced that are sound and complete with respect to L. Henkin's general model semantics. Our higher order logics are based on a restricted type structure in the sense of A. Church, they have typed function symbols and predicate symbols, but no sorts.|Manfred Kerber","15037|IJCAI|1993|Nonmonotonic Model Inference-A Formalization of Student Modeling|A student model description language and its synthesis method are presented. The language called SMDL is based on a logic programming language taking  truth values such as true, false, unknown and fail. A modeling method called HSMIS is a new nonmonotonic model inference system and has the following major characteristics () Model inference of logic program taking  truth values, () Treatment of nonmonotonicity of both student's belief and inference process itself. HSMIS incorporates de Kleer's ATMS as a vehicle for formulating the nonmonotonicity. Both SMDL interpreter and HSMIS have been implemented in Common ESP(Extended Self-contained Prolog) and incorporated into a framework for ITS, called FITS.|Mitsuru Ikeda,Yasuyuki Kono,Riichiro Mizoguchi","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","15064|IJCAI|1993|First-Order Modal Logic Theorem Proving and Functional Simulation|We propose a translation approach from modal logics to first-order predicate logic which combines advantages from both, the (standard) relational translation and the (rather compact) functional translation method and avoids many of their respective disadvantages (exponential growth versus equality handling). In particular in the application to serial modal logics it allows considerable simplifications such that often even a simple unit clause suffices in order to express the accessibility relation properties. Although we restrict the approach here to first-order modal logic theorem proving it has been shown to be of wider interest, as e.g. sorted logic or terminological logic.|Andreas Nonnengart","14578|IJCAI|1989|Logic Programming with General Clauses and Defaults Based on Model Elimination|The foundations of a class of logic programming systems with the expressive power of full first-order logic and a non-monotonic component is addressed. The underlying refutation method is an extended version of weak model elimination. The first question addressed is how to compute answers with weak model elimination when queries and programs are sets of arbitrary clauses, which is completely settled by a soundness and completeness result. The question of computing only definite answers is also settled. Then, the problem of computing answers is rediscussed when the logic programs also include a finite set of defaults.|Marco A. Casanova,Ramiro A. de T. Guerreiro,Andrea Silva","15000|IJCAI|1993|An Abductive Framework for General Logic Programs and other Nonmonotonic Systems|We present an abductive semantics for general propositional logic programs which defines the meaning of a logic program in terms of its extensions. This approach extends the stable model semantics for normal logic programs in a natural way. The new semantics is equivalent to stable semantics for a logic program P whenever P is normal and has a stable model. The abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner. Our approach is based on an idea recently proposed by Konolige for causal reasoning. Instead of maximizing the set of hypotheses alone we maximize the union of the hypotheses, along with possible hypotheses that are excused or refuted by the theory.|Gerhard Brewka,Kurt Konolige"],["16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","16879|IJCAI|2009|Predictive Projections|This paper addresses the problem of learning control policies in very high dimensional state spaces. We propose a linear dimensionality reduction algorithm that discovers predictive projections projections in which accurate predictions of future states can be made using simple nearest neighbor style learning. The goal of this work is to extend the reach of existing reinforcement learning algorithms to domains where they would otherwise be inapplicable without extensive engineering of features. The approach is demonstrated on a synthetic pendulum balancing domain, as well as on a robot domain requiring visually guided control.|Nathan Sprague","14366|IJCAI|1987|The Use of Explanations for Similarity-based Learning|Due to the difficult nature of Machine Learning, it has often been looked at in the context of \"toy\" domains or in more realistic domains with simplifying assumptions. We propose an integrated learning approach that combines Explanation-Based and Similarity-Based Learning methods to make learning in an inherently complex domain feasible. We discuss the use of explanations for Similarity-Based Learning and present an example from a program which applies thee ideas to the domain of terrorist events.|Andrea Pohoreckyj Danyluk","16870|IJCAI|2009|Knowledge Transfer on Hybrid Graph|In machine learning problems, labeled data are often in short supply. One of the feasible solution for this problem is transfer learning. It can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain. In this paper, we propose a transfer learning framework based on similarity matrix approximation to tackle such problems. Two practical algorithms are proposed, which are the label propagation and the similarity propagation. In these methods, we build a hybrid graph based on all available data. Then the information is transferred cross domains through alternatively constructing the similarity matrix for different part of the graph. Among all related methods, similarity propagation approach can make maximum use of all available similarity information across domains. This leads to more efficient transfer and better learning result. The experiment on real world text mining applications demonstrates the promise and effectiveness of our algorithms.|Zheng Wang,Yangqiu Song,Changshui Zhang","66776|AAAI|2010|g-Planner Real-time Motion Planning and Global Navigation using GPUs|This paper proposed a pursuit-evasion algorithm based on the Option method from hierarchical reinforcement learning and applied it into multi-robot pursuit-evasion game in D-Dynamic environment. The algorithm efficiency is studied by comparing it with Q-learning. We decompose the complex task with option method, and divide the learning process into two parts High-level learning and Low-level learning, then design a new mechanism in order to make the learning process perform parallel. The simulation result shows the Option algorithm can efficiently reduce the complexity of pursuit-evasion task, avoid traditional reinforcement learning curse of dimensionality, and improve the learning result.|Jia Pan,Christian Lauterbach,Dinesh Manocha","15217|IJCAI|1995|Determining What to Learn Through Component-Task Modeling|Research in machine learning has typically addressed the problem of how and when to learn, and ignored the problem of formulating learning tasks in the first place. This paper addresses this issue in the context of the CASTLE system, that dynamically formulates learning tasks for a given situation. Our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. CASTLE can then focus the learning process on the issues involved in improving the performance of the particular component.|Bruce Krulwich,Lawrence Birnbaum,Gregg Collins","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","14818|IJCAI|1991|The Problem of Induction and Machine Learning|Are we justified in inferring a general rule from observations that frequently confirm it This is the usual statement of the problem of induction. The present paper argues that this question is relevant for the understanding of Machine Learning, but insufficient. Research in Machine Learning has prompted another, more fundamental question the number of possible rules grows exponentially with the size of the examples, and many of them are somehow confirmed by the data - how are we to choose effectively some rules that have good chances of being predictive We analyze if and how this problem is approached in standard accounts of induction and show the difficulties that are present. Finally, we suggest that the Explanation-based Learning approach and related methods of knowledge intensive induction could be a partial solution to some of these problems, and help understanding the question of valid induction from a new perspective.|Francesco Bergadano","66231|AAAI|2007|Learning by Combining Observations and User Edits|We introduce a new collaborative machine learning paradigm in which the user directs a learning algorithm by manually editing the automatically induced model. We identify a generic architecture that supports seamless interweaving of automated learning from training samples and manual edits of the model, and we discuss the main difficulties that the framework addresses. We describe Augmentation-Based Learning (ABL), the first learning algorithm that supports interweaving of edits and learning from training samples. We use examples based on ABL to outline selected advantages of the approach--dealing with bad data by manually removing their effects from the model, and learning a model with fewer training samples.|Vittorio Castelli,Lawrence D. Bergman,Daniel Oblinger","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["65218|AAAI|2004|Complete Local Search for Propositional Satisfiability|Algorithms based on following local gradient information are surprisingly effective for certain classes of constraint satisfaction problems. Unfortunately, previous local search algorithms are notoriously incomplete They are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability. We present an algorithmic framework for complete local search and discuss in detail an instantiation for the propositional satisfiability problem (SAT). The fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minima. Although the algorithm has worst-case exponential space complexity, we present empirical resulls on challenging SAT competition benchmarks that suggest that our implementation can perform as well as state-of-the-art solvers based on more mature techniques. Our framework suggests a range of possible algorithms lying between tree-based search and local search.|Hai Fang,Wheeler Ruml","15351|IJCAI|1997|An Approximate - Edge-Labeling Algorithm for Constrained Bin-Packing Problem|This paper describes a constrained bin-packing problem (CBPP) and an approximate, anytime algorithm for solutions. A CBPP is a constrained version of the bin-packing problem, in which a set of items allocated to a bin are ordered in a way to satisfy constraints defined on them and achieve near-optimality. The algorithm for CBPP uses a heuristic search for labeling edges with a binary value, together with a beam search and constraint propagation. Some experimental results are provided. This algorithm has been successfully applied to industrial-scale scheduling problems.|Ho Soo Lee,Mark Trumbo","15045|IJCAI|1993|Understanding the Role of Negotiation in Distributed Search Among Heterogereous Agents|In our research, we explore the role of negotiation for conflict resolution in distributed search among heterogeneous and reusable agents. We present negotiated search, an algorithm that explicitly recognizes and exploits conflict to direct search activity across a set of agents. In negotiated search, loosely coupled agents interleave the tasks of ) local search for a solution to some subproblem ) integration of local subproblem solutions into a shared solution ) information exchange to define and refine the shared search space of the agents and ) assessment and reassessment of emerging solutions. Negotiated search is applicable to diverse application areas and problem-solving environments. It requires only basic search operators and allows maximum flexibility in the distribution of those operators. These qualities make the algorithm particularly appropriate for the integration of heterogeneous agents into application systems. The algorithm is implemented in a multi-agent framework, TEAM, that provides the infrastructure required for communication and cooperation.|Susan E. Lander,Victor R. Lesser","65475|AAAI|2005|Neighborhood Interchangeability and Dynamic Bundling for Non-Binary Finite CSPs|Neighborhood Interchangeability (NI) identifies the equivalent values in the domain of a variable of a Constraint Satisfaction Problem (CSP) by considering only the constraints that directly apply to the variable. Freuder described an algorithm for efficiently computing NI values in binary CSPs. In this paper, we show that the generalization of this algorithm to non-binary CSPs is not straightforward, and introduce an efficient algorithm for computing. NI values in the presence of non-binary constraints. Further, we show how to interleave this mechanism with search for solving CSPs, thus yielding a dynamic bundling strategy. While the goal of dynamic bundling is to produce multiple robust solutions, we empirically show that it does not increase (but significantly decreases) the cost of search.|Anagh Lal,Berthe Y. Choueiry,Eugene C. Freuder","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","14879|IJCAI|1991|Moving Target Search|We consider the case of heuristic search where the location of the goal may change during the course of the search. For example, the goal may be a target that is actively avoiding the problem solver. We present a moving target search algorithm (MTS) to solve this problem. We prove that if the average speed of the target is slower than that of the problem solver, then the problem solver is guaranteed to eventually reach the target. An implementation with randomly positioned obstacles confirms that the MTS algorithm is highly effective in various situations.|Toru Ishida,Richard E. Korf","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan","15308|IJCAI|1995|Theoretical Analysis of Davis-Putnam Procedure and Propositional Satisfiability|This paper presents a statistical analysis of the Davis-Putnam procedure and propositional satisfiability problems (SAT). SAT has been researched in AI because of its strong relationship to automated reasoning and recently it is used as a benchmark problem of constraint satisfaction algorithms. The Davis-Putnam procedure is a well-known satisfiability checking algorithm based on tree search technique. In this paper, I analyze two average case complexities for the Davis-Putnam procedure, the complexity for satisfiability checking and the complexity for finding all solutions. I also discuss the probability of satisfiability. The complexities and the probability strongly depend on the distribution of formulas to be tested and I use the fixed clause length model as the distribution model. The result of the analysis coincides with the experimental result well.|Nobuhiro Yugami","16299|IJCAI|2005|Value Ordering for Finding All Solutions|In finding all solutions to a constraint satisfaction problem, or proving that there are none, with a search algorithm that backtracks chronologically and forms k-way branches, the order in which the values are assigned is immaterial. However, we show that if the values of a variable are assigned instead via a sequence of binary choice points, and the removal of the value just tried from the domain of the variable is propagated before another value is selected, the value ordering can affect the search effort. We show that this depends on the problem constraints for some types of constraints, we show that the savings in search effort can be significant, given a good value ordering.|Barbara M. Smith,Paula Sturdy"]]},"title":{"entropy":5.868926854473514,"topics":["algorithm for, problem solving, search for, constraint satisfaction, for planning, heuristic search, planning with, search, and search, planning, case study, for and, local search, planning and, constraint, combinatorial auctions, the problem, algorithm and, for problem, for constraint","for system, data base, for data, data, and data, system, artificial intelligence, the system, expert system, database, the data, database system, data management, for database, and system, data system, data streams, management system, relational database, data model","the and, natural language, reasoning about, for logic, and, the, logic, and logic, for and, and reasoning, reasoning, description logic, logic programs, for reasoning, logic programming, knowledge representation, language and, for the, theorem proving, answer set","learning, learning for, model for, reinforcement learning, neural networks, learning and, mobile robot, model, using, for networks, bayesian networks, sense disambiguation, networks, and image, learning with, from, decision processes, for recognition, word sense, markov processes","problem solving, constraint satisfaction, for problem, the problem, for constraint, and constraint, and problem, problem, constraint, with constraint, search problem, constraint problem, the constraint, constraint optimization, algorithm problem, for solving, for temporal, satisfaction problem, temporal and, distributed constraint","case study, for information, combinatorial auctions, for games, and information, information extraction, strategies for, with information, information, the games, online for, for auctions, information the, games, for negotiation, incomplete information, for and, coalitional games, for space, games playing","data base, for data, and data, for base, data, data system, the data, data model, base system, for database, tool for, relational database, database system, and database, for large, knowledge base, from data, large base, relational data, the database","data streams, management system, architecture for, for management, data management, and application, for application, for streams, architecture system, for data, adaptive for, the management, and management, access data, the architecture, data application, framework for, for and, view data, data integration","natural language, language and, language for, representation and, the language, framework for, and its, the use, representation for, the representation, and natural, knowledge representation, the understanding, speech understanding, understanding and, the speech, and system, framework and, for natural, semantic and","and logic, for logic, for and, description logic, the logic, and, theorem proving, the and, with and, logic, description and, for description, logic with, default logic, method and, and application, modal logic, and inference, method for, the description","learning for, learning and, learning, reinforcement learning, learning with, the learning, learning from, machine learning, learning using, learning concept, learning model, from, transfer learning, extracting from, learning examples, learning data, from examples, learning networks, and from, dimensionality reduction","mobile robot, for robot, for agents, using for, matrix factorization, using and, with robot, using, robot, combining and, and robot, for mobile, learning robot, robot control, and agents, gaussian process, the robot, modeling for, agents, human and"],"ranking":[["14902|IJCAI|1991|Localized Search for Multiagent Planning|This paper describes the localized search mechanism of the GEMPLAN multiagent planner. Both formal complexity results and empirical results are provided, demonstrating the benefits of localized search, A localized domain description is one that decomposes domain activities and requirements into a set of regions. This description is used to infer how domain requirements are semantically localized and, as a result, to enable the decomposition of the planning search space into a set of spaces, one for each domain region. Benefits of localization include a smaller and cheaper overall search space as well as heuristic guidance in controlling search. Such benefits are critical if current planning technologies and other types of reasoning are to be scaled up to large, complex domains.|Amy L. Lansky","16753|IJCAI|2007|A Heuristic Search Approach to Planning with Temporally Extended Preferences|Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.|Jorge A. Baier,Fahiem Bacchus,Sheila A. McIlraith","16803|IJCAI|2007|Using Learned Policies in Heuristic-Search Planning|Many current state-of-the-art planners rely on forward heuristic search. The success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. Such estimates are effective in guiding search for many domains, but there remain many other domains where current heuristics are inadequate to guide forward search effectively. In some of these domains, it is possible to learn reactive policies from example plans that solve many problems. However, due to the inductive nature of these learning techniques, the policies are often faulty, and fail to achieve high success rates. In this work, we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. We propose a simple approach that uses the policy to augment the states expanded during each search step. In particular, during each search node expansion, we add not only its neighbors, but all the nodes along the trajectory followed by the policy from the node until some horizon. Empirical results show that our proposed approach benefits both of the leveraged automated techniques, learning and heuristic search, outperforming the state-of-the-art in most benchmark planning domains.|Sung Wook Yoon,Alan Fern,Robert Givan","14099|IJCAI|1985|A Study of Search Methods The Effect of Constraint Satisfaction and Adventurousness|This research addresses how constraint satisfaction interacts with the search mode, and how the ratio of breadth of effort to depth of effort can be controlled. Four search paradigms, each the best of its kind for non adversary problems, are investigated. One is depth first, and the others best first. All methods except one highly informed best first search use the same knowledge, and each of these methods is tested with and without the use of a constraint satisfaction procedure on sets of progressively more difficult problems. As expected, the most informed search does better than the less informed as the problems get more difficult. Constraint satisfaction is found to have a pronouncedly greater effect when coupled with the most informed algorithm. Large performance increments over A* can be produced by the use of a coefficient associated with the h term, and this algorithm produces solutions that are only % worse than optimal. This is a known phenomenon however, the range of this coefficient is very narrow. We term this coefficient, which controls the ratio of depth of effort to breadth of effort, the adventurousness coefficient. The less tractable a problem the greater the adventurousness should be. We present evidence to support this.|Hans J. Berliner,Gordon Goetsch","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","15377|IJCAI|1997|Combining Local Search and Look-Ahead for Scheduling and Constraint Satisfaction Problems|We propose a solution technique for scheduling and constraint satisfaction problems that combines backtracking-free constructive methods and local search techniques. Our technique incrementally constructs the solution, performing a local search on partial solutions each time the construction reaches a dead-end. Local search on the space of partial solutions is guided by a cost function based on three components the distance to feasibility of the partial solution, a look-ahead factor, and (for optimization problems) a lower bound of the objective function. In order to improve search effectiveness, we make use of an adaptive relaxation of constraints and an interleaving of different lookahead factors. The new technique has been successfully experimented on two real-life problems university course scheduling and sport tournament scheduling.|Andrea Schaerf","15652|IJCAI|2001|Local Search Topology in Planning Benchmarks An Empirical Analysis|Many state-of-the-art heuristic planners derive their heuristic function by relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. Looking at a collection of planning benchmarks, we measure topological properties of state spaces with respect to that relaxation. The results suggest that, given the heuristic based on the relaxation, many planning benchmarks are simple in structure. This sheds light on the recent success of heuristic planners employing local search.|J√∂rg Hoffmann","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","16777|IJCAI|2007|Recent Progress in Heuristic Search A Case Study of the Four-Peg Towers of Hanoi Problem|We integrate a number of new and recent advances in heuristic search, and apply them to the fourpeg Towers of Hanoi problem. These include frontier search, disk-based search, parallel processing, multiple, compressed, disjoint, and additive pattern database heuristics, and breadth-first heuristic search. New ideas include pattern database heuristics based on multiple goal states, a method to reduce coordination among multiple parallel threads, and a method for reducing the number of heuristic calculations. We perform the first complete breadth-first searches of the  and -disc fourpeg Towers of Hanoi problems, and extend the verification of \"presumed optimal solutions\" to this problem from  to  discs. Verification of the -disc problem is in progress.|Richard E. Korf,Ariel Felner","66469|AAAI|2008|Anytime Local Search for Distributed Constraint Optimization|Most former studies of Distributed Constraint Optimization Problems (DisCOPs) search considered only complete search algorithms, which are practical only for relatively small problems. Distributed local search algorithms can be used for solving DisCOPs. However, because of the differences between the global evaluation of a system's state and the private evaluation of states by agents, agents are unaware of the global best state which is explored by the algorithm. Previous attempts to use local search algorithms for solving DisCOPs reported the state held by the system at the termination of the algorithm, which was not necessarily the best state explored. A general framework for implementing distributed local search algorithms for DisCOPs is proposed. The proposed framework makes use of a BFS-tree in order to accumulate the costs of the system's state in its different steps and to propagate the detection of a new best step when it is found. The resulting framework enhances local search algorithms for DisCOPs with the anytime property. The proposed framework does not require additional network load. Agents are required to hold a small (linear) additional space (beside the requirements of the algorithm in use). The proposed framework preserves privacy at a higher level than complete Dis-COP algorithms which make use ofa pseudo-tree (ADOPT, DPOP).|Roie Zivan"],["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","80067|VLDB|1981|The Active Information System A Data-Driven System for the Analysis of Imprecise Data|The concept of Active information System is introduced. Active Information Systems are collections of autonomous, data-driven tools operating in a data base environment and seeking to produce results that enhance the understanda- bility of the contents of the data base. The activities of the corresponding multiple concurrent processes are coordi- nated by control mechanisms that perform in a goal oriented manner. The data models supported by Active Systems are characterized by the existence of numerous data elements that are known only in imprecise or uncertain terms. The different types of components of Active Systems are identified and their organization into a prototypical architec- ture is described. Emphasis is placed on algorithms that assist users in hypothesis generation and verification tasks. These algorithms are based on concepts from data base technology, knowledge based systems, system science and multivalued logic. The technological approaches utilized in the development of an Active System testbed to treat problems of natural language understanding and uncertainty representation are discussed in detail.|Christine A. Montgomery,Enrique H. Ruspini","13593|IJCAI|1977|A Deductive Question Answering System on Relational Data Bases|This paper describes a new formalization of a deductive question answering system on a relational data base using a theorem proving technique. A theorem proving procedure for a finite domain is investigated and a direct proof procedure based on substitutions of equivalent formulas which employs the breadth first search is introduced. The search strategy is then expanded to set operations of the relational algebra which are in corporated into the proof procedure in order to increase the data base search efficiency. Virtual relations are realized by means of introducing several axioms and utilizing the deductive capability of the logical system. Furthermore, a conditional domain is, introduced as one of the virtual domains and is used to give a relational view to a pseudo relational data base which can represent exceptional cases using some link information. A query transformation system called DBAP (Data Base Access Planner) which embodies those features is implemented in QJJSP.|Koichi Furukawa"],["13947|IJCAI|1983|Logic Modelling of Cognitive Reasoning|Logic modelling is presented as an approach for exploring cognitive reasoning. The notion of mental construction and execution of propositional models is introduced. A model is constructed through inclusions and exclusions of assertions and assumptions about the task. A constructed model is executed in a logical control structure. Formal rules of inference are argued to be an essential feature of this architecture. A few examples are given for purpose of illustration.|G√∂ran Hagert,√\u2026ke Hansson","15791|IJCAI|2003|A Logic For Causal Reasoning|We introduce a logical formalism of irreflexive casual production relations that possesses both a standard monotonic semantics, and a natural nonmonotonic semantics. The formalism is shown to provide a complete characterization for the casual reasoning behind casual theories from McCain and Turner, . It is shown also that any causal relation is reducible to its Horn sub-relation with respect to the nonmonotonic semantics. We describe also a general correspondence between casual relations and abductive systems, which shows, in effect, that casual relations allow to express abductive reasoning. The results of the study seem to suggest causal production relations as a viable general framework for nonmonotonic reasoning.|Alexander Bochman","16542|IJCAI|2007|Using the Probabilistic Logic Programming Language P-log for Causal and Counterfactual Reasoning and Non-Naive Conditioning|P-log is a probabilistic logic programming language, which combines both logic programming style knowledge representation and probabilistic reasoning. In earlier papers various advantages of P-log have been discussed. In this paper we further elaborate on the KR prowess of P-log by showing that (i) it can be used for causal and counterfactual reasoning and (ii) it provides an elaboration tolerant way for non-naive conditioning.|Chitta Baral,Matt Hunsaker","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16866|IJCAI|2009|A Logic for Reasoning about Counterfactual Emotions|The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. An example of this kind of emotions is regret. In order to meet this objective, we exploit the well-known STIT logic Belnap et al.,  Horty, . STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable. In this work we study a decidable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions.|Emiliano Lorini,Fran√ßois Schwarzentruber","15759|IJCAI|2001|EPDL A Logic for Causal Reasoning|This paper presents an extended system EPDL of propositional dynamic logic by allowing a proposition as a modality for representing and specifying direct and indirect effects of actions in a unified logical structure. A set of causal logics based on the framework are proposed to model causal propagations through logical relevancy and iterated effects of causation. It is shown that these logics capture the basic properties of causal reasoning.|Dongmo Zhang,Norman Y. Foo","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang"],["16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","15381|IJCAI|1997|An Effective Learning Method for Max-Min Neural Networks|Max and min operations have interesting properties that facilitate the exchange of information between the symbolic and real-valued domains. As such, neural networks that employ max-min activation functions have been a subject of interest in recent years. Since max-min functions are not strictly differentiate, we propose a mathematically sound learning method based on using Fourier convergence analysis of side-derivatives to derive a gradient descent technique for max-min error functions. This method is applied to a \"typical\" fuzzy-neural network model employing max-rnin activation functions. We show how this network can be trained to perform function approximation its performance was found to be better than that of a conventional feedforward neural network.|Loo-Nin Teow,Kia-Fock Loe","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","66607|AAAI|2010|Latent Variable Model for Learning in Pairwise Markov Networks|In this paper, an efficient rate-control scheme for H.AVC video encoding is proposed. The redesign of the quantization scheme in H.AVC results in that the relationship between the quantization parameter and the true quantization stepsize is no longer linear. Based on this observation, we propose a new rate-distortion (R-D) model by utilizing the true quantization stepsize and then develop an improved rate-control scheme for the H.AVC encoder based on this new R-D model. In general, the current R-D optimization (RDO) mode-selection scheme in H.AVC test model is difficult for rate control, because rate control usually requires a predetermined set of motion vectors and coding modes to select the quantization parameter, whereas the RDO does in the different order and requires a predetermined quantization parameter to select motion vectors and coding modes. To tackle this problem, we develop a complexity-adjustable rate-control scheme based on the proposed R-D model. Briefly, the proposed scheme is a one-pass process at frame level and a partial two-pass process at macroblock level. Since the number of macroblocks with the two-pass processing can be controlled by an encoder parameter, the fully one-pass implementation is a subset of the proposed algorithm. An additional topic discussed in this paper is about video buffering. Since a hypothetical reference decoder (HRD) has been defined in H.AVC to guarantee that the buffers never overflow or underflow, the more accurate rate-allocation schemes are proposed to satisfy these requirements of HRD.|Saeed Amizadeh,Milos Hauskrecht","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","66549|AAAI|2008|Reinforcement Learning for Vulnerability Assessment in Peer-to-Peer Networks|Proactive assessment of computer-network vulnerability to unknown future attacks is an important but unsolved computer security problem where AI techniques have significant impact potential. In this paper, we investigate the use of reinforcement learning (RL) for proactive security in the context of denial-of-service (DoS) attacks in peer-to-peer (PP) networks. Such a tool would be useful for network administrators and designers to assess and compare the vulnerability of various network configurations and security measures in order to optimize those choices for maximum security. We first discuss the various dimensions of the problem and how to formulate it as RL. Next we introduce compact parametric policy representations for both single attacker and botnets and derive a policy-gradient RL algorithm. We evaluate these algorithms under a variety of network configurations that employ recent fair-use DoS security mechanisms. The results show that nur RL-based approach is able to significantly outperform a number of heuristic strategies in terms of the severity of the attacks discovered. The results also suggest some possible network design lessons for reducing the attack potential of an intelligent attacker.|Scott Dejmal,Alan Fern,Thinh Nguyen","65014|AAAI|1987|Modular Learning in Neural Networks|In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.|Dana H. Ballard"],["66116|AAAI|2007|Solving a Stochastic Queueing Design and Control Problem with Constraint Programming|A facility with front room and back room operations has the option of hiring specialized or, more expensive, cross-trained workers. Assuming stochastic customer arrival and service times, we seek a smallest-cost combination of cross-trained and specialized workers satisfying constraints on the expected customer waiting time and expected number of workers in the back room. A constraint programming approach using logic-based Benders' decomposition is presented. Experimental results demonstrate the strong performance of this approach across a wide variety of problem parameters. This paper provides one of the first links between queueing optimization problems and constraint programming.|Daria Terekhov,J. Christopher Beck,Kenneth N. Brown","16689|IJCAI|2007|On Modeling Multiagent Task Scheduling as a Distributed Constraint Optimization Problem|This paper investigates how to represent and solve multiagent task scheduling as a Distributed Constraint Optimization Problem (DCOP). Recently multiagent researchers have adopted the CTMS language as a standard for multiagent task scheduling. We contribute an automated mapping that transforms CTMS into a DCOP. Further, we propose a set of representational compromises for CTMS that allow existing distributed algorithms for DCOP to be immediately brought to bear on CTMS problems. Next, we demonstrate a key advantage of a constraint based representation is the ability to leverage the representation to do efficient solving. We contribute a set of pre-processing algorithms that leverage existing constraint propagation techniques to do variable domain pruning on the DCOP. We show that these algorithms can result in % reduction in state space size for a given set of CTMS problems. Finally, we demonstrate up to a % increase in the ability to optimally solve CTMS problems in a reasonable amount of time and in a distributed manner as a result of applying our mapping and domain pruning algorithms.|Evan Sultanik,Pragnesh Jay Modi,William C. Regli","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","14989|IJCAI|1993|Diagnosing and Solving Over-Determined Constraint Satisfaction Problems|Constraint relaxation is a frequently used technique for managing over-determined constraint satisfaction problems. A problem in constraint relaxation is the selection of the appropriate constraints. We show that methods developed in model-based diagnosis solve this problem. The resulting method, DOC, an abbreviation for Diagnosis of Over-determined Constraint Satisfaction Problems, identifies the set of least important constraints that should be relaxed to solve the remaining constraint satisfaction problem. If the solution is not acceptable for a user, DOC selects next-best sets of least-important constraints until an acceptable solution has been generated. The power of DOC is illustrated by a case study of scheduling the Dutch major league soccer competition. The current schedule is made using human insight and Operations Research methods. Using DOC, the - schedule has been improved by reducing the number and importance of the violated constraints by %. The case study revealed that efficiency improvement is a major issue in order to apply this method to large-scale over-determined scheduling and constraint satisfaction problems.|R. R. Bakker,F. Dikker,F. Tempelman,P. M. Wognum","14841|IJCAI|1991|On the Feasibility of Distributed Constraint Satisfaction|This paper characterizes connectionist-type architectures that allow a distributed solution for classes of constraint-satisfaction problems. The main issue addressed is whether there exists a uniform model of computation (where all nodes are indistinguishable) that guarantees convergence to a solution from every initial state of the system, whenever such a solution exists. We show that even for relatively simple constraint networks, such as rings, there is no general solution using a completely uniform, asynchronous, model. However, some restricted topologies like trees can accommodate the uniform, asynchronous, model and a protocol demonstrating this fact is presented. An almost* uniform, asynchronous, network-consistency protocol is also presented. We show that the algorithms are guaranteed to be self-stabilizing, which makes them suitable for dynamic or error-prone environments.|Zeev Collin,Rina Dechter,Shmuel Katz","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","66290|AAAI|2008|A Global Constraint for Bin-Packing with Precedences Application to the Assembly Line Balancing Problem|Assembly line balancing problems (ALBP) are of capital importance for the industry since the first assembly line for the Ford T by Henry Ford. Their objective is to optimize the design of production lines while satisfying the various constraints. Precedence constraints among the tasks are always present in ALBP. The objective is then to place the tasks among various workstations such that the production rate is maximized. This problem can be modeled as a bin packing problem with precedence constraints (BPPC) where the bins are the workstations and the items are the tasks. Paul Shaw introduced a global constraint for bin-packing (without precedence). Unfortunately this constraint does not capture the precedence constraints of BPPC. In this paper, we first introduce redundant constraints for BPPC combining the precedences and the bin-packing, allowing to solve instances which are otherwise intractable in constraint programming. We also design a global constraint for BPPC, introducing even more pruning in the search tree. We finally used our CP model for BPPC to solve ALBP. We propose two search heuristics, and show the efficiency of our approach on standard ALBP benchmarks. Compared to standard non CP approaches, our method is more flexible as it can handle new constraints that might appear in real applications.|Pierre Schaus,Yves Deville","65241|AAAI|2004|Robust Solutions for Constraint Satisfaction and Optimization|Super solutions are solutions in which, if a small number of variables lose their values, we are guaranteed to be able to repair the solution with only a few changes. In this paper, we stress the need to extend the super solution framework along several dimensions to make it more useful practically. We demonstrate the usefulness of those extensions on an example from jobshop scheduling, an optimization problem solved through constraint satisfaction. In such a case there is indeed a trade-off between optimality and robustness, however robustness may be increased without sacrificing optimality.|Emmanuel Hebrard","66003|AAAI|2007|A Distributed Constraint Optimization Solution to the PP Video Streaming Problem|The future success of application layer video multicast depends on the availability of video stream distribution methods that can scale in the number of stream senders and receivers. Previous work on the problem of application layer video streaming has not effectively addressed scalability in the number of receivers and senders. Therefore, new solutions that are amenable to analysis and can achieve scalable PP video streaming are needed. In this work we propose the use of automated negotiation algorithms to construct video streaming trees at the application layer. We show that automated negotiation can effectively solve the problem of distributing a video stream to a large number of receivers.|Theodore Elhourani,Nathan Denny,Michael M. Marefat"],["17037|IJCAI|2009|Computing Equilibria in Multiplayer Stochastic Games of Imperfect Information|Computing a Nash equilibrium in multiplayer stochastic games is a notoriously difficult problem. Prior algorithms have been proven to converge in extremely limited settings and have only been tested on small problems. In contrast, we recently presented an algorithm for computing approximate jamfold equilibrium strategies in a three-player nolimit Texas hold'em tournament--a very large real-world stochastic game of imperfect information . In this paper we show that it is possible for that algorithm to converge to a non-equilibrium strategy profile. However, we develop an ex post procedure that determines exactly how much each player can gain by deviating from his strategy and confirm that the strategies computed in that paper actually do constitute an -equilibrium for a very small  (.% of the tournament entry fee). Next, we develop several new algorithms for computing a Nash equilibrium in multiplayer stochastic games (with perfect or imperfect information) which can provably never converge to a non-equilibrium. Experiments show that one of these algorithms outperforms the original algorithm on the same poker tournament. In short, we present the first algorithms for provably computing an -equilibrium of a large stochastic game for small . Finally, we present an efficient algorithm that minimizes external regret in both the perfect and imperfect information cases.|Sam Ganzfried,Tuomas Sandholm","66824|AAAI|2010|A General Game Description Language for Incomplete Information Games|A novel improved linear discriminant analysis (ILDA) method is presented. Comparing with LDA, under the condition of d  c -, d and c are the dimensionality of feature subspace and the number of classes respectively, ILDA uniformly preserves the class distances of classpairs by rearranging the contribution of each class-pair to the generalized between-class scatter matrix after whitening within-class scatter matrix. Experiment results based on simulating data and measured radar data both show that, under the condition of d  c -, the features extracted by ILDA are more efficient for multi-class classification than those extracted by LDA.|Michael Thielscher","66764|AAAI|2010|Reasoning about Imperfect Information Games in the Epistemic Situation Calculus|In the past years dynamic voltage and frequency scaling (DVFS) has been an effective technique that allowed microprocessors to match a predefined power budget. However, as process technology shrinks, DVFS becomes less effective (because of the increasing leakage power) and it is getting closer to a point where DVFS won't be useful at all (when static power exceeds dynamic power). In this paper we propose the use of microarchitectural techniques to accurately match a power constraint while maximizing the energy efficiency of the processor. We predict the processor power consumption at a basic block level, using the consumed power translated into tokens to select between different power-saving micro-architectural techniques. These techniques are orthogonal to DVFS so they can be simultaneously applied. We propose a two-level approach where DVFS acts as a coarse-grained technique to lower the average power while microarchitectural techniques remove all the power spikes efficiently. Experimental results show that the use of power-saving microarchitectural techniques in conjunction with DVFS is up to six times more precise, in terms of total energy consumed (area) over the power budget, than using DVFS alone for matching a predefined power budget. Furthermore, in a near future DVFS will become DFS because lowering the supply voltage will be too expensive in terms of leakage power. At that point, the use of power-saving microarchitectural techniques will become even more energy efficient.|Vaishak Belle,Gerhard Lakemeyer","14978|IJCAI|1991|Incomplete Information and Deception in Multi-Agent Negotiation|Much distributed artificial intelligence research on negotiation assumes complete knowledge among the interacting agents andor truthful agents. These assumptions in many domains will not be realistic, and this paper extends previous work to begin dealing with the case of inter-agent negotiation with incomplete information. A discussion of our existing negotiation framework sets out the rules by which agents operate during this phase of their interaction. The concept of a \"solution\" within this framework is presented the same solution concept serves for interactions between agents with incomplete information as it did for complete information interactions. The possibility of incomplete information among agents opens up the possibility of deception as part of the negotiation strategy of an agent. Deception during negotiation among autonomous agents is thus analyzed in the constrained Blocks Domain, and it is shown that beneficial lies do exist in some scenarios. The three types of interactions, cooperative, compromise, and conflict, are examined. An analysis is made of how each affects the possibility of beneficial deception by a negotiating agent.|Gilad Zlotkin,Jeffrey S. Rosenschein","65849|AAAI|2006|Overconfidence or Paranoia Search in Imperfect-Information Games|We derive a recursive formula for expected utility values in imperfect- information game trees, and an imperfect-information game tree search algorithm based on it. The formula and algorithm are general enough to incorporate a wide variety of opponent models. We analyze two opponent models. The \"paranoid\" model is an information-set analog of the minimax rule used in perfect-information games. The \"overconfident\" model assumes the opponent moves randomly. Our experimental tests in the game of kriegspiel chess (an imperfect-information variant of chess) produced surprising results () against each other, and against one of the kriegspiel algorithms presented at IJCAI-, the overconfident model usually outperformed the paranoid model () the performance of both models depended greatly on how well the model corresponded to the opponent's behavior. These results suggest that the usual assumption of perfect-information game tree search--that the opponent will choose the best possible move--isn't as useful in imperfect-information games.|Austin Parker,Dana S. Nau,V. S. Subrahmanian","66099|AAAI|2007|Joint Inference in Information Extraction|The goal of information extraction is to extract database records from text or semi-structured sources. Traditionally, information extraction proceeds by first segmenting each candidate record separately, and then merging records that refer to the same entities. While computationally efficient, this approach is suboptimal, because it ignores the fact that segmenting one candidate record can help to segment similar ones. For example, resolving a well-segmented field with a less-clear one can disambiguate the latter's boundaries. In this paper we propose a joint approach to information extraction, where segmentation of all records and entity resolution are performed together in a single integrated inference process. While a number of previous authors have taken steps in this direction (eg., Pasula et al. (), Wellner et al. ()), to our knowledge this is the first fully joint approach. In experiments on the CiteSeer and Cora citation matching datasets, joint inference improved accuracy, and our approach outperformed previous ones. Further, by using Markov logic and the existing algorithms for it, our solution consisted mainly of writing the appropriate logical formulas, and required much less engineering than previous ones.|Hoifung Poon,Pedro Domingos","66492|AAAI|2008|Expectation-Based Versus Potential-Aware Automated Abstraction in Imperfect Information Games An Experimental Comparison Using Poker|Automated abstraction algorithms for sequential imperfect information games have recently emerged as a key component in developing competitive game theory-based agents. The existing literature has not investigated the relative performance of different abstraction algorithms. Instead, agents whose construction has used automated abstraction have only been compared under confounding effects different granularities of abstraction and equilibrium-finding algorithms that yield different accuracies when solving the abstracted game. This paper provides the first systematic evaluation of abstraction algorithms. Two families of algorithms have been proposed. The distinguishing feature is the measure used to evaluate the strategic similarity between game states. One algorithm uses the probability of winning as the similarity measure. The other uses a potential-aware similarity measure based on probability distributions over future states. We conduct experiments on Rhode Island Hold'em poker. We compare the algorithms against each other, against optimal play, and against each agent's nemesis. We also compare them based on the resulting game's value. Interestingly, for very coarse abstractions the expectation-based algorithm is better, but for moderately coarse and fine abstractions the potential-aware approach is superior. Furthermore, agents constructed using the expectation-based approach are highly exploitable beyond what their performance against the game's optimal strategy would suggest.|Andrew Gilpin,Tuomas Sandholm","80027|VLDB|1980|Functional Dependencies and Incomplete Information|Functional dependencies play an important role in relational database design. They are defined in the context of a single relation which at all times must contain tuples with non-null entries. In this paper we examine an extension of the functional dependency interpretation to handle null values, that is, entries in tuples that represent incomplete information in a relational database. A complete axiomatization of inference rules for extended functional dependencies is also presented. Only after having such results is it possible to talk about decompositions and normalization theory in a context of incomplete information. Finally, we show that there are several practical advantages in using nulls and a weaker notion of constraint satisfiability.|Yannis Vassiliou","15210|IJCAI|1995|Generating and Solving Imperfect Information Games|Work on game playing in AI has typically ignored games of imperfect information such as poker. In this paper we present a framework for dealing with such games. We point out several important issues that arise only in the context of imperfect information games particularly the insufficiency of a simple game tree model to represent the players information state and the need for randomization in the players optimal strategies. We describe Gala an implemented system that provides the user with a very natural and expressive language for describing games. From a game description Gala creates an augmented game tree with information sets which can be used by various algorithms in order to find optimal strategies for that game. In particular Gala implements the first practical algorithm for finding optimal randomized strategies in two player imperfect information competitive games Koller et al . The running time of this algorithm is palinomial in the size of the game tree whereas previous algorithms were exponential. We present experimental results showing that this algorithm is also efficient in practice and can therefore form the basis for a game playing system.|Daphne Koller,Avi Pfeffer","15721|IJCAI|2001|Relational Learning via Propositional Algorithms An Information Extraction Case Study|This paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional means. This paradigm suggests different tradeoffs than those in the traditional approach to this problem - the ILP approach - and as a result it enjoys several significant advantages over it. In particular, the new paradigm is more flexible and allows the use of any propositional algorithm, including probabilistic algorithms, within it. We evaluate the new approach on an important and relation-intensive task - Information Extraction - and show that it outperforms existing methods while being orders of magnitude more efficient.|Dan Roth,Wen-tau Yih"],["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","80028|VLDB|1980|On Retrieval from a Small Version of a Large Data Base|A person in the higher levels of a hierarchical organization may wish not to use a fully detailed data base, but rather an abstracted data base, perhaps dropping into detail in only a few areas. We compare answers from queries put to an abstracted data base with answers obtained by querying a full data base and then abstracting the result of the query. We show that, for some common relational retrievals, querying an abstracted data base always yields the correct information, plus, in some cases, some incorrect information, and we give simple conditions on the abstraction which ensure that only the correct information is fetched. For the cases in which the conditions may not hold, we suggest an ordering, called succinctness, for comparing the quality of different abstractions.|Adrian Walker","79910|VLDB|1978|On Bubble Memories and Relational Data Base|Slightly modified majorminor loop bubble chips can accommodate storage and access for relational data base quite well. With the addition of dynamic indexing loops, very efficient data retrieval and manipulation can be accomplished. This paper specifies a set of relationally complete instructions. A variety of queries are executed to demonstrate the versatility of the instructions as well as understand the nature of our hardware and software operations. Since the bubble hardware is intrinsically similar to the data model, and adapted to the access requirements. we believe the overall system is simpler both in operation and in programming. The present work is also examined in the persepctive of data base trends, bubble technology development, and other hardware approaches to data base (viz. logic per track associative processing).|H. Chang","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","80100|VLDB|1985|An Efficient Implementation of a Relational Data Base|In the paper an approach to implement efficiently a relational data base is presented. The appro- ach combines a new method of physical repre- sentation with a novel database machine (DBM) architecture. The dispersed representation assumes a relation consisting of three parts the identification, value, and link (optionally). The parts are separated and the identification and link parts are converted to be expressed with pointers, and then the parts are dispersed among three memories the IDENTIFICATION, VALUE, and LINK. The DBM architecture attempts to implement the dispersed representa- tion efficiently and uses modified cellular asso- ciative memories (CAM) to store and process the value and link parts. The modification consists in providing a CAM with a respond register called a global mark register (GMR) which can be readily loaded and unloaded externally.|Marian S. Furman","79866|VLDB|1977|Self-Descriptive Relational Data Base|An architectural model of a self-descriptive relational data base is presented and the implementation problems are investigated. It is argued that self-descriptivity brings excellent properties of the data base management system. For example, users can easily extend the capability of the DBMS or fewer basic access commands can cover all the accesses to the data base. Thus, the self-descriptive data base architecture is an excellent candidate for a hardware data base management system where simple and yet universal access mechanism is badly needed. It is shown that a close relation exists between a self-descriptive data base and the DDD. In fact, it is theoretically necessary that a DBMS be self-descriptive. The notions 'type' and 'occurrence' are also applied to relations, thus a relation type and corresponding relation occurrences are introduced and shown to be effective in representing the real world. An example of a self-descriptive data base relation is exhibited and the general characteristics of it is discussed. To make the system motions secure, a monitoring program is introduced and explained. A typical command process is described and its internal logic is shown in an ALGOL-like program codes. Four basic atomic commands, executed by a data base machine, are introduced and explained. The method to extend the basic data base management facility by using selfdescriptivity and exit routines is introduced and an example to enhance the integrity checking capability is demonstrated. The hierarchical locking method is applied and the problems peculiar to the self-descriptive architecture are investigated. Finally the implementation of temporary relations useful in various conditions is considered.|Ryosuke Hotaka,Masaaki Tsubaki","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","79837|VLDB|1976|Relational Transformation and a Redundancy in Relational Data Base|This paper considers some transformation and replacement of relations in a relational data base, called association and dissociation, which are intended to decrease the time of jobs processing and files updating, although it is accompanied by an increase of information redundancy. Some examples are examined. It is shown that in a considerable number of practical cases association and dissociation are effective means of processing time reduction and data base adaptation.|Eliezer L. Lozinskii","80063|VLDB|1981|Application of Data Compression to a Large Bibliographic Data Base|This paper examines the applicability and benefits of various data compression techniques to a large bibliographic database that forms the core of The University of California's public access on-line Union Catalog. Particular attention is given to the interplay between compression and retrieval, and to the feasibility of implementing high performance Huffman coding algorithms in microcode.|Clifford A. Lynch,E. B. Brownrigg"],["79879|VLDB|1977|A Survey The Application of Data Base Management Computers in Distributed Systems|Data Base Management Computers (DBMC) are special purpose computers in distributed systems. The dedicated role of the DBMC is to control the devices upon which data bases reside and to provide data base management services for other computers in the system. and how it relates to other computers, Depending upon how it operates a given DBMC can be classified either as an intelligent controller, a backend or a datacomputer. Prototypes of each class have been proposed and implemented. In addition there is a growing body of literature devoted to DBMC technology -- objectives, requirements, problems, performance issues, etc. The purpose of this paper is to present a survey of recent research and then to examine special DBMC requirements in the increasingly important areas of very large data bases and distributed data bases.|Eugene I. Lowenthal","79799|VLDB|1975|An Experiment in Dedicated Data Management|There is growing concern among users of data base management systems regarding the cost and performance of such systems. As is typical of highly complex, generalized software, data management systems can be expensive to run, particularly in terms of processing time. A possible solution to this problem is to offload the data management functions into an inexpensive, dedicated minicomputer, called a dedicated data management computer, or DDM. This \"backend\" approach to the problem has been investigated by Bell Telephone Laboratories , using a large DBTG-based system. Their results indicated that such an approach is feasible, though no performance figures have been published. In an effort to obtain more detailed data on the performance implications of a DDM system, two experiments were performed by the Sperry Corporate Research Center. The first experiment was designed to obtain estimates of the percentage of CP time which could be offloaded from the host to a DDM. The second experiment, a prototype implementation of a DDM configuration using a standard minicomputer, was designed to demonstrate the credibility of such a system.|H. C. Heacox,E. S. Cosloy,J. B. Cohen","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","79955|VLDB|1979|Spatial Management of Data Abstract|Spatial Data Management is a technique for organizing and retrieving information by positioning it in a Graphical Data Space (GDS). This Graphical Data Space is viewed through a color raster scan display which enables users to traverse the GDS surface or zoom into the image to obtain greater detail. In contrast to conventional database management systems - in which users access data by asking questions in a formal query language, a Spatial Data Management System (SDMS) presents the information graphically in a form which seems to encourage browsing and to require less prior knowledge of the contents and organization of the database. This paper presents an overview of the SDMS concept and describes its implementation in a prototype system for retrieving information from both a symbolic database management system and from an optical videodisk.|Christopher F. Herot","80756|VLDB|2007|Regulatory-Compliant Data Management|Digital societies and markets increasingly mandate consistent procedures for the access, processing and storage of information. In the United States alone, over , such regulations can be found in financial, life sciences, health - care and government sectors, including the Gramm - Leach - Bliley Act, Health Insurance Portability and Accountability Act, and Sarbanes - Oxley Act. A recurrent theme in these regulations is the need for regulatory - compliant data management as an underpinning to ensure data confidentiality, access integrity and authentication provide audit trails, guaranteed deletion, and data migration and deliver Write Once Read Many (WORM) assurances, essential for enforcing long - term data retention and life - cycle policies.|Radu Sion,Marianne Winslett","80698|VLDB|2006|Next Generation Data Management in Enterprise Application Platforms|As a leading provider of applications and application infrastructure software, SAP has been always interested in the entire spectrum of enterprise data management from transactional to analytical, structured and unstructured, as well as high-volatility event data streams. The underlying architecture for enterprise applications has fundamentally changed in the last decade, with the adoption of service-oriented architectures representing the latest shift. However, DBMS architecture has not evolved sufficiently to meet the challenges that these new application characteristics pose. As a result, at SAP we have been rethinking the way enterprise applications manage their data. In this talk, we will present some key aspects of this rethinking. We will start with a description of the shift in application architecture and the challenges that this shift poses on data management. We will then describe the failings of a single overarching DBMS architecture against these needs, and then describe some examples of usage-specific data management in enterprise application platforms. In particular we will focus on our approach to managing analytical, transactional and master data. We will present some results that describe how, with a combination of better utilization of main-memory based data management techniques, addressing the needs of the next generation application infrastructure and advances in the underlying computing and storage infrastructure, we can do significantly more efficient data management.|Vishal Sikka","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","79834|VLDB|1976|A Deductive Capability for Data Management|This paper examines some of the problems and issues involved in designing a practical deductive inference processor to augment a data management system, as well as some of the benefits that can be expected from such an augmentation. A deductive processor design is presented that incorporates new techniques for selecting, from large collections of mostly irrelevant general assertions and specific facts, the small number needed for deriving an answer to a particular query.|Charles Kellogg,Philip Klahr,Larry Travis","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","80170|VLDB|2002|Monitoring Streams - A New Class of Data Management Applications|This paper introduces monitoring applications, which we will show differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS that is currently under construction at Brandeis University, Brown University, and M.I.T. We describe the basic system architecture, a stream-oriented set of operators, optimization tactics, and support for real-time operation.|Donald Carney,Ugur √\u2021etintemel,Mitch Cherniack,Christian Convey,Sangdon Lee,Greg Seidman,Michael Stonebraker,Nesime Tatbul,Stanley B. Zdonik"],["13693|IJCAI|1977|PRUF - A Language for the Representation of Meaning in Natural Languages|PRUF--an acronym for Possibilistic Relational Universal Fuzzy--is a designation for a novel type of synthetic language which is intended to serve as a target language for the representation of meaning of expressions in a natural language.|Lotfi A. Zadeh","13953|IJCAI|1983|Graph Grammar Approach to Natural Language Parsing and Understanding|String grammars have been found in many ways inadequate for parsing inflectional languages with \"free\" word order. To overcome these problems we have replaced linear string grammars and tree transformations by their multidimensional generalization, graph grammars. In our approach parsing is seen as a transformation between two graph languages, namely the sets of morphological and semantic representations of natural language sentences. An experimental Finnish question-answering system SUVI based on graph grammars has been implemented. In SUVI the role of individual words is active. Each word is associated to a syntactico-senantic constituent type that is represented by a transition network - like graph whose transitions correspond to transformations in the derivation graph. Parsing is performed by interpreting the constituent type graphs corresponding to the words of the current sentence.|Eero Hyv√∂nen","14404|IJCAI|1987|Understanding System Specifications Written in Natural Language|This paper describes research in understanding system specifications written in natural language. This research involves the implementation of a natural language interface, PHRANSPAN, for specifying the abstract behavior of digital systems in restricted English text. A neutral formal representation for the behavior is described using the USC Design Data Structure. A small set of concepts that characterize digital system behavior are presented using this representation. An intermediate representation loosely based on Conceptual Dependency is presented. Its use with a semantic-based parser to translate from English to the formal representation is illustrated by examples.|John J. Granacki Jr.,Alice C. Parker,Yigal Arens","14382|IJCAI|1987|Representation and Interpretation of Determiners in Natural Language|Following the principles of locality and compositionality during semantic interpretation, we propose a semantic representation formalism which manages to deal with reference problems, i.e. with the interpretation of NPs (in particular, of those beginning with an article) the addressed problems are generic  specific  class readings, and collective  distributive interpretations. The formalism follows the semantic net approach, and uses different representation plans (semantic, content and reference) and particular structures, called ambiguity spaces, that hide the ambiguities from the other parts of the sentence and remain neutral with respect to the various interpretations until certain disambiguation clues are found. Besides presenting the formalism, the paper discusses which clues may be used to disambiguate among the various interpretations and shows the way this representation is used to update the hearer's knowledge base.|Barbara Di Eugenio,Leonardo Lesmo","14433|IJCAI|1987|A Knowledge Framework for Natural Language Analysis|Recent research in language analysis and language generation has highlighted the role of knowledge representation in both processes. Certain knowledge representation foundations, such as structured inheritance networks and feature-based linguistic representations, have proved useful in a variety of language processing tasks. Augmentations to this common framework, however, are required to handle particular issues, such as the ROLE RELATIONSHIP problem the task of determining how roles, or slots, of a given frame, are filled based on knowledge about other roles. Three knowledge structures are discussed that address this problem. The semantic interpreter of an analyzer called TRUMP (TRansportable Understanding Mechanism Package) uses these structures to determine the fillers of roles effectively without requiring excessive specialized information about each frame.|Paul S. Jacobs","14812|IJCAI|1991|Efficient Representation of Linguistic Knowledge for Continuous Speech Understanding|This paper describes a linguistic knowledge representation technique suitable for reducing analysis time and memory requirements in a parser for continuous speech. Parsing speech, having to process a lattice of word hypotheses instead of a string of words, involves a tremendous amount of search and the generation of a high number of phrase hypotheses. The aim is, while using powerful and flexible formalisms for syntax and semantics, to generate \"compact\" phrase hypotheses, each one accounting for many syntactic rules simultaneously. The proposed method is able to cope with, and to take advantage from, the fact that short words are often missing from the lattice. A detailed example is given to clarify this method. Finally experimental data arc presented and discussed, showing the effectiveness of the proposed technique.|Paolo Baggia,Elisabetta Gerbino,Egidio P. Giachin,Claudio Rullent","13657|IJCAI|1977|A State Logic for the Representation of Natural Language Based Intelligent Systems|The work described herein introduces a general logic based formalism for the actions of an intelligent system understanding natural language sentences, executing commands and answering questions|Camilla Schwind","65579|AAAI|2005|Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation|I propose a representation formalism and algorithms to be used in a new language generation mechanism for text-to-text applications. The generation process is driven by both text-specific information encoded via probability distributions over words and phrases derived from the input text, and general language knowledge captured by n-gram and syntactic language models.|Radu Soricut","13771|IJCAI|1981|Variable-Depth Natural Language Understanding|Standard A I representations of knowledge operate at fixed depth (i.e. the objects manipulated are described by an amount of information which remains constant for every task). Contrary to this approach, Variable Depth Processing (VDP) uses a progressive description of objects, tries different strategies according to the quality of the result it needs, and continually controls this quality by means of an evaluation of the approximations it makes. Contextual Production Rules are shown to be an effective way to implement some features of VDP. We are currently developing a VDP question - answering system which works on texts concerning a non-technical subject, namely an excerpt of a general public - oriented encyclopaedia.|Daniel Kayser,Daniel Coulon","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II"],["16682|IJCAI|2007|Conjunctive Query Answering for the Description Logic SHIQ|Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, it was an open problem whether conjunctive query answering over DL knowledge bases is decidable if transitive roles are admitted in the query. In this paper, we consider conjunctive queries over knowledge bases formulated in the popular DL SHIQ and allow transitive roles in both the query and the knowledge base. We show that query answering is decidable and establish the following complexity bounds regarding combined complexity, we devise a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query. Regarding data complexity, we prove co-NP-completeness.|Birte Glimm,Ian Horrocks,Carsten Lutz,Ulrike Sattler","65822|AAAI|2006|Finding Maximally Satisfiable Terminologies for the Description Logic ALC|For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.|Thomas Andreas Meyer,Kevin Lee,Richard Booth,Jeff Z. Pan","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","65720|AAAI|2006|On the Update of Description Logic Ontologies at the Instance Level|We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.|Giuseppe De Giacomo,Maurizio Lenzerini,Antonella Poggi,Riccardo Rosati","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15777|IJCAI|2003|Terminological Cycles in a Description Logic with Existential Restrictions|Cyclic definitions in description logics have until now been investigated only for description logics allowing for value restrictions. Even for the most basic language FL, which allows for conjunction and value restrictions only, deciding subsumption in the presence of terminological cycles is a PSPACE-complete problem. This paper investigates subsumption in the presence of terminological cycles for the language EL, Which allows for conjunction, existential restrictions, and the topconcept. In contrast to the results for FL., subsumption in EL remains polynomial, independent of whether we use least fixpoint semantics, greatest fixpoint semantics, or descriptive semantics.|Franz Baader","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski"],["16592|IJCAI|2007|Learning from Partial Observations|We present a general machine learning framework for modelling the phenomenon of missing information in data. We propose a masking process model to capture the stochastic nature of information loss. Learning in this context is employed as a means to recover as much of the missing information as is recoverable. We extend the Probably Approximately Correct semantics to the case of learning from partial observations with arbitrarily hidden attributes. We establish that simply requiring learned hypotheses to be consistent with observed values suffices to guarantee that hidden values are recoverable to a certain accuracy we also show that, in some sense, this is an optimal strategy for achieving accurate recovery. We then establish that a number of natural concept classes, including all the classes of monotone formulas that are PAC learnable by monotone formulas, and the classes of conjunctions, disjunctions, k-CNF, k-DNF, and linear thresholds, are consistently learnable from partial observations. We finally show that the concept classes of parities and monotone term -decision lists are not properly consistently learnable from partial observations, if RP  NP. This implies a separation of what is consistently learnable from partial observations versus what is learnable in the complete or noisy setting.|Loizos Michael","14370|IJCAI|1987|A Formal Approach to Learning From Examples|This paper presents a formal, foundational approach to learning from examples in machine learning. It is assumed that a learning system is presented with a stream of facts describing a domain of application. The task of the system is to form and modify hypotheses characterising the relations in the domain, based on this information. Presumably the set of hypotheses that may be so formed will require continual revision as further information is received. The emphasis in this paper is to characterise those hypotheses that may potentially be formed, rather than to specify the subset of the hypotheses that, for whatever reason, should be held. To this end. formal systems are derived from which the set of potential hypotheses that may be formed is precisely specified. A procedure is also derived for restoring the consistency of a set of hypotheses after conflicting evidence is encountered. In addition, this work is extended to where a learning system may be \"told\" arbitrary sentences concerning a domain The approach is intended to provide a basic framework lor the development of systems that learn from examples, as well as a neutral point from which such systems may be viewed and compared.|James P. Delgrande","14240|IJCAI|1985|Learning Procedures from Examples and by Doing|This paper describes a program that learns procedures by examining worked-out examples in a textbook and bv working problems two kinds of production (If-then) rules are created working forward rules that produce an action when a proceduie is executed and difference rules that suggest operators from observed transformations. Dining example learning, the program examines two states in an example, fig tires out the operator that produced the second state and creates a production with some part ot the first line in the condition with the operator-tor in the action. During learning by working problems, the program generates its own example trace by problem solving and uses the same example learning techniques.|David M. Neves","66326|AAAI|2008|Learning from Multiple Heuristics|Heuristic functions for single-agent search applications estimate the cost of the optimal solution. When multiple heuristics exist, taking their maximum is an effective way to combine them. A new technique is introduced for combining multiple heuristic values. Inspired by the evaluation functions used in two-player games, the different heuristics in a single-agent application are treated as features of the problem domain. An ANN is used to combine these features into a single heuristic value. This idea has been implemented for the sliding-tile puzzle and the -peg Towers of Hanoi, two classic single-agent search domains. Experimental results show that this technique can lead to a large reduction in the search effort at a small cost in the quality of the solution obtained.|Mehdi Samadi,Ariel Felner,Jonathan Schaeffer","14051|IJCAI|1983|Learning Equation Solving Methods From Examples|This paper describes LP, a program that learns new techniques for solving equations by examining worked examples. Unlike most of the work in this field, e.g. (Neves, ), where the equations used have been very simple, LP uses complex equations, see below. LP can learn from one example, using concepts from the planning field. In order to be able to successfully use a new technique, LP learns many different types of information. To learn new rewrite rules, LP compares consecutive lines in the worked example, finding differences between them. It also learns the strategic purpose of the steps, by considering the worked example as a type of plan for solving the equation. LP extracts the necessary information, and builds a plan which is stored for future use. LP executes the plan in a flexible way to solve new equations.|Bernard Silver","14272|IJCAI|1985|Learning Concept Descriptions from Examples with Errors|This paper presents a scheme for learning complex descriptions, such as logic formulas, from examples with errors. The basis for learning is provided by a selection criterion which minimizes a combined measure of discrepancy of a description with training data, and complexity of a description. Learning rules for two types of descriptors are derived one for finding descriptors with good average discrimination over a set of concepts, second for selecting the best descriptor for a specific concept. Once these descriptors are found, an unknown instance can be identified by a search using the descriptors of the first type for a fast screening of candidate concepts, and the second for the final selection of the closest concept.|Jakub Segen","14534|IJCAI|1987|Guiding Constructive Induction for Incremental Learning from Examples|LAIR is a system that incrementally learns conjunctive concept descriptions from positive and negative examples. These concept descriptions are used to create and extend a domain theory that is applied, by means of constructive induction, to later learning tasks. Important issues for constructive induction are when to do it and how to control it LAIR demonstrates how constructive induction can be controlled by () reducing it to simpler operations, () constraining the simpler operations to preserve relative correctness, () doing deductive inference on an as-needed basis to meet specific information requirements of learning subtasks, and () constraining the search space by subtask-dependent constraints.|Larry Watanabe,Renee Elio","13883|IJCAI|1983|Learning Word Meanings From Examples|This paper describes work in progress on a computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple English example stories. The central idea is an extension of Winston's (Winston ) program that learned the structural descriptions of blocks world scenes. In the new research, English verbs take the place of blocks world objects like ARCH and TOWER, with frame-based descriptions of causal relationships serving as the structural descriptions. Syntactic constraints derived from the parsing of story plots are used to drive an analogical matching procedure. Analogical matching gives a way to compare descriptions of known words to unknown words. The \"meaning\" of a new verb is learned by matching pan of the causal network description of a story precis containing the unknown word to a set of such descriptions derived from similar stories that contain only known words. The best match forges an assignment between objects and relations such that the unknown verb is matched to a known verb, with the assignment being guided by syntactic constraints. The causal network surrounding the unknown item is then used as a scaffolding to construct a network representing the use of the novel word in a particular context. Words (and their associated stories) that are \"best matches\" are grouped together into a similarity network, according to the match score.|Robert C. Berwick","14965|IJCAI|1991|Learning Structural Decision Trees from Examples|STRUCT is a system that learns structural decision trees from positive and negative examples. The algorithm uses a modification of Pagallo and Haussler's FRINGE algorithm to construct new features in a first-order representation. Experiments compare the effects of different hypothesis evaluation strategies, domain representation, and feature construction. STRUCT is also compared with Quinlan's FOIL on two domains. The results show that a modified FRINGE algorithm improves accuracy, but that it is sensitive to the distribution of the examples.|Larry Watanabe,Larry A. Rendell","13791|IJCAI|1981|Learning Complex Structural Descriptions from Examples|We present a formalization of an intuitively sound strategy for learning a description from examples  within a partition examples are grouped according to greatest resemblances and examples not in the same subset show a maximum of differences.|Regine Loisel,Yves Kodratoff"],["66264|AAAI|2007|Hybrid Inference for Sensor Network Localization Using a Mobile Robot|In this paper, we consider a hybrid solution to the sensor network position inference problem, which combines a real-time filtering system with information from a more expensive, global inference procedure to improve accuracy and prevent divergence. Many online solutions for this problem make use of simplifying assumptions, such as Gaussian noise models and linear system behaviour and also adopt a filtering strategy which may not use available information optimally. These assumptions allow near real-time inference, while also limiting accuracy and introducing the potential for ill-conditioning and divergence. We consider augmenting a particular real-time estimation method, the extended Kalman filter (EKF), with a more complex, but more highly accurate, inference technique based on Markov Chain Monte Carlo (MCMC) methodology. Conventional MCMC techniques applied to this problem can entail significant and time consuming computation to achieve convergence. To address this, we propose an intelligent bootstrapping process and the use of parallel, communicative chains of different temperatures, commonly referred to as parallel tempering. The combined approach is shown to provide substantial improvement in a realistic simulated mapping environment and when applied to a complex physical system involving a robotic platform moving in an office environment instrumented with a camera sensor network.|Dimitri Marinakis,David Meger,Ioannis M. Rekleitis,Gregory Dudek","15933|IJCAI|2003|Non-Invasive Brain-Actuated Control of a Mobile Robot|Recent experiments have indicated the possibility to use the brain electrical activity to directly control the movement of robotics or prosthetic devices. In this paper we report results with a portable non-invasive brain-computer interface that makes possible the continuous control of a mobile robot in a house-like environment. The interface uses  surface electrodes to measure electroencephalogram (EEG) signals from which a statistical classifier recognizes  different mental states. Until now, brain-actuated control of robots has relied on invasive approaches-requiring surgical implantation of electrodes-since EEG-based systems have been considered too slow for controlling rapid and complex sequences of movements. Here we show that, after a few days of training, two human subjects successfully moved a robot between several rooms by mental control only. Furthermore, mental control was only marginally worse than manual control on the same task.|Jos√© del R. Mill√°n,Fr√©d√©ric Renkens,Josep Mouri√±o,Wulfram Gerstner","16581|IJCAI|2007|Using a Mobile Robot for Cognitive Mapping|When animals (including humans) first explore a new environment, what they remember is fragmentary knowledge about the places visited. Yet, they have to use such fragmentary knowledge to find their way home. Humans naturally use more powerful heuristics while lower animals have shown to develop a variety of methods that tend to utilize two key pieces of information, namely distance and orientation information. Their methods differ depending on how they sense their environment. Could a mobile robot be used to investigate the nature of such a process, commonly referred to in the psychological literature as cognitive mapping What might be computed in the initial explorations and how is the resulting \"cognitive map\" be used to return home In this paper, we presented a novel approach using a mobile robot to do cognitive mapping. Our robot computes a \"cognitive map\" and uses distance and orientation information to find its way home. The process developed provides interesting insights into the nature of cognitive mapping and encourages us to use a mobile robot to do cognitive mapping in the future, as opposed to its popular use in robot mapping.|Chee K. Wong,Jochen Schmidt,Wai K. Yeap","15324|IJCAI|1997|Active Mobile Robot Localization|Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effectors during localization. This paper proposes an active localization approach. The approach provides rational criteria for () setting the robot's motion direction (exploration), and () determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment.|Wolfram Burgard,Dieter Fox,Sebastian Thrun","66384|AAAI|2008|Planning for Human-Robot Interaction Using Time-State Aggregated POMDPs|In order to interact successfully in social situations, a robot must be able to observe others' actions and base its own behavior on its beliefs about their intentions. Many interactions take place in dynamic environments, and the outcomes of people's or the robot's actions may be time-dependent. In this paper, such interactions are modeled as a POMDP with a time index as part of the state, resulting in a fully Markov model with a potentially very large state space. The complexity of finding even an approximate solution often limits POMDP's practical applicability for large problems. This difficulty is addressed through the development of an algorithm for aggregating states in POMDPs with a time-indexed state space. States that represent the same physical configuration of the environment at different times are chosen to be combined using reward-based metrics, preserving the structure of the original model while producing a smaller model that is faster to solve. We demonstrate that solving the aggregated model produces a policy with performance comparable to the policy from the original model. The example domains used are a simulated elevator-riding task and a simulated driving task based on data collected from human drivers.|Frank Broz,Illah R. Nourbakhsh,Reid G. Simmons","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","14951|IJCAI|1991|Mobile Robot Navigation by an Active Control of the Vision System|In this paper, we argue that a mobile robot's environment can be determined by computing local maps surrounding feature points, called fixation points. These fixation points are obtained by searching the scene for points which present some interesting cue for robot navigation. This -D computation is based on a monocular active vision system composed of a camera, mounted on a rotating table accurately controlled by a computer, which gazes the fixation point as the robot moves. The system then computes the local map and updates it with each new observation in order to increase its accuracy and robustness. Real experimentation in a complex indoor scene illustrates that the -D scene coordinates can be obtained with a good accuracy by integrating several observations.|Patrick Stelmaszyk,Hiroshi Ishiguro,Saburo Tsuji","14529|IJCAI|1987|Visual Path Planning by a Mobile Robot|Stereo-vision-based mobile robots observe their environments and acquire - data with considerable errors in range. Rather than to use the conventional - maps, described with these Inaccurate data in the absolute coordinate system, a flexible relational model of the global world can be built with local maps suited for each function of robots such as path planning or manipulation. For the path planning, a perspective representation of the local world, the image on which the scene Interpretation is mapped, is proposed. Using world constraints and sensory Information of camera orientation, a stereo-image analyzer determines vertical projections of edge points on the floor in the image. A method for planning of promising paths to the specified goal using this representation is presented.|Saburo Tsuji,Jiang Yu Zheng","14556|IJCAI|1989|Building a World Model for a Mobile Robot Using Dynamic Semantic Constraints|We are developing a new paradigm for a world model construction system which interprets a scene and builds a world model for a mobile robot using dynamic semantic constraints. The system represents a world model in hierarchical form sensor-based maps to a global map with both numerical and symbolic descriptions. At the beginning of interpretation, sensory data (video and range images) are analyzed in bottom-up fashion. A range image is transformed into a height map, and analyzed for the purpose of generating a geometrical property list for both obstacle and traversable regions that is used as the initial input to the interpretation process. At each step of the scene interpretation process, the most reliable feature of an object is selected in the region property list to propagate semantic constraints on other objects close to it. Geometrical modeling for individual objects in the scene is performed, and parameters of each model are dynamically refined by the scene interpretation process. These model parameters and their interrelationships make spatial reasoning robust. Preliminary results with video and range images are shown.|Minoru Asada,Yoshiaki Shirai","13710|IJCAI|1981|Learning of Sensory-Motor Schemas in a Mobile Robot|A learning system is described which was used to control a simple robot vehicle and to autonomously learn behaviour patterns. The system is loosely based on Becker's model of Intermediate Cognition.|Alan H. Bond,David H. Mott"]]}}