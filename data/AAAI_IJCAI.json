{"abstract":{"entropy":6.51802688779137,"topics":["natural language, knowledge base, logic, knowledge representation, theorem proving, description logic, language, reasoning, logic programming, present approach, knowledge system, language system, knowledge, theorem prover, program, situation calculus, temporal reasoning, describes system, case-based reasoning, nonmonotonic logic","problem, constraint satisfaction, solving problem, constraint problem, markov decision, markov processes, search algorithm, search, heuristic search, satisfaction problem, consider problem, partially observable, decision processes, present algorithm, recent years, planning problem, solutions problem, planning domains, algorithm, planning","machine learning, learning, neural networks, bayesian networks, data, present learning, present novel, reinforcement learning, widely used, dimensionality reduction, information extraction, learning domains, learning data, belief revision, sense disambiguation, approach learning, word sense, learning algorithm, problem data, explanation-based learning","artificial intelligence, system, play role, expert system, mobile robot, agents, describes system, model-based diagnosis, robot, autonomous agents, intelligent system, real world, applications, multi-agent system, research intelligence, agents environment, received attention, autonomous robot, research artificial, vision system","logic programming, theorem proving, theorem prover, logic program, question answering, computer program, programming language, programming, answer question, present programming, general framework, present general, programming system, present methodology, programming program, inference rules, present program, language specification, general system, programming asp","present approach, describes approach, reasoning system, case-based reasoning, temporal reasoning, qualitative reasoning, reasoning, approach reasoning, approach based, describes reasoning, approach, reasoning based, present reasoning, reasoning knowledge, present called, present model, reasoning case, case-based cbr, model based, approach system","markov decision, markov processes, partially observable, decision processes, recent years, decision tree, markov mdps, decision making, decision mdps, decision problem, observable markov, processes mdps, partially markov, decision, markov model, partially decision, observable decision, observable pomdps, partially pomdps, observable processes","planning problem, consider problem, planning domains, planning plan, solve problem, planning, reinforcement learning, approach planning, classical planning, problem plan, problem domains, resource allocation, planning actions, optimization problem, present planning, distributed optimization, planning nondeterministic, improve performance, shortest path, problem learning","neural networks, bayesian networks, belief revision, networks, semantic networks, networks based, discriminant analysis, model, collaborative filtering, data structure, directed graph, social networks, linear discriminant, networks probabilistic, sensor networks, present model, iterated revision, networks model, hidden model, belief","problem data, data, dimensionality reduction, data mining, support vector, algorithm data, machine data, address problem, natural processing, clustering data, system data, information processing, vector machine, large data, vector svm, relational database, information retrieval, labeled data, support machine, paper address","mobile robot, describes system, intelligent system, expert system, system, knowledge-based system, system user, web services, intelligent tutoring, dynamic system, problem system, learning system, system architecture, user interface, recommender system, stereo vision, tutoring system, robot navigation, information system, hybrid system","model-based diagnosis, diagnosis system, control system, development system, video coding, components system, model-based system, research system, well known, behavior system, system developed, system model, software system, dynamical system, system implemented, diagnostic system, model behavior, system provide, qualitative system, diagnosis"],"ranking":[["16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","14929|IJCAI|1991|Representing Diagnostic Knowledge for Probabilistic Horn Abduction|This paper presents a simple logical framework for abduction, with probabilities associated with hypotheses. The language is an extension to pure Prolog, and it has straight-forward implementations using branch and bound search with either logic-programming technology or ATMS technology. The main focus of this paper is arguing for a form of representational adequacy of this very simple system for diagnostic reasoning. It is shown how it can represent model-based knowledge, with and without faults, and with and without nonintermittency assumptions. It is also shown how this representation can represent any probabilistic knowledge representable in a Bayesian belief network.|David Poole","65074|AAAI|1987|Inferring Formal Software Specifications from Episodic Descriptions|The WATSON automatic programming system computes formal behavior specifications for process-control software from informal \"scenarios\" traces of typical system operation. It first generalizes scenarios into stimulus-response rules, then modifies and augments these rules to repair inconsistency and incompleteness. It finally produces a formal specification for the class of computations which implement that scenario and which are also compatible with a set of \"domain axioms\". A particular automaton from that class is constructed as an executable prototype for the specification. WATSON's inference engine combines theorem proving in a very weak temporal logic with faster and stronger, but approximate, model-based reasoning. The use of models and of closed-world reasoning over \"snapshots\" of an evolving knowledge base leads to an interesting special case of non-monotonic reasoning.|Van E. Kelly,Uwe Nonnenmann","13879|IJCAI|1983|Integrating Logic Programs and Schemata|(orn clauseSchema Representation Language) is the result of an athor to combine the- tools of logic program-niinp. and schema based knowledge represention into a single hybrid system. knowledge, compressed in schemala can be accessed during the execution of logic programs, and the retrieval of the values of a SLOT in a schema can involve- the execution of logic programs that attempt. to declare the- values prior to presenting- to inheritence, should the slot be empty. ISRI. supports the implementation of programs that take advantage of the best controles. of the logical and objec oriented approaches to knowledge represenlation.|Bradley P. Allen,J. Mark Wright","15005|IJCAI|1993|Proving Theorems in a Multi-Source Environment|This paper describes a logic for reasoning in a multi-source environment and a theorem prover for this logic. We assume the existence of several sources of information (dataknowledge bases), each of them providing information. The main problem dealt with here is the problem of the consistency of the information  even if each separate source is consistent, the global set of information may be inconsistent. In our approach, we assume that the different sources are totally ordered, according to their reliability. This order is then used in order to avoid inconsistency. The logic we define for reasoning in this case is based on a classical logic augmented with pseudo-modalities. Its semantic is first detailed. Then a sound and complete axiomatic is given. Finally, a theorem prover is specified at the meta-level. We prove that it is correct with regard to the logic. We then implement it in a PROLOG-like language.|Laurence Cholvy","14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi","15474|IJCAI|1999|Stable Model Checking Made Easy|Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (V-free) logic programs liecause stable model checking - deciding whether a given model is a stable model of a propositional DLP program is co-NP-complete, while it is polynomial for normal logic programs. This paper proposes a new transformation M(P) which reduces stable model checking to UNSAT - i.e., to deciding whether a given CNF formula is unsatisfiable. Thus, the stability of a model AI for a program P can be verified by calling a Satisfiability Checker on the CNF formula M(P). The transformation is parsimonious and efficiently computable, as it runs in logarithmic space. Moreover, the size of the generated CNF formula never exceeds the size of the input. The proposed approach to stable model checking lias been implemented in a DLP system, and a number of experiments and benchmarks have been run.|Christoph Koch,Nicola Leone","15058|IJCAI|1993|Using Classification as a Programming Language|Our experience in the IDAS natural language generation project has shown us that IDAS'S KLONE-like classifier, originally built solely to hold a domain knowledge base, could also be used to perform many of the computations required by a natural-language generation system in fact it seems possible to use the classifier to encode and execute arbitrary programs. We discuss IDAS'S classification system and how it differs from other such systems (perhaps most notably in the presence of template' constructs that enable recursion to be encoded) give examples of program fragments encoded in the classification system and compare the classification approach to other AI programming paradigms (e.g., logic programming).|Chris Mellish,Ehud Reiter","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"],["15126|IJCAI|1995|Exploiting Structure in Policy Construction|Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods.|Craig Boutilier,Richard Dearden,Moisés Goldszmidt","16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","15875|IJCAI|2003|A Planning Algorithm for Predictive State Representations|We address the problem of optimally controlling stochastic environments that are partially observable. The standard method for tackling such problems is to define and solve a Partially Observable Markov Decision Process (POMDP). However, it is well known that exactly solving POMDPs is very costly computationally. Recently, Littman, Sutton and Singh () have proposed an alternative representation of partially observable environments, called predictive state representations (PSRs). PSRs are grounded in the sequence of actions and observations of the agent, and hence relate the state representation directly to the agent's experience. In this paper, we present a policy iteration algorithm for finding policies using PSRs. In preliminary experiments, our algorithm produced good solutions.|Masoumeh T. Izadi,Doina Precup","66436|AAAI|2008|Symbolic Heuristic Search Value Iteration for Factored POMDPs|We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.|Hyeong Seop Sim,Kee-Eung Kim,Jin Hyung Kim,Du-Seong Chang,Myoung-Wan Koo","65199|AAAI|2004|Stochastic Local Search for POMDP Controllers|The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.|Darius Braziunas,Craig Boutilier","15146|IJCAI|1995|Decomposition Techniques for Planning in Stochastic Domains|This paper is concerned with modeling planning problems involving uncertainty as discrete-time, finite-state stochastic automata Solving planning problems is reduced to computing policies for Markov decision processes. Classical methods for solving Markov decision processes cannot cope with the size of the state spaces for typical problems encountered in practice. As an alternative, we investigate methods that decompose global planning problems into a number of local problems solve the local problems separately and then combine the local solutions to generate a global solution. We present algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space. The local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the subsets of the state space defined by the partition. One algorithm relies on constructing and solving an abstract version of the original decision problem. A second algorithm iteratively approximates parameters of the local problems to converge to an optimal solution. We show how properties of a specified partition affect the time and storage required for these algorithms.|Thomas Dean,Shieu-Hong Lin","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","15972|IJCAI|2003|Modular self-organization for a long-living autonomous agent|The aim of this paper is to provide a sound framework for addressing a difficult problem the automatic construction of an autonomous agent's modular architecture. We briefly present two apparently uncorrelated frameworks Autonomous planning through Markov Decision Processes and Kernel Clustering. Our fundamental idea is that the former addresses autonomy whereas the latter allows to tackle self-organizing issues. Relying on both frameworks, we show that modular self-organization can be formalized as a clustering problem in the space of MDPs. We derive a modular self-organizing algorithm in which an autonomous agent learns to efficiently spread n planning problems over m initially blank modules with m  n.|Bruno Scherrer","14618|IJCAI|1989|Partial Constraint Satisfaction|A constraint satisfaction problem involves finding values for variables subject to constraints on which combinations of values are allowed. In some cases it may be impossible or impractical to solve these problems completely. We may seek to partially solve the problem in an \"optimal\" or \"sufficient\" sense. A formal model is presented for defining and studying such partial constraint satisfaction problems. The basic components of this model are a constraint satisfaction problem, a problem space, and a metric on that space. Algorithms for solving partial constraint satisfaction problems are discussed. A specific branch and bound algorithm is described. Some initial experimental experience with this algorithm is presented.|Eugene C. Freuder","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan"],["65905|AAAI|2006|Cross-Domain Knowledge Transfer Using Structured Representations|Previous work in knowledge transfer in machine learning has been restricted to tasks in a single domain. However, evidence from psychology and neuroscience suggests that humans are capable of transferring knowledge across domains. We present here a novel learning method, based on neuroevolution, for transferring knowledge across domains. We use many-layered, sparsely-connected neural networks in order to learn a structural representation of tasks. Then we mine frequent sub-graphs in order to discover sub-networks that are useful for multiple tasks. These sub-networks are then used as primitives for speeding up the learning of subsequent related tasks, which may be in different domains.|Samarth Swarup,Sylvian R. Ray","65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine","16879|IJCAI|2009|Predictive Projections|This paper addresses the problem of learning control policies in very high dimensional state spaces. We propose a linear dimensionality reduction algorithm that discovers predictive projections projections in which accurate predictions of future states can be made using simple nearest neighbor style learning. The goal of this work is to extend the reach of existing reinforcement learning algorithms to domains where they would otherwise be inapplicable without extensive engineering of features. The approach is demonstrated on a synthetic pendulum balancing domain, as well as on a robot domain requiring visually guided control.|Nathan Sprague","16349|IJCAI|2007|A Subspace Kernel for Nonlinear Feature Extraction|Kernel based nonlinear Feature Extraction (KFE) or dimensionality reduction is a widely used preprocessing step in pattern classification and data mining tasks. Given a positive definite kernel function, it is well known that the input data are implicitly mapped to a feature space with usually very high dimensionality. The goal of KFE is to find a low dimensional subspace of this feature space, which retains most of the information needed for classification or data analysis. In this paper, we propose a subspace kernel based on which the feature extraction problem is transformed to a kernel parameter learning problem. The key observation is that when projecting data into a low dimensional subspace of the feature space, the parameters that are used for describing this subspace can be regarded as the parameters of the kernel function between the projected data. Therefore current kernel parameter learning methods can be adapted to optimize this parameterized kernel function. Experimental results are provided to validate the effectiveness of the proposed approach.|Mingrui Wu,Jason D. R. Farquhar","15818|IJCAI|2003|Hierarchical Semantic Classification Word Sense Disambiguation with World Knowledge|We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \"world knowledge\". The model compiles knowledge contained in a dictionary-ontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \"hierarchical classifier\" outperforms a state-of-the-art standard \"flat\" one.|Massimiliano Ciaramita,Thomas Hofmann,Mark Johnson","15542|IJCAI|1999|Combining Weak Knowledge Sources for Sense Disambiguation|There has been a tradition of combining different knowledge sources in Artificial Intelligence research. We apply this methodology to word sense disambiguation (WSD), a long-standing problem in Computational Linguistics. We report on an implemented sense tagger which uses a machine readable dictionary to provide both a set of senses and associated forms of information on which to base disambiguation decisions. The system is based on an architecture which makes use of different sources of lexical knowledge in two ways and optimises their combination using a learning algorithm. Tested accuracy of our approach on a general corpus exceeds %, demonstrating the viability of allword disambiguation as opposed to restricting oneself to a small sample.|Mark Stevenson,Yorick Wilks","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","65490|AAAI|2005|Distribution-Free Learning of Bayesian Network Structure in Continuous Domains|In this paper we present a method for learning the structure of Bayesian networks (BNs) without making any assumptions on the probability distribution of the domain. This is mainly useful for continuous domains, where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the Bayesian network, and only a few have been examined analytically. We therefore focus on BN structure learning in continuous domains. We address the problem by developing a conditional independence test for continuous variables, which can be readily used by any existing independence-based BN structure learning algorithm. Our test is non-parametric, making no assumptions on the distribution of the domain. We also provide an effective and computationally efficient method for calculating it from data. We demonstrate the learning of the structure of graphical models in continuous domains from real-world data, to our knowledge for the first time using independence-based methods and without distributional assumptions. We also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization, and verify desirable properties such as statistical consistency.|Dimitris Margaritis"],["66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","16093|IJCAI|2005|Learning Forward Models for Robots|Forward models enable a robot to predict the effects of its actions on its own motor system and its environment. This is a vital aspect of intelligent behaviour, as the robot can use predictions to decide the best set of actions to achieve a goal. The ability to learn forward models enables robots to be more adaptable and autonomous this paper describes a system whereby they can be learnt and represented as a Bayesian network. The robot's motor system is controlled and explored using 'motor babbling'. Feedback about its motor system comes from computer vision techniques requiring no prior information to perform tracking. The learnt forward model can be used by the robot to imitate human movement.|Anthony M. Dearden,Yiannis Demiris","66461|AAAI|2008|Incorporating Mental Simulation for a More Effective Robotic Teammate|How can we facilitate human-robot teamwork The teamwork literature has identified the need to know the capabilities of teammates. How can we integrate the knowledge of another agent's capabilities for a justifiably intelligent teammate This paper describes extensions to the cognitive architecture, ACT-R, and the use of artificial intelligence (AI) and cognitive science approaches to produce a more cognitively-plausible, autonomous robotic system that \"mentally\" simulates the decision-making of its teammate. The extensions to ACT-R added capabilities to interact with the real world through the robot's sensors and effectors and simulate the decision-making of its teammate. The AI applications provided visual sensor capabilities by methods clearly different than those used by humans. The integration of these approaches into intelligent team-based behavior is demonstrated on a mobile robot. Our \"TeamBot\" matches the descriptive work and theories on human teamwork. We illustrate our approach in a spatial, team-oriented task of a guard force responding appropriately to an alarm condition that requires the human and robot team to \"man\" two guard stations as soon as possible after the alarm.|William G. Kennedy,Magdalena D. Bugajska,William Adams,Alan C. Schultz,J. Gregory Trafton","13689|IJCAI|1977|Planning in the World of the Air Traffic Controller|An enroute air traffic control (ATC) simulation has provided the basis for research into the marriage of discrete simulation and artificial intelligence techniques. A program which simulates, using real world data, the movement of aircraft in an ATC environment forms a robot's world model. Using a production system to respond to events in the simulated world, the robot is able to look ahead and form a plan of instructions which guarantees safe, expedient aircraft transit. A distinction is made between the real world, where pilots can make mistakes, change their minds, etc., and an idealized plan-ahead world which the robot uses the over-all simulation alternates between updating the real world and planning in the idealized one to investigate the robot's ability to plan in the face of uncertainty.|Robert B. Wesson","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz","13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson","13398|IJCAI|1973|Planning Considerations for a Roving Robot with Arm|The Jet Propulsion Laboratory is engaged in a robot research program. The program is aimed at the development and demonstration of technology required to integrate a variety of robntic functions (locomotion, manipulation, sensing and perception, decision making, and man-robot interaction) into a working robot unit operating in a real world environment and dealing with both man-made and natural objects. This paper briefly describes the hardware and software system architecture of the robot breadboard and summarizes the developments to date. The content of the paper is focused on the unique planning considerations involved in incorporating a manipulator as part of an autonomous robot system. In particular, the effects of system architecture, arm trajectory calculations, and arm dynamics and control are discussed in the context of planning arm motion in complex and changing sensory and workspace environments.|Richard A. Lewis,Antal K. Bejczy","14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue","14284|IJCAI|1985|A Robot Planning Structure Using Production Rules|Robot plan generation is a field which engendered the development of AI languages and rule-based expert systems. Utilization of these latter concepts permits a flexible formalism for robot planning research. We present a robot plan-generation architecture and its application to a real-world mobile robot system. The system undergoes tests through its utilization in the IIILARE robot project (Ciralt, et al., ). Though the article concentrates on planning, execution monitoring and error recovery are discussed. The system includes models of its synergistic environment as well SR of its sensors and effectors (i.e. operators). Its rules embody both planning specific and domair specific knowledge. The system gains generality and adaptiveness through the use of planning variables which provide constraints to the plan generation system. It is implemented in an efficient compiled Production System language (PSL).|Ralph P. Sobek"],["16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","15035|IJCAI|1993|A Parameterised Module System for Constructing Typed Logic Programs|The paper is concerned with the design of a module system for logic programming so as to satisfy many of the requirements of software engineering. The design is based on the language Godel which is a logic programming language which already has a simple type and module system. The module system described here extends the Godel module system so as to include parameterised modules. In particular, this extended system allows general purpose predicates that depend on facts and rules for specific applications to be defined in modules that are independent of their applications.|Patricia M. Hill","15037|IJCAI|1993|Nonmonotonic Model Inference-A Formalization of Student Modeling|A student model description language and its synthesis method are presented. The language called SMDL is based on a logic programming language taking  truth values such as true, false, unknown and fail. A modeling method called HSMIS is a new nonmonotonic model inference system and has the following major characteristics () Model inference of logic program taking  truth values, () Treatment of nonmonotonicity of both student's belief and inference process itself. HSMIS incorporates de Kleer's ATMS as a vehicle for formulating the nonmonotonicity. Both SMDL interpreter and HSMIS have been implemented in Common ESP(Extended Self-contained Prolog) and incorporated into a framework for ITS, called FITS.|Mitsuru Ikeda,Yasuyuki Kono,Riichiro Mizoguchi","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","16856|IJCAI|2009|Automated Theorem Proving for General Game Playing|A general game player is a system that understands the rules of an unknown game and learns to play this game well without human intervention. To succeed in this endeavor, systems need to be able to extract and prove game-specific knowledge from the mere game rules. We present a practical approach to this challenge with the help of Answer Set Programming. The key idea is to reduce the automated theorem proving task to a simple proof of an induction step and its base case. We prove correctness of this method and report on experiments with an off-the-shelf Answer Set Programming system in combination with a successful general game player.|Stephan Schiffel,Michael Thielscher","65043|AAAI|1987|Forward Chaining Logic Programming with the ATMS|Two powerful reasoning tools have recently appeared, logic programming and assumption-based truth maintenance systems (ATMS). An ATMS offers significant advantages to a problem solver assumptions are easily managed and the search for solutions can be carried out in the most general context first and in any order. Logic programming allows us to program a problem solver declaratively--describe what the problem is, rather than describe how to solve the problem. However, we are currently limited when using an ATMS with our problem solvers, because we are forced to describe the problem in terms of a simple language of forward implications. In this paper we present a logic programming language, called FORLOG, that raises the level of programming the ATMS to that of a powerful logic programming language. FORLOG supports the use of \"logical variables\" and both forward and backward reasoning. FORLOG programs are compiled into a data-flow language (similar to the RETE network) that efficiently implements deKleer's consumer architecture. FORLOG has been implemented in Interlisp-D.|Nicholas S. Flann,Thomas G. Dietterich,Dan R. Corpon","66508|AAAI|2008|A Meta-Programming Technique for Debugging Answer-Set Programs|Answer-set programming (ASP) is widely recognised as a viable tool for declarative problem solving. However, there is currently a lack of tools for developing answer-set programs. In particular, providing tools for debugging answer-set programs has recently been identified as a crucial prerequisite for a wider acceptance of ASP. In this paper, we introduce a meta-programming technique for debugging in ASP. The basic question we address is why interpretations expected to be answer sets are not answer sets of the program to debug. We thus deal with finding semantical errors of programs. The explanations provided by our method are based on an intuitive scheme of errors that relies on a recent characterisation of the answer-set semantics. Furthermore, as we are using a meta-programming technique, debugging queries are expressed in terms of answer-set programs themselves, which has several benefits For one, we can directly use ASP solvers for processing debugging queries. Indeed, our technique can easily be implemented, and we devised a corresponding prototype debugging system. Also, our approach respects the declarative nature of ASP, and the capabilities of the system can easily be extended to incorporate differing debugging features.|Martin Gebser,Jörg Pührer,Torsten Schaub,Hans Tompits","13644|IJCAI|1977|Program Inference from Traces using Multiple Knowledge Sources|This paper presents an overview of a framework for the synthesis of high-level program descriptions from traces and example pairs in an automatic programming system. The framework is described in terms of a methodology and a rule base for generating control and data structure specifications for the program to be synthesized, in a format suitable for transformation into program code in a given target language.|J. V. Phillips","65231|AAAI|2004|SAT-Based Answer Set Programming|The relation between answer set programming (ASP) and propositional satisfiability (SAT) is at the center of many research papers, partly because of the tremendous performance boost of SAT solvers during last years. Various translations from ASP to SAT are known but the resulting SAT formula either includes many new variables or may have an unpractical size. There are also well known results showing a one-to-one correspondence between the answer sets of a logic program and the model of its completion. Unfortunately, these results only work for specific classes of problems. In this paper we present a SAT-based decision procedure for answer set programming that (i) deals with any (non disjunctive) logic program, (ii) works on a SAT formula without additional variables, and (iii) is guaranteed to work in polynomial space. Further, our procedure can be extended to compute all the answer sets still working in polynomial space. The experimental results of a prototypical implementation show that the approach can pay off sometimes by orders of magnitude.|Enrico Giunchiglia,Yuliya Lierler,Marco Maratea"],["14736|IJCAI|1989|Combining Case-Based and Rule-Based Reasoning A Heuristic Approach|In this paper we discuss a heuristically controlled approach to combining reasoning with cases and reasoning with rules. Our task is interpretation of under-defined terms that occur in legal statutes (like the Internal Revenue Code) where certain terms must be applied to particular cases even though their meanings are not defined by the statute and the statutory rules are unclear as to scope and meaning. We describe this problem, known as statutory interpretation, provide examples of it, describe the need for melding case-based and rule-based reasoning, and discuss heuristics used in guiding reasoning on such problems. We conclude with a discussion of our on-going work to model this mode of expert reasoning.|Edwina L. Rissland,David B. Skalak","14195|IJCAI|1985|A Process Model of Cased-Based Reasoning in Problem Solving|Much of the problem solving done by both novices and experts uses \"case-based\" reasoning, or reasoning by analogy to previous similar cases. We explore the ways in which case-based reasoning can help in problem solving. According to our model, transfer of knowledge between cases is guided largely by the problem solving process itself. Our model shows the interactions between problem solving processes and memory for experience. Our computer program, called the MEDIATOR, illustrates case-based reasoning in interpreting and resolving common sense disputes.|Janet L. Kolodner,Robert L. Simpson Jr.,Katia Sycara-Cyranski","16114|IJCAI|2005|Representing Flexible Temporal Behaviors in the Situation Calculus|In this paper we present an approach to representing and managing temporally-flexible behaviors in the Situation Calculus based on a model of time and concurrent situations. We define a new hybrid framework combining temporal constraint reasoning and reasoning about actions. We show that the Constraint Based Interval Planning approach can be imported into the Situation Calculus by defining a temporal and concurrent extension of the basic action theory. Finally, we provide a version of the Golog interpreter suitable for managing flexible plans on multiple timelines.|Alberto Finzi,Fiora Pirri","65597|AAAI|2005|CORMS AI Decision Support System for Monitoring US Maritime Environment|Rule based reasoning and case based reasoning have emerged as two important and complementary reasoning methodologies in artificial intelligence (AI). This paper describes the approach for the development of CORMS AI, a decision support system which employs rule-based and case-based reasoning to assist NOAA's Center for Operational Oceanographic Products and Services watch standing personnel in monitoring the quality of marine environmental data and information. CORMS AI has been in operation since July . The system accurately and reliably identifies suspect data and network disruptions, and has decreased the amount of time it takes to identify and troubleshoot sensor, network, and server failures. CORMS AI has proven to be robust, extendable, and cost effective. It is estimated that CORMS AI will save government over one million dollars per year when its full range of quality control monitoring capabilities is implemented.|Haleh Vafaie,Carl Cecere","14496|IJCAI|1987|The Classification Detection and Handling of Imperfect Theory Problems|In recent years knowledge-based techniques like explanation-based learning, qualitative reasoning and case-based reasoning have been gaining considerable popularity in AI. Such knowledge-based methods face two difficult problems ) the performance of the system is fundamentally limited by the knowledge initially encoded into its domain theory ) the encoding of just the right knowledge to enable the system to function properly over a wide range of tasks and situations is virtually impossible for a complex domain. This paper describes research directed towards the construction of a system that will detect and correct problems with domain theories. This will enable knowledge-based systems to operate with imperfect domain theories and automatically correct the imperfections whenever they pose problems. This paper discusses the classification of imperfect theory problems, strategies for their detection and an approach based on experiment design to handle different types of imperfect theory problems.|Shankar A. Rajamoney,Gerald DeJong","15458|IJCAI|1999|Toward a Probabilistic Formalization of Case-Based Inference|We propose a formal framework for modelling case-based inference (CBI), which is a crucial part of the case-based reasoning (CBR) methodology. As a representation of the similarity structure of a system, the concept of a similarity profile is introduced. This concept makes it possible to formalize the CBR hypothesis that \"similar problems have similar solutions\" and to realize CBI in the form of constraint-based inference. In order to exploit the similarity structure more efficiently, a probabilistic generalization of the constraintbased view is developed. This formalization allows for realizing CBI in the context of probabilistic reasoning and statistical inference and, hence, makes a powerful methodological framework accessible to CBR. Within the generalized setting, a (formalized) CBR hypothesis corresponds to the assumption of a certain stochastic model, and a memory of cases can be seen as statistical data underlying the inference process. As a particular result we establish an approximate probabilistic reasoning scheme which generalizes the constraint-based approach.|Eyke Hüllermeier","16591|IJCAI|2007|Case Base Mining for Adaptation Knowledge Acquisition|In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.|Mathieu d'Aquin,Fadi Badra,Sandrine Lafrogne,Jean Lieber,Amedeo Napoli,Laszlo Szathmary","66160|AAAI|2007|Extending Cognitive Architecture with Episodic Memory|In this paper, we explore the hypothesis that episodic memory is a critical component for cognitive architectures that support general intelligence. Episodic memory overlaps with case-based reasoning (CBR) and can be seen as a task-independent, architectural approach to CBR. We define the design space for episodic memory systems and the criteria any implementation must meet to be useful in a cognitive architecture. We present an implementation and demonstrate how episodic memory, combined with other components of a cognitive architecture, supports a wealth of cognitive capabilities that are difficult to attain without it.|Andrew Nuxoll,John E. Laird","16222|IJCAI|2005|Automating the Discovery of Recommendation Knowledge|In case-based reasoning (CBR) systems for product recommendation, the retrieval of acceptable products based on limited information is an important and challenging problem. As we show in this paper, basic retrieval strategies such as nearest neighbor are potentially unreliable when applied to incomplete queries. To address this issue, we present techniques for automating the discovery of recommendation rules that are provably reliable and non-conflicting while requiring minimal information for their application in a rule-based approach to the retrieval of recommended cases.|David McSherry,Christopher Stretch","14877|IJCAI|1991|Reasoning of Geometric Concepts based on Algebraic Constraint-directed Method|We present an algebraic approach to geometric reasoning and learning. The purpose of this research is to avoid the usual difficulties in symbolic handling of geometric concepts. Our system GREW is grounded on a reasoning scheme that integrate the symbolic reasoning and algebraic reasoning of Wu's method. The basic principle of this scheme is to describe mathematical knowledge in terms of symbolic logic and to execute the subsidiary reasoning for Wu's method. The validity of our approach and GREW is shown by experiments, such as applying to learning-by-example of computer vision heuristics or solving locus problems.|Hitoshi Iba,Hirochika Inoue"],["16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","66436|AAAI|2008|Symbolic Heuristic Search Value Iteration for Factored POMDPs|We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.|Hyeong Seop Sim,Kee-Eung Kim,Jin Hyung Kim,Du-Seong Chang,Myoung-Wan Koo","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","66277|AAAI|2008|Exploiting Symmetries in POMDPs for Point-Based Algorithms|We extend the model minimization technique for partially observable Markov decision processes (POMDPs) to handle symmetries in the joint space of states, actions, and observations. The POMDP symmetry we define in this paper cannot be handled by the model minimization techniques previously published in the literature. We formulate the problem of finding the symmetries as a graph automorphism (GA) problem, and although not yet known to be tractable, we experimentally show that the sparseness of the graph representing the POMDP allows us to quickly find symmetries. We show how the symmetries in POMDPs can be exploited for speeding up point-based algorithms. We experimentally demonstrate the effectiveness of our approach.|Kee-Eung Kim","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|Håkan L. S. Younes","65240|AAAI|2004|Dynamic Programming for Partially Observable Stochastic Games|We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.|Eric A. Hansen,Daniel S. Bernstein,Shlomo Zilberstein"],["65408|AAAI|2005|State Agnostic Planning Graphs and the Application to Belief-Space Planning|Planning graphs have been shown to be a rich source of heuristic information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states. The naive technique enumerates the graphs individually. This is equivalent to solving an all-pairs shortest path problem by iterating a single-source algorithm over each source. We introduce a structure, the state agnostic planning graph, that directly, solves the all-pairs problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in classical planning. The more prominent application of tnis technique is in belief-space planning, where an optimization results in drastically improved theoretical complexity. Our experimental evaluation quantifies this performance boost. and demonstrates that heuristic belief-space progression planning using our technique is competitive with the state of t the art.|William Cushing,Daniel Bryce","15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri","15707|IJCAI|2001|Planning as Model Checking for Extended Goals in Non-deterministic Domains|Recent research has addressed the problem of planning in non-deterministic domains. Classical planning has also been extended to the case of goals that can express temporal properties. However, the combination of these two aspects is not trivial. In non-deterministic domains, goals should take into account the fact that a plan may result in many possible different executions and that some requirements can be enforced on all the possible executions, while others may be enforced only on some executions. In this paper we address this problem. We define a planning algorithm that generates automatically plans for extended goals in nondeterministic domains. We also provide preliminary experimental results based on an implementation of the planning algorithm that uses symbolic model checking techniques.|Marco Pistore,Paolo Traverso","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","66431|AAAI|2008|HTN-MAKER Learning HTNs with Minimal Additional Knowledge Engineering Required|We describe HTN-MAKER, an algorithm for learning hierarchical planning knowledge in the form of decomposition methods for Hierarchical Task Networks (HTNs). HTN-MAKER takes as input the initial states from a set of classical planning problems in a planning domain and solutions to those problems, as well as a set of semantically-annotated tasks to be accomplished. The algorithm analyzes this semantic information in order to determine which portions of the input plans accomplish a particular task and constructs HTN methods based on those analyses. Our theoretical results show that HTN-MAKER is sound and complete. We also present a formalism for a class of planning problems that are more expressive than classical planning. These planning problems can be represented as HTN planning problems. We show that the methods learned by HTN-MAKER enable an HTN planner to solve those problems. Our experiments confirm the theoretical results and demonstrate convergence in three well-known planning domains toward a set of HTN methods that can be used to solve nearly any problem expressible as a classical planning problem in that domain, relative to a set of goals.|Chad Hogg,Héctor Muñoz-Avila,Ugur Kuter","15121|IJCAI|1995|Fast Planning Through Planning Graph Analysis|We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a Planning Graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest-possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem.|Avrim Blum,Merrick L. Furst","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","16574|IJCAI|2007|Planning for Temporally Extended Goals as Propositional Satisfiability|Planning for temporally extended goals (TEGs) expressed as formulae of Linear-time Temporal Logic (LTL) is a proper generalization of classical planning, not only allowing to specify properties of a goal state but of the whole plan execution. Additionally, LTL formulae can be used to represent domain-specific control knowledge to speed up planning. In this paper we extend SATbased planning for LTL goals (akin to bounded LTL model-checking in verification) to partially ordered plans, thus significantly increasing planning efficiency compared to purely sequential SAT planning. We consider a very relaxed notion of partial ordering and show how planning for LTL goals (without the next-time operator) can be translated into a SAT problem and solved very efficiently. The results extend the practical applicability of SATbased planning to a wider class of planning problems. In addition, they could be applied to solving problems in bounded LTL model-checking more efficiently.|Robert Mattmüller,Jussi Rintanen","14569|IJCAI|1989|Solving Time-Dependent Planning Problems|A planning problem is time-dependent, if the time spent planning affects the utility of the system's performance. In Dean and Boddy, , we define a framework for constructing solutions to time-dependent planning problems, called expectation-driven iterative refinement. In this paper, we analyze and solve a moderately complex time-dependent planning problem involving path planning for a mobile robot, as a way of exploring a methodology for applying expectation-driven iterative refinement. The fact that we construct a solution to the proposed problem without appealing to luck or extraordinary inspiration provides evidence that expectation-driven iterative refinement is an appropriate framework for solving time-dependent planning problems.|Mark S. Boddy,Thomas Dean","16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"],["66038|AAAI|2007|Probabilistic Community Discovery Using Hierarchical Latent Gaussian Mixture Model|Complex networks exist in a wide array of diverse domains, ranging from biology, sociology, and computer science. These real-world networks, while disparate in nature, often comprise of a set of loose clusters(a.k.a communities), whose members are better connected to each other than to the rest of the network. Discovering such inherent community structures can lead to deeper understanding about the networks and therefore has raised increasing interests among researchers from various disciplines. This paper describes GWN-LDA (Generic weighted network-Latent Dirichlet Allocation) model, a hierarchical Bayesian model derived from the widely-received LDA model, for discovering probabilistic community profiles in social networks. In this model, communities are modeled as latent variables and defined as distributions over the social actor space. In addition, each social actor belongs to every community with different probability. This paper also proposes two different network encoding approaches and explores the impact of these two approaches to the community discovery performance. This model is evaluated on two research collaborative networks CiteSeer and NanoSCI. The experimental results demonstrate that this approach is promising for discovering community structures in large-scale networks.|Haizheng Zhang,C. Lee Giles,Henry C. Foley,John Yen","16499|IJCAI|2007|Computational Aspects of Analyzing Social Network Dynamics|Motivated by applications such as the spread of epidemics and the propagation of influence in social networks, we propose a formal model for analyzing the dynamics of such networks. Our model is a stochastic version of discrete dynamical systems. Using this model, we formulate and study the computational complexity of two fundamental problems (called reachability and predecessor existence problems) which arise in the context of social networks. We also point out the implications of our results on other computational models such as Hopfield networks, communicating finite state machines and systolic arrays.|Christopher L. Barrett,Harry B. Hunt III,Madhav V. Marathe,S. S. Ravi,Daniel J. Rosenkrantz,Richard Edwin Stearns,Mayur Thakur","16071|IJCAI|2005|Sensitivity Analysis in Markov Networks|This paper explores the topic of sensitivity analysis in Markov networks, by tackling questions similar to those arising in the context of Bayesian networks the tuning of parameters to satisfy query constraints, and the bounding of query changes when perturbing network parameters. Even though the distribution induced by a Markov network corresponds to ratios of multi-linear functions, whereas the distribution induced by a Bayesian network corresponds to multi-linear functions, the results we obtain for Markov networks are as effective computationally as those obtained for Bayesian networks. This similarity is due to the fact that conditional probabilities have the same functional form in both Bayesian and Markov networks, which turns out to be the more influential factor. The major difference we found, however, is in how changes in parameter values should be quantified, as such parameters are interpreted differently in Bayesian networks and Markov networks.|Hei Chan,Adnan Darwiche","16509|IJCAI|2007|Adaptation of Organizational Models for Multi-Agent Systems Based on Max Flow Networks|Organizational models within multi-agent systems literature are of a static nature. Depending upon circumstances adaptation of the organizational model can be essential to ensure a continuous successful function of the system. This paper presents an approach based on max flow networks to dynamically adapt organizational models to environmental fluctuation. First, a formal mapping between a well-known organizational modeling framework and max flow networks is presented. Having such a mapping maintains the insightful structure of an organizational model whereas specifying efficient adaptation algorithms based on max flow networks can be done as well. Thereafter two adaptation mechanisms based on max flow networks are introduced each being appropriate for different environmental characteristics.|Mark Hoogendoorn","14815|IJCAI|1991|Integration of Neural Networks and Expert Systems for Process Fault Diagnosis|The main thrust of this research is the development of an artificial intelligence (AI) system to be used as an operators' aid in the diagnosis of faults in large-scale chemical process plants. The operator advisory system involves the integration of two fundamentally different AI techniques expert systems and neural networks. A diagnostic strategy based on the hierarchical use of neural networks is used as a first level filter to diagnose faults commonly encountered in chemical process plants. Once the faults are localized within the process by the neural networks, the deep knowledge expert system analyzes the results, and either confirms the diagnosis or offers alternative solutions. The model-based expert system contains information of the plant's structure and function within its object-oriented knowledge base. The diagnostic strategy can handle novel or previously unencountered faults, noisy process sensor measurements, and multiple faults. The operator advisory system is demonstrated using a multi-column distillation plant as a case study.|Warren R. Becraft,Peter L. Lee,Robert B. Newell","15372|IJCAI|1997|On the Role of Hierarchy for Neural Network Interpretation|In this paper, we concentrate on the expressive power of hierarchical structures in neural networks. Recently, the so-called SplitNet model was introduced. It develops a dynamic network structure based on growing and splitting Kohonen chains and it belongs to the class of topology preserving networks. We briefly introduce the basics of this model and explain the different sources of information built up during the training phase, namely the neuron distribution, the final topology of the network, and the emerging hierarchical structure. In contrast to most other neural models in which the structure is only a means to get desired results, in SplitNet the structure itself is part of the aim. Our focus then lies on the interpretation of the hierarchy produced by the training algorithm and we relate our findings to a common data analysis method, the hierarchical cluster analysis. We illustrate the results of network application to a real medical diagnosis and monitoring task in the domain of nerve lesions of the human hand.|Jürgen Rahmel,Christian Blum,Peter Hahn","16068|IJCAI|2005|The Inferential Complexity of Bayesian and Credal Networks|This paper presents new results on the complexity of graph-theoretical models that represent probabilities (Bayesian networks) and that represent interval and set valued probabilities (credal networks). We define a new class of networks with bounded width, and introduce a new decision problem for Bayesian networks, the maximin a posteriori. We present new links between the Bayesian and credal networks, and present new results both for Bayesian networks (most probable explanation with observations, maximin a posteriori) and for credal networks (bounds on probabilities a posteriori, most probable explanation with and without observations, maximum a posteriori).|Cassio Polpo de Campos,Fabio Gagliardi Cozman","15375|IJCAI|1997|PRISM A Language for Symbolic-Statistical Modeling|We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs.|Taisuke Sato,Yoshitaka Kameya","15275|IJCAI|1995|Local Learning in Probabilistic Networks with Hidden Variables|Probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks Because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process, thereby significantly improving the learning rate Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.|Stuart J. Russell,John Binder,Daphne Koller,Keiji Kanazawa","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka"],["65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","15529|IJCAI|1999|Transduction with Confidence and Credibility|In this paper we follow the same general ideology as in Gammerman et al., , and describe a new transductive learning algorithm using Support Vector Machines. The algorithm presented provides confidence values for its predicted classifications of new examples. We also obtain a measure of \"credibility\" which serves as an indicator of the reliability of the data upon which we make our prediction. Experiments compare the new algorithm to a standard Support Vector Machine and other transductive methods which use Support Vector Machines, such as Vapnik's margin transduction. Empirical results show that the new algorithm not only produces confidence and credibility measures, but is comparable to, and sometimes exceeds the performance of the other algorithms.|Craig Saunders,Alexander Gammerman,Volodya Vovk","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","15177|IJCAI|1995|Advances of the DBLearn System for Knowledge Discovery in Large Databases|A prototyped data mining system, DBLearn, was developed in Simon Fraser Univ., which integrates machine learning methodologies with database technologies and efficiently and effectively extracts characteristic and discriminant rules from relational databases. Further developments, of DBLearn lead to a new generation data mining system DBMiner, with the following features () mining new kinds of rules from large databases, including multiple-level association rules, classification rules, cluster description rules, etc., () automatic generation and refinement of concept hierarchies, () high level SQL-like and graphical data mining interfaces, and () clientserver architecture and performance improvements for large applications. The major features of the system are demonstrated with experiments in a research grant information database.|Jiawei Han,Yongjian Fu,Simon Tang","65672|AAAI|2006|Integrated AI in Space The Autonomous Sciencecraft on Earth Observing One|The Earth Observing One spacecraft has been under the control of AI software for several years - experimentally since  and since November  as the primary operations system. This software includes model-based planning and scheduling, procedural execution, and event detection software learned by support vector machine (SVM) techniques. This software has enabled a x increase in the mission science return per data downlinked and a $Myear reduction in operations costs. In this paper we discuss the AI software used, the impact of the software, and lessons learned with implications for future AI research.|Steve Chien","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","65867|AAAI|2006|Closest Pairs Data Selection for Support Vector Machines|This paper presents data selection procedures for support vector machines (SVM). The purpose of data selection is to reduce the dataset by eliminating as many non support vectors (non-SVs) as possible. Based on the fact that support vectors (SVs) are those vectors close to the decision boundary, data selection keeps only the closest pair vectors of opposite classes. The selected dataset will replace the full dataset as the training component for any standard SVM algorithm.|Chaofan Sun","66870|AAAI|2010|Algorithms for Finding Approximate Formations in Games|Practical applications call for efficient model selection criteria for multiclass support vector machine (SVM) classification. To solve this problem, this paper develops two model selection criteria by combining or redefining the radius-margin bound used in binary SVMs. The combination is justified by linking the test error rate of a multiclass SVM with that of a set of binary SVMs. The redefinition, which is relatively heuristic, is inspired by the conceptual relationship between the radius-margin bound and the class separability measure. Hence, the two criteria are developed from the perspective of model selection rather than a generalization of the radius-margin bound for multiclass SVMs. As demonstrated by extensive experimental study, the minimization of these two criteria achieves good model selection on most data sets. Compared with the k-fold cross validation which is often regarded as a benchmark, these two criteria give rise to comparable performance with much less computational overhead, particularly when a large number of model parameters are to be optimized.|Patrick R. Jordan,Michael P. Wellman","66498|AAAI|2008|Structure Learning on Large Scale Common Sense Statistical Models of Human State|Research has shown promise in the design of large scale common sense probabilistic models to infer human state from environmental sensor data. These models have made use of mined and preexisting common sense data and traditional probabilistic machine learning techniques to improve recognition of the state of everyday human life. In this paper, we demonstrate effective techniques for structure learning on graphical models designed for this domain, improving the SRCS system of (Pentney et al. ) by learning additional dependencies between variables. Because the models used for common sense reasoning typically involve a large number of variables, issues of scale arise in searching for additional dependencies we discuss how we use data mining techniques to address this problem. We show experimentally that these techniques improve the accuracy of state prediction, and that, with a good prior model, the use of a common sense model with structure learning provides better prediction of unlabeled variables as well as labeled variables. The results also demonstrate that it is possible to collect new common sense information about daily life using such a statistical model and labeled data.|William Pentney,Matthai Philipose,Jeff A. Bilmes","16796|IJCAI|2007|Parametric Kernels for Sequence Data Analysis|A key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. Many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. However, finding an effective transformation scheme for sequence (e.g. time series) data is a difficult task. In this paper, we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwritten character recognition and object recognition from sensor readings. Ordering information is represented by values of a parameter associated with each input data element. A similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a Mercer kernel suitable for use in methods such as support vector machine (SVM). This scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. We apply our method to object and handwritten character recognition tasks and compare against current approaches. The results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. Our contribution is the introduction of a general technique for designing SVM kernels tailored for the classification of sequence data.|Young-In Shin,Donald S. Fussell"],["15867|IJCAI|2003|A Revised Algorithm for Latent Semantic Analysis|The intelligent tutoring system AutoTutor uses latent semantic analysis to evaluate student answers to the tutor's questions. By comparing a student's answer to a set of expected answers, the system determines how much information is covered and how to continue the tutorial. Despite the success of LSA in tutoring conversations, the system sometimes has difficulties determining at an early stage whether or not an expectation is covered. A new LSA algorithm significantly improves the precision of AutoTutor's natural language understanding and can be applied to other natural language understanding applications.|Xiangen Hu,Zhiqiang Cai,Max M. Louwerse,Andrew Olney,Phanni Penumatsa,Arthur C. Graesser","15970|IJCAI|2003|A Logic-based Algorithm for Image Sequence Interpretation and Anchoring|This paper describes a logic-based framework for interpretation of sequences of scenes captured by a stereo vision system of a mobile robot. An algorithm for anchoring and interpretation of such sequences is also proposed.|Paulo Santos,Murray Shanahan","16093|IJCAI|2005|Learning Forward Models for Robots|Forward models enable a robot to predict the effects of its actions on its own motor system and its environment. This is a vital aspect of intelligent behaviour, as the robot can use predictions to decide the best set of actions to achieve a goal. The ability to learn forward models enables robots to be more adaptable and autonomous this paper describes a system whereby they can be learnt and represented as a Bayesian network. The robot's motor system is controlled and explored using 'motor babbling'. Feedback about its motor system comes from computer vision techniques requiring no prior information to perform tracking. The learnt forward model can be used by the robot to imitate human movement.|Anthony M. Dearden,Yiannis Demiris","15038|IJCAI|1993|Vision Based Robot Behavior Tools and Testbeds for Real-World AI Research|Vision is a key function not only for robotics but also for AI more generally. Today realtime visual processing is becoming possible this means that vision based behavior can become more dynamic, opening fertile areas for applications. One aspect of this is real-time visual tracking. We have built a real-time tracking vision system and incorporated it in an integrated robot programming environment. Using this, we have performed experiments in vision based robot behavior and human-robot interaction. In particular, we have developed a robotic system capable of \"learning by seeing\". In general, it is important for the AI community not to lose sight of the problems and progress of robotics. After all, an AI system which acts in real-time in the real-world is no less (and no more) than an intelligent robot.|Hirochika Inoue","65871|AAAI|2006|Deeper Natural Language Processing for Evaluating Student Answers in Intelligent Tutoring Systems|This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.|Vasile Rus,Arthur C. Graesser","16308|IJCAI|2005|Clinical-Reasoning Skill Acquisition through Intelligent Group Tutoring|This paper describes COMET, a collaborative intelligent tutoring system for medical problembased learning. COMET uses Bayesian networks to model individual student knowledge and activity, as well as that of the group. Generic domainindependent tutoring algorithms use the models to generate tutoring hints. We present an overview of the system and then the results of two evaluation studies. The validity of the modeling approach is evaluated in the areas of head injury, stroke and heart attack. Receiver operating characteristic (ROC) curve analysis indicates that, the models are accurate in predicting individual student actions. Comparison of learning outcomes shows that student clinical reasoning gains from our system are significantly higher than those obtained from human tutored sessions (Mann-Whitney, p  .).|Siriwan Suebnukarn,Peter Haddawy","13869|IJCAI|1981|An Information Presentation System|AIPS is a system for graphically presenting information. It promotes a high degree of interactivity between a user and a knowledge base or knowlege-based system, and is designed to be utmost domain independent and extensible. This paper describes the concept of an Information Presentation System (IPS), the intimate relationship between IPS goals and knowledge representation issues, and some of the architecture of AIPS.|Frank Zdybel,Norton Greenfeld,Martin D. Yonke,Jeff Gibbons","14702|IJCAI|1989|Robot Navigation|Robot navigation is a problem that encompasses most of the major areas of AI research. Machine architecture, search, knowledge acquisition and representation, planning, scheduling, reaction, perception, and of course robotics, all can play integral roles in a mobile robot navigation system. For this reason, a large part of the AI community is now interested (and has been since STRIPS was used to guide the Shakey robot) in mobile robot navigation.|David P. Miller","14835|IJCAI|1991|Integration-Kid A Learning Companion System|This paper describes a learning companion system called Integration-Kid in the domain of learning indefinite integration. A learning companion system is an intelligent tutoring system of a new breed. Apart from the teacher, a learning companion models after an additional agent, called the learning companion. The learning companion acts as a companion for the human student in learning. Thus the companion performs the learning task at about the same level as the student and both the student and the companion exchange ideas while being presented the same material by the computer teacher. The computer companion might make mistakes, just like a human student.|Tak-Wai Chan","65752|AAAI|2006|A Dynamic Mixture Model to Detect Student Motivation and Proficiency|Unmotivated students do not reap the full rewards of using a computer-based intelligent tutoring system. Detection of improper behavior is thus an important component of an online student model. To meet this challenge, we present a dynamic mixture model based on Item Response Theory. This model, which simultaneously estimates a student's proficiency and changing motivation level, was tested with data of high school students using a geometry tutoring system. By accounting for student motivation, the dynamic mixture model can more accurately estimate proficiency and the probability of a correct response. The model's generality is an added benefit, making it applicable to many intelligent tutoring systems as well as other domains.|Jeffrey Johns,Beverly Park Woolf"],["14399|IJCAI|1987|An Improved Constraint-Propagation Algorithm for Diagnosis|Diagnosing a system requires the identification of a set of components whose abnormal behavior could explain the faulty system behavior. Previously, model-based diagnosis schemes have proceeded through a cycle of assumptions - predictions observations assumptions-adjustment, where the basic assumptions entail the proper functioning of those components whose failure is not established. Here we propose a scheme in which every component's status is treated as a variable therefore, predictions covering all possible behavior of the system can be generated. Remarkably, the algorithm exhibits a drastic reduction in complexity for a large family of system-models. Additionally, the intermediate computations provide useful guidance for selecting new tests. The proposed scheme may be considered as either an enhancement of the scheme proposed in de Kleer,  or an adaptation of the probabilistic propagation scheme proposed in Pearl,  for the diagnosis of deterministic systems.|Hector Geffner,Judea Pearl","65509|AAAI|2005|Model-Based Monitoring and Diagnosis of Systems with Software-Extended Behavior|Model-based diagnosis has largely operated on hard-ware systems. However, in most complex systems today, hardware is augmented with software functions that influence the system's behavior. In this paper, hard-ware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Prior work introduced probabilistic, hierarchical, constraint-based automata (PHCA) to allow the uniform and compact encoding of both hard-ware and software behavior. This paper focuses on PHCA-based monitoring and diagnosis to ensure the robustness of complex systems. We introduce a novel approach that frames diagnosis over a finite time horizon as a soft constraint optimization problem (COP), allowing us to leverage an extensive body of efficient solution methods for COPs. The solutions to the COP correspond to the most likely evolutions of the complex system. We demonstrate our approach on a vision-based rover navigation system, and models of the SPHERES and Earth Observing One spacecraft.|Tsoline Mikaelian,Brian C. Williams,Martin Sachenbacher","13754|IJCAI|1981|How Expert Should an Expert System Be|A computer system which aids computer engineers in fault diagnosis is described. The system, called CRIB (Computer Retrieval Incidence Bank) is shown to fit into the class of pattern-directed inference systems. Emphasis is placed on the \"before\" and \"after\" phases of system generation and it is shown why, to be called an expert system, these phases are important. The forms of knowledge used in CRIB are shown to be adequate for diagnosis and yet possess little of the structural or functional knowledge of more advanced expert systems. Summaries are given of the three phases of implementation elicitation, implementation of knowledge structures, validation and improvement. The idea of an expert system as a \"model of competence\" is mentioned and the transferrance of the system architecture to software diagnosis, using the same model, is described. There are short discussions of system performance and the nature of expert systems.|Roger T. Hartley","15991|IJCAI|2003|Automatic Abstraction in Component-Based Diagnosis Driven by System Observability|The paper addresses the problem of automatic abstraction of component variables in the context of Model Based Diagnosis, in order to produce models capable of deriving fewer and more general diagnoses when the current observability of the system is reduced. The notion of indiscriminability among faults of a set of components is introduced and constitutes the basis for a formal definition of admissible abstractions which preserve all the distinctions that are relevant for diagnosis given the current observability of the system. The automatic synthesis of abstract models further restricts abstractions such that the behavior of abstract components is expressed in terms of a simple and intuitive combination of the behavior of their subcomponents. As a validation of our proposal, we present experimental results which show the reduction in the number of diagnoses returned by a diagnostic agent for a space robotic arm.|Gianluca Torta,Pietro Torasso","14815|IJCAI|1991|Integration of Neural Networks and Expert Systems for Process Fault Diagnosis|The main thrust of this research is the development of an artificial intelligence (AI) system to be used as an operators' aid in the diagnosis of faults in large-scale chemical process plants. The operator advisory system involves the integration of two fundamentally different AI techniques expert systems and neural networks. A diagnostic strategy based on the hierarchical use of neural networks is used as a first level filter to diagnose faults commonly encountered in chemical process plants. Once the faults are localized within the process by the neural networks, the deep knowledge expert system analyzes the results, and either confirms the diagnosis or offers alternative solutions. The model-based expert system contains information of the plant's structure and function within its object-oriented knowledge base. The diagnostic strategy can handle novel or previously unencountered faults, noisy process sensor measurements, and multiple faults. The operator advisory system is demonstrated using a multi-column distillation plant as a case study.|Warren R. Becraft,Peter L. Lee,Robert B. Newell","66411|AAAI|2008|Pervasive Diagnosis The Integration of Diagnostic Goals into Production Plans|In model-based control, a planner uses a system description to create a plan that achieves production goals (Fikes & Nilsson ). The same description can be used by model-based diagnosis to infer the condition of components in a system from partially informative sensors. Prior work has demonstrated that diagnosis can be used to adapt the control of a system to changes in its components. However diagnosis must either make inferences from passive observations of production, or production must be halted to take diagnostic actions. We observe that the declarative nature of model-based control allows the planner to achieve production goals in multiple ways. This exibility can be exploited with a novel paradigm we call pervasive diagnosis which produces diagnostic production plans that simultaneously achieve production goals while uncovering additional information about component health. We present an efficient heuristic search for these diagnostic production plans and show through experiments on a model of an industrial digital printing press that the theoretical increase in information can be realized on practical real-time systems. We obtain higher long-run productivity than a decoupled combination of planning and diagnosis.|Lukas Kuhn,Bob Price,Johan de Kleer,Minh Binh Do,Rong Zhou","14604|IJCAI|1989|Model-Based Monitoring of Dynamic Systems|Industrial process plants such as chemical refineries and electric power generation are examples of continuous-variable dynamic systems (CVDS) whose operation is continuously monitored for abnormal behavior. CVDSs pose a challenging diagnostic problem in which values are continuous (not discrete), relatively few parameters are observable, parameter values keep changing, and diagnosis must be performed while the system operates. We present a novel method for monitoring CVDSs which exploits the system's dynamic behavior for diagnostic clues. The key techniques are modeling the physical system with dynamic qualitativequantitative models, inducing diagnostic knowledge from qualitative simulations, continuously comparing observations against fault-model predictions, and incrementally creating and testing multiple-fault hypotheses. The important result is that the diagnosis is refined as the physical system's dynamic behavior is revealed over time.|Daniel Dvorak,Benjamin Kuipers","14766|IJCAI|1989|Physical Negation Integrating Fault Models into the General Diagnostic Engine|The General Diagnostic Engine (GDE) provides an elegant and general framework for model-based diagnosis. However, like many other diagnostic systems, GDE's device models capture only the correct, or intended, behavior of its components. It is lacking an important part of diagnostic reasoning knowledge about how components may behave when they are faulty. This fact can limit the performance of GDE considerably. We present a solution for integrating the use of fault models into GDE in a very homogeneous way, a system called GDE +. Unlike the basic GDE, it can not only exploit contradictions between the assumed correct behavior of components and the observations, but also analyze whether the faultiness of components would really explain the observations. Based on an extended version of the ATMS, GDE + is able to prove the correctness of components and to rule out implausible diagnostic hypotheses.|Peter Struss,Oskar Dressler","14900|IJCAI|1991|Integrating Model-Based Monitoring and Diagnosis of Complex Dynamic Systems|We present a new approach to model-based monitoring and diagnosis of dynamic systems. The presented DIAMON algorithm uses hierarchical models to monitor and diagnose dynamic systems. DIAMON is based on the integration of teleological parameter-based monitoring models and repair-oriented device-based diagnosis models. It combines consistency-based diagnosis with model-based monitoring and uses an extension of the QSIM-language for the representation of qualitative system models. Furthermore, DIAMON is able to detect and localize a broad range of nonpermanent faults and thus extends traditional diagnosis which exclusively deals with permanent faulty behavior. The operation of DIAMON will be demonstrated on a real-world example in a multiple-faults scenario.|Franz Lackinger,Wolfgang Nejdl","16556|IJCAI|2007|Conflict-Based Diagnosis Adding Uncertainty to Model-based Diagnosis|Consistency-based diagnosis concerns using a model of the structure and behaviour of a system in order to analyse whether or not the system is malfunctioning. A well-known limitation of consistency-based diagnosis is that it is unable to cope with uncertainty. Uncertainty reasoning is nowadays done using Bayesian networks. In this field, a conflict measure has been introduced to detect conflicts between a given probability distribution and associated data. In this paper, we use a probabilistic theory to represent logical diagnostic systems and show that in this theory we are able to determine consistent and inconsistent states as traditionally done in consistency-based diagnosis. Furthermore, we analyse how the conflict measure in this theory offers a way to favour particular diagnoses above others. This enables us to add uncertainty reasoning to consistency-based diagnosis in a seamless fashion.|Ildikó Flesch,Peter J. F. Lucas,Theo P. van der Weide"]]},"title":{"entropy":6.176459177221551,"topics":["artificial intelligence, sense disambiguation, and image, information extraction, word sense, speech understanding, matrix factorization, word disambiguation, from, and analysis, and recognition, pattern recognition, and intelligence, from data, object recognition, from image, for disambiguation, image interpretation, and artificial, the intelligence","problem solving, algorithm for, constraint satisfaction, the problem, heuristic search, search for, for planning, and search, planning with, search, planning and, local search, planning, constraint, for problem, problem, and constraint, for constraint, and problem, with constraint","natural language, the and, reasoning about, and, for system, for logic, for and, for language, the system, and logic, expert system, logic, reasoning, the, and reasoning, description logic, for the, for reasoning, logic programming, logic programs","learning, learning for, model for, reinforcement learning, for, neural networks, learning and, mobile robot, for networks, method for, bayesian networks, learning with, using, model, combinatorial auctions, decision processes, algorithm for, for robot, networks, support vector","matrix factorization, and image, from image, from data, and from, from the, learning from, from, from examples, from web, vision system, from motion, nonnegative matrix, and processing, motion and, and perception, for matching, for image, mining web, nonnegative factorization","and recognition, and analysis, pattern recognition, for recognition, image interpretation, activity recognition, the recognition, object recognition, and object, structure and, the structure, for analysis, recognition, analysis, for interpretation, structure generation, and image, the pattern, for object, scene analysis","constraint satisfaction, and constraint, for constraint, with constraint, constraint, with and, arc consistency, for csp, local global, the constraint, temporal constraint, consistency for, for with, global constraint, with, algorithm constraint, distributed constraint, and csp, for set, and application","search for, and search, heuristic search, search with, case study, theorem proving, efficient for, the search, search, and space, for space, search space, the space, decision tree, parallel for, and tree, efficient algorithm, best-first search, for temporal, temporal and","reasoning about, and reasoning, for reasoning, for and, and, the and, reasoning, reasoning with, representation and, situation calculus, reasoning the, and action, between and, about action, with and, complexity and, the representation, the calculus, temporal reasoning, the situation","natural language, for system, for language, the system, and system, expert system, language and, system, semantic for, the language, the semantic, for natural, semantic and, and natural, semantic web, expert for, for and, language system, semantic, integration and","for robot, mobile robot, for agents, control for, with robot, for mobile, and control, robot, for interactive, for environments, behavior for, and robot, learning robot, dimensionality reduction, control, robot control, and agents, agents, robot using, learning control","method for, for, algorithm for, for and, decision processes, for system, markov processes, decision tree, markov decision, dynamic for, for decision, for with, collaborative filtering, online for, feature for, predictive representation, and method, for adaptive, decision, representation for"],"ranking":[["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","13538|IJCAI|1975|Inductive Inference Theory - A Unified Approach to Problems in Pattern Recognition and Artificial Intelligence|Recent results in induction theory are reviewed that demonstrate the general adequacy of the induction system of Solomoncff and Willis. Several problems in pattern recognition and A.I. are investigated through these methods. The theory is used to obtain the a priori probabilities that are necessary in the application cf stochastic languages to pattern recognition. A simple, quantitative solution is presented for part of Winston's problem of learning structural descriptions from exandples. In contrast to work in non-probabilistic prediction, the present methods give probability values that can be used with decision. theory to make critical decisions.|R. Solomonott","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Kornél G. Markó,Stefan Schulz,Udo Hahn","15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin","13328|IJCAI|1971|Pattern Recognition by an Artificial Tactile Sense|This paper proposes an artificial tactile pattern recognition system, which combines recognition by touching the object surface with an artificial tactile sense and recognition by grasping the object with an artificial hand. The inspiration for this proposition was found in the function of the tactile sense of a human hand. The fundermental principle of artificial tactile pattern recognition is to process a stress distribution that the unknown object produces in the artificial tactile sense elements. In the proposed method, the -dimensional stress distribution is partitioned into a -dimensional peripheral pattern and a threshold decrement by analogy with threshold phenomena in the living body. The object surface is recognized as a sequence of the peripheral processings at each threshold decrement. A simple experiment classifying cylinders and square pillars was performed by the artificial hand with on-off switches instead of the pressure sense elements. As the result, a high reliability of recognition is obtained.|Gen-ichiro Kinoshita,Shuhei Aida,Masahiro Mori","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["66094|AAAI|2007|Synthesis of Constraint-Based Local Search Algorithms from High-Level Models|The gap in automation between MIPSAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location.|Pascal Van Hentenryck,Laurent D. Michel","66116|AAAI|2007|Solving a Stochastic Queueing Design and Control Problem with Constraint Programming|A facility with front room and back room operations has the option of hiring specialized or, more expensive, cross-trained workers. Assuming stochastic customer arrival and service times, we seek a smallest-cost combination of cross-trained and specialized workers satisfying constraints on the expected customer waiting time and expected number of workers in the back room. A constraint programming approach using logic-based Benders' decomposition is presented. Experimental results demonstrate the strong performance of this approach across a wide variety of problem parameters. This paper provides one of the first links between queueing optimization problems and constraint programming.|Daria Terekhov,J. Christopher Beck,Kenneth N. Brown","14099|IJCAI|1985|A Study of Search Methods The Effect of Constraint Satisfaction and Adventurousness|This research addresses how constraint satisfaction interacts with the search mode, and how the ratio of breadth of effort to depth of effort can be controlled. Four search paradigms, each the best of its kind for non adversary problems, are investigated. One is depth first, and the others best first. All methods except one highly informed best first search use the same knowledge, and each of these methods is tested with and without the use of a constraint satisfaction procedure on sets of progressively more difficult problems. As expected, the most informed search does better than the less informed as the problems get more difficult. Constraint satisfaction is found to have a pronouncedly greater effect when coupled with the most informed algorithm. Large performance increments over A* can be produced by the use of a coefficient associated with the h term, and this algorithm produces solutions that are only % worse than optimal. This is a known phenomenon however, the range of this coefficient is very narrow. We term this coefficient, which controls the ratio of depth of effort to breadth of effort, the adventurousness coefficient. The less tractable a problem the greater the adventurousness should be. We present evidence to support this.|Hans J. Berliner,Gordon Goetsch","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","14338|IJCAI|1987|An Investigation of Opportunistic Constraint Satisfaction in Space Planning|We are investigating constraint directed heuristic search as a means for performing design in the field of space planning. Space planning is selecting, dimensioning, locating and shaping design units to create two dimensional layouts based on functional, topological and geometrical considerations. Search is carried out using operators at different abstraction levels and design objects at different levels of detail. Constraints are used to represent domain knowledge, to define the search space by specifying operators in means ends analysis manner, and to rate the partial candidate solutions using importances associated with each constraint. Search is carried out opportunistically. The philosophy behind opportunism is that understanding the approximate topology of the search space will lead to efficient search. Uncertainty associated with constraints is derived and used to identify islands of certainty in the search space, which are used as starting points and anchors for search. The knowledge that enables us to identify opportunistic decisions are interactions between constraints and the usefulness of a constraint in different situations. The resulting uncertainty measure will be tested by observing the problem solving behavior it causes in different search spaces.|Can A. Baykan,Mark S. Fox","16229|IJCAI|2005|Combination of Local Search Strategies for Rotating Workforce Scheduling Problem|Rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work places (e.g. industrial plants). Solving this problem is of a high practical relevance. In this paper we propose the combination of tabu search with random walk and min conflicts strategy to solve this problem. Computational results for benchmark examples in literature and other real life examples show that combination of tabu search with random walk and min conflicts strategy improves the performance of tabu search for this problem. The methods proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedules.|Nysret Musliu","13695|IJCAI|1981|Tuning of Search of the Problem Space for Geometry Proofs|In planning a proof, a student searches through a space of inferences leading forward from the givens of the problem and backward from the to-be-proven statement. One dimension of growth of expertise is that students become more tuned in the search of this problem space. This can be shown to result from the application of various learning operators to production embodiments of the inference rules. Rules are evaluated after the solution of a problem according to whether they led to or led away from the solution. Rules that contributed to a solution are strengthened and an attempt is made to formulate general versions of these rules that will apply in other situations. Rules that led away from the solution are weakened and a discrimination process is evoked to try to add features to the rules that will try to restrict them to the correct circumstances of application. Composition is a learning process that collapses successful sequences of rule operations into single macro-rule productions. There is also a process that converts the backward reasoning rules formed by composition into forward reasoning rules. The effect of these learning processes is to put into production conditions tests for problem features that are heunstically predictive of the rule's success.|John R. Anderson","15377|IJCAI|1997|Combining Local Search and Look-Ahead for Scheduling and Constraint Satisfaction Problems|We propose a solution technique for scheduling and constraint satisfaction problems that combines backtracking-free constructive methods and local search techniques. Our technique incrementally constructs the solution, performing a local search on partial solutions each time the construction reaches a dead-end. Local search on the space of partial solutions is guided by a cost function based on three components the distance to feasibility of the partial solution, a look-ahead factor, and (for optimization problems) a lower bound of the objective function. In order to improve search effectiveness, we make use of an adaptive relaxation of constraints and an interleaving of different lookahead factors. The new technique has been successfully experimented on two real-life problems university course scheduling and sport tournament scheduling.|Andrea Schaerf","14053|IJCAI|1983|Flexible Learning of Problem Solving Heuristics Through Adaptive Search|Noting that the methods employed by existing learning systems are often bound to the intended task domain and have little applicability outside that domain, this paper considers an alternative learning system design that offers greater flexibility without sacrificing performance. An operational prototype, constructed around a powerful adaptive search technique, is presented and applied to the problem of acquiring problem solving heuristics through experience. Some performance results obtained with the system in a poker betting domain are reported and compared with those of a previously investigated learning system in the same domain. It is seen that comparable levels of performance are achieved by the two systems, despite the latter's dependence on a considerable amount of domain specific knowledge for effective operation.|Stephen F. Smith","66469|AAAI|2008|Anytime Local Search for Distributed Constraint Optimization|Most former studies of Distributed Constraint Optimization Problems (DisCOPs) search considered only complete search algorithms, which are practical only for relatively small problems. Distributed local search algorithms can be used for solving DisCOPs. However, because of the differences between the global evaluation of a system's state and the private evaluation of states by agents, agents are unaware of the global best state which is explored by the algorithm. Previous attempts to use local search algorithms for solving DisCOPs reported the state held by the system at the termination of the algorithm, which was not necessarily the best state explored. A general framework for implementing distributed local search algorithms for DisCOPs is proposed. The proposed framework makes use of a BFS-tree in order to accumulate the costs of the system's state in its different steps and to propagate the detection of a new best step when it is found. The resulting framework enhances local search algorithms for DisCOPs with the anytime property. The proposed framework does not require additional network load. Agents are required to hold a small (linear) additional space (beside the requirements of the algorithm in use). The proposed framework preserves privacy at a higher level than complete Dis-COP algorithms which make use ofa pseudo-tree (ADOPT, DPOP).|Roie Zivan"],["13947|IJCAI|1983|Logic Modelling of Cognitive Reasoning|Logic modelling is presented as an approach for exploring cognitive reasoning. The notion of mental construction and execution of propositional models is introduced. A model is constructed through inclusions and exclusions of assertions and assumptions about the task. A constructed model is executed in a logical control structure. Formal rules of inference are argued to be an essential feature of this architecture. A few examples are given for purpose of illustration.|Göran Hagert,\u2026ke Hansson","15791|IJCAI|2003|A Logic For Causal Reasoning|We introduce a logical formalism of irreflexive casual production relations that possesses both a standard monotonic semantics, and a natural nonmonotonic semantics. The formalism is shown to provide a complete characterization for the casual reasoning behind casual theories from McCain and Turner, . It is shown also that any causal relation is reducible to its Horn sub-relation with respect to the nonmonotonic semantics. We describe also a general correspondence between casual relations and abductive systems, which shows, in effect, that casual relations allow to express abductive reasoning. The results of the study seem to suggest causal production relations as a viable general framework for nonmonotonic reasoning.|Alexander Bochman","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16866|IJCAI|2009|A Logic for Reasoning about Counterfactual Emotions|The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. An example of this kind of emotions is regret. In order to meet this objective, we exploit the well-known STIT logic Belnap et al.,  Horty, . STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable. In this work we study a decidable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions.|Emiliano Lorini,François Schwarzentruber","15759|IJCAI|2001|EPDL A Logic for Causal Reasoning|This paper presents an extended system EPDL of propositional dynamic logic by allowing a proposition as a modality for representing and specifying direct and indirect effects of actions in a unified logical structure. A set of causal logics based on the framework are proposed to model causal propagations through logical relevancy and iterated effects of causation. It is shown that these logics capture the basic properties of causal reasoning.|Dongmo Zhang,Norman Y. Foo","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","14109|IJCAI|1985|A Logic Programming and Verification System for Recursive Quantificational Logic|In this paper, we describe a logic programming and program verification system which is based on quantifier elimination techniques and axiomatization rather than on more common method of doing logic programming using the Herbrand-Prawitz-Robinson unification algorithm without occur-check. This system is shown to have interesting properties for logic programming and includes a number of advanced features. Among these features are user-defined data objects, user-defined recursive relations and functions, either of which may involve quantifiers in the body of their definitions, and automatic termination and consistency checking for recursively defined concept. In addition, it has a correct implementation of negation in contrast to PROLOG implementation of negation as failure, a smooth interaction between LISP-like functions and PROLOG-like relations, and a smooth interaction between specifications and programs. Finally, it provides a method of mathematical induction applicable to recursive definitions involving quantifiers.|Frank M. Brown,Peiya Liu","14466|IJCAI|1987|A Parsing System Based on Logic Programming|The paper presents a practical parsing system based on logic programming. A restricted Definite Clause Grammar is assumed as grammar description and the grammar is translated into a parsing program written in Prolog. The system employs a bottom-up parsing strategy with top-down prediction. The major advantages of our system are that the system works in a bottom-up manner so that the left-recursive rules do not cause difficulties, the parsing process does not involve backtracking, and there is no duplicated construction of same syntactic structures. Experiments are shown to estimate the efficiency of the system.|Yuji Matsumoto,Ryôichi Sugimura"],["15381|IJCAI|1997|An Effective Learning Method for Max-Min Neural Networks|Max and min operations have interesting properties that facilitate the exchange of information between the symbolic and real-valued domains. As such, neural networks that employ max-min activation functions have been a subject of interest in recent years. Since max-min functions are not strictly differentiate, we propose a mathematically sound learning method based on using Fourier convergence analysis of side-derivatives to derive a gradient descent technique for max-min error functions. This method is applied to a \"typical\" fuzzy-neural network model employing max-rnin activation functions. We show how this network can be trained to perform function approximation its performance was found to be better than that of a conventional feedforward neural network.|Loo-Nin Teow,Kia-Fock Loe","16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","17031|IJCAI|2009|Learning Conditional Preference Networks with Queries|We investigate the problem of eliciting CP-nets in the well-known model of exact learning with equivalence and membership queries. The goal is to identify a preference ordering with a binary-valued CP-net by guiding the user through a sequence of queries. Each example is a dominance test on some pair of outcomes. In this setting, we show that acyclic CP-nets are not learnable with equivalence queries alone, while they are learnable with the help of membership queries if the supplied examples are restricted to swaps. A similar property holds for tree CP-nets with arbitrary examples. In fact, membership queries allow us to provide attribute-efficient algorithms for which the query complexity is only logarithmic in the number of attributes. Such results highlight the utility of this model for eliciting CP-nets in large multi-attribute domains.|Frédéric Koriche,Bruno Zanuttini","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik","66607|AAAI|2010|Latent Variable Model for Learning in Pairwise Markov Networks|In this paper, an efficient rate-control scheme for H.AVC video encoding is proposed. The redesign of the quantization scheme in H.AVC results in that the relationship between the quantization parameter and the true quantization stepsize is no longer linear. Based on this observation, we propose a new rate-distortion (R-D) model by utilizing the true quantization stepsize and then develop an improved rate-control scheme for the H.AVC encoder based on this new R-D model. In general, the current R-D optimization (RDO) mode-selection scheme in H.AVC test model is difficult for rate control, because rate control usually requires a predetermined set of motion vectors and coding modes to select the quantization parameter, whereas the RDO does in the different order and requires a predetermined quantization parameter to select motion vectors and coding modes. To tackle this problem, we develop a complexity-adjustable rate-control scheme based on the proposed R-D model. Briefly, the proposed scheme is a one-pass process at frame level and a partial two-pass process at macroblock level. Since the number of macroblocks with the two-pass processing can be controlled by an encoder parameter, the fully one-pass implementation is a subset of the proposed algorithm. An additional topic discussed in this paper is about video buffering. Since a hypothetical reference decoder (HRD) has been defined in H.AVC to guarantee that the buffers never overflow or underflow, the more accurate rate-allocation schemes are proposed to satisfy these requirements of HRD.|Saeed Amizadeh,Milos Hauskrecht","14535|IJCAI|1987|Learning Phonetic Features Using Connectionist Networks|A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectraltemporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. A simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated % of  other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items.|Raymond L. Watrous,Lokendra Shastri","15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller","66549|AAAI|2008|Reinforcement Learning for Vulnerability Assessment in Peer-to-Peer Networks|Proactive assessment of computer-network vulnerability to unknown future attacks is an important but unsolved computer security problem where AI techniques have significant impact potential. In this paper, we investigate the use of reinforcement learning (RL) for proactive security in the context of denial-of-service (DoS) attacks in peer-to-peer (PP) networks. Such a tool would be useful for network administrators and designers to assess and compare the vulnerability of various network configurations and security measures in order to optimize those choices for maximum security. We first discuss the various dimensions of the problem and how to formulate it as RL. Next we introduce compact parametric policy representations for both single attacker and botnets and derive a policy-gradient RL algorithm. We evaluate these algorithms under a variety of network configurations that employ recent fair-use DoS security mechanisms. The results show that nur RL-based approach is able to significantly outperform a number of heuristic strategies in terms of the severity of the attacks discovered. The results also suggest some possible network design lessons for reducing the attack potential of an intelligent attacker.|Scott Dejmal,Alan Fern,Thinh Nguyen","65014|AAAI|1987|Modular Learning in Neural Networks|In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.|Dana H. Ballard"],["14360|IJCAI|1987|Image Reconstruction from Zero-Crossings|In the computational theory of human vision, developed by D. Marr and his school, zero-crossings detection is proposed as one of the first stages of human visual information processing. The justification of this theory reguires that the zero crossings are rich in information about the original image. In a paper, T. Poggio, H.K. Nishihara and K.R.K. Nielsen stated that a successful extension of B. Logan's analysis to two-dimensional images represents one of the critical unsolved steps of this computational theory. Moreover, such an extension is crucial to the question of spatiotemporal interpolation in vision. In this paper, we shall provide a solution to this problem using a theory of spherical perspective and spherical harmonics.|Su-Shing Chen","15692|IJCAI|2001|Mining Soft-Matching Rules from Textual Data|Text mining concerns the discovery of knowledge from unstructured textual data. One important task is the discovery of rules that relate specific words and phrases. Although existing methods for this task learn traditional logical rules, soft-matching methods that utilize word-frequency information generally work better for textual data. This paper presents a rule induction system, TEXTRISE, that allows for partial matching of text-valued features by combining rule-based and instance-based learning. We present initial experiments applying TEXTRISE to corpora of book descriptions and patent documents retrieved from the web and compare its results to those of traditional rule and instance based methods.|Un Yong Nahm,Raymond J. Mooney","66437|AAAI|2008|MakeD Depth Perception from a Single Still Image|Humans have an amazing ability to perceive depth from a single still image however, it remains a challenging problem for current computer vision systems. In this paper, we will present algorithms for estimating depth from a single still image. There are numerous monocular cues--such as texture variations and gradients, defocus, colorhaze, etc.--that can be used for depth perception. Taking a supervised learning approach to this problem, in which we begin by collecting a training set of single images and their corresponding ground-truth depths, we learn the mapping from image features to the depths. We then apply these ideas to create -d models that are visually-pleasing as well as quantitatively accurate from individual images. We also discuss applications of our depth perception algorithm in robotic navigation, in improving the performance of stereovision, and in creating large-scale -d models given only a small number of images.|Ashutosh Saxena,Min Sun,Andrew Y. Ng","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","66512|AAAI|2008|Mining Translations of Web Queries from Web Click-through Data|Query translation for Cross-Lingual Information Retrieval (CLIR) has gained increasing attention in the research area. Previous work mainly used machine translation systems, bilingual dictionaries, or web corpora to perform query translation. However, most of these approaches require either expensive language resources or complex language models, and cannot achieve timely translation for new queries. In this paper, we propose a novel solution to automatically acquire query translation pairs from the knowledge hidden in the click-through data, that are represented by the URL a user clicks after submitting a query to a search engine. Our proposed solution consists of two stages identitying bilingual URL pair patterns in the click-through data and matching query translation pairs based on user click behavior. Experimental results on a real dataset show that our method not only generates existing query translation pairs with high precision, but also generates many timely query translation pairs that could not be obtained by previous methods. A comparative study between our system and two commercial online translation systems shows the advantage of our proposed method.|Rong Hu,Weizhu Chen,Jian Hu,Yansheng Lu,Zheng Chen,Qiang Yang","13792|IJCAI|1981|The Interpretation of Three-Dimensional Structure from Image Curves|Genera constraints on the interpretation of image curves are described and implemented We illustrate the use of these constraints to interpret three dimensional structure from an image up to the volumetric level These constraints do not use any knowledge regarding the specific objects in the image, but rely on general assumptions regarding illumination, occlusion, object geometry, and the imaging process They are based on coincidence assumptions that various coincidental lcatures or alignments in an image are unlikely to arise without reason. The strength of these coincidence assumptions depends on the accuracy of the low-level description of an image. Since any one feature can be the result of pure coincidence or errors in the detection process, it is necessary to use a reasoning system which can use these hypotheses to derive consistent global interpretations, while maintaining the ability to remove the implications of hypotheses which are disproved in the face of further evidence We demonstrate the successful interpretation of some hand derived image curves up to the volumetric level, including the construction of a three- space model.|David G. Lowe,Thomas O. Binford","14175|IJCAI|1985|Spatial Object Perception from an Image|In this paper we address the problem of finding the spatial position and orientation of an object from a single image. It is assumed that the image formation process and an object model are known in advance. Sets of image lines are backprojected and constraints on their spatial interpretations are derived. A search space is then constructed where each node represents a space feature with a model assignment. Next, a hypothesize-and-test recognition strategy is used to select a solution, that is to determine six degrees of freedom of a part from a set of features. Finally we discuss the efficiency and the reliability of the method.|Radu Horaud","14239|IJCAI|1985|Determining -D Motion of Planar Objects from Image Brightness Patterns|The brightness patterns in two successive image frames are used to recover the motion of a planar object without computing the optical flow as an intermediate step. Based on a least-squares formulation, a set of nine nonlinear equations are derived. A simple iterative scheme for solving these equations is presented. Using a selected example, it is shown that in general, the scheme may converge to cither of two possible solutions depending on the initial condition. Only in the special case where the translational motion vector is perpendicular to the surface does our algorithm converge to a unique solution.|Shahriar Negahdaripour,Berthold K. P. Horn","66616|AAAI|2010|Commonsense Knowledge Mining from the Web|In this article, two learning classifier systems based on evolutionary techniques are described to classify remote sensing images. Usually, these images contain voluminous, complex, and sometimes erroneous and noisy data. The first approach implements ICU, an evolutionary rule discovery system, generating simple and robust rules. The second approach applies the real-valued accuracy-based classification system XCSR. The two algorithms are detailed and validated on hyperspectral data.|Chi-Hsin Yu,Hsin-Hsi Chen","16095|IJCAI|2005|Inferring Image Templates from Classification Decisions|Assuming human image classification decisions are based on estimating the degree of match between a small number of stored internal templates and certain regions of the input images, we present an algorithm which infers observers classification templates from their classification decisions on a set of test images. The problem is formulated as learning prototypes from labeled data under an adjustable, prototype-specific elliptical metric. The matrix of the elliptical metric indicates the pixels that the template responds to. The model was applied to human psychophysical data collected in a simple image classification experiment.|Arnab Dhua,Florin Cutzu"],["16159|IJCAI|2005|Combining Structural Descriptions and Image-based Representations for Image Object and Scene Recognition|Object and scene learning and recognition is a major issue in computer vision, in robotics and in cognitive sciences. This paper presents the principles and results of an approach which extracts structured view-based representations for multi-purpose recognition. The structures are hierarchical and distributed and provide for generalization and categorization. A tracking process enables to bind views over time and to link consecutive views. Scenes can also be recognized using objects as components. Illustrative results are presented.|Nicolas Do Huu,Williams Paquier,Raja Chatila","14827|IJCAI|1991|HyperBF Networks for Real Object Recognition|Even if represented in a way which is invariant to illumination conditions, a D object gives rise to an infinite number of D views, depending on its pose. It has been recently shown () that it is possible to synthesize a module that can recognize a specific D object from any viewpoint, by using a new technique of learning from examples, which are, in this case, a small set of D views of the object. In this paper we extend the technique, a) to deal with real objects (isolated paper clips) that suffer from noise and occlusions and b) to exploit negative examples during the learning phase. We also compare different versions of the multi-layer networks corresponding to our technique among themselves and with a standard Nearest Neighbor classifier. The simplest version, which is a Radial Basis Functions network, performs less well than a Nearest Neighbor classifier. The more powerful versions, trained with positive and negative examples, perform significantly better. Our results, which may have interesting implications for computer vision despite the relative simplicity of the task studied, are especially interesting for understanding the process of object recognition in biological vision.|Roberto Brunelli,Tomaso Poggio","13361|IJCAI|1971|Hypothesis of Simplicity in Pattern Recognition|This paner deals with the problem of constructing a general method for pattern recognition. It is proposed to realize this method based on a hypothe sis of simplicity which is formulated in an appropriate manner. The proposed method is illustrated with an example.|N. G. Zagoruyko,K. F. Samokhvalov","65223|AAAI|2004|Rapid Object Recognition from Discriminative Regions of Interest|Object recognition and detection represent a relevant component in cognitive computer vision systems, such as in robot vision, intelligent video surveillance systems, or multimodal interfaces. Object identification from local information has recently been investigated with respect to its potential for robust recognition, e.g., in case of partial object occlusions, scale variation, noise, and background clutter in detection tasks. This work contributes to this research by a thorough analysis of the discriminative power of local appearance patterns and by proposing to exploit local information content to model object representation and recognition. We identify discriminative regions in the object views from a posterior entropy measure, and then derive object models from selected discriminative local patterns. For recognition, we determine rapid attentive search for locations of high information content from learned decision trees. The recognition system is evaluated by various degrees of partial occlusion and Gaussian image noise, resulting in highly robust recognition even in the presence of severe occlusion effects.|Gerald Fritz,Christin Seifert,Lucas Paletta,Horst Bischof","14088|IJCAI|1985|Object Recognition Using Vision and Touch|A system is described that integrates vision and tactile sensing in a robotics environment to perform object recognition tasks. It uses multiple sensor systems (active touch and passive stereo vision) to compute three dimensional primitives that can be matched against a model data base of complex curved surface objects containing holes and cavities. The low level sensing elements provide local surface and feature matches which are constrained by relational criteria embedded in the models. Once a model has been invoked, a verification procedure establishes confidence measures for a correct recognition. The three dimen* sional nature of the sensed data makes the matching process more robust as does the system's ability to sense visually occluded areas with touch. The model is hierarchic in nature and allows matching at different levels to provide support or inhibition for recognition.|Peter K. Allen,Ruzena Bajcsy","13890|IJCAI|1983|Oil-Well Data Interpretation Using Expert System and Pattern Recognition Technique|This paper describes a program for interpretation of oil-well measurements. Two techniques have been merged for this purpose the expert system technique (production rules) which lends itself to the symbolic reasoning aspect of the problem and pattern recognition technique (mainly the split and merge algorithm) to interpret curves measuring physical properties of rocks. Part of the knowledge of an expert in geology has been coded into  production rules together with hierarchy trees (such as petrography, paleontology) which allow the program an access to the rules containing assertions more general than the input data. There are two geological laws underlying the design of the program () All sources of data are accounted for whenever they are available, () A description of a rock has no significance out of its context, which means that the identification of rocks must be guided by the knowledge of the environmental conditions. A session with LITHO is partially displayed.|Alain Bonnet,Claude Dahan","17107|IJCAI|2009|Boosting Constrained Mutual Subspace Method for Robust Image-Set Based Object Recognition|Object recognition using image-set or video sequence as input tends to be more robust since image-set or video sequence provides much more information than single snap-shot about the variability in the appearance of the target subject. Constrained Mutual Subspace Method (CMSM) is one of the state-of-the-art algorithms for imageset based object recognition by first projecting the image-set patterns onto the so-called generalized difference subspace then classifying based on the principal angle based mutual subspace distance. By treating the subspace bases for each image-set patterns as basic elements in the grassmann manifold, this paper presents a framework for robust image-set based recognition by CMSM-based ensemble learning in a boosting way. The proposed Boosting Constrained Mutual Subspace Method(BCMSM) improves the original CMSM in the following ways a) The proposed BCMSM algorithm is insensitive to the dimension of the generalized differnce subspace while the performance of the original CMSM algorithm is quite dependent on the dimension and the selecting of optimum choice is quite empirical and case-dependent b) By taking advantage of both boosting and CMSM techniques, the generalization ability is improved and much higher classification performance can be achieved. Extensive experiments on real-life data sets (two face recognition tasks and one D object category classification task) show that the proposed method outperforms the previous state-of-the-art algorithms greatly in terms of classification accuracy.|Xi Li,Kazuhiro Fukui,Nanning Zheng","13543|IJCAI|1975|Recognition Of An Object in A Stack Of Industrial Parts|This paper describes a method for analyzing an input scene of a stack of industrial parts in order to recognize an object which is not obscured by others. Detecting a simple familiar pattern such as an ellipse in a set of strong feature points, an analyzer selects models of the machine parts from the attributes of other feature points around the pattern under the constraints of the proposed models. Finally one of the models is verified through processes of matching the detailed structures of the models to the less obvious feature points.|Saburo Tsuji,A. Nakamura","66351|AAAI|2008|A Fast Data Collection and Augmentation Procedure for Object Recognition|When building an application that requires object class recognition, having enough data to learn from is critical for good performance, and can easily determine the success or failure of the system. However, it is typically extremely labor-intensive to collect data, as the process usually involves acquiring the image, then manual cropping and hand-labeling. Preparing large training sets for object recognition has already become one of the main bottlenecks for such emerging applications as mobile robotics and object recognition on the web. This paper focuses on a novel and practical solution to the dataset collection problem. Our method is based on using a green screen to rapidly collect example images we then use a probabilistic model to rapidly synthesize a much larger training set that attempts to capture desired invariants in the object's foreground and background. We demonstrate this procedure on our own mobile robotics platform, where we achieve x savings in the timeeffort needed to obtain a training set. Our data collection method is agnostic to the learning algorithm being used, and applies to any of a large class of standard object recognition methods. Given these results, we suggest that this method become a standard protocol for developing scalable object recognition systems. Further, we used our data to build reliable classifiers that enabled our robot to visually recognize an object in an office environment, and thereby fetch an object from an office in response to a verbal request.|Benjamin Sapp,Ashutosh Saxena,Andrew Y. Ng","13468|IJCAI|1975|A Program For Geometrical Pattern Recognition Based On The Linguistic Method Of The Description And Analysis Of Geometrical Structures|The organization of a program for linguistic analysis of geometrical patterns is described. The program belongs to the system of pattern recognition for industrial drawings and objects ,, We started from the formalism suggested by Evans , . In completing practical tasks, we haved eveloped the methods of economical grammar construction, exact description of various geometrical relations between pattern elements and our program implementation. The program is written in FORTRAN with the use of the LIDI-T list processing system L, . The grammar is represented by a stiitic lint structure. Geometrical relations and structural transformations of quantitative information, included in the sytitem, are implemented as a set of subroutines. The program algorithm to organize the analysis is grammar-independent, but the passing of the program control to the program segments to process the quantitative information is controlled by the grammar and vice-versa, i.e. the taking of information out of the grammar is controlled by the results. of the processing of the quantitative information about the actual object (and by the given atrategy of the use of the grammar as well).|V. Gallo"],["65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","66695|AAAI|2010|A Stronger Consistency for Soft Global Constraints in Weighted Constraint Satisfaction|Atomic section is an important language feather in multi- thread synchronizing. So far, it can only be implemented by using pessimistic or optimistic concurrent control singly. This paper introduces a flexible hybrid concurrent control system which could harmonize the two modes of concurrent control. Accordingly, a new atomic section is proposed as language level support to open an interface for both manual and compiler-assisted optimization.|Jimmy Lee,K. L. Leung","15401|IJCAI|1999|Path Consistency on Triangulated Constraint Graphs|Among the local consistency techniques used in the resolution of constraint satisfaction problems (CSPs), path consistency (PC) has received a great deal of attention. A constraint graph G is PC if for any valuation of a pair of variables that satisfy the constraint in G between them, one can find values for the intermediate variables on any other path in G between those variables so that all the constraints along that path are satisfied. On complete graphs, Montanari showed that PC holds if and only if each path of length two is PC. By convention, it is therefore said that a CSP is PC if the completion of its constraint graph is PC. In this paper, we show that Montanari's theorem extends to triangulated graphs. One can therefore enforce PC on sparse graphs by triangulating instead of completing them. The advantage is that with triangulation much less universal constraints need to be added. We then compare the pruning capacity of the two approaches. We show that when the constraints are convex, the pruning capacity of PC on triangulated graphs and their completion are identical on the common edges. Furthermore, our experiments show that there is little difference for general nonconvex problems.|Christian Bliek,Djamila Sam-Haroud","17085|IJCAI|2009|Towards Efficient Consistency Enforcement for Global Constraints in Weighted Constraint Satisfaction|Powerful consistency techniques, such as AC* and FDAC*, have been developed for Weighted Constraint Satisfaction Problems (WCSPs) to reduce the space in solution search, but are restricted to only unary and binary constraints. On the other hand, van Hoeve et al. developed efficient graph-based algorithms for handling soft constraints as classical constraint optimization problems. We prove that naively incorporating van Hoeve's method into the WCSP framework can enforce a strong form of -Inverse Consistency, which can prune infeasible values and deduce good lower bound estimates. We further show how Van Hoeve's method can be modified so as to handle cost projection and extension to maintain the stronger AC* and FDAC* generalized for non-binary constraints. Using the soft allDifferent constraint as a testbed, preliminary results demonstrate that our proposal gives improvements up to an order of magnitude both in terms of time and pruning.|Jimmy Ho-Man Lee,Ka Lun Leung","65206|AAAI|2004|Collapsibility and Consistency in Quantified Constraint Satisfaction|The concept of consistency has pervaded studies of the constraint satisfiction problem. We introduce two concepts, which are inspired by consistency, for the more general framework of the quantified constraint satisfaction problem (QCSP). We use these concepts to derive, in a uniform fashion, proofs of polynomial-time tractability and corresponding algorithms for certain cases of the QCSP where the types of allowed relations are restricted. We not only unify existing tractability results and algorithms, but also identify new classes of tractable QCSPs.|Hubie Chen","16978|IJCAI|2009|Set Branching in Constraint Optimization|Branch and bound is an effective technique for solving constraint optimization problems (COP's). However, its search space expands very rapidly as the domain sizes of the problem variables grow. In this paper, we present an algorithm that clusters the values of a variable's domain into sets. Branch and bound can then branch on these sets of values rather than on individual values, thereby reducing the branching factor of its search space. The aim of our clustering algorithm is to construct a collection of sets such that branching on these sets will still allow effective bounding. In conjunction with the reduced branching factor, the size of the explored search space is thus significantly reduced. We test our method and show empirically that it can yield significant performance gains over existing state-of-the-art techniques.|Matthew Kitching,Fahiem Bacchus","14841|IJCAI|1991|On the Feasibility of Distributed Constraint Satisfaction|This paper characterizes connectionist-type architectures that allow a distributed solution for classes of constraint-satisfaction problems. The main issue addressed is whether there exists a uniform model of computation (where all nodes are indistinguishable) that guarantees convergence to a solution from every initial state of the system, whenever such a solution exists. We show that even for relatively simple constraint networks, such as rings, there is no general solution using a completely uniform, asynchronous, model. However, some restricted topologies like trees can accommodate the uniform, asynchronous, model and a protocol demonstrating this fact is presented. An almost* uniform, asynchronous, network-consistency protocol is also presented. We show that the algorithms are guaranteed to be self-stabilizing, which makes them suitable for dynamic or error-prone environments.|Zeev Collin,Rina Dechter,Shmuel Katz","66125|AAAI|2007|Transposition Tables for Constraint Satisfaction|In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.|Christophe Lecoutre,Lakhdar Sais,Sébastien Tabary,Vincent Vidal","14618|IJCAI|1989|Partial Constraint Satisfaction|A constraint satisfaction problem involves finding values for variables subject to constraints on which combinations of values are allowed. In some cases it may be impossible or impractical to solve these problems completely. We may seek to partially solve the problem in an \"optimal\" or \"sufficient\" sense. A formal model is presented for defining and studying such partial constraint satisfaction problems. The basic components of this model are a constraint satisfaction problem, a problem space, and a metric on that space. Algorithms for solving partial constraint satisfaction problems are discussed. A specific branch and bound algorithm is described. Some initial experimental experience with this algorithm is presented.|Eugene C. Freuder","16842|IJCAI|2009|A Soft Global Precedence Constraint|Hard and soft precedence constraints play a key role in many application domains. In telecommunications, one application is the configuration of callcontrol feature subscriptions where the task is to sequence a set of user-selected features subject to a set of hard (catalogue) precedence constraints and a set of soft (user-selected) precedence constraints. When no such consistent sequence exists, the task is to find an optimal relaxation by discarding some features or user precedences. For this purpose, we present the global constraint SOFTPREC. Enforcing Generalized Arc Consistency (GAC) on SOFTPREC is NP-complete. Therefore, we approximate GAC based on domain pruning rules that follow from the semantics of SOFTPREC this pruning is polynomial. Empirical results demonstrate that the search effort required by SOFTPREC is up to one order of magnitude less than the previously known best CP approach for the feature subscription problem. SOFTPREC is also applicable to other problem domains including minimum cutset problems for which initial experiments confirm the interest.|David Lesaint,Deepak Mehta,Barry O'Sullivan,Luis Quesada,Nic Wilson"],["15457|IJCAI|1999|SAT-Encodings Search Space Structure and Local Search Performance|Stochastic local search (SLS) algorithms for prepositional satisfiability testing (SAT) have become popular and powerful tools for solving suitably encoded hard combinatorial from different domains like, e.g., planning. Consequently, there is a considerable interest in finding SAT-encodings which facilitate the efficient application of SLS algorithms. In this work, we study how two encodings schemes for combinatorial problems, like the well-known Constraint Satisfaction or Hamilton Circuit Problem, affect SLS performance on the SAT-encoded instances. To explain the observed performance differences, we identify features of the induces search spaces which affect SLS performance. We furthermore present initial results of a comparitive analysis of the performance of the SAT-encoding and-solving approach versus that of native SLS algorithms directly applied to the unencoded problem instances.|Holger H. Hoos","66110|AAAI|2007|Approximate Counting by Sampling the Backtrack-free Search Space|We present a new estimator for counting the number of solutions of a Boolean satisfiability problem as a part of an importance sampling framework. The estimator uses the recently introduced SampleSearch scheme that is designed to overcome the rejection problem associated with distributions having a substantial amount of determinism. We show here that the sampling distribution of SampleSearch can be characterized as the backtrack-free distribution and propose several schemes for its computation. This allows integrating Sample-Search into the importance sampling framework for approximating the number of solutions and also allows using Sample-Search for computing a lower bound measure on the number of solutions. Our empirical evaluation demonstrates the superiority of our new approximate counting schemes against recent competing approaches.|Vibhav Gogate,Rina Dechter","66449|AAAI|2008|Limits and Possibilities of BDDs in State Space Search|The idea of using BDDs for optimal sequential planning is to reduce the memory requirements for the state sets as problem sizes increase. State variables are encoded binary and ordered along their causal graph dependencies. Sets of planning states are represented in form of Boolean functions, and actions are formalized as transition relations. This allows to compute the successor state set, which determines all states reached by applying one action to the states in the input set. Iterating the process (starting with the representation of the initial state) yields a symbolic implementation of breadth-first search. This paper studies the causes for good and bad BDD performance by providing lower and upper bounds for BDD growth in various domains. Besides general applicability to planning benchmarks, our approach covers different cost models it applies to step-optimal propositional planning as well as planning with additive action costs.|Stefan Edelkamp,Peter Kissmann","15642|IJCAI|2001|UNSEARCHMO Eliminating Redundant Search Space on Backtracking for Forward Chaining Theorem Proving|This paper introduces how to eliminate redundant search space for forward chaining theorem proving as much as possible. We consider how to keep on minimal useful consequent atom sets for necessary branches in a proof tree. In the most cases, an unnecessary non-Horn clause used for forward chaining will be split only once. The increase of the search space by invoking unnecessary forward chaining clauses will be nearly linear, not exponential anymore. In a certain sense, we \"unsearch\" more than necessary. We explain the principle of our method, and provide an example to show that our approach is powerful for forward chaining theorem proving.|Lifeng He","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","15069|IJCAI|1993|Genetic State-Space Search for Constrained Optimization Problems|This paper introduces GSSS (Genetic State-Space Search). The integration of two general search paradigms--genetic search and state-space-search - provides a general framework which can be applied to a large variety of search problems. Here, we show how GSSS solves constrained optimization problems (COPs). Basically, it searches for \"promising search states\" from which good solutions can be easily found. Domain knowledge in the form of constraints is used to limit the space to be searched. Interestingly, our approach allows the handling of constraints within genetic search at a general domain independent level. First, we introduce a genetic representation of search states. Next, we provide empirical results which compare the relative merit of the introduction of constraints during the generation of the initial population, during the fitness calculation, and during the application of genetic operators. Finally, we describe some extensions to our method which came about when applying it to factory floor scheduling problems.|Jan Paredis","13695|IJCAI|1981|Tuning of Search of the Problem Space for Geometry Proofs|In planning a proof, a student searches through a space of inferences leading forward from the givens of the problem and backward from the to-be-proven statement. One dimension of growth of expertise is that students become more tuned in the search of this problem space. This can be shown to result from the application of various learning operators to production embodiments of the inference rules. Rules are evaluated after the solution of a problem according to whether they led to or led away from the solution. Rules that contributed to a solution are strengthened and an attempt is made to formulate general versions of these rules that will apply in other situations. Rules that led away from the solution are weakened and a discrimination process is evoked to try to add features to the rules that will try to restrict them to the correct circumstances of application. Composition is a learning process that collapses successful sequences of rule operations into single macro-rule productions. There is also a process that converts the backward reasoning rules formed by composition into forward reasoning rules. The effect of these learning processes is to put into production conditions tests for problem features that are heunstically predictive of the rule's success.|John R. Anderson","66782|AAAI|2010|Search Space Reduction Using Swamp Hierarchies|We describe a method to extract content text from diverse Web pages by using the HTML document's text-to-tag ratio rather than specific HTML cues that may not be constant across various Web pages. We describe how to compute the text-to-tag ratio on a line-by-line basis and then cluster the results into content and non-content areas. With this approach we then show surprisingly high levels of recall for all levels of precision, and a large space savings.|Nir Pochter,Aviv Zohar,Jeffrey S. Rosenschein,Ariel Felner","15415|IJCAI|1999|Genetic Heuristic for Search Space Exploration|This paper deals with the way dual genetic algorithms (DGA), an extension of the standard ones, explore the search space. After a brief introduction presenting genetic algorithms and dualism, the fitness distance correlation is discussed in the context of dualism. From this discussion, a conjecture is made about the genetic heuristic used by dual genetic algorithms to explore the search space. This conjecture is reinforced by the visualization of the population centroid trajectories in the plane fitness distance. These trajectories help to point out \"leg-up\" behaviors, which allow the dual genetic algorithm to reach the global optimum from walks on deceptive paths.|Manuel Clergue,Philippe Collard","16655|IJCAI|2007|State Space Search for Risk-Averse Agents|We investigate search problems under risk in state-space graphs, with the aim of finding optimal paths for risk-averse agents. We consider problems where uncertainty is due to the existence of different scenarios of known probabilities, with different impacts on costs of solution-paths. We consider various non-linear decision criteria (EU, RDU, Yaari) to express risk averse preferences then we provide a general optimization procedure for such criteria, based on a path-ranking algorithm applied on a scalarized valuation of the graph. We also consider partial preference models like second order stochastic dominance (SSD) and propose a multiobjective search algorithm to determine SSD-optimal paths. Finally, the numerical performance of our algorithms are presented and discussed.|Patrice Perny,Olivier Spanjaard,Louis-Xavier Storme"],["15112|IJCAI|1995|Reasoning about Noisy Sensors in the Situation Calculus|Agents interacting with an incompletely known dynamic world need to be able to reason about the effects of their actions, and to gain further information about that world using sensors of some sort. Unfortunately, sensor information is inherently noisy, and in general serves only to increase the agent's degree of confidence in various propositions. Building on a general logical theory of action formalized in the situation calculus developed by Reiter and others, we propose a simple axiomatization of the effect on an agent's state of belief of taking a reading from a noisy sensor. By exploiting Reiter's solution to the frame problem, we automatically obtain that these sensor actions leave the rest of the world unaffected, and further, that non-sensor actions change the state of belief of the agent in appropriate ways.|Fahiem Bacchus,Joseph Y. Halpern,Hector J. Levesque","15971|IJCAI|2003|Reasoning about the Interaction of Knowledge Time and Concurrent Actions in the Situation Calculus|A formal framework for specifying and developing agentsrobots must handle not only knowledge and sensing actions, but also time and concurrency. Researchers have extended the situation calculus to handle knowledge and sensing actions. Other researchers have addressed the issue of adding time and concurrent actions. Here both of these features are combined into a united logical theory of knowledge, sensing, time, and concurrency. The result preserves the solution to the frame problem of previous work, maintains the distinction between indexical and objective knowledge of time, and is capable of representing the various ways in which concurrency interacts with time and knowledge. Furthermore, a method based on regression is developed for solving the projection problem for theories specified in this version of the situation calculus.|Richard B. Scherl","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","66764|AAAI|2010|Reasoning about Imperfect Information Games in the Epistemic Situation Calculus|In the past years dynamic voltage and frequency scaling (DVFS) has been an effective technique that allowed microprocessors to match a predefined power budget. However, as process technology shrinks, DVFS becomes less effective (because of the increasing leakage power) and it is getting closer to a point where DVFS won't be useful at all (when static power exceeds dynamic power). In this paper we propose the use of microarchitectural techniques to accurately match a power constraint while maximizing the energy efficiency of the processor. We predict the processor power consumption at a basic block level, using the consumed power translated into tokens to select between different power-saving micro-architectural techniques. These techniques are orthogonal to DVFS so they can be simultaneously applied. We propose a two-level approach where DVFS acts as a coarse-grained technique to lower the average power while microarchitectural techniques remove all the power spikes efficiently. Experimental results show that the use of power-saving microarchitectural techniques in conjunction with DVFS is up to six times more precise, in terms of total energy consumed (area) over the power budget, than using DVFS alone for matching a predefined power budget. Furthermore, in a near future DVFS will become DFS because lowering the supply voltage will be too expensive in terms of leakage power. At that point, the use of power-saving microarchitectural techniques will become even more energy efficient.|Vaishak Belle,Gerhard Lakemeyer","15348|IJCAI|1997|Reasoning with Incomplete Initial Information and Nondeterminism in Situation Calculus|Situation Calculus is arguably the most widely studied and used formalism for reasoning about action and change. The main reason for its popularity is the ability to reason about different action sequences as explicit objects. In particular, planning can be formulated as an existence problem. This paper shows how these properties break down when incomplete information about the initial state and nondeterministic action effects are introduced, basically due to the fact that this incompleteness is not adequately manifested on the object level. A version of Situation Calculus is presented which adequately models the alternative ways the world can develop relative to a choice of actions.|Lars Karlsson","15332|IJCAI|1997|Reasoning about Action in Polynomial Time|Although many formalisms for reasoning about action exist, surprisingly few approaches have taken computational complexity into consideration. The contributions of this paper are the following a temporal logic with a restriction for which deciding satisfiability is tractable, a tractable extension for reasoning about action, and NP-completeness results for the unrestricted problems. Many interesting reasoning problems can be modelled, involving nondeterminism, concurrency and memory of actions. The reasoning process is proved to be sound and complete.|Thomas Drakengren,Marcus Bjäreland","13637|IJCAI|1977|Reasoning About Knowledge and Action|This paper discusses the problems of representing and reasoning with information about knowledge and action. The first section discusses the importance of having systems that understand the concept of knowledge, and how knowledge is related to action. Section  points out some of the special problems that are involved in reasoning about knowledge, and section S presents a logic of knowledge based on the idea of possible worlds. Section  integrates this with a logic of actions and gives an example of reasoning in the combined system. Section  makes some concluding comments.|Robert C. Moore","15337|IJCAI|1997|Reasoning about Concurrent Execution Prioritized Interrupts and Exogenous Actions in the Situation Calculus|As an alternative to planning, an approach to highlevel agent control based on concurrent program execution is considered. A formal definition in the situation calculus of such a programming language is presented and illustrated with a detailed example. The language includes facilities for prioritizing the concurrent execution, interrupting the execution when certain conditions become true, and dealing with exogenous actions. The language differs from other procedural formalisms for concurrency in that the initial state can be incompletely specified and the primitive actions can be user-defined by axioms in the situation calculus.|Giuseppe De Giacomo,Yves Lespérance,Hector J. Levesque","16348|IJCAI|2007|Decidable Reasoning in a Modified Situation Calculus|We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter's version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.|Yilan Gu,Mikhail Soutchanski","15429|IJCAI|1999|Expressive Reasoning about Action in Nondeterministic Polynomial Time|The rapid development of efficient heuristics for deciding satisfiability for propositional logic motivates thorough investigations of the usability of NP-complete problems in general. In this paper we introduce a logic of action and change which is expressive in the sense that it can represent most propositional benchmark examples in the literature, and some new examples involving parallel composition of actions, and actions that may or may not be executed. We prove that satisfiability of a scenario in this logic is NP-complete, and that it subsumes an NP-complete logic (which in turn includes a nontrivial polynomial-time fragment) previously introduced by Drakengren and Bjareland.|Thomas Drakengren,Marcus Bjäreland"],["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","13405|IJCAI|1973|Mechanism of Deduction in a Question-Answering System with Natural Language Inputd|We have constructed a deductive question answering system which accepts natural language input in Japanese. The semantic trees of aseertional input sentences are stored in a semantic network and interrelationships -- conditional, implicational, and so forth -- are established among them. A matching routine looks for the semantic trees which have some relations to a query, and returns the mismatch information (difference) to a deduction routine. The deduction routine produces sub-goals to diminish t h i s difference. This process takes place recursively until the difference is completely resolved (success), or there is no other possibility of matching in the semantic network (failure). Standard problem solving techniques are used in this process. As the result the system is very powerful in handling deductive responses. In this paper only the part of the logical deduction is explained in detail.|Makoto Nagao,Jun-ichi Tsujii","13554|IJCAI|1975|On The System Of Concepts Relations And Outline Of The Natural Language System|Systems that simulates thinking processes of a man through natural language are called natural language systems. We present here outline of a natural language system in connection with the system of concepts relations which plays important role in our system. The system of concepts relations is a set of words or short sentences connected through various basic view-points. Semantic analyses procedure using the system of concepts relations is shown by an example. We regard desire as basic cause of thinking in our system. Outline of the relation between desire system and thinking processes using natural language as a base Is given in later chapters.|S. Yoshida","13317|IJCAI|1971|Experiments with a Natural Language Problem-Solving System|Development work on a computer program called HAPPINESS which solves basic probability problems phrased in English is presented. Emphasis is placed on () the application of heuristics to the examination of input language structure for the purpose of determining those phases richest in semantic content () the piecewise construction of combinatorial formulas for problem solution The language analysis is accomplished in several discrete stages, involving simple sentence transformation, keyword and semantic scanning, and syntactic analysis based on a simplified context-free grammar. The descriptor list result of this analysis is used as the basis for a four-stage solution procedure. A description of the implementation, and a discussion of its limitations, extensions, and applications, is also given.|Jack P. Gelb","14404|IJCAI|1987|Understanding System Specifications Written in Natural Language|This paper describes research in understanding system specifications written in natural language. This research involves the implementation of a natural language interface, PHRANSPAN, for specifying the abstract behavior of digital systems in restricted English text. A neutral formal representation for the behavior is described using the USC Design Data Structure. A small set of concepts that characterize digital system behavior are presented using this representation. An intermediate representation loosely based on Conceptual Dependency is presented. Its use with a semantic-based parser to translate from English to the formal representation is illustrated by examples.|John J. Granacki Jr.,Alice C. Parker,Yigal Arens","13991|IJCAI|1983|Transportability and Generality in a Natural-Language Interface System|This paper describes the design of a transportable natural language (NL) interface to databases and the constraints that transportability places on each component of such a system. By a transportable NL system, we mean an NL processing system that is constructed so that a domain expert (rather than an AI or linguistics expert) can move the system to a new application domain. After discussing the general problems presented by transportability, this paper describes TEAM (an acronym for Transportable English database Access Medium), a demonstratable prototype of such a system. The discussion of TEAM shows how domain-independent and domain-dependent information can be separated in the different components of a NL interface system, and presents one method of obtaining domain-specific information from a domain expert.|Paul A. Martin,Douglas E. Appelt,Fernando C. N. Pereira","15304|IJCAI|1995|Semantic Inference in Natural Language Validating a Tractable Approach|This paper is concerned with an inferential approach to information extraction reporting in particular on the results of an empirical study that was performed to validate the approach. The study brings together two lines of research () the RHO framework for tractable terminological knowledge representation and () the Alembic message understanding system. There are correspondingly two principal aspects of interest to this work From the knowledge representation perspective the present study serves to validate experimentally a normal form hypothesis that guarantees tractability of inference in the RHO framework. From the message processing perspective this study substantiates the utility of limited inference to information extraction.|Marc B. Vilain","13509|IJCAI|1975|An Adaptive Natural Language System That Listens Asks And Learns|Whan a user interacts with a natural language system, ha may wall uaa words and expressions which ware not anticipated by tha systea designers. This paper describes a systea which can play TIC-TAC-TOE, and discuss tha game while it Is in progress. If the system encounters new words, new expressions, or inadvertent ungrammatlcalltles, It attempts to understand what was meant, through contextual inference, and by asking intelligent clarifying questions of the user. The system than records the meaning of any new words or expressions, thus augmenting its linguistic knowledge in the course of user interaction.|Perry Lowell Miller","13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons"],["14531|IJCAI|1987|A Parallel Blackboard System for Robot Control|The blackboard architecture is a powerful expert systems architecture. Its main advantages are flexibility of control and integration of different kinds of knowledge representations and inferencing techniques. At our Laboratories we built a blackboard shell, based on the principles as developed in the system Hearsay-II and the blackboard shells Hearsay-III and BB. This paper describes how we modify this blackboard shell, in order to use it for the control of a robot cell. This cell consists of a number of independent devices, which will have to execute their tasks concurrently. This means a modification of the blackboard shell towards parallelism. Ve see great possibilities for using our system not only for simulating robot cell configurations and proto typing flexible control of robot cells, but also for other systems with multiple processors.|Hugo Velthuijsen,Ben J. Lippolt,Jeanette C. Vonk","15933|IJCAI|2003|Non-Invasive Brain-Actuated Control of a Mobile Robot|Recent experiments have indicated the possibility to use the brain electrical activity to directly control the movement of robotics or prosthetic devices. In this paper we report results with a portable non-invasive brain-computer interface that makes possible the continuous control of a mobile robot in a house-like environment. The interface uses  surface electrodes to measure electroencephalogram (EEG) signals from which a statistical classifier recognizes  different mental states. Until now, brain-actuated control of robots has relied on invasive approaches-requiring surgical implantation of electrodes-since EEG-based systems have been considered too slow for controlling rapid and complex sequences of movements. Here we show that, after a few days of training, two human subjects successfully moved a robot between several rooms by mental control only. Furthermore, mental control was only marginally worse than manual control on the same task.|José del R. Millán,Frédéric Renkens,Josep Mouriño,Wulfram Gerstner","13514|IJCAI|1975|Walking Robot A Non-Deterministic Model Of Control|The development of a model of motion control for walking robots is reported. The goal of the project is the elaboration of methods to construct the tree main levels of a control system of the robot's motion and a program complex to realise the robot's behaviour simulation in complex topography. A notable amount of achievements has been reached by now in developing the model of behaviour control for robots in various \" activity areas\". Most of the models, however, are, to our mind, too \"deterministic\". The hope to obtain interesting results in the field with a model based, in a sense, on the principle of \"maximum indetenninism\" gave rise to the project 'discussed in the present paper. The corresponding ideology has been almost literally transferred here from the asynchronous programming theory as to the \"walking\", it was chosen as a model domain because it offers combination of both general and special problems.|A. Narinyani,V. Pyatkin,P. Kim,V. Domentyev","15634|IJCAI|2001|Combining Probabilities Failures and Safety in Robot Control|We present a formal framework for treating both incomplete information in the initial database and possible failures during an agent's execution of a course of actions. These two aspects of uncertainty are formalized by two different notions of probability. We introduce also a concept of expected probability, which is obtained by combining the two previous notions. Expected probability accounts for the probability of a sentence on the hypothesis that the sequence of actions needed to make it true might have failed. Expected probability leads to the possibility of comparing courses of actions and verifying which is more safe.|Alberto Finzi,Fiora Pirri","13269|IJCAI|1969|Robot Control Strategy|A behavioral theory of information processing based on ethological observations has been tested experimentally by programming a robot control system. The resultant software has successfully simulated robot operation. Three basic types of software modules are assumed as elements. These accept input stimuli and generate detailedstrings of motor response \"units\" to attain design goals. A complex process of interaction between the modules permits the consideration of many alternative actions while focusing attention on a selected set of input signals. ' The organization is shown to have especially flexible characteristics adapted to robot control and permits a recursive definition of some of the modules. Brief comparisons with other robot systems in being indicate some of the relationships between the different design approaches so far proposed.|Leonard Friedman","15263|IJCAI|1995|Logic Programming for Robot Control|This paper proposes logic programs as a specification for robot control. These provide a formal specification of what an agent should do depending on what it senses, and its previous sensory inputs and actions. We show how to axiomatise reactive agents, events as an interface between continuous and discrete time, and persistence, as well as axiomatising integration and differentiation over time (in terms of the limit of sums and differences). This specification need not be evaluated as a Prolog program we use can the fact that it will be evaluated in time to get a more efficient agent. We give a detailed example of a nonholonomic maze travelling robot, where we use the same language to model both the agent and the environment. One of the main motivations for this work is that there is a clean interface between the logic programs here and the model of uncertainty embedded in probabilistic Horn abduction. This is one step towards building a decision-theoretic planning system where the output of the planner is a plan suitable for actually controlling a robot.|David Poole","65554|AAAI|2005|Remote Supervisory Control of a Humanoid Robot|For this demonstration, participants have the opportunity to control a humanoid robot located hundreds of miles away. The general task is to reach, grasp, and transport various objects in the vicinity of the robot. Although remote \"pick-and-place\" operations of this sort form the basis of numerous practical applications, they are frequently error-prone and fatiguing for human operators. Participants can experience the relative difficulty of remote manipulation both with and without the use of an assistive interface. This interface simplifies the task by injecting artificial intelligence in key places without seizing higher-level control from the operator. In particular, we demonstrate the benefits of two key components of the system a video display of predicted operator intentions, and a haptic-based controller for automated grasping.|Michael T. Rosenstein,Andrew H. Fagg,Robert Platt Jr.,John Sweeney,Roderic A. Grupen","14951|IJCAI|1991|Mobile Robot Navigation by an Active Control of the Vision System|In this paper, we argue that a mobile robot's environment can be determined by computing local maps surrounding feature points, called fixation points. These fixation points are obtained by searching the scene for points which present some interesting cue for robot navigation. This -D computation is based on a monocular active vision system composed of a camera, mounted on a rotating table accurately controlled by a computer, which gazes the fixation point as the robot moves. The system then computes the local map and updates it with each new observation in order to increase its accuracy and robustness. Real experimentation in a complex indoor scene illustrates that the -D scene coordinates can be obtained with a good accuracy by integrating several observations.|Patrick Stelmaszyk,Hiroshi Ishiguro,Saburo Tsuji","13439|IJCAI|1975|Transport Robot With Network Control System|A network control system of Transport Automatic Integral Robot (TAIP) motion in the real environment is suggested. Input data and general structure of the network are described. A program is reported of further efforts to investigate and refine the network control systems.|N. Amosov,E. Kussul,V. Fomenko","14926|IJCAI|1991|Planning Robot Control Parameter Values with Qualitative Reasoning|A qualitative reasoning planner for determining robot control parameters to drive manipulation actions has been developed, integrated into a telerobot system, and demonstrated for a match striking task. The planner consists of a qualitative reasoner and a numerical execution history which interact to jointly direct and narrow the search for reliable numerical control parameter values. The planner algorithm, implementation, and an execution example are described. The relationship to previous qualitative reasoning work is also discussed.|Stephen F. Peters,Shigeoki Hirai,Toru Omata,Tomomasa Sato"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","15471|IJCAI|1999|A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes|An issue that is critical for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or even infinite state spaces, traditional planning and reinforcement learning algorithms are often inapplicable, since their running time typically scales linearly with the state space size. In this paper we present a new algorithm that, given only a generative model (simulator) for an arbitrary MDP, performs near-optimal planning with a running time that has no dependence on the number of states. Although the running time is exponential in the horizon time (which depends only on the discount factor  and the desired degree of approximation to the optimal policy), our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large, unstructured MDPs. Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs KMN.|Michael J. Kearns,Yishay Mansour,Andrew Y. Ng","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","65220|AAAI|2004|Metrics for Finite Markov Decision Processes|We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.|Norm Ferns,Prakash Panangaden,Doina Precup","16367|IJCAI|2007|A Fast Analytical Algorithm for Solving Markov Decision Processes with Real-Valued Resources|Agents often have to construct plans that obey deadlines or, more generally, resource limits for real-valued resources whose consumption can only be characterized by probability distributions, such as execution time or battery power. These planning problems can be modeled with continuous state Markov decision processes (MDPs) but existing solution methods are either inefficient or provide no guarantee on the quality of the resulting policy. We therefore present CPH, a novel solution method that solves the planning problems by first approximating with any desired accuracy the probability distributions over the resource consumptions with phasetype distributions, which use exponential distributions as building blocks. It then uses value iteration to solve the resulting MDPs by exploiting properties of exponential distributions to calculate the necessary convolutions accurately and efficiently while providing strong guarantees on the quality of the resulting policy. Our experimental feasibility study in a Mars rover domain demonstrates a substantial speedup over Lazy Approximation, which is currently the leading algorithm for solving continuous state MDPs with quality guarantees.|Janusz Marecki,Sven Koenig,Milind Tambe","17020|IJCAI|2009|Generalized First Order Decision Diagrams for First Order Markov Decision Processes|First order decision diagrams (FODD) were recently introduced as a compact knowledge representation expressing functions over relational structures. FODDs represent numerical functions that, when constrained to the Boolean range, use only existential quantification. Previous work developed a set of operations over FODDs, showed how they can be used to solve relational Markov decision processes (RMDP) using dynamic programming algorithms, and demonstrated their success in solving stochastic planning problems from the International Planning Competition in the system FODD-Planner. A crucial ingredient of this scheme is a set of operations to remove redundancy in decision diagrams, thus keeping them compact. This paper makes three contributions. First, we introduce Generalized FODDs (GFODD) and combination algorithms for them, generalizing FODDs to arbitrary quantification. Second, we show how GFODDs can be used in principle to solve RMDPs with arbitrary quantification, and develop a particularly promising case where an arbitrary number of existential quantifiers is followed by an arbitrary number of universal quantifiers. Third, we develop a new approach to reduce FODDs and GFODDs using model checking. This yields a reduction that is complete for FODDs and provides a sound reduction procedure for GFODDs.|Saket Joshi,Kristian Kersting,Roni Khardon","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aurélie Beynier,Abdel-Illah Mouaddib","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"]]}}