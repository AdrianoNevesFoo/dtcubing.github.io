{"abstract":{"entropy":6.696279312272143,"topics":["data, recent years, semantic web, web, web pages, schema mapping, data management, search engine, dimensionality reduction, large data, data systems, present novel, heuristic search, data integration, widely used, management systems, recent research, systems information, query processing, data stream","description logic, knowledge base, logic, preferences voting, logic programming, modal logic, games player, actions uncertainty, situation calculus, logic knowledge, aggregation voting, real world, logic programs, voting rules, aggregation preferences, effects causal, games playing, commonly used, games, multiple voting","artificial intelligence, machine learning, reinforcement learning, learning, bayesian network, important applications, sensor network, transfer knowledge, play role, present learning, present approach, transfer learning, problem learning, knowledge learning, knowledge task, learning task, spatial reasoning, research focuses, improve learning, important problem","markov decision, markov processes, decision processes, natural language, partially observable, decision making, constraint satisfaction, constraint problem, sense disambiguation, word sense, solving problem, multi-agent systems, present algorithm, observable pomdps, partially pomdps, observable markov, partially markov, consider problem, observable decision, partially decision","recent years, recent research, ontology web, text web, recent, clustering data, web become, recent interest, past decade, years research, data items, highly data, research data, recent data, quality data, recent database, years interest, growing interest, past years, web research","semantic web, web pages, present novel, search engine, dimensionality reduction, heuristic search, search algorithm, web, web services, support vector, use web, information extraction, search problem, web information, search, novel approach, data web, search web, ontologies semantic, techniques based","games player, negotiation agents, query scoring, query function, incomplete information, efficient parsing, present games, solutions problem, games, top-k query, information games, query, solutions, function, relational, same, pattern, evaluation, information, efficient","games playing, situation calculus, present general, commonly used, general framework, general playing, planners plans, present framework, framework, plans, theory, recently, values, programs, max-sat, power, define, search, basis, general","machine learning, reinforcement learning, problem learning, learning, present learning, address problem, learning data, research focuses, problem reinforcement, research learning, learning methods, objects data, semi-supervised learning, learning training, learning techniques, consider problem, classification problem, learning representation, simultaneously learning, applying learning","present approach, problem given, addresses problem, present corpus-based, satisfiability problem, location mobile, approach, problem, given, belief change, counting problem, solutions problem, problem finding, problem points, local problem, finding, approach problem, input, problem sequence, belief","constraint problem, constraint satisfaction, solving problem, present algorithm, sense disambiguation, word sense, consider problem, satisfaction problem, distributed optimization, satisfaction csp, word disambiguation, constraint csp, problem, algorithm problem, constraint, algorithm, optimization problem, combinatorial problem, problem csp, problem dynamic","challenging problem, mechanism design, intelligent systems, intelligent tutoring, teaching students, problem systems, design problem, design settings, design, challenging task, systems, model-based systems, systems general, complex systems, describe systems, design systems, develop, effective, students, developed"],"ranking":[["80738|VLDB|2007|Indexable PLA for Efficient Similarity Search|Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the \"dimensionality curse\". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).|Qiuxia Chen,Lei Chen 0002,Xiang Lian,Yunhao Liu,Jeffrey Xu Yu","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","80797|VLDB|2007|SOR A Practical System for Ontology Storage Reasoning and Search|Ontology, an explicit specification of shared conceptualization, has been increasingly used to define formal data semantics and improve data reusability and interoperability in enterprise information systems. In this paper, we present and demonstrate SOR (Scalable Ontology Repository), a practical system for ontology storage, reasoning, and search. SOR uses Relational DBMS to store ontologies, performs inference over them, and supports SPARQL language for query. Furthermore, a faceted search with relationship navigation is designed and implemented for ontology search. This demonstration shows how to efficiently solve three key problems in practical ontology management in RDBMS, namely storage, reasoning, and search. Moreover, we show how the SOR system is used for semantic master data management.|Jing Lu,Li Ma,Lei Zhang,Jean-S√©bastien Brunner,Chen Wang,Yue Pan,Yong Yu","66002|AAAI|2007|Towards Efficient Dominant Relationship Exploration of the Product Items on the Web|In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.|Zhenglu Yang,Lin Li,Botao Wang,Masaru Kitsuregawa","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Cal√¨,Michael Kifer","80846|VLDB|2007|iTrails Pay-as-you-go Information Integration in Dataspaces|Dataspace management has been recently identified as a new agenda for information management ,  and information integration . In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration , as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.|Marcos Antonio Vaz Salles,Jens-Peter Dittrich,Shant Kirakos Karakashian,Olivier Ren√© Girard,Lukas Blunschi","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["65701|AAAI|2006|Forgetting and Conflict Resolving in Disjunctive Logic Programming|We establish a declarative theory of forgetting for disjunctive logic programs. The suitability of this theory is justified by a number of desirable properties. In particular, one of our results shows that our notion of forgetting is completely captured by the classical forgetting. A transformation-based algorithm is also developed for computing the result of forgetting. We also provide an analysis of computational complexity. As an application of our approach, a fairly general framework for resolving conflicts in inconsistent knowledge bases represented by disjunctive logic programs is defined. The basic idea of our framework is to weaken the preferences of each agent by forgetting certain knowledge that causes inconsistency. In particular, we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming.|Thomas Eiter,Kewen Wang","16738|IJCAI|2007|Quantified Coalition Logic|We add a limited but useful form of quantification to Coalition Logic, a popular formalism for reasoning about cooperation in game-like multi-agent systems. The basic constructs of Quantified Coalition Logic (QCL) allow us to express properties as \"there exists a coalition C satisfying property P such that C can achieve . We give an axiomatization of QCL, and show that while it is no more expressive than Coalition Logic, it is exponentially more succinct. The time complexity of QCL model checking for symbolic and explicit state representations is shown to be no worse than that of Coalition Logic. We illustrate the formalism by showing how to succinctly specify such social choice mechanisms as majority voting, which in Coalition Logic require specifications that are exponentially long in the number of agents.|Thomas √\u2026gotnes,Wiebe van der Hoek,Michael Wooldridge","16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","65692|AAAI|2006|Unifying Logical and Statistical AI|Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system.|Pedro Domingos,Stanley Kok,Hoifung Poon,Matthew Richardson,Parag Singla","65962|AAAI|2007|ESP A Logic of Only-Knowing Noisy Sensing and Acting|When reasoning about actions and sensors in realistic domains, the ability to cope with uncertainty often plays an essential role. Among the approaches dealing with uncertainty, the one by Bacchus, Halpern and Levesque, which uses the situation calculus, is perhaps the most expressive. However, there are still some open issues. For example, it remains unclear what an agent's knowledge base would actually look like. The formalism also requires second-order logic to represent uncertain beliefs, yet a first-order representation clearly seems preferable. In this paper we show how these issues can be addressed by incorporating noisy sensors and actions into an existing logic of only-knowing.|Alfredo Gabaldon,Gerhard Lakemeyer","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","16348|IJCAI|2007|Decidable Reasoning in a Modified Situation Calculus|We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter's version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.|Yilan Gu,Mikhail Soutchanski","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski","16705|IJCAI|2007|Vote and Aggregation in Combinatorial Domains with Structured Preferences|In many real-world collective decision problems, the set of alternatives is a Cartesian product of finite value domains for each of a given set of variables. The prohibitive size of such domains makes it practically impossible to represent preference relations explicitly. Now, AI has been developing languages for representing preferences on such domains in a succinct way, exploiting structural properties such as conditional preferential independence. Here we reconsider voting and aggregation rules in the case where voters' preferences have a common preferential independence structure, and address the decompossition a voting rule or an aggregation function following a linear order over variables.|J√©r√¥me Lang"],["16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","65967|AAAI|2007|Using Multiresolution Learning for Transfer in Image Classification|Our work explores the transfer of knowledge at multiple levels of abstraction to improve learning. By exploiting the similarities between objects at various levels of detail, multiresolution learning can facilitate transfer between image classification tasks. We extract features from images at multiple levels of resolution, then use these features to create models at different resolutions. Upon receiving a new task, the closest-matching stored model can be generalized (adapted to the appropriate resolution) and transferred to the new task.|Eric Eaton,Marie desJardins,John Stevenson","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","16634|IJCAI|2007|Transfer Learning in Real-Time Strategy Games Using Hybrid CBRRL|The goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. In this paper, we present a multilayered architecture named CAse-Based Reinforcement Learner (CARL). It uses a novel combination of Case-Based Reasoning (CBR) and Reinforcement Learning (RL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTSTM, a commercial Real Time Strategy game. Our experiments demonstrate that CARL not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.|Manu Sharma,Michael P. Holmes,Juan Carlos Santamar√≠a,Arya Irani,Charles Lee Isbell Jr.,Ashwin Ram","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","65697|AAAI|2006|Multi-Resolution Learning for Knowledge Transfer|Related objects may look similar at low-resolutions differences begin to emerge naturally as the resolution is increased. By learning across multiple resolutions of input, knowledge can be transfered between related objects. My dissertation develops this idea and applies it to the problem of multitask transfer learning.|Eric Eaton","16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern√°ndez,Ricardo Aler,Daniel Borrajo","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","16658|IJCAI|2007|AEMS An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs|Solving large Partially Observable Markov Decision Processes (POMDPs) is a complex task which is often intractable. A lot of effort has been made to develop approximate offline algorithms to solve ever larger POMDPs. However, even state-of-the-art approaches fail to solve large POMDPs in reasonable time. Recent developments in online POMDP search suggest that combining offline computations with online computations is often more efficient and can also considerably reduce the error made by approximate policies computed offline. In the same vein, we propose a new anytime online search algorithm which seeks to minimize, as efficiently as possible, the error made by an approximate value function computed offline. In addition, we show how previous online computations can be reused in following time steps in order to prevent redundant computations. Our preliminary results indicate that our approach is able to tackle large state space and observation space efficiently and under real-time constraints.|St√©phane Ross,Brahim Chaib-draa","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,Fran√ßois Charpillet","65786|AAAI|2006|Incremental Least Squares Policy Iteration for POMDPs|We present a new algorithm, called incremental least squares policy iteration (ILSPI), for finding the infinite-horizon stationary policy for partially observable Markov decision processes (POMDPs). The ILSPI algorithm computes a basis representation of the infinite-horizon value function by minirnizing the square of Bellman residual and performs policy improvenent in reachable belief states. A number of optimal basis functions are determined by the algorithm to minimize the Bellman residual incrementally, via efficient computations. We show that, by using optimally determined basis functions, the policy can be improved successively on a set of most probable belief points sampled from the reachable belief set. As the ILSPI is based on belief sample points, it represents a point-based policy iteration method. The results on four benchmark problems show that the ILSPI compares competitively to its value-iteration counterparts in terms of both performance and computational efficiency.|Hui Li,Xuejun Liao,Lawrence Carin","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein"],["80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","66204|AAAI|2007|Using Eye-Tracking Data for High-Level User Modeling in Adaptive Interfaces|In recent years, there has been substantial research on exploring how AI can contribute to Human-Computer Interaction by enabling an interface to understand a user's needs and act accordingly. Understanding user needs is especially challenging when it involves assessing the user's high-level mental states not easily reflected by interface actions. In this paper, we present our results on using eye-tracking data to model such mental states during interaction with adaptive educational software. We then discuss the implications of our research for Intelligent User Interfaces.|Cristina Conati,Christina Merten,Saleema Amershi,Kasia Muldner","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","80639|VLDB|2006|Answering Top-k Queries Using Views|The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Dimitris Tsirogiannis","80687|VLDB|2006|The Making of TPC-DS|For the last decade, the research community and the industry have used TPC-D and its successor TPC-H to evaluate performance of decision support technology. Recognizing a paradigm shift in the industry the Transaction Processing Performance Council has developed a new Decision Support benchmark, TPC-DS, expected to be released this year. From an ease of benchmarking perspective it is similar to past benchmarks. However, it adjusts for new technology and new approaches the industry has embarked on in recent years. This paper describes the main characteristics of TPC-DS, explains why some of the key decisions were made and which performance aspects of decision support system it measures.|Raghunath Othayoth Nambiar,Meikel Poess","66002|AAAI|2007|Towards Efficient Dominant Relationship Exploration of the Product Items on the Web|In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.|Zhenglu Yang,Lin Li,Botao Wang,Masaru Kitsuregawa","16515|IJCAI|2007|A Scalable Kernel-Based Algorithm for Semi-Supervised Metric Learning|In recent years, metric learning in the semisupervised setting has aroused a lot of research interests. One type of semi-supervised metric learning utilizes supervisory information in the form of pairwise similarity or dissimilarity constraints. However, most methods proposed so far are either limited to linear metric learning or unable to scale up well with the data set size. In this paper, we propose a nonlinear metric learning method based on the kernel approach. By applying low-rank approximation to the kernel matrix, our method can handle significantly larger data sets. Moreover, our low-rank approximation scheme can naturally lead to out-of-sample generalization. Experiments performed on both artificial and real-world data show very promising results.|Dit-Yan Yeung,Hong Chang,Guang Dai","65766|AAAI|2006|Lessons on Applying Automated Recommender Systems to Information-Seeking Tasks|Automated recommender systems predict user preferences by applying machine learning techniques to data on products, users, and past user preferences for products. Such systems have become increasingly popular in entertainment and e-commerce domains, but have thus far had little success in information-seeking domains such as identifying published research of interest. We report on several recent publications that show how recommenders can be extended to more effectively address information-seeking tasks by expanding the focus from accurate prediction of user preferences to identifying a useful set of items to recommend in response to the user's specific information need. Specific research demonstrates the value of diversity in recommendation lists, shows how users value lists of recommendations as something different from the sum of the individual recommendations within, and presents an analytic model for customizing a recommender to match user information-seeking needs.|Joseph A. Konstan,Sean M. McNee,Cai-Nicolas Ziegler,Roberto Torres,Nishikant Kapoor,John Riedl","80606|VLDB|2006|Query Co-Processing on Commodity Processors|The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), Cell processors and graphics processing units (GPUs). This tutorial will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips.|Anastassia Ailamaki,Naga K. Govindaraju,Stavros Harizopoulos,Dinesh Manocha"],["16451|IJCAI|2007|Web Page Clustering Using Heuristic Search in the Web Graph|Effective representation of Web search results remains an open problem in the Information Retrieval community. For ambiguous queries, a traditional approach is to organize search results into groups (clusters), one for each meaning of the query. These groups are usually constructed according to the topical similarity of the retrieved documents, but it is possible for documents to be totally dissimilar and still correspond to the same meaning of the query. To overcome this problem, we exploit the thematic locality of the Web--relevant Web pages are often located close to each other in the Web graph of hyperlinks. We estimate the level of relevance between each pair of retrieved pages by the length of a path between them. The path is constructed using multi-agent beam search each agent starts with one Web page and attempts to meet as many other agents as possible with some bounded resources. We test the system on two types of queries ambiguous English words and people names. The Web appears to be tightly connected about % of the agents meet with each other after only three iterations of exhaustive breadth-first search. However, when heuristics are applied, the search becomes more focused and the obtained results are substantially more accurate. Combined with a content-driven Web page clustering technique, our heuristic search system significantly improves the clustering results.|Ron Bekkerman,Shlomo Zilberstein,James Allan","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","16730|IJCAI|2007|Exploiting Image Contents in Web Search|Web search is a challenging task. Previous research mainly exploits texts on the Web pages or link information between the pages, while multimedia information is largely ignored. This paper proposes a new framework for Web search, which exploits image contents to help improve the search performance. In this framework, candidate images are retrieved at first by considering their associated text information. Then, images related to the query are identified by analyzing the density of the visual feature space. After that, an image-based rank of the Web pages is generated, which is combined with the traditional keyword-based search result to produce the final search result. Experiments demonstrate the promise of the proposed framework.|Zhi-Hua Zhou,Hong-Bin Dai","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","16483|IJCAI|2007|Learning Semantic Descriptions of Web Information Sources|The Internet is full of information sources providing various types of data from weather forecasts to travel deals. These sources can be accessed via web-forms, Web Services or RSS feeds. In order to make automated use of these sources, one needs to first model them semantically. Writing semantic descriptions for web sources is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions for web sources, in which we actively invoke sources and compare the data they produce with that of known sources of information. We perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. The paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.|Mark James Carman,Craig A. Knoblock","80721|VLDB|2006|Automatic Extraction of Dynamic Record Sections From Search Engine Result Pages|A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features () it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and () it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.|Hongkun Zhao,Weiyi Meng,Clement T. Yu","65807|AAAI|2006|Bookmark Hierarchies and Collaborative Recommendation|GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely. The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web. GiveALink does not use tags, content, or links in the submitted Web pages. Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users. In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network. Search results can be personalized using the bookmarks submitted by a user. We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.|Benjamin Markines,Lubomira Stoilova,Filippo Menczer","66025|AAAI|2007|TableRank A Ranking Algorithm for Table Search and Retrieval|Tables are ubiquitous in web pages and scientific documents. With the explosive development of the web, tables have become a valuable information repository. Therefore, effectively and efficiently searching tables becomes a challenge. Existing search engines do not provide satisfactory search results largely because the current ranking schemes are inadequate for table search and automatic table understanding and extraction are rather difficult in general. In this work, we design and evaluate a novel table ranking algorithm-TableRank to improve the performance of our table search engine Table-Seer. Given a keyword based table query, TableRank facilities TableSeer to return the most relevant tables by tailoring the classic vector space model. TableRank adopts an innovative term weighting scheme by aggregating multiple weighting factors from three levels term, table and document. The experimental results show that our table search engine out-performs existing search engines on table search. In addition, incorporating multiple weighting factors can significantly improve the ranking results.|Ying Liu,Kun Bai,Prasenjit Mitra,C. Lee Giles","66167|AAAI|2007|Robust Estimation of Google Counts for Social Network Extraction|Various studies within NLP and Semantic Web use the so-called Google count, which is the hit count on a query returned by a search engine (not only Google). However, sometimes the Google count is unreliable, especially when the count is large, or when advanced operators such as OR and NOT are used. In this paper, we propose a novel algorithm that estimates the Google count robustly. It (i) uses the co-occurrence of terms as evidence to estimate the occurrence of a given word, and (ii) integrates multiple evidence for robust estimation. We evaluated our algorithm for more than  queries on three datasets using Google, Yahoo and MSN search engine. Our algorithm also provides estimate counts for any classifier that judges a web page as positive or negative. Consequently, we can estimate the number of documents with included references of a particular person (among namesakes) on the entire web.|Yutaka Matsuo,Hironori Tomobe,Takuichi Nishimura"],["80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","66274|AAAI|2007|Fluxplayer A Successful General Game Player|General Game Playing (GGP) is the art of designing programs that are capable of playing previously unknown games of a wide variety by being told nothing but the rules of the game. This is in contrast to traditional computer game players like Deep Blue, which are designed for a particular game and can't adapt automatically to modifications of the rules, let alone play completely different games. General Game Playing is intended to foster the development of integrated cognitive information processing technology. In this article we present an approach to General Game Playing using a novel way of automatically constructing a position evaluation function from a formal game description. Our system is being tested with a wide range of different games. Most notably, it is the winner of the AAAI GGP Competition .|Stephan Schiffel,Michael Thielscher","16545|IJCAI|2007|Complexity of Pure Equilibria in Bayesian Games|In this paper we make a comprehensive study of the complexity of the problem of deciding the existence of equilibria in strategic games with incomplete information, in case of pure strategies. In particular, we show that this is NP-complete in general Bayesian Games in Standard Normal Form, and that it becomes PP-hard (and, in fixed-precision scenarios, PP-complete), when the game is represented succinctly in General Normal Form. Suitable restrictions in case of graphical games that make the problem tractable are also discussed.|Georg Gottlob,Gianluigi Greco,Toni Mancini","80805|VLDB|2007|Depth Estimation for Ranking Query Optimization|A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just \"enough\" of the input in order to generate just \"enough\" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.|Karl Schnaitter,Joshua Spiegel,Neoklis Polyzotis","66266|AAAI|2007|Thresholded Rewards Acting Optimally in Timed Zero-Sum Games|In timed, zero-sum games, the goal is to maximize the probability of winning, which is not necessarily the same as maximizing our expected reward. We consider cumulative intermediate reward to be the difference between our score and our opponent's score the \"true\" reward of a win, loss, or tie is determined at the end of a game by applying a threshold function to the cumulative intermediate reward. We introduce thresholded-rewards problems to capture this dependency of the final reward outcome on the cumulative intermediate reward. Thresholded-rewards problems reflect different real-world stochastic planning domains, especially zero-sum games, in which time and score need to be considered. We investigate the application of thresholded rewards to finite-horizon Markov Decision Processes (MDPs). In general, the optimal policy for a thresholded-rewards MDP will be non-stationary, depending on the number of time steps remaining and the cumulative intermediate reward. We introduce an efficient value iteration algorithm that solves thresholded-rewards MDPs exactly, but with running time quadratic on the number of states in the MDP and the length of the time horizon. We investigate a number of heuristic-based techniques that efficiently find approximate solutions for MDPs with large state spaces or long time horizons.|Colin McMillen,Manuela M. Veloso","80639|VLDB|2006|Answering Top-k Queries Using Views|The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Dimitris Tsirogiannis","80718|VLDB|2006|Answering Top-k Queries with Multi-Dimensional Selections The Ranking Cube Approach|Observed in many real applications, a top-k query often consists of two components to reflect a user's preference a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server  show that our proposed approaches have significant improvement over the previous methods.|Dong Xin,Jiawei Han,Hong Cheng,Xiaolei Li","65707|AAAI|2006|Exploring GnuGos Evaluation Function with a SVM|While computers have defeated the best human players in many classic board games, progress in Go remains elusive. The large branching factor in the game makes traditional adversarial search intractable while the complex interaction of stones makes it difficult to assign a reliable evaluation function. This is why most existing programs rely on hand-tuned heuristics and pattern matching techniques. Yet none of these solutions perform better than an amateur player. Our work introduces a composite approach, aiming to integrate the strengths of the proved heuristic algorithms, the AI-based learning techniques, and the knowledge derived from expert games. Specifically, this paper presents an application of the Support Vector Machine (SVM) for training the GnuGo evaluation function.|Christopher Fellows,Yuri Malitsky,Gregory Wojtaszczyk","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","65984|AAAI|2007|Efficient Statistical Methods for Evaluating Trading Agent Performance|Market simulations, like their real-world counterparts, are typically domains of high complexity, high variability, and incomplete information. The performance of autonomous agents in these markets depends both upon the strategies of their opponents and on various market conditions, such as supply and demand. Because the space for possible strategies and market conditions is very large, empirical analysis in these domains becomes exceedingly difficult. Researchers who wish to evaluate their agents must run many test games across multiple opponent sets and market conditions to verify that agent performance has actually improved. Our approach is to improve the statistical power of market simulation experiments by controlling their complexity, thereby creating an environment more conducive to structured agent testing and analysis. We develop a tool that controls variability across games in one such market environment, the Trading Agent Competition for Supply Chain Management (TAC SCM), and demonstrate how it provides an efficient, systematic method for TAC SCM researchers to analyze agent performance.|Eric Sodomka,John Collins,Maria L. Gini"],["66124|AAAI|2007|Heuristic Evaluation Functions for General Game Playing|A general game playing program plays games that it has not previously encountered. A game manager program sends the game playing programs a description of a game's rules and objectives in a well-defined game description language. A central challenge in creating effective general game playing programs is that of constructing heuristic evaluation functions from game descriptions. This paper describes a method for constructing evaluation functions that represent exact values of simplified games. The simplified games are abstract models that incorporate the most essential aspects of the original game, namely payoff, control, and termination. Results of applying this method to a sampling of games suggest that heuristic evaluation functions based on our method are both comprehensible and effective.|James Clune","16669|IJCAI|2007|Exploiting Inference Rules to Compute Lower Bounds for MAX-SAT Solving|In this paper we present a general logical framework for (weighted) MAX-SAT problem, and study properties of inference rules for branch and bound MAX-SAT solver. Several rules, which are not equivalent but -equivalent, are proposed, and we show that -equivalent rules are also sound. As an example, we show how to exploit inference rules to achieve a new lower bound function for a MAX--SAT solver. Our new function is admissible and consistently better than the well-known lower bound function. Based on the study of inference rules, we implement an efficient solver and the experimental results demonstrate that our solver outperforms the most efficient solver that has been implemented very recently Heras and Larrosa, , especially for large instances.|Han Lin,Kaile Su","65701|AAAI|2006|Forgetting and Conflict Resolving in Disjunctive Logic Programming|We establish a declarative theory of forgetting for disjunctive logic programs. The suitability of this theory is justified by a number of desirable properties. In particular, one of our results shows that our notion of forgetting is completely captured by the classical forgetting. A transformation-based algorithm is also developed for computing the result of forgetting. We also provide an analysis of computational complexity. As an application of our approach, a fairly general framework for resolving conflicts in inconsistent knowledge bases represented by disjunctive logic programs is defined. The basic idea of our framework is to weaken the preferences of each agent by forgetting certain knowledge that causes inconsistency. In particular, we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming.|Thomas Eiter,Kewen Wang","66274|AAAI|2007|Fluxplayer A Successful General Game Player|General Game Playing (GGP) is the art of designing programs that are capable of playing previously unknown games of a wide variety by being told nothing but the rules of the game. This is in contrast to traditional computer game players like Deep Blue, which are designed for a particular game and can't adapt automatically to modifications of the rules, let alone play completely different games. General Game Playing is intended to foster the development of integrated cognitive information processing technology. In this article we present an approach to General Game Playing using a novel way of automatically constructing a position evaluation function from a formal game description. Our system is being tested with a wide range of different games. Most notably, it is the winner of the AAAI GGP Competition .|Stephan Schiffel,Michael Thielscher","16394|IJCAI|2007|Emotions as Durative Dynamic State for Action Selection|This paper presents a representation system for maintaining interacting durative states to replicate realistic emotional control. Our model, the Dynamic Emotion Representation (DER) integrates emotional responses and keeps track of emotion intensities changing over time. The developer can specify an interacting network of emotional states with appropriate onsets, sustains and decays. The levels of these states can be used as input for action selection, including emotional expression. We present both a general representational framework and a specific instance of a DER network constructed for a virtual character. The character's DER uses three types of emotional state as classified by duration timescales, in keeping with current emotional theory. The system is demonstrated with a virtual actor.|Emmanuel Tanguy,Philip J. Willis,Joanna Bryson","16710|IJCAI|2007|Towards an Integration of Golog and Planning|The action language Golog has been applied successfully to the control of robots, among other things. Perhaps its greatest advantage is that a user can write programs which constrain the search for an executable plan in a flexible manner. However, when general planning is needed, Golog supports this only in principle, but does not measure up with state-of-the-art planners. In this paper we propose an integration of Golog and planning in the sense that planning problems, formulated as part of a Golog program, are solved by a modern planner during the execution of the program. Here we focus on the ADL subset of the plan language PDDL. First we show that the semantics of ADL can be understood as progression in the situation calculus, which underlies Golog, thus providing us with a correct embedding of ADL within Golog. We then show how Golog can be integrated with an existing ADL planner for closed-world initial databases and compare the performance of the resulting system with the original Golog.|Jens Cla√üen,Patrick Eyerich,Gerhard Lakemeyer,Bernhard Nebel","16572|IJCAI|2007|On Reversing Actions Algorithms and Complexity|Reversing actions is the following problem After executing a sequence of actions, which sequence of actions brings the agent back to the state just before this execution (an action reversal). Notably, this problem is different from a vanilla planning problem since the state we have to get back to is in general unknown. It emerges, for example, if an agent needs to find out which action sequences are undoable, and which ones are committed choices. It has applications related to plan execution and monitoring in nondeterministic domains, such as recovering from a failed execution by partially undoing the plan, dynamically switching from one executed plan to another, or restarting plans. We formalize action reversal in a logic-based action framework and characterize its computational complexity. Since unsurprisingly, the problem is intractable in general, we present a knowledge compilation approach that constructs offline a reverse plan library for efficient (in some cases, linear time) on-line computation of action reversals. Our results for the generic framework can be easily applied for expressive action languages such as C+ or .|Thomas Eiter,Esra Erdem,Wolfgang Faber","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","16364|IJCAI|2007|Correlation Clustering for Crosslingual Link Detection|The crosslingual link detection problem calls for identifying news articles in multiple languages that report on the same news event. This paper presents a novel approach based on constrained clustering. We discuss a general way for constrained clustering using a recent, graph-based clustering framework called correlation clustering. We introduce a correlation clustering implementation that features linear program chunking to allow processing larger datasets. We show how to apply the correlation clustering algorithm to the crosslingual link detection problem and present experimental results that show correlation clustering improves upon the hierarchical clustering approaches commonly used in link detection, and, hierarchical clustering approaches that take constraints into account.|Jurgen Van Gael,Xiaojin Zhu","66156|AAAI|2007|Counting CSP Solutions Using Generalized XOR Constraints|We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.|Carla P. Gomes,Willem Jan van Hoeve,Ashish Sabharwal,Bart Selman"],["16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","65627|AAAI|2006|QUICR-Learning for Multi-Agent Coordination|Coordinating multiple agents that need to perform a sequence of actions to maximize a system level reward requires solving two distinct credit assignment problems. First, credit must be assigned for an action taken at time step t that results in a reward at time step t  t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first credit assignment problem is typically addressed with temporal difference methods such as Q-learning. The second credit assignment problem is typically addressed by creating custom reward functions. To address both credit assignment problems simultaneously, we propose the \"Q Updates with Immediate Counterfactual Rewards-learning\" (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods.|Adrian K. Agogino,Kagan Tumer","65718|AAAI|2006|Incremental Least-Squares Temporal Difference Learning|Approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning, usually solved using temporal difference (TD) algorithms. In this paper we introduce a new variant of linear TD learning, called incremental least-squares TD learning, or iLSTD. This method is more data efficient than conventional TD algorithms such as TD() and is more computationally efficient than non-incremental least-squares TD methods such as LSTD (Bradtke & Barto  Boyan ). In particular, we show that the per-time-step complexities of iLSTD and TD() are O(n), where n is the number of features, whereas that of LSTD is O(n). This difference can be decisive in modern applications of reinforcement learning where the use of a large number features has proven to be an effective solution strategy. We present empirical comparisons, using the test problem introduced by Boyan (), in which iLSTD converges faster than TD() and almost as fast as LSTD.|Alborz Geramifard,Michael H. Bowling,Richard S. Sutton","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","66058|AAAI|2007|Efficient Structure Learning in Factored-State MDPs|We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.|Alexander L. Strehl,Carlos Diuk,Michael L. Littman","16488|IJCAI|2007|Deictic Option Schemas|Deictic representation is a representational paradigm, based on selective attention and pointers, that allows an agent to learn and reason about rich complex environments. In this article we present a hierarchical reinforcement learning framework that employs aspects of deictic representation. We also present a Bayesian algorithm for learning the correct representation for a given sub-problem and empirically validate it on a complex game environment.|Balaraman Ravindran,Andrew G. Barto,Vimal Mathew","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","65812|AAAI|2006|A Simple and Effective Method for Incorporating Advice into Kernel Methods|We propose a simple mechanism for incorporating advice (prior knowledge), in the form of simple rules, into support-vector methods for both classification and regression. Our approach is based on introducing inequality constraints associated with datapoints that match the advice. These constrained datapoints can be standard examples in the training set, but can also be unlabeled data in a semi-supervised, advice-taking approach. Our new approach is simpler to implement and more efficiently solved than the knowledge-based support vector classification methods of Fung, Mangasarian and Shavlik ( ) and the knowledge-based support vector regression method of Mangasarian, Shavlik, and Wild (), while performing approximately as well as these more complex approaches. Experiments using our new approach on a synthetic task and a reinforcement-learning problem within the RoboCup soccer simulator show that our advice-taking method can significantly outperform a method without advice and perform similarly to prior advice-taking, support-vector machines.|Richard Maclin,Jude W. Shavlik,Trevor Walker,Lisa Torrey","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["16477|IJCAI|2007|Diagnosability Testing with Satisfiability Algorithms|We show how testing whether a system is diagnosable can be reduced to the satisfiability problem and how satisfiability algorithms yield a very efficient approach to testing diagnosability. Diagnosability is the question whether it is always possible to know whether a given system has exhibited a failure behavior. This is a basic question that underlies diagnosis, and it is also closely related to more general questions about the possibility to know given facts about system behavior. The work combines the twin plant construct of Jiang et al., which is the basis of diagnosability testing of systems with an enumerative representation, and SAT-based techniques to AI planning which form a very promising approach to finding paths in very large transition graphs.|Jussi Rintanen,Alban Grastien","16529|IJCAI|2007|Inferring Long-term User Properties Based on Users Location History|Recent development of location technologies enables us to obtain the location history of users. This paper proposes a new method to infer users' longterm properties from their respective location histories. Counting the instances of sensor detection for every user, we can obtain a sensor-user matrix. After generating features from the matrix, a machine learning approach is taken to automatically classify users into different categories for each user property. Inspired by information retrieval research, the problem to infer user properties is reduced to a text categorization problem. We compare weightings of several features and also propose sensor weighting. Our algorithms are evaluated using experimental location data in an office environment.|Yutaka Matsuo,Naoaki Okazaki,Kiyoshi Izumi,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideyuki Nakashima","16764|IJCAI|2007|Belief Change Based on Global Minimisation|A general framework for minimisation-based belief change is presented. A problem instance is made up of an undirected graph, where a formula is associated with each vertex. For example, vertices may represent spatial locations, points in time, or some other notion of locality. Information is shared between vertices via a process of minimisation over the graph. We give equivalent semantic and syntactic characterisations of this minimisation. We also show that this approach is general enough to capture existing minimisation-based approaches to belief merging, belief revision, and (temporal) extrapolation operators. While we focus on a set-theoretic notion of minimisation, we also consider other approaches, such as cardinality-based and priority-based minimisation.|James P. Delgrande,J√©r√¥me Lang,Torsten Schaub","66110|AAAI|2007|Approximate Counting by Sampling the Backtrack-free Search Space|We present a new estimator for counting the number of solutions of a Boolean satisfiability problem as a part of an importance sampling framework. The estimator uses the recently introduced SampleSearch scheme that is designed to overcome the rejection problem associated with distributions having a substantial amount of determinism. We show here that the sampling distribution of SampleSearch can be characterized as the backtrack-free distribution and propose several schemes for its computation. This allows integrating Sample-Search into the importance sampling framework for approximating the number of solutions and also allows using Sample-Search for computing a lower bound measure on the number of solutions. Our empirical evaluation demonstrates the superiority of our new approximate counting schemes against recent competing approaches.|Vibhav Gogate,Rina Dechter","65724|AAAI|2006|Model Counting A New Strategy for Obtaining Good Bounds|Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained.|Carla P. Gomes,Ashish Sabharwal,Bart Selman","65942|AAAI|2006|Optimal Unbiased Estimators for Evaluating Agent Performance|Evaluating the performance of an agent or group of agents can be, by itself, a very challenging problem. The stochastic nature of the environment plus the stochastic nature of agents' decisions can result in estimates with intractably large variances This paper examines the problem of finding low variance estimates of agent performance. In particular, we assume that some agent-environment dynamics are known, such as the random outcome of drawing a card or rolling a die. Other dynamics are unknown, such as the reasoning of a human or other black-box agent. Using the known dynamics, we describe the complete set of all unbiased estimators, that is, for any possible unknown dynamics the estimate's expectation is always the agent's expected utility. Then, given a belief abcut the unknown dynamics, we identify the unbiased estimator with minimum variance. If the belief is correct our estimate is optimal, and if the belief is wrong it is at least unbiased. Finally, we apply our unbiased estimator to the game of poker, demonstrating dramatically reduced variance and faster evaluation.|Martin Zinkevich,Michael H. Bowling,Nolan Bard,Morgan Kan,Darse Billings","16789|IJCAI|2007|Local Search for Balanced Submodular Clusterings|In this paper, we consider the problem of producing balanced clusterings with respect to a submodular objective function. Submodular objective functions occur frequently in many applications, and hence this problem is broadly applicable. We show that the results of Patkar and Narayanan  can be applied to cases when the submodular function is derived from a bipartite object-feature graph, and moreover, in this case we have an efficient flow based algorithm for finding local improvements. We show the effectiveness of this approach by applying it to the clustering of words in language models.|Mukund Narasimhan,Jeff Bilmes","16790|IJCAI|2007|Co-Localization from Labeled and Unlabeled Data Using Graph Laplacian|This paper addresses the problem of recovering the locations of both mobile devices and access points from radio signals, a problem which we call colocalization, by exploiting both labeled and unlabeled data from mobile devices and access points. We first propose a solution using Latent Semantic Indexing to construct the relative locations of the mobile devices and access points when their absolute locations are unknown. We then propose a semi-supervised learning algorithm based on manifold to obtain the absolute locations of the devices. Both solutions are finally combined together in terms of graph Laplacian. Extensive experiments are conducted in wireless local-area networks, wireless sensor networks and radio frequency identification networks. The experimental results showthat we can achieve high accuracy with much less calibration effort as compared to several previous systems.|Jeffrey Junfeng Pan,Qiang Yang","80697|VLDB|2006|The Spatial Skyline Queries|In this paper, for the first time, we introduce the concept of Spatial Skyline Queries (SSQ). Given a set of data points P and a set of query points Q each data point has a number of derived spatial attributes each of which is the point's distance to a query point. An SSQ retrieves those points of P which are not dominated by any other point in P considering their derived spatial attributes. The main difference with the regular skyline query is that this spatial domination depends on the location of the query points Q SSQ has application in several domains such as emergency response and online maps. The main intuition and novelty behind our approaches is that we exploit the geometric properties of the SSQ problem space to avoid the exhaustive examination of all the point pairs in P and Q. Consequently, we reduce the complexity of SSQ search from O(PQ) to O(SC+P), where S and C are the solution size and the number of vertices of the convex hull of Q, respectively.We propose two algorithms, BS and VS, for static query points and one algorithm, VCS, for streaming Q whose points change location over time (e.g., are mobile). VCS exploits the pattern of change in Q to avoid unnecessary re-computation of the skyline and hence efficiently perform updates. Our extensive experiments using real-world datasets verify that both R-tree-based BS and Voronoi-based VS out perform the best competitor approach in terms of processing time by a wide margin (- times better in most cases).|Mehdi Sharifzadeh,Cyrus Shahabi","16796|IJCAI|2007|Parametric Kernels for Sequence Data Analysis|A key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. Many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. However, finding an effective transformation scheme for sequence (e.g. time series) data is a difficult task. In this paper, we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwritten character recognition and object recognition from sensor readings. Ordering information is represented by values of a parameter associated with each input data element. A similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a Mercer kernel suitable for use in methods such as support vector machine (SVM). This scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. We apply our method to object and handwritten character recognition tasks and compare against current approaches. The results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. Our contribution is the introduction of a general technique for designing SVM kernels tailored for the classification of sequence data.|Young-In Shin,Donald S. Fussell"],["16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessi√®re,Remi Coletta,Fr√©d√©ric Koriche,Barry O'Sullivan","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","65631|AAAI|2006|The Impact of Balancing on Problem Hardness in a Highly Structured Domain|Random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider random problem distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCPQWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness.|Carlos Ans√≥tegui,Ram√≥n B√©jar,C√®sar Fern√°ndez,Carla P. Gomes,Carles Mateu","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","16784|IJCAI|2007|A Flexible Unsupervised PP-Attachment Method Using Semantic Information|In this paper we revisit the classical NLP problem of prepositional phrase attachment (PP-attachment). Given the pattern V -NP-P-NP in the text, where V is verb, NP is a noun phrase, P is the preposition and NP is the other noun phrase, the question asked is where does P - NP attach V or NP This question is typically answered using both the word and the world knowledge. Word Sense Disambiguation (WSD) and Data Sparsity Reduction (DSR) are the two requirements for PP-attachment resolution. Our approach described in this paper makes use of training data extracted from raw text, which makes it an unsupervised approach. The unambiguous V - P - N and N - P -N tuples of the training corpus TEACH the system how to resolve the attachments in the ambiguous V - N - P - N tuples of the test corpus. A graph based approach to word sense disambiguation (WSD) is used to obtain the accurate word knowledge. Further, the data sparsity problem is addressed by (i) detecting synonymy using the wordnet and (ii) doing a form of inferencing based on the matching of Vs and Ns in the unambiguous patterns of V - P - NP, NP - P - NP. For experimentation, Brown Corpus provides the training data andWall Street Journal Corpus the test data. The accuracy obtained for PP-attachment resolution is close to %. The novelty of the system lies in the flexible use of WSD and DSR phases.|Srinivas Medimi,Pushpak Bhattacharyya","66125|AAAI|2007|Transposition Tables for Constraint Satisfaction|In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.|Christophe Lecoutre,Lakhdar Sais,S√©bastien Tabary,Vincent Vidal","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith"],["80626|VLDB|2006|To Tune or not to Tune A Lightweight Physical Design Alerter|In recent years there has been considerable research on automating the physical design in database systems. Current techniques provide good recommendations, but are resource intensive. This makes DBAs somewhat conservative when deciding to launch a resource-intensive tuning session. In this paper, we introduce an alerter that helps determining when a physical design tool should be invoked. The alerter is a lightweight mechanism that provides guaranteed lower (and upper bounds) on the improvement that a DBA could expect by invoking a comprehensive physical design tool. Moreover, it produces an accompanying recommendation that serves as a \"proof\" for the lower bound. We show experimentally that the alerter handles large workloads with little overhead, and help judiciously decide on launching subsequent tuning sessions.|Nicolas Bruno,Surajit Chaudhuri","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","66267|AAAI|2007|Responding to Student Affect and Efficacy through Empathetic Companion Agents in Interactive Learning Environments|Because many students experience frustration during learning, it is important to develop affective strategies to support students' coping with frustration in interactive learning environments. First, we must devise affect recognition models to detect student affect. Second, we need to determine when to intervene these conditions are likely to be different for each student. To determine how much frustration a student can persist through, we should utilize models of student self-efficacy to predict a student's frustration threshold. Third, we should devise techniques for responding empathetically before the student reaches her threshold of frustration. We propose an approach to support students' coping with frustration in intelligent tutoring systems that utilizes induced models of affect, self-efficacy and empathetic behavior to effectively reason about precisely when and how to intervene in frustration-ridden learning situations.|Scott W. McQuiggan","80783|VLDB|2007|Executing Stream Joins on the Cell Processor|Low-latency and high-throughput processing are key requirements of data stream management systems (DSMSs). Hence, multi-core processors that provide high aggregate processing capacity are ideal matches for executing costly DSMS operators. The recently developed Cell processor is a good example of a heterogeneous multi-core architecture and provides a powerful platform for executing data stream operators with high-performance. On the down side, exploiting the full potential of a multi-core processor like Cell is often challenging, mainly due to the heterogeneous nature of the processing elements, the software managed local memory at the co-processor side, and the unconventional programming model in general. In this paper, we study the problem of scalable execution of windowed stream join operators on multi-core processors, and specifically on the Cell processor. By examining various aspects of join execution flow, we determine the right set of techniques to apply in order to minimize the sequential segments and maximize parallelism. Concretely, we show that basic windows coupled with low-overhead pointer-shifting techniques can be used to achieve efficient join window partitioning, column-oriented join window organization can be used to minimize scattered data transfers, delay-optimized double buffering can be used for effective pipelining, rate-aware batching can be used to balance join throughput and tuple delay, and finally SIMD (single-instruction multiple-data) optimized operator code can be used to exploit data parallelism. Our experimental results show that, following the design guidelines and implementation techniques outlined in this paper, windowed stream joins can achieve high scalability (linear in the number of co-processors) by making efficient use of the extensive hardware parallelism provided by the Cell processor (reaching data processing rates of &ap  GBsec) and significantly surpass the performance obtained form conventional high-end processors (supporting a combined input stream rate of  tuplessec using  minutes windows and without dropping any tuples, resulting in &ap . times higher output rate compared to an SSE implementation on dual .Ghz Intel Xeon).|Bugra Gedik,Philip S. Yu,Rajesh Bordawekar","65871|AAAI|2006|Deeper Natural Language Processing for Evaluating Student Answers in Intelligent Tutoring Systems|This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.|Vasile Rus,Arthur C. Graesser","65642|AAAI|2006|Classifying Learner Engagement through Integration of Multiple Data Sources|Intelligent tutoring systems (ITS) can provide effective instruction, but learners do not always use such systems effectively. In the present study, high school students' action sequences with a mathematics ITS were machine-classified into five finite-state machines indicating guessing strategies, appropriate help use, and independent problem solving over % of problem events were categorized. Students were grouped via cluster analyses based on self reports of motivation. Motivation grouping predicted ITS strategic approach better than prior math achievement (as rated by classroom teachers). Learners who reported being disengaged in math were most likely to exhibit appropriate help use while working with the ITS, relative to average and high motivation learners. The results indicate that learners can readily report their motivation state and that these data predict how learners interact with the ITS.|Carole R. Beal,Lei Qu,Hyokyeong Lee","65947|AAAI|2007|Integrated Introspective Case-Based Reasoning for Intelligent Tutoring Systems|Many intelligent tutoring systems (ITSs) have been developed, deployed, assessed, and proven to facilitate learning. However, most of these systems do not generally adapt to new circumstances, do not self-evaluate and self-configure their own strategies, and do not monitor the usage history of the learning content being delivered or presented to the students. These shortcomings force ITS developers to often spend much development time in manual revision and fine-tuning of the learning and instructional contents of an ITS. In this paper, we describe an intelligent agent that delivers learning material adaptively to different students, factoring in the usage history of the learning materials and student profiles as observed by the agent. Student-tutor interaction includes the activities of going through learning material, such as a topical tutorial, a set of examples, and a set of problems. Our assumption is that our agent will be able to capture and utilize these student activities as the primer to select the appropriate examples or problems to administer to the student. Using an integrated introspective case-based reasoning approach, our agent further learns from its experience and refines its reasoning process-including the instructional strategies-to adapt to student needs. Moreover, our agent monitors the usage history of the learning materials to improve its performance. We have built an end-to-end ITS using an agent powered by this integrated introspective case-based reasoning engine. We have deployed the ITS in a CS course. Results indicate that the ITS was able to learn to deliver more appropriate examples and problems to the students.|Leen-Kiat Soh","65752|AAAI|2006|A Dynamic Mixture Model to Detect Student Motivation and Proficiency|Unmotivated students do not reap the full rewards of using a computer-based intelligent tutoring system. Detection of improper behavior is thus an important component of an online student model. To meet this challenge, we present a dynamic mixture model based on Item Response Theory. This model, which simultaneously estimates a student's proficiency and changing motivation level, was tested with data of high school students using a geometry tutoring system. By accounting for student motivation, the dynamic mixture model can more accurately estimate proficiency and the probability of a correct response. The model's generality is an added benefit, making it applicable to many intelligent tutoring systems as well as other domains.|Jeffrey Johns,Beverly Park Woolf","66160|AAAI|2007|Extending Cognitive Architecture with Episodic Memory|In this paper, we explore the hypothesis that episodic memory is a critical component for cognitive architectures that support general intelligence. Episodic memory overlaps with case-based reasoning (CBR) and can be seen as a task-independent, architectural approach to CBR. We define the design space for episodic memory systems and the criteria any implementation must meet to be useful in a cognitive architecture. We present an implementation and demonstrate how episodic memory, combined with other components of a cognitive architecture, supports a wealth of cognitive capabilities that are difficult to attain without it.|Andrew Nuxoll,John E. Laird","80760|VLDB|2007|Self-Tuning Database Systems A Decade of Progress|In this paper we discuss advances in self-tuning database systems over the past decade, based on our experience in the AutoAdmin project at Microsoft Research. This paper primarily focuses on the problem of automated physical database design. We also highlight other areas where research on self-tuning database technology has made significant progress. We conclude with our thoughts on opportunities and open issues.|Surajit Chaudhuri,Vivek R. Narasayya"]]},"title":{"entropy":6.3328902309173625,"topics":["algorithm for, for data, heuristic search, data, named entity, search, search for, value iteration, query processing, answering queries, data management, efficient for, constraint satisfaction, local search, new for, web search, distributed optimization, top-k queries, search space, query for","learning for, reinforcement learning, learning, method for, using, models for, transfer learning, for and, learning and, bayesian networks, neural networks, for networks, image using, decision tree, learning with, real-time video, random fields, probabilistic and, using models, and method","description logic, for logic, the logic, mechanism design, planning with, situation calculus, logic, for planning, and logic, with, logic programs, with preferences, for and, logic programming, for agent, answer set, constraint problems, the problems, language and, the modal","the web, semantic web, the, reasoning about, decision processes, markov processes, sense disambiguation, markov decision, web services, and reasoning, system for, the and, for reasoning, for the, the semantic, large scale, web and, for and, word disambiguation, system and","for data, efficient for, data, and data, data management, techniques for, schema matching, efficient with, efficient over, efficient, efficient and, data with, for xml, techniques xml, over data, models xml, for fusion, multiagent for, for management, efficient xml","algorithm for, new for, algorithm and, distributed optimization, for optimization, for constraint, constraint optimization, new distributed, new algorithm, for and, bounds for, anytime algorithm, for text, for optimal, algorithm large, dynamic for, for configuration, optimal algorithm, for access, anytime for","learning for, learning and, learning, reinforcement learning, using learning, learning models, transfer learning, learning with, machine learning, for reinforcement, good the, dynamic for, efficient learning, learning algorithm, evolutionary algorithm, semi-supervised learning, reinforcement with, learning data, and relations, relational learning","and evaluation, for based, decision tree, real-time video, image using, using wikipedia, for clustering, using tree, value for, pomdps value, evaluation functions, for evaluation, value functions, based, and image, value and, for functions, for detection, based and, for image","for agent, mechanism design, for information, the partial, and agent, automated mechanism, incomplete information, for online, automated design, partial mechanism, agent, information, mechanism for, partial, online, from, elicitation, properties, under, decentralized","logic programs, planning with, for programs, the problems, with and, with, with preferences, for problems, constraint problems, planning goal, with constraint, for with, the with, for abstract, the constraint, solving problems, for domains, logic with, for solving, heuristic planning","reasoning about, for reasoning, and reasoning, from, knowledge from, from and, reasoning, extracting from, extracting and, from observation, from behavior, behavior, ranking, general, through, control, more, generic, predicting, automatic","the web, for the, the and, the, semantic web, web and, web services, belief and, the semantic, semantic for, services and, for web, the impact, the services, for services, services composition, from the, semantic and, using web, web planning"],"ranking":[["66248|AAAI|2007|Automatic Algorithm Configuration Based on Local Search|The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variablevalue ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at httpwww.cs.ubc.calabsbetaProjectsParamILS.|Frank Hutter,Holger H. Hoos,Thomas St√ºtzle","80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","16451|IJCAI|2007|Web Page Clustering Using Heuristic Search in the Web Graph|Effective representation of Web search results remains an open problem in the Information Retrieval community. For ambiguous queries, a traditional approach is to organize search results into groups (clusters), one for each meaning of the query. These groups are usually constructed according to the topical similarity of the retrieved documents, but it is possible for documents to be totally dissimilar and still correspond to the same meaning of the query. To overcome this problem, we exploit the thematic locality of the Web--relevant Web pages are often located close to each other in the Web graph of hyperlinks. We estimate the level of relevance between each pair of retrieved pages by the length of a path between them. The path is constructed using multi-agent beam search each agent starts with one Web page and attempts to meet as many other agents as possible with some bounded resources. We test the system on two types of queries ambiguous English words and people names. The Web appears to be tightly connected about % of the agents meet with each other after only three iterations of exhaustive breadth-first search. However, when heuristics are applied, the search becomes more focused and the obtained results are substantially more accurate. Combined with a content-driven Web page clustering technique, our heuristic search system significantly improves the clustering results.|Ron Bekkerman,Shlomo Zilberstein,James Allan","16522|IJCAI|2007|A Multiobjective Frontier Search Algorithm|The paper analyzes the extension of frontier search to the multiobjective framework. A frontier multiobjective A* search algorithm is developed, some formal properties are presented, and its performance is compared to those of other multiobjective search algorithms. The new algorithm is adequate for both monotone and non-monotone heuristics.|Lawrence Mandow,Jos√©-Luis P√©rez-de-la-Cruz","16630|IJCAI|2007|Building Structure into Local Search for SAT|Local search procedures for solving satisfiability problems have attracted considerable attention since the development of GSAT in . However, recentwork indicates that for many real-world problems, complete search methods have the advantage, because modern heuristics are able to effectively exploit problem structure. Indeed, to develop a local search technique that can effectively deal with variable dependencies has been an open challenge since . In this paper we show that local search techniques can effectively exploit information about problem structure producing significant improvements in performance on structured problem instances. Building on the earlier work of Ostrowski et al. we describe how information about variable dependencies can be built into a local search, so that only independent variables are considered for flipping. The cost effect of a flip is then dynamically calculated using a dependency lattice that models dependent variables using gates (specifically and, or and equivalence gates). The experimental study on hard structured benchmark problems demonstrates that our new approach significantly outperforms the previously reported best local search techniques.|Duc Nghia Pham,John Thornton,Abdul Sattar","16401|IJCAI|2007|Forward Search Value Iteration for POMDPs|Recent scaling up of POMDP solvers towards realistic applications is largely due to point-based methods which quickly converge to an approximate solution formedium-sized problems. Of this family HSVI, which uses trial-based asynchronous value iteration, can handle the largest domains. In this paper we suggest a new algorithm, FSVI, that uses the underlying MDP to traverse the belief space towards rewards, finding sequences of useful backups, and show how it scales up better than HSVI on larger benchmarks.|Guy Shani,Ronen I. Brafman,Solomon Eyal Shimony","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","16437|IJCAI|2007|GUNSAT A Greedy Local Search Algorithm for Unsatisfiability|Local search algorithms for satisfiability testing are still the best methods for a large number of problems, despite tremendous progresses observed on complete search algorithms over the last few years. However, their intrinsic limit does not allow them to address UNSAT problems. Ten years ago, this question challenged the community without any answer was it possible to use local search algorithm for UNSAT formulae We propose here a first approach addressing this issue, that can beat the best resolution-based completemethods. We define the landscape of the search by approximating the number of filtered clauses by resolution proof. Furthermore, we add high-level reasoning mechanism, based on Extended Resolution and Unit Propagation Look-Ahead to make this new and challenging approach possible. Our new algorithm also tends to be the first step on two other challenging problems obtaining short proofs for UNSAT problems and build a real local-search algorithm for QBF.|Gilles Audemard,Laurent Simon","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"],["65967|AAAI|2007|Using Multiresolution Learning for Transfer in Image Classification|Our work explores the transfer of knowledge at multiple levels of abstraction to improve learning. By exploiting the similarities between objects at various levels of detail, multiresolution learning can facilitate transfer between image classification tasks. We extract features from images at multiple levels of resolution, then use these features to create models at different resolutions. Upon receiving a new task, the closest-matching stored model can be generalized (adapted to the appropriate resolution) and transferred to the new task.|Eric Eaton,Marie desJardins,John Stevenson","16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","16634|IJCAI|2007|Transfer Learning in Real-Time Strategy Games Using Hybrid CBRRL|The goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. In this paper, we present a multilayered architecture named CAse-Based Reinforcement Learner (CARL). It uses a novel combination of Case-Based Reasoning (CBR) and Reinforcement Learning (RL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTSTM, a commercial Real Time Strategy game. Our experiments demonstrate that CARL not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.|Manu Sharma,Michael P. Holmes,Juan Carlos Santamar√≠a,Arya Irani,Charles Lee Isbell Jr.,Ashwin Ram","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","16471|IJCAI|2007|A Tighter Error Bound for Decision Tree Learning Using PAC Learnability|Error bounds for decision trees are generally based on depth or breadth of the tree. In this paper, we propose a bound for error rate that depends both on the depth and the breadth of a specific decision tree constructed from the training samples. This bound is derived from sample complexity estimate based on PAC learnability. The proposed bound is compared with other traditional error bounds on several machine learning benchmark data sets as well as on an image data set used in Content Based Image Retrieval (CBIR). Experimental results demonstrate that the proposed bound gives tighter estimation of the empirical error.|Chaithanya Pichuka,Raju S. Bapi,Chakravarthy Bhagvati,Arun K. Pujari,Bulusu Lakshmana Deekshatulu","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","16786|IJCAI|2007|An Extension to Conformant Planning Using Logic Programming|In this paper we extend the logic programming based conformant planner described in Son et al., a to allow it to work on planning problems with more complex descriptions of the initial states. We also compare the extended planner with other concurrent conformant planners.|A. Ricardo Morales,Phan Huy Tu,Tran Cao Son","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","66095|AAAI|2007|Logic for Automated Mechanism Design - A Progress Report|Over the past half decade, we have been exploring the use of logic in the specification and analysis of computational economic mechanisms. We believe that this approach has the potential to bring the same benefits to the design and analysis of computational economic mechanisms that the use of temporal logics and model checking have brought to the specification and analysis of reactive systems. In this paper, we give a survey of our work. We first discuss the use of cooperation logics such as Alternating-time Temporal Loglc (ATL) for the specification and verification of mechanisms such as social choice procedures. We motivate the approach, and then discuss the work we have done on extensions to ATL to support incomplete information, preferences, and quantification over coalition. We then discuss is the use of ATL-like cooperation logics in the development of social laws.|Michael Wooldridge,Thomas √\u2026gotnes,Paul E. Dunne,Wiebe van der Hoek","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski","65891|AAAI|2006|Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms|In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs. The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms. We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.|Tran Cao Son,Enrico Pontelli,Phan Huy Tu"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","66013|AAAI|2007|Towards Large Scale Argumentation Support on the Semantic Web|This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW) a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Users can also attack or support parts of existing arguments, use existing parts of an argument in the creation of new arguments, or create new argumentation schemes. As such, this initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.|Iyad Rahwan,Fouad Zablith,Chris Reed","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","65754|AAAI|2006|An End-to-End Supervised Target-Word Sense Disambiguation System|We present an extensible supervised Target-Word Sense Disambiguation system that leverages upon GATE (General Architecture for Text Engineering), NSP (Ngram Statistics Package) and WEKA (Waikato Environment for Knowledge Analysis) to present an end-to-end solution that integrates feature identification, feature extraction, preprocessing and classification.|Mahesh Joshi,Serguei V. S. Pakhomov,Ted Pedersen,Richard Maclin,Christopher G. Chute","66010|AAAI|2007|A Unification of Extensive-Form Games and Markov Decision Processes|We describe a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization is that it places arbitrary convex optimization problems at decision nodes, in place of the finite action sets typically considered. The possibly-infinite action sets mean we must \"forget\" the exact action taken (feasible solution to the optimization problem), remembering instead only some statistic sufficient for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games.|H. Brendan McMahan,Geoffrey J. Gordon","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas W√∂lfl","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"],["80625|VLDB|2006|Efficient Allocation Algorithms for OLAP Over Imprecise Data|Recent work proposed extending the OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform a given imprecise fact table into a form, called the Extended Database, that can be readily used to answer OLAP aggregation queries.In this work, we present scalable, efficient algorithms for creating the Extended Database (i.e., performing allocation) for a given imprecise fact table. Many allocation policies require multiple iterations over the imprecise fact table, and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, and has not been considered previously to the best of our knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straight-foward approaches.|Douglas Burdick,Prasad M. Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80615|VLDB|2006|Inference of Concise DTDs from XML Data|We consider the problem to infer a concise Document Type Definition (DTD) for a given set of XML-documents, a problem which basically reduces to learning of concise regular expressions from positive example strings. We identify two such classes single occurrence regular expressions (SOREs) and chain regular expressions (CHAREs). Both classes capture the far majority of the regular expressions occurring in practical DTDs and are succinct by definition. We present the algorithm iDTD (infer DTD) that learns SOREs from strings by first inferring an automaton by known techniques and then translating that automaton to a corresponding SORE, possibly by repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. We show that iDTD outperforms existing systems in accuracy, conciseness and speed. In a scenario where only a very small amount of XML data is available, for instance when generated by Web service requests or by answers to queries, iDTD produces regular expressions which are too specific. Therefore, we introduce a novel learning algorithm CRX that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that CRX performs very well within its target class on very small data sets. Finally, we discuss incremental computation, noise, numerical predicates, and the generation of XML Schemas.|Geert Jan Bex,Frank Neven,Thomas Schwentick,Karl Tuyls","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80645|VLDB|2006|Efficient Discovery of XML Data Redundancies|As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often \"casually designed\" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.|Cong Yu,H. V. Jagadish","80611|VLDB|2006|On the Path to Efficient XML Queries|XQuery and SQLXML are powerful new languages for querying XML data. However, they contain a number of stumbling blocks that users need to be aware of to get the expected results and performance. For example, certain language features make it hard if not impossible to exploit XML indexes.The major database vendors provide XQuery and SQLXML support in their current or upcoming product releases. In this paper, we identify common pitfalls gleaned from the experiences of early adopters of this functionality. We illustrate these pitfalls through concrete examples, explain the unexpected query behavior, and show alternative formulations of the queries that behave and perform as anticipated. As results we provide guidelines for XQuery and SQLXML users, feedback on the language standards, and food for thought for emerging languages and APIs.|Andrey Balmin,Kevin S. Beyer,Fatma √\u2013zcan,Matthias Nicola","80672|VLDB|2006|Efficient Incremental Maintenance of Data Cubes|The data cube provides users with aggregated results that are group-bys for all possible combinations of dimension attributes. When the number of dimension attributes is n, the data cube computes n group-bys, each of which is called a cuboid. A data cube is often precomputed and stored as materialized views in data warehouses. The data cube needs to be updated when source relations change. The incremental maintenance of a data cube is to compute and propagate only changes of source relations rather than recompute the entire data cube from the source relations.To maintain a data cube incrementally, previous methods compute a delta cube which represents the change of the data cube. We call a cuboid in a delta cube a delta cuboid. For a data cube with n cuboids, a delta cube consists of n delta cuboids. Thus, as the number of dimension attributes increases, the cost of computing the delta cube increases significantly. In this paper, we propose an incremental maintenance method for data cubes that can maintain a data cube by using only (n n) delta cuboids. As a result, the cost of computing delta cuboids is substantially reduced. Through various experiments, we show the performance advantages of our method over the previous methods.|Ki Yong Lee,Myoung-Ho Kim","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"],["16658|IJCAI|2007|AEMS An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs|Solving large Partially Observable Markov Decision Processes (POMDPs) is a complex task which is often intractable. A lot of effort has been made to develop approximate offline algorithms to solve ever larger POMDPs. However, even state-of-the-art approaches fail to solve large POMDPs in reasonable time. Recent developments in online POMDP search suggest that combining offline computations with online computations is often more efficient and can also considerably reduce the error made by approximate policies computed offline. In the same vein, we propose a new anytime online search algorithm which seeks to minimize, as efficiently as possible, the error made by an approximate value function computed offline. In addition, we show how previous online computations can be reused in following time steps in order to prevent redundant computations. Our preliminary results indicate that our approach is able to tackle large state space and observation space efficiently and under real-time constraints.|St√©phane Ross,Brahim Chaib-draa","66227|AAAI|2007|Possibilistic Causal Networks for Handling Interventions A New Propagation Algorithm|This paper contains two important contributions for the development of possibilistic causal networks. The first one concerns the representation of interventions in possibilistic networks. We provide the counterpart of the \"DO\" operator, recently introduced by Pearl, in possibility theory framework. We then show that interventions can equivalently be represented in different ways in possibilistic causal networks. The second main contribution is a new propagation algorithm for dealing with both observations and interventions. We show that our algorithm only needs a small extra cost for handling interventions and is more appropriate for handling sequences of observations and interventions.|Salem Benferhat,Salma Smaoui","80679|VLDB|2006|Analysis of Two Existing and One New Dynamic Programming Algorithm for the Generation of Optimal Bushy Join Trees without Cross Products|Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph.|Guido Moerkotte,Thomas Neumann","16629|IJCAI|2007|MB-DPOP A New Memory-Bounded Algorithm for Distributed Optimization|In distributed combinatorial optimization problems, dynamic programming algorithms like DPOP (Petcu and Faltings, ) require only a linear number of messages, thus generating low communication overheads. However, DPOP's memory requirements are exponential in the induced width of the constraint graph, which may be prohibitive for problems with large width. We present MB-DPOP, a new hybrid algorithm that can operate with bounded memory. In areas of low width, MB-DPOP operates like standard DPOP (linear number of messages). Areas of high width are explored with bounded propagations using the idea of cycle-cuts Dechter, . We introduce novel DFS-based mechanisms for determining the cycle-cutset, and for grouping cycle-cut nodes into clusters. We use caching (Darwiche, ) between clusters to reduce the complexity to exponential in the largest number of cycle cuts in a single cluster. We compare MB-DPOP with ADOPT Modi et al., , the current state of the art in distributed search with bounded memory. MB-DPOP consistently outperforms ADOPT on  problem domains, with respect to  metrics, providing speedups of up to  orders of magnitude.|Adrian Petcu,Boi Faltings","65963|AAAI|2007|A New Algorithm for Generating Equilibria in Massive Zero-Sum Games|In normal scenarios, computer scientists often consider the number of states in a game to capture the difficulty of learning an equilibrium. However, players do not see games in the same light most consider Go or Chess to be more complex than Monopoly. In this paper, we discuss a new measure of game complexity that links existing state-of-the-art algorithms for computing approximate equilibria to a more human measure. In particular, we consider the range of skill in a game, i.e. how many different skill levels exist. We then modify existing techniques to design a new algorithm to compute approximate equilibria whose performance can be captured by this new measure. We use it to develop the first near Nash equilibrium for a four round abstraction of poker, and show that it would have been able to win handily the bankroll competition from last year's AAAI poker competition.|Martin Zinkevich,Michael H. Bowling,Neil Burch","65728|AAAI|2006|Analysis of Privacy Loss in Distributed Constraint Optimization|Distributed Constraint Optimization (DCOP) is rapidly emerging as a prominent technique for multi agent coordination. However, despite agent privacy being a key motivation for applying DCOPs in many applications, rigorous quantitative evaluations of privacy loss in DCOP algorithms have been lacking. Recently, Maheswaran et al.  introduced a framework for quantitative evaluations of privacy in DCOP algorithms, showing that some DCOP algorithms lose more privacy than purely centralized approaches and questioning the motivation for applying DCOPs. This paper addresses the question of whether state-of-the art DCOP algorithms suffer from a similar shortcoming by investigating several of the most efficient DCOP algorithms, including both DPOP and ADOPT. Furthermore, while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions (e.g. constraint-graph topology, asynchrony, message-contents), this paper examines the privacy aspect of such decisions, providing an improved understanding of privacy-efficiency tradeoffs.|Rachel Greenstadt,Jonathan P. Pearce,Milind Tambe","65853|AAAI|2006|ODPOP An Algorithm for OpenDistributed Constraint Optimization|We propose ODPOP, a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domains (Faltings & Macho-Gonzalez ). The ODPOP algorithm explores the same search space as the dynamic programming algorithm DPOP (Petcu & Faltings b) or ADOPT (Modi et at. ). but does so in an incremental, best-first fashion suitable for open problems. ODPOP has several advantages over DPOP. First, it uses messages whose size only grows linearly with the treewidth of the problem. Second, by letting agents explore values in a best-first order, it avoids incurring always the worst case complexity as DPOP, and on average it saves a significant amount of computation and information exchange. To show the merits of our approach, we report on experiments with practically sized distributed meeting scheduling problems on a multiagent system.|Adrian Petcu,Boi Faltings","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","16791|IJCAI|2007|PC-DPOP A New Partial Centralization Algorithm for Distributed Optimization|Fully decentralized algorithms for distributed constraint optimization often require excessive amounts of communication when applied to complex problems. The OptAPO algorithm of Mailler and Lesser,  uses a strategy of partial centralization to mitigate this problem. We introduce PC-DPOP, a new partial centralization technique, based on the DPOP algorithm of Petcu and Faltings, . PC-DPOP provides better control over what parts of the problem are centralized and allows this centralization to be optimal with respect to the chosen communication structure. Unlike OptAPO, PC-DPOP allows for a priory, exact predictions about privacy loss, communication, memory and computational requirements on all nodes and links in the network. Upper bounds on communication and memory requirements can be specified. We also report strong efficiency gains over OptAPO in experiments on three problem domains.|Adrian Petcu,Boi Faltings,Roger Mailler","16459|IJCAI|2007|Quality Guarantees on k-Optimal Solutions for Distributed Constraint Optimization Problems|A distributed constraint optimization problem (DCOP) is a formalism that captures the rewards and costs of local interactions within a team of agents. Because complete algorithms to solve DCOPs are unsuitable for some dynamic or anytime domains, researchers have explored incomplete DCOP algorithms that result in locally optimal solutions. One type of categorization of such algorithms, and the solutions they produce, is k- optimality a k-optimal solution is one that cannot be improved by any deviation by k or fewer agents. This paper presents the first known guarantees on solution quality for k-optimal solutions. The guarantees are independent of the costs and rewards in the DCOP, and once computed can be used for any DCOP of a given constraint graph structure.|Jonathan P. Pearce,Milind Tambe"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","66101|AAAI|2007|Efficient Reinforcement Learning with Relocatable Action Models|Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.|Bethany R. Leffler,Michael L. Littman,Timothy Edmunds","16661|IJCAI|2007|An Experts Algorithm for Transfer Learning|A long-lived agent continually faces new tasks in its environment. Such an agent may be able to use knowledge learned in solving earlier tasks to produce candidate policies for its current task. There may, however, be multiple reasonable policies suggested by prior experience, and the agent must choose between them potentially without any a priori knowledge about their applicability to its current situation. We present an \"experts\" algorithm for efficiently choosing amongst candidate policies in solving an unknown Markov decision process task. We conclude with the results of experiments on two domains in which we generate candidate policies from solutions to related tasks and use our experts algorithm to choose amongst them.|Erik Talvitie,Satinder Singh","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","16494|IJCAI|2007|Building Portable Options Skill Transfer in Reinforcement Learning|The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.|George Konidaris,Andrew G. Barto","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","16732|IJCAI|2007|Utile Distinctions for Relational Reinforcement Learning|We introduce an approach to autonomously creating state space abstractions for an online reinforcement learning agent using a relational representation. Our approach uses a tree-based function approximation derived from McCallum's  UTree algorithm. We have extended this approach to use a relational representation where relational observations are represented by attributed graphs McGovern et al., . We address the challenges introduced by a relational representation by using stochastic sampling to manage the search space Srinivasan,  and temporal sampling to manage autocorrelation Jensen and Neville, . Relational UTree incorporates Iterative Tree Induction Utgoff et al.,  to allow it to adapt to changing environments. We empirically demonstrate that Relational UTree performs better than similar relational learning methods Finney et al.,  Driessens et al.,  in a blocks world domain. We also demonstrate that Relational UTree can learn to play a sub-task of the game of Go called Tsume-Go Ramon et al., .|William Dabney,Amy McGovern"],["66124|AAAI|2007|Heuristic Evaluation Functions for General Game Playing|A general game playing program plays games that it has not previously encountered. A game manager program sends the game playing programs a description of a game's rules and objectives in a well-defined game description language. A central challenge in creating effective general game playing programs is that of constructing heuristic evaluation functions from game descriptions. This paper describes a method for constructing evaluation functions that represent exact values of simplified games. The simplified games are abstract models that incorporate the most essential aspects of the original game, namely payoff, control, and termination. Results of applying this method to a sampling of games suggest that heuristic evaluation functions based on our method are both comprehensible and effective.|James Clune","16774|IJCAI|2007|Improving Anytime Point-Based Value Iteration Using Principled Point Selections|Planning in partially-observable dynamical systems (such as POMDPs and PSRs) is a computationally challenging task. Popular approximation techniques that have proven successful are point-based planning methods including pointbased value iteration (PBVI), which works by approximating the solution at a finite set of points. These point-based methods typically are anytime algorithms, whereby an initial solution is obtained using a small set of points, and the solution may be incrementally improved by including additional points. We introduce a family of anytime PBVI algorithms that use the information present in the current solution for identifying and adding new points that have the potential to best improve the next solution. We motivate and present two different methods for choosing points and evaluate their performance empirically, demonstrating that high-quality solutions can be obtained with significantly fewer points than previous PBVI approaches.|Michael R. James,Michael E. Samples,Dmitri A. Dolgov","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","80623|VLDB|2006|Using Partial Evaluation in Distributed Query Evaluation|A basic idea in parallel query processing is that one is prepared to do more computation than strictly necessary at individual sites in order to reduce the elapsed time, the network traffic, or both in the evaluation of the query. We develop this idea for the evaluation of boolean XPath queries over a tree that is fragmented, both horizontally and vertically over a number of sites. The key idea is to send the whole query to each site which partially evaluates, in parallel, the query and sends the results as compact boolean functions to a coordinator which combines these to obtain the result. This approach has several advantages. First, each site is visited only once, even if several fragments of the tree are stored at that site. Second, no prior constraints on how the tree is decomposed are needed, nor is any structural information about the tree required, such as a DTD. Third, there is a satisfactory bound on the total computation performed on all sites and on the total network traffic. We also develop a simple incremental maintenance algorithm that requires communication only with the sites at which changes have taken place moreover the network traffic depends neither on the data nor on the update. These results, we believe, illustrate the usefulness and potential of partial evaluation in distributed systems as well as centralized xml stores for evaluating XPath queries and beyond.|Peter Buneman,Gao Cong,Wenfei Fan,Anastasios Kementsietsidis","16635|IJCAI|2007|From Generic Knowledge to Specific Reasoning for Medical Image Interpretation Using Graph based Representations|In several domains of spatial reasoning, such as medical image interpretation, spatial relations between structures play a crucial role since they are less prone to variability than intrinsic properties of structures. Moreover, they constitute an important part of available knowledge. We show in this paper how this knowledge can be appropriately represented by graphs and fuzzy models of spatial relations, which are integrated in a reasoning process to guide the recognition of individual structures in images. However pathological cases may deviate substantially from generic knowledge. We propose a method to adapt the knowledge representation to take into account the influence of the pathologies on the spatial organization of a set of structures, based on learning procedures. We also propose to adapt the reasoning process, using graph based propagation and updating.|Jamal Atif,C√©line Hudelot,Geoffroy Fouquier,Isabelle Bloch,Elsa D. Angelini","66276|AAAI|2007|Visualization and Adjustment of Evaluation Functions Based on Evaluation Values and Win Probability|We present a method of visualizing and adjusting the evaluation functions in game programming in this paper. It is widely recognized that an evaluation function should assign a higher evaluation value to a position with greater probability of a win. However, this relation has not been utilized directly to tune evaluation functions because of the difficulty of measuring the probability of wins in deterministic games. We present the use of win percentage to utilize this relation in positions having the same evaluation value as win probability, where the positions we used were stored in a large database of game records. We introduce an evaluation curve formed by evaluation values and win probabilities, to enable evaluation functions to be visualized. We observed that evaluation curves form a sigmoid in various kinds of games and that these curves may split depending on the properties of positions. Because such splits indicate that an evaluation function that is visualized misestimates positions with less probability of winning, we can improve this by fitting evaluation curves to one. Our experiments with Chess and Shogi revealed that deficiencies in evaluation functions could be successfully visualized, and that improvements by automatically adjusting their weights were confirmed by self-plays.|Shogo Takeuchi,Tomoyuki Kaneko,Kazunori Yamaguchi,Satoru Kawai","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","65795|AAAI|2006|Functional Value Iteration for Decision-Theoretic Planning with General Utility Functions|We study how to find plans that maximize the expected total utility for a given MDP, a planning objective that is important for decision making in high-stakes domains. The optimal actions can now depend on the total reward that has been accumulated so far in addition to the current state. We extend our previous work on functional value iteration from one-switch utility functions to all utility functions that can be approximated with piecewise linear utility functions (with and without exponential tails) by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function. Functional value iteration does not maintain a value for every state but a value function that maps the total reward that has been accumulated so far into a value. We describe how functional value iteration represents these value functions in finite form, how it performs dynamic programming by manipulating these representations and what kinds of approximation guarantees it is able to make. We also apply it to a probabilistic blocksworld problem, a standard test domain for decision-theoretic planners.|Yaxin Liu,Sven Koenig","16703|IJCAI|2007|Image Modeling Using Tree Structured Conditional Random Fields|In this paper we present a discriminative framework based on conditional random fields for stochastic modeling of images in a hierarchical fashion. The main advantage of the proposed framework is its ability to incorporate a rich set of interactions among the image sites. We achieve this by inducing a hierarchy of hidden variables over the given label field. The proposed tree like structure of our model eliminates the need for a huge parameter space and at the same time permits the use of exact and efficient inference procedures based on belief propagation. We demonstrate the generality of our approach by applying it to two important computer vision tasks, namely image labeling and object detection. The model parameters are trained using the contrastive divergence algorithm. We report the performance on real world images and compare it with the existing approaches.|Pranjal Awasthi,Aakanksha Gagrani,Balaraman Ravindran"],["65980|AAAI|2007|Design of a Mechanism for Promoting Honesty in E-Marketplaces|In this paper, we explore the use of the web as an environment for electronic commerce. In particular, we develop a novel mechanism that creates incentives for honesty in electronic marketplaces where human users are represented by buying and selling agents. In our mechanism, buyers model other buyers and select the most trustworthy ones as their neighbors from which they can ask advice about sellers. In addition, however, sellers model the reputation of buyers. Reputable buyers provide fair ratings of sellers, and are likely to be neighbors of many other buyers. Sellers will provide more attractive products to reputable buyers, in order to build their reputation. We discuss how a marketplace operating with our mechanism leads to better profit both for honest buyers and sellers. With honesty encouraged, our work promotes the acceptance of web-based agent-oriented e-commerce by human users.|Jie Zhang,Robin Cohen","66244|AAAI|2007|Automated Online Mechanism Design and Prophet Inequalities|Recent work on online auctions for digital goods has explored the role of optimal stopping theory -- particularly secretary problems -- in the design of approximately optimal online mechanisms. This work generally assumes that the size of the market (number of bidders) is known a priori, but that the mechanism designer has no knowledge of the distribution of bid values. However, in many real-world applications (such as online ticket sales), the opposite is true the seller has distributional knowledge of the bid values (e.g., via the history of past transactions in the market), but there is uncertainty about market size. Adopting the perspective of automated mechanism design, introduced by Conitzer and Sandholm, we develop algorithms that compute an optimal, or approximately optimal, online auction mechanism given access to this distributional knowledge. Our main results are twofold. First, we show that when the seller does not know the market size, no constant-approximation to the optimum efficiency or revenue is achievable in the worst case, even under the very strong assumption that bid values are i.i.d. samples from a distribution known to the seller. Second, we show that when the seller has distributional knowledge of the market size as well as the bid values, one can do well in several senses. Perhaps most interestingly, by combining dynamic programming with prophet inequalities (a technique from optimal stopping theory) we are able to design and analyze online mechanisms which are temporally strategyproof (even with respect to arrival and departure times) and approximately efficiency (revenue)-maximizing. In exploring the interplay between automated mechanism design and prophet inequalities, we prove new prophet inequalities motivated by the auction setting.|Mohammad Taghi Hajiaghayi,Robert D. Kleinberg,Tuomas Sandholm","16536|IJCAI|2007|Incremental Mechanism Design|Mechanism design has traditionally focused almost exclusively on the design of truthful mechanisms. There are several drawbacks to this . in certain settings (e.g. voting settings), no desirable strategy proof mechanisms exist . truthful mechanisms are unable to take advantage of the fact that computationally bounded agents may not be able to find the best manipulation, and . when designing mechanisms automatically, this approach leads to constrained optimization problems for which current techniques do not scale to very large instances. In this paper, we suggest an entirely different approach we start with a nave (manipulable) mechanism, and incrementally make it more strategy proof over a sequence of iterations. We give examples of mechanisms that (variants of) our approach generate, including the VCG mechanism in general settings with payments, and the plurality-with-runoff voting rule. We also provide several basic algorithms for automatically executing our approach in general settings. Finally, we discuss how computationally hard it is for agents to find any remaining beneficial manipulation.|Vincent Conitzer,Tuomas Sandholm","16486|IJCAI|2007|Mechanism Design with Partial Revelation|Classic direct mechanisms require full utility revelation from agents, which can be very difficult in practical multi-attribute settings. In this work, we study partial revelation within the framework of one-shot mechanisms. Each agent's type space is partitioned into a finite set of partial types and agents (should) report the partial type within which their full type lies. A classic result implies that implementation in dominant strategies is impossible in this model. We first show that a relaxation to Bayes-Nash implementation does not circumvent the problem. We then propose a class of partial revelation mechanisms that achieve approximate dominant strategy implementation, and describe a computationally tractable algorithm for myopically optimizing the partitioning of each agent's type space to reduce manipulability and social welfare loss. This allows for the automated design of one-shot partial revelation mechanisms with worst-case guarantees on both manipulability and efficiency.|Nathanael Hyafil,Craig Boutilier","65943|AAAI|2006|Mechanisms for Partial Information Elicitation The Truth but Not the Whole Truth|We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios.|Aviv Zohar,Jeffrey S. Rosenschein","66095|AAAI|2007|Logic for Automated Mechanism Design - A Progress Report|Over the past half decade, we have been exploring the use of logic in the specification and analysis of computational economic mechanisms. We believe that this approach has the potential to bring the same benefits to the design and analysis of computational economic mechanisms that the use of temporal logics and model checking have brought to the specification and analysis of reactive systems. In this paper, we give a survey of our work. We first discuss the use of cooperation logics such as Alternating-time Temporal Loglc (ATL) for the specification and verification of mechanisms such as social choice procedures. We motivate the approach, and then discuss the work we have done on extensions to ATL to support incomplete information, preferences, and quantification over coalition. We then discuss is the use of ATL-like cooperation logics in the development of social laws.|Michael Wooldridge,Thomas √\u2026gotnes,Paul E. Dunne,Wiebe van der Hoek","16637|IJCAI|2007|An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource Allocation Mechanism|Multiagent distributed resource allocation requires that agents act on limited, localized information with minimum communication overhead in order to optimize the distribution of available resources. When requirements and constraints are dynamic, learning agents may be needed to allow for adaptation. One way of accomplishing learning is to observe past outcomes, using such information to improve future decisions. When limits in agents' memory or observation capabilities are assumed, one must decide on how large should the observation window be. We investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems, namely dispersion games. We show by numerical experiments over a specific dispersion game (the Minority Game) that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. We then provide an information-theoretic explanation for the observed behaviors, showing that a downward causation effect takes place.|Ricardo M. Araujo,Lu√≠s C. Lamb","16400|IJCAI|2007|Inferring Complex Agent Motions from Partial Trajectory Observations|Tracking the movements of a target based on limited observations plays a role in many interesting applications. Existing probabilistic tracking techniques have shown considerable success but the majority assume simplistic motion models suitable for short-term, local motion prediction. Agent movements are often governed by more sophisticated mechanisms such as a goal-directed path-planning algorithm. In such contexts we must go beyond estimating a target's current location to consider its future path and ultimate goal. We show how to use complex, \"black box\" motion models to infer distributions over a target's current position, origin, and destination, using only limited observations of the full path. Our approach accommodates motion models defined over a graph, including complex pathing algorithms such as A*. Robust and practical inference is achieved by using hidden semi-Markov models (HSMMs) and graph abstraction. The method has also been extended to effectively track multiple, indistinguishable agents via a greedy heuristic.|Finnegan Southey,Wesley Loh,Dana F. Wilkinson","65964|AAAI|2007|Partial Revelation Automated Mechanism Design|In most mechanism design settings, optimal general-purpose mechanisms are not known. Thus the automated design of mechanisms tailored to specific instances of a decision scenario is an important problem. Existing techniques for automated mechanism design (AMD) require the revelation of full utility information from agents, which can be very difficult in practice. In this work, we study the automated design of mechanisms that only require partial revelation of utilities. Each agent's type space is partitioned into a finite set of partial types, and agents (should) report the partial type within which their full type lies. We provide a set of optimization routines that can be combined to address the trade-offs between the amount of communication, approximation of incentive properties, and objective value achieved by a mechanism. This allows for the automated design of partial revelation mechanisms with worst-case guarantees on incentive properties for any objective function (revenue, social welfare, etc.).|Nathanael Hyafil,Craig Boutilier","66211|AAAI|2007|An Ironing-Based Approach to Adaptive Online Mechanism Design in Single-Valued Domains|Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method (\"ironing\") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.|David C. Parkes,Quang Duong"],["16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","65844|AAAI|2006|Compiling Uncertainty Away Solving Conformant Planning Problems using a Classical Planner Sometimes|Even under polynomial restrictions on plan length, conformant planning remains a very hard computational problem as plan verification itself can take exponential time. This heavy price cannot be avoided in general although in many cases conformant plans are verifiable efficiently by means of simple forms of disjunctive inference. This raises the question of whether it is possible to identify and use such forms of inference for developing an efficient but incomplete planner capable of solving non-trivial problems quickly. In this work, we show that this is possible by mapping conformant into classical problems that are then solved by an off-the-shelf classical planner. The formulation is sound as the classical plans obtained are all conformant, but it is incomplete as the inverse relation does not always hold. The translation accommodates 'reasoning by cases' by means of an 'split-protect-and-merge' strategy namely, atoms LXi that represent conditional beliefs 'if Xi then L' are introduced in the classical encoding, that are combined by suitable actions to yield the literal L when the disjunction X  ...  Xn holds and certain invariants in the plan are verified. Empirical results over a wide variety of problems illustrate the power of the approach.|H√©ctor Palacios,Hector Geffner","16700|IJCAI|2007|Constraint Partitioning for Solving Planning Problems with Trajectory Constraints and Goal Preferences|The PDDL specifications include soft goals and trajectory constraints for distinguishing highquality plans among the many feasible plans in a solution space. To reduce the complexity of solving a large PDDL planning problem, constraint partitioning can be used to decompose its constraints into subproblems of much lower complexity. However, constraint locality due to soft goals and trajectory constraints cannot be effectively exploited by existing subgoal-partitioning techniques developed for solving PDDL. problems. In this paper, we present an improved partition-andresolve strategy for supporting the new features in PDDL. We evaluate techniques for resolving violated global constraints, optimizing goal preferences, and achieving subgoals in a multivalued representation. Empirical results on the th International Planning Competition (IPC) benchmarks show that our approach is effective and significantly outperforms other competing planners.|Chih-Wei Hsu,Benjamin W. Wah,Ruoyun Huang,Yixin Chen","16503|IJCAI|2007|Reducing Accidental Complexity in Planning Problems|Although even propositional STRIPS planning is a hard problem in general, many instances of the problem, including many of those commonly used as benchmarks, are easy. In spite of this, they are often hard to solve for domain-independent planners, because the encoding of the problem into a general problem specification formalism such as STRIPS hides structure that needs to be exploited to solve problems easily. We investigate the use of automatic problem transformations to reduce this \"accidental\" problem complexity. The main tool is abstraction we identify a new, weaker, condition under which abstraction is \"safe\", in the sense that any solution to the abstracted problem can be refined to a concrete solution (in polynomial time, for most cases) and also show how different kinds of problem reformulations can be applied to create greater opportunities for such safe abstraction.|Patrik Haslum","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","16457|IJCAI|2007|The Design of ESSENCE A Constraint Language for Specifying Combinatorial Problems|ESSENCE is a new formal language for specifying combinatorial problems in a manner similar to natural rigorous specifications that use a mixture of natural language and discrete mathematics. ESSENCE provides a high level of abstraction, much of which is the consequence of the provision of decision variables whose values can be combinatorial objects, such as tuples, sets, multisets, relations, partitions and functions. ESSENCE also allows these combinatorial objects to be nested to arbitrary depth, thus providing, for example, sets of partitions, sets of sets of partitions, and so forth. Therefore, a problem that requires finding a complex combinatorial object can be directly specified by using a decision variable whose type is precisely that combinatorial object.|Alan M. Frisch,Matthew Grum,Christopher Jefferson,Bernadette Mart√≠nez Hern√°ndez,Ian Miguel","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","16392|IJCAI|2007|Constraint and Variable Ordering Heuristics for Compiling Configuration Problems|To facilitate interactive design, the solutions to configuration problems can be compiled into a decision diagram. We develop three heuristics for reducing the time and space required to do this. These heuristics are based on the distinctive clustered and hierarchical structure of the constraint graphs of configuration problems. The first heuristic attempts to limit the growth in the size of the decision diagram by providing an order in which constraints are added to the decision diagram. The second heuristic provides an initial order for the variables within the decision diagram. Finally, the third heuristic groups variables together so that they can be reordered by a dynamic variable reordering procedure used during the construction of the decision diagram. These heuristics provide one to two orders magnitude improvement in the time to compile a wide range of configuration.|Nina Narodytska,Toby Walsh","16459|IJCAI|2007|Quality Guarantees on k-Optimal Solutions for Distributed Constraint Optimization Problems|A distributed constraint optimization problem (DCOP) is a formalism that captures the rewards and costs of local interactions within a team of agents. Because complete algorithms to solve DCOPs are unsuitable for some dynamic or anytime domains, researchers have explored incomplete DCOP algorithms that result in locally optimal solutions. One type of categorization of such algorithms, and the solutions they produce, is k- optimality a k-optimal solution is one that cannot be improved by any deviation by k or fewer agents. This paper presents the first known guarantees on solution quality for k-optimal solutions. The guarantees are independent of the costs and rewards in the DCOP, and once computed can be used for any DCOP of a given constraint graph structure.|Jonathan P. Pearce,Milind Tambe","65891|AAAI|2006|Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms|In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs. The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms. We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.|Tran Cao Son,Enrico Pontelli,Phan Huy Tu"],["65816|AAAI|2006|Extracting Knowledge about Users Activities from Raw Workstation Contents|A long-standing goal of AI is the development of intelligent workstation-based personal agents to assist users in their daily lives. A key impediment to this goal is the unrealistic cost of developing and maintaining a detailed knowledge base describing the user's different activities, and which people, meetings, emails, etc. are affiliated with each such activity. This paper presents a clustering approach to automatically acquiring such a knowledge base by analyzing the raw contents of the workstation, including emails, contact person names, and online calendar meetings. Our approach analyzes the distribution of email words, the social network of email senders and recipients, and the results of Google Desktop Search queried with text from online calendar entries and person contact names. For each cluster it constructs, the program outputs a frame-based representation of the conesponding user activity. This paper describes our approach and experimentally assesses its perfonnance over the workstations of three different users.|Tom M. Mitchell,Sophie H. Wang,Yifen Huang,Adam Cheyer","66050|AAAI|2007|Reasoning from Desires to Intentions A Dialectical Framework|Here, we define a framework where defeasible argumentation is used for reasoning about beliefs, desires and intentions. A dialectical filtering process is introduced to obtain a subset of the agent's desires containing only those that are achievable in the current situation. Different agents types can be defined in the framework affecting the way in which current desires are obtained. The agent is provided with a set of intention rules that specifies under what conditions an intention could be achieved. When more than one intention is present, a policy will be used to choose among them. Thus, intention policies provide the agent with a mechanism for deciding which intention is selected in the current situation. Several application examples will be given.|Nicol√°s D. Rotstein,Alejandro Javier Garc√≠a,Guillermo Ricardo Simari","16603|IJCAI|2007|Extracting Chatbot Knowledge from Online Discussion Forums|This paper presents a novel approach for extracting high-quality thread-title, reply pairs as chat knowledge from online discussion forums so as to efficiently support the construction of a chatbot for a certain domain. Given a forum, the high-quality thread-title, reply pairs are extracted using a cascaded framework. First, the replies logically relevant to the thread title of the root message are extracted with an SVM classifier from all the replies, based on correlations such as structure and content. Then, the extracted thread-title, reply pairs are ranked with a ranking SVM based on their content qualities. Finally, the Top-N thread-title, reply pairs are selected as chatbot knowledge. Results from experiments conducted within a movie forum show the proposed approach is effective.|Jizhou Huang,Ming Zhou,Dan Yang","16491|IJCAI|2007|Extracting and Visualizing Trust Relationships from Online Auction Feedback Comments|Buyers and sellers in online auctions are faced with the task of deciding who to entrust their business to based on a very limited amount of information. Current trust ratings on eBay average over % positive  and are presented as a single number on a user's profile. This paper presents a system capable of extracting valuable negative information from the wealth of feedback comments on eBay, computing personalized and feature-based trust and presenting this information graphically.|John O'Donovan,Barry Smyth,Vesile Evrim,Dennis McLeod","16635|IJCAI|2007|From Generic Knowledge to Specific Reasoning for Medical Image Interpretation Using Graph based Representations|In several domains of spatial reasoning, such as medical image interpretation, spatial relations between structures play a crucial role since they are less prone to variability than intrinsic properties of structures. Moreover, they constitute an important part of available knowledge. We show in this paper how this knowledge can be appropriately represented by graphs and fuzzy models of spatial relations, which are integrated in a reasoning process to guide the recognition of individual structures in images. However pathological cases may deviate substantially from generic knowledge. We propose a method to adapt the knowledge representation to take into account the influence of the pathologies on the spatial organization of a set of structures, based on learning procedures. We also propose to adapt the reasoning process, using graph based propagation and updating.|Jamal Atif,C√©line Hudelot,Geoffroy Fouquier,Isabelle Bloch,Elsa D. Angelini","65665|AAAI|2006|Progress in Textual Case-Based Reasoning Predicting the Outcome of Legal Cases from Text|This paper reports on a project that explored reasoning with textual cases in the context of legal reasoning. The work is anchored in both Case-Based Reasoning (CBR) and AI and Law. It introduces the SMILE+IBP framework that generates a case-based analysis and prediction of the outcome of a legal case given a brief textual summary of the case facts. The focal research question in this work was to find a good text representation for text classification. An evaluation showed that replacing case-specific names by roles and adding NLP lead to higher performance for assigning CBR indices. The NLP-based representation produced the best results for reasoning with the automatically indexed cases.|Stefanie Br√ºninghaus,Kevin D. Ashley","66218|AAAI|2007|Automatic Synthesis of a Global Behavior from Multiple Distributed Behaviors|We consider the problem of synthesizing a team of local behavior controllers to realize a fully controllable target behavior from a set of available partially controllable behaviors that execute distributively within a shared partially predictable, but fully observable, environment. Available behaviors stand for existing distributed components and are represented with (finite) nondeterministic transition systems. The target behavior is assumed to be fully deterministic and stands for the collective behavior that the system as a whole needs to guarantee. We formally define the problem within a general framework, characterize its computational complexity, and propose techniques to actually generate a solution. Also, we investigate the relationship between the distributed solutions and the centralized ones, in which a single global controller is conceivable.|Sebastian Sardi√±a,Fabio Patrizi,Giuseppe De Giacomo","16656|IJCAI|2007|Learning from the Report-writing Behavior of Individuals|We describe a briefing system that learns to predict the contents of reports generated by users who create periodic (weekly) reports as part of their normal activity. The system observes content-selection choices that users make and builds a predictive model that could, for example, be used to generate an initial draft report. Using a feature of the interface the system also collects information about potential user-specific features. The system was evaluated under realistic conditions, by collecting data in a project-based university course where student group leaders were tasked with preparing weekly reports for the benefit of the instructors, using the material from individual student reports. This paper addresses the question of whether data derived from the implicit supervision provided by end-users is robust enough to support not only model parameter tuning but also a form of feature discovery. Results indicate that this is the case system performance improves based on the feedback from user activity. We find that individual learned models (and features) are user-specific, although not completely idiosyncratic. Thismay suggest that approaches which seek to optimizemodels globally (say over a large corpus of data) may not in fact produce results acceptable to all individuals.|Mohit Kumar,Nikesh Garera,Alexander I. Rudnicky","65826|AAAI|2006|Phoebus A System for Extracting and Integrating Data from Unstructured and Ungrammatical Sources|With the proliferation of online classifieds and auctions comes a new need to meaningfully search and organize the items for sale. However, since the seller's item descriptions are not structured and do not conform to a standard set of values (think \"Chevy\" versus \"Chevrolet\"), searching and organizing this data is difficult. This paper describes a working demonstration of the Phoebus system which uses both record linkage and information extraction to parse out the meaningful attributes of an item description and assign them standard values. This allows the data to be sorted, searched and linked to other data sources where standard values for the attributes are required to link the sources together.|Matthew Michelson,Craig A. Knoblock","16712|IJCAI|2007|Extracting Keyphrases to Represent Relations in Social Networks from Web|Social networks have recently garnered considerable interest. With the intention of utilizing social networks for the Semantic Web, several studies have examined automatic extraction of social networks. However, most methods have addressed extraction of the strength of relations. Our goal is extracting the underlying relations between entities that are embedded in social networks. To this end, we propose a method that automatically extracts labels that describe relations among entities. Fundamentally, the method clusters similar entity pairs according to their collective contexts in Web documents. The descriptive labels for relations are obtained from results of clustering. The proposed method is entirely unsupervised and is easily incorporated with existing social network extraction methods. Our experiments conducted on entities in researcher social networks and political social networks achieved clustering with high precision and recall. The results showed that our method is able to extract appropriate relation labels to represent relations among entities in the social networks.|Junichiro Mori,Mitsuru Ishizuka,Yutaka Matsuo"],["66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy L√©cu√©,Alexandre Delteil","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","66048|AAAI|2007|A Planning Approach for Message-Oriented Semantic Web Service Composition|In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm.|Zhen Liu,Anand Ranganathan,Anton Riabov","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"]]}}