{"abstract":{"entropy":6.682490131467319,"topics":["data management, data, web pages, data integration, markov decision, query, data stream, actions uncertainty, markov processes, materialized views, applications data, processing stream, queries data, decision processes, top-k queries, partially observable, rdf ontologies, recent years, semantic web, planning uncertainty","scene understanding, description logic, constraint satisfaction, heuristics planning, planning domains, present based, based reasoning, constraint problem, reasoning actions, develop theory, solving problem, solutions problem, case-based reasoning, satisfaction problem, constraint, constraint programming, recent years, modal logic, recent research, algorithm solving","machine learning, natural language, artificial intelligence, reinforcement learning, learning, search engine, address problem, bayesian network, computer science, learning problem, present novel, intelligence research, search algorithm, example learning, text classification, semi-supervised learning, data clustering, present learning, present engine, learning data","systems, combinatorial auctions, multi-agent systems, agents, decision making, cognitive architecture, emerging applications, systems need, wide web, agents task, negotiation agents, world wide, analysis critical, agents able, consider problem, data similar, applications requires, task environments, autonomous need, multi-agent agents","markov decision, markov processes, partially observable, rdf ontologies, decision processes, business processes, time, processes, provide, important, significantly, study, model, rich, pomdps, relationships, improve, table, mdp, first","web pages, actions uncertainty, recent years, web search, planning uncertainty, uncertainty source, web, data uncertainty, large data, large sets, recent data, information web, data web, large database, web engine, documents web, recent database, uncertainty, recent, large","constraint satisfaction, solutions problem, problem, present algorithm, recent years, constraint problem, solving problem, solutions number, satisfaction problem, algorithm solving, constraint, algorithm, sat problem, general problem, search constraint, algorithm constraint, recent, problem number, search algorithm, problem different","scene understanding, addresses problem, problem dynamic, important problem, important, approach, programming, dynamic, idea, research, interactive, data, computational, sentences","natural language, bayesian network, present semantic, semantic, data semantic, network, knowledge, systems, efficient, parsing, automatically, inference, performance, consists, components, nodes, project, generate, context, heterogeneous","reinforcement learning, learning problem, learning, research learning, computer science, research traditional, data traditional, learning model, work, work learning, traditional, computer, modeling, tree, study, probability, sequence, previous, programming, class","multi-agent agents, multi-agent systems, decision making, systems need, systems, information systems, systems based, diagnosis systems, problem systems, present systems, systems online, systems agents, design systems, state systems, describe systems, describe, order, state, complexity, interaction","consider problem, emerging applications, systems challenge, key systems, applications requires, key challenge, emerging requires, applications, services, key, requires, challenge, consider, increasingly, games, behavior, activity, negotiation, team, size"],"ranking":[["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","66001|AAAI|2007|DL-Lite in the Light of First-Order Logic|The use of ontologies in various application domains, such as Data Integration, the Semantic Web, or ontology-based data management, where ontologies provide the access to large amounts of data, is posing challenging requirements w.r.t. a trade-off between expressive power of a DL and efficiency of reasoning. The logics of the DL-Lite family were specifically designed to meet such requirements and optimized w.r.t. the data complexity of answering complex types of queries. In this paper we propose DL-Litebool, an extension of DL-Lite with full Booleans and number restrictions, and study the complexity of reasoning in DL-Litebool and its significant sub-logics. We obtain our results, together with useful insights into the properties of the studied logics, by a novel reduction to the one-variable fragment of first-order logic. We study the computational complexity of satisfiability and subsumption, and the data complexity of answering positive existential queries (which extend unions of conjunctive queries). Notably, we extend the LOGSPACE upper bound for the data complexity of answering unions of conjunctive queries in DL-Lite to positive queries and to the possibility of expressing also number restrictions, and hence local functionality in the TBox.|Alessandro Artale,Diego Calvanese,Roman Kontchakov,Michael Zakharyaschev","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","80840|VLDB|2007|Data Integration with Uncertainty|This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels, and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, queries to the system may be posed with keywords rather than in a structured form. Third, the data from the sources may be extracted using information extraction techniques and so may yield imprecise data. As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings by-table semantics assumes that there exists a correct mapping but we don't know what it is by-tuple semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of approximate schema mappings, and we describe an algorithm for efficiently computing the top-k answers to queries in such a setting.|Xin Luna Dong,Alon Y. Halevy,Cong Yu","80803|VLDB|2007|Request Window an Approach to Improve Throughput of RDBMS-based Data Integration System by Utilizing Data Sharing Across Concurrent Distributed Queries|This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently issue data requests and fetch data over the network. This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a .x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.|Rubao Lee,Minghong Zhou,Huaming Liao","80524|VLDB|2005|Caching with Good Enough Currency Consistency and Completeness|SQL extensions that allow queries to explicitly specify data quality requirements in terms of currency and consistency were proposed in an earlier paper. This paper develops a data quality-aware, finer grained cache model and studies cache design in terms of four fundamental properties presence, consistency, completeness and currency. The model provides an abstract view of the cache to the query processing layer, and opens the door for adaptive cache management. We describe an implementation approach that builds on the MTCache framework for partially materialized views. The optimizer checks most consistency constraints and generates a dynamic plan that includes currency checks and inexpensive checks for dynamic consistency constraints that cannot be validated during optimization. Our solution not only supports transparent caching but also provides fine grained data currency and consistency guarantees.|Hongfei Guo,Per-√\u2026ke Larson,Raghu Ramakrishnan"],["66161|AAAI|2007|Forgetting Actions in Domain Descriptions|Forgetting irrelevantproblematic actions in a domain description can be useful in solving reasoning problems, such as query answering, planning, conftict resolution, prediction, postdiction, etc.. Motivated by such applications, we study what forgetting is, how forgetting can be done, and for which applications forgetting can be useful and how, in the context of reasoning about actions. We study these questions in the action language C (a formalism based on causal explanations), and relate it to forgetting in classical logic and logic programming.|Esra Erdem,Paolo Ferraris","65538|AAAI|2005|SAT-Based versus CSP-Based Constraint Weighting for Satisfiability|Recent research has focused on bridging the gap between the satisfiability (SAT) and constraint satisfaction problem (CSP) formalisms. One approach has been to develop a many-valued SAT formula (MV-SAT) as an intermediate paradigm between SAT and CSP, and then to translate existing highly efficient SAT solvers to the MV-SAT domain. Experimental results have shown this approach can achieve significant improvements in performance compared with the traditional SAT and CSP approaches. In this paper, we follow a different route, developing SAT solvers that can automatically recognise CSP structure hidden in SAT encodings. This allows us to look more closely at how constraint weighting can be implemented in the SAT and CSP domains. Our experimental results show that a SAT-based approach to handle weights, together with CSP-based approach to variable instantiation, is superior to other combinations of SAT and CSP-based approaches. A further experiment on the round robin scheduling problem indicates that this many-valued constraint weighting approach outperforms other state-of-the-art solvers.|Duc Nghia Pham,John Thornton,Abdul Sattar,Abdelraouf Ishtaiwi","66120|AAAI|2007|Filtering Decomposition and Search Space Reduction for Optimal Sequential Planning|We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.|St√©phane Grandcolas,C. Pain-Barre","65467|AAAI|2005|Heterogeneous Multirobot Coordination with Spatial and Temporal Constraints|Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.|Mary Koes,Illah R. Nourbakhsh,Katia P. Sycara","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","65558|AAAI|2005|Performing Bayesian Inference by Weighted Model Counting|Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman ). Solving such NP-complete tasks by \"compilation to SAT\" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. ) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether \"compilation to model-counting\" could be a practical technique for solving real-world P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.|Tian Sang,Paul Beame,Henry A. Kautz","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith"],["66184|AAAI|2007|A Text-to-Picture Synthesis System for Augmenting Communication|We present a novel Text-to-Picture system that synthesizes a picture from general, unrestricted natural language text. The process is analogous to Text-to-Speech synthesis, but with pictorial output that conveys the gist of the text. Our system integrates multiple AI components, including natural language processing, computer vision, computer graphics, and machine learning. We present an integration framework that combines these components by first identifying infonnative and 'picturable' text units, then searching for the most likely image parts conditioned on the text, and finally optimizing the picture layout conditioned on both the text and image parts. The effectiveness of our system is assessed in two user studies using children's books and news articles. Experiments show that the synthesized pictures convey as much infonnation about children's stories as the original artists' illustrations, and much more information about news articles than their original photos alone. These results suggest that Text-to-Picture synthesis has great potential in augmenting human-computer and human-human communication modalities, with applications in education and health care, among others.|Xiaojin Zhu,Andrew B. Goldberg,Mohamed Eldawy,Charles R. Dyer,Bradley Strock","65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","66164|AAAI|2007|Transferring Naive Bayes Classifiers for Text Classification|A basic assumption in traditional machine learning is that the training and test data distributions should be identical. This assumption may not hold in many situations in practice, but we may be forced to rely on a different-distribution data to learn a prediction model. For example, this may be the case when it is expensive to label the data in a domain of interest, although in a related but different domain there may be plenty of labeled data available. In this paper, we propose a novel transfer-learning algorithm for text classification based on an EM-based Naive Bayes classifiers. Our solution is to first estimate the initial probabilities under a distribution Dl of one labeled data set, and then use an EM algorithm to revise the model for a different distribution Du of the test data which are unlabeled. We show that our algorithm is very effective in several different pairs of domains, where the distances between the different distributions are measured using the Kullback-Leibler (KL) divergence. Moreover, KL-divergence is used to decide the trade-off parameters in our algorithm. In the experiment, our algorithm outperforms the traditional supervised and semi-supervised learning algorithms when the distributions of the training and test sets are increasingly different.|Wenyuan Dai,Gui-Rong Xue,Qiang Yang,Yong Yu","65616|AAAI|2005|Hidden Naive Bayes|The conditional independence assumption of naive Bayes essentially ignores attribute dependencies and is often violated. On the other hand, although a Bayesian network can represent arbitrary attribute dependencies, learning an optimal Bayesian network from data is intractable. The main reason is that learning the optimal structure of a Bayesian network is extremely time consuming. Thus, a Bayesian model without structure learning is desirable. In this paper, we propose a novel model, called hidden naive Bayes (HNB). In an HNB, a hidden parent is created for each attribute which combines the influences from all other attributes. We present an approach to creating hidden parents using the average of weighted one-dependence estimators. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. We propose an algorithm for learning HNB based on conditional mutual information. We experimentally test HNB in terms of classification accuracy, using the  UCI data sets recommended by Weka (Witten & Frank ), and compare it to naive Bayes (Langley, Iba, & Thomas ), C. (Quinlan ), SBC (Langley & Sage ), NBTree (Kohavi ), CL-TAN (Friedman, Geiger, & Goldszmidt ), and AODE (Webb, Boughton, & Wang ). The experimental results show that HNB outperforms naive Bayes, C., SBC, NBTree, and CL-TAN, and is competitive with AODE.|Harry Zhang,Liangxiao Jiang,Jiang Su","65384|AAAI|2005|A Comparison of Novel and State-of-the-Art Polynomial Bayesian Network Learning Algorithms|Learning the most probable a posteriori Bayesian network from data has been shown to be an NP-Hard problem and typical state-of-the-art algorithms are exponential in the worst case. However, an important open problem in the field is to identify the least restrictive set of assumptions and corresponding algorithms under which learning the optimal network becomes polynomial. In this paper, we present a technique for learning the skeleton of a Bayesian network, called Polynomial Max-Min Skeleton (PMMS), and compare It with Three Phase Dependency Analysis, another state-of-the-art polynomial algorithm. This analysis considers both the theoretical and empirical differences between the two algorithms, and demonstrates PMMS's advantages in both respects. When extended with a greedy hill-climbing Bayesian-scoring search to orient the edges, the novel algorithm proved more time efficient, scalable, and accurate in quality of reconstruction than most state-of-the-art Bayesian network learning algorithms. The results show promise of the existence of polynomial algorithms that are provably correct under minimal distributional assumptions.|Laura E. Brown,Ioannis Tsamardinos,Constantin F. Aliferis","80842|VLDB|2007|XSeek A Semantic XML Search Engine Using Keywords|We present XSeek, a keyword search engine that enables users to easily access XML data without the need of learning XPath or XQuery and studying possibly complex data schemas. XSeek addresses a challenge in XML keyword search that has been neglected in the literature how to determine the desired return information, analogous to inferring a \"return\" clause in XQuery. To infer the search semantics, XSeek recognizes possible entities and attributes in the data, differentiates search predicates and return specifications in the keywords, and generates meaningful search results based on the analysis.|Ziyang Liu,Jeffrey Walker,Yi Chen","66058|AAAI|2007|Efficient Structure Learning in Factored-State MDPs|We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.|Alexander L. Strehl,Carlos Diuk,Michael L. Littman","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","65490|AAAI|2005|Distribution-Free Learning of Bayesian Network Structure in Continuous Domains|In this paper we present a method for learning the structure of Bayesian networks (BNs) without making any assumptions on the probability distribution of the domain. This is mainly useful for continuous domains, where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the Bayesian network, and only a few have been examined analytically. We therefore focus on BN structure learning in continuous domains. We address the problem by developing a conditional independence test for continuous variables, which can be readily used by any existing independence-based BN structure learning algorithm. Our test is non-parametric, making no assumptions on the distribution of the domain. We also provide an effective and computationally efficient method for calculating it from data. We demonstrate the learning of the structure of graphical models in continuous domains from real-world data, to our knowledge for the first time using independence-based methods and without distributional assumptions. We also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization, and verify desirable properties such as statistical consistency.|Dimitris Margaritis","66201|AAAI|2007|Improving Learning in Networked Data by Combining Explicit and Mined Links|This paper is about using multiple types of information for classification of networked data in a semi-supervised setting given a fully described network (nodes and edges) with known labels for some of the nodes, predict the labels of the remaining nodes. One method recently developed for doing such inference is a guilt-by-association model. This method has been independently developed in two different settings-relational learning and semi-supervised learning. In relational learning, the setting assumes that the networked data has explicit links such as hyperlinks between webpages or citations between research papers. The semi-supervised setting assumes a corpus of non-relational data and creates links based on similarity measures between the instances. Both use only the known labels in the network to predict the remaining labels but use very different information sources. The thesis of this paper is that if we combine these two types of links, the resulting network will carry more information than either type of link by itself. We test this thesis on six benchmark data sets, using a within-network learning algorithm, where we show that we gain significant improvements in predictive performance by combining the links. We describe a principled way of combining mUltiple types of edges with different edge-weights and semantics using an objective graph measure called node-based assortativity. We investigate the use of this measure to combine text-mined links with explicit links and show that using our approach significantly improves performance of our classifier over naively combining these two types of links.|Sofus A. Macskassy"],["66207|AAAI|2007|An Integrated Development Environment and Architecture for Soar-Based Agents|It is well known how challenging is the task of coding complex agents for virtual environments. This difficulty in developing and maintaining complex agents has been plaguing commercial applications of advanced agent technology in virtual environments. In this paper, we discuss development of a commercial-grade integrated development environment (IDE) and agent architecture for simulation and training in a high-fidelity virtual environment. Specifically, we focus on two key areas of contribution. First, we discuss the addition of an explicit recipe mechanism to Soar, allowing reflection. Second, we discuss the development and usage of an IDE for building agents using our architecture the approach we take is to tightly-couple the IDE to the architecture. The result is a complete development and deployment environment for agents situated in a complex dynamic virtual world.|Ari Yakir,Gal A. Kaminka","65458|AAAI|2005|Towards Model-Based Diagnosis of Coordination Failures|With increasing deployment of multi-agent and distributed systems, there is an increasing need for failure diagnosis systems. While successfully tackling key challenges in multi-agent settings, model-based diagnosis has left open the diagnosis of coordination failures, where failures often lie in the boundaries between agents, and thus the inputs to the model--with which the diagnoser simulates the system to detect discrepancies--are not known. However, it is possible to diagnose such failures using a model of the coordination between agents. This paper formalizes model-based coordination diagnosis, using two coordination primitives (concurrence and mutual exclusion). We define the consistency-based and abductive diagnosis problems within this formalization, and show that both are NP-Hard by mapping them to other known problems.|Meir Kalech,Gal A. Kaminka","66075|AAAI|2007|Allocating Goods on a Graph to Eliminate Envy|We introduce a distributed negotiation framework for multi-agent resource allocation where interactions between agents are limited by a graph defining a negotiation topology. A group of agents may only contract a deal if that group is fully connected according to the negotiation topology. An important criterion for assessing the quality of an allocation of resources, in terms of fairness, is envy-freeness an agent is said to envy another agent if it would prefer to swap places with that other agent. We analyse under what circumstances a sequence of deals respecting the negotiation topology may be expected to converge to a state where no agent envies any of the agents it is directly connected to. We also analyse the computational complexity of a related decision problem, namely the problem of checking whether a given negotiation state admits any deal that would both be beneficial to every agent involved and reduce envy in the agent society.|Yann Chevaleyre,Ulrich Endriss,Nicolas Maudet","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","65428|AAAI|2005|Agent-Organized Networks for Multi-Agent Production and Exchange|As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.|Matthew E. Gaston,Marie desJardins","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","66036|AAAI|2007|Centralized Distributed or Something Else Making Timely Decisions in Multi-Agent Systems|In multi-agent systems, agents need to share information in order to make good decisions. Who does what in order to achieve this matters a lot. The assignment of responsibility influences delay and consequently affects agents' abilities to make timely decisions. It is often unclear which approaches are best. We develop a model where one can easily test the impact of different assignments and information sharing protocols by focusing only on the delays caused by computation and communication. Using the model, we obtain interesting results that provide insight about the types of assignments that perform well in various domains and how slight variations in protocols can make great differences in feasibility.|Tim Harbers,Rajiv T. Maheswaran,Pedro A. Szekely","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","65374|AAAI|2005|Coordination and Adaptation in Impromptu Teams|Coordinating a team of autonomous agents is one of the major challenges in building effective multiagcnt systems. Many techniques have been devised for this problem. and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques. though. is that the team members are developed together as a whole. In many multi agent scenarios. this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills. models. strategies. and coordination mechanisms. and no external organization is imposed upon them. In particular. we propose two techniques. one adaptive and one predictive. for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.|Michael H. Bowling,Peter McCracken","66211|AAAI|2007|An Ironing-Based Approach to Adaptive Online Mechanism Design in Single-Valued Domains|Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method (\"ironing\") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.|David C. Parkes,Quang Duong"],["65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","66123|AAAI|2007|Continuous State POMDPs for Object Manipulation Tasks|My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. ) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.|Emma Brunskill","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65422|AAAI|2005|Efficient Maximization in Solving POMDPs|We present a simple, yet effective improvement to the dynamic programming algorithm for solving partially observable Markov decision processes. The technique targets the vector pruning operation during the maximization step, a key source of complexity in POMDP algorithms. We identify two types of structures in the belief space and exploit them to reduce significantly the number of constraints in the linear programs used for pruning. The benefits of the new technique are evaluated both analytically and experimentally, showing that it can lead to significant performance improvement. The results open up new research opportunities to enhance the performance and scalability of several POMDP algorithms.|Zhengzhu Feng,Shlomo Zilberstein","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes"],["66166|AAAI|2007|Learning Large Scale Common Sense Models of Everyday Life|Recent work has shown promise in using large, publicly available, hand-contributed commonsense databases as joint models that can be used to infer human state from day-to-day sensor data. The parameters of these models are mined from the web. We show in this paper that learning these parameters using sensor data (with the mined parameters as priors) can improve performance of the models significantly. The primary challenge in learning is scale. Since the model comprises roughly , irregularly connected nodes in each time slice, it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice. We show how to solve the resulting semi-supervised learning problem by combining a variety of conventional approximation techniques and a novel technique for simplifying the model called context-based pruning. We show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various techniques contribute to the performance.|William Pentney,Matthai Philipose,Jeff A. Bilmes,Henry A. Kautz","66019|AAAI|2007|Web Service Composition as Planning Revisited In Between Background Theories and Initial State Uncertainty|Thanks to recent advances, AI Planning has become the underlying technique for several applications. Amongst these, a prominent one is automated Web Service Composition (WSC). One important issue in this context has been hardly addressed so far WSC requires dealing with background ontologies. The support for those is severely limited in current planning tools. We introduce a planning formalism that faithfully represents WSC. We show that, unsurprisingly, planning in such a formalism is very hard. We then identify an interesting special case that covers many relevant WSC scenarios, and where the semantics are simpler and easier to deal with. This opens the way to the development of effective support tools for WSC. Furthermore, we show that if one additionally limits the amount and form of outputs that can be generated, then the set of possible states becomes static, and can be modelled in terms of a standard notion of initial state uncertainty. For this, effective tools exist these can realize scalable WSC with powerful background ontologies. In an initial experiment, we show how scaling WSC instances are comfortably solved by a tool incorporating modern planning heuristics.|J√∂rg Hoffmann,Piergiorgio Bertoli,Marco Pistore","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","66002|AAAI|2007|Towards Efficient Dominant Relationship Exploration of the Product Items on the Web|In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.|Zhenglu Yang,Lin Li,Botao Wang,Masaru Kitsuregawa","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","80519|VLDB|2005|Database Change Notifications Primitives for Efficient Database Query Result Caching|Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier , . The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.|C√©sar A. Galindo-Legaria,Torsten Grabs,Christian Kleinerman,Florian Waas"],["65503|AAAI|2005|Augmenting Disjunctive Temporal Problems with Finite-Domain Constraints|We present a general framework for augmenting instances of the Disjunctive Temporal Problem (DTP) with finite-domain constraints. In this new formalism, the bounds of the temporal constraints become conditional on the finite-domain assignment. This hybridization makes it possible to reason simultaneously about temporal relationships between events as well as their nontemporal properties. We provide a special case of this hybridization that allows reasoning about a limited form of spatial constraints namely, the travel time induced by the locations of a set of activities. We develop a least-commitment algorithm for efficiently finding solutions to this combined constraint system and provide empirical results demonstrating the effectiveness of our approach.|Michael D. Moffitt,Bart Peintner,Martha E. Pollack","65441|AAAI|2005|Finding Diverse and Similar Solutions in Constraint Programming|It is useful in a wide range of situations to find solutions which are diverse (or similar) to each other. We therefore define a number of different classes of diversity and similarity problems. For example, what is the most diverse set of solutions of a constraint satisfaction problem with a given cardinality We first determine the computational complexity of these problems. We then propose a number of practical solution methods, some of which use global constraints for enforcing diversity (or similarity) between solutions. Empirical evaluation on a number of problems show promising results.|Emmanuel Hebrard,Brahim Hnich,Barry O'Sullivan,Toby Walsh","65538|AAAI|2005|SAT-Based versus CSP-Based Constraint Weighting for Satisfiability|Recent research has focused on bridging the gap between the satisfiability (SAT) and constraint satisfaction problem (CSP) formalisms. One approach has been to develop a many-valued SAT formula (MV-SAT) as an intermediate paradigm between SAT and CSP, and then to translate existing highly efficient SAT solvers to the MV-SAT domain. Experimental results have shown this approach can achieve significant improvements in performance compared with the traditional SAT and CSP approaches. In this paper, we follow a different route, developing SAT solvers that can automatically recognise CSP structure hidden in SAT encodings. This allows us to look more closely at how constraint weighting can be implemented in the SAT and CSP domains. Our experimental results show that a SAT-based approach to handle weights, together with CSP-based approach to variable instantiation, is superior to other combinations of SAT and CSP-based approaches. A further experiment on the round robin scheduling problem indicates that this many-valued constraint weighting approach outperforms other state-of-the-art solvers.|Duc Nghia Pham,John Thornton,Abdul Sattar,Abdelraouf Ishtaiwi","65475|AAAI|2005|Neighborhood Interchangeability and Dynamic Bundling for Non-Binary Finite CSPs|Neighborhood Interchangeability (NI) identifies the equivalent values in the domain of a variable of a Constraint Satisfaction Problem (CSP) by considering only the constraints that directly apply to the variable. Freuder described an algorithm for efficiently computing NI values in binary CSPs. In this paper, we show that the generalization of this algorithm to non-binary CSPs is not straightforward, and introduce an efficient algorithm for computing. NI values in the presence of non-binary constraints. Further, we show how to interleave this mechanism with search for solving CSPs, thus yielding a dynamic bundling strategy. While the goal of dynamic bundling is to produce multiple robust solutions, we empirically show that it does not increase (but significantly decreases) the cost of search.|Anagh Lal,Berthe Y. Choueiry,Eugene C. Freuder","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","66118|AAAI|2007|Best-First Search for Treewidth|Finding the exact treewidth of a graph is central to many operations in a variety of areas, including probabilistic reasoning and constraint satisfaction. Treewidth can be found by searching over the space of vertex elimination orders. This search space differs from those where best-first search is typically applied, because a solution path is evaluated by its maximum edge cost instead of the sum of its edge costs. We show how to make best-first search admissible on max-cost problem spaces. We also employ breadth-first heuristic search to reduce the memory requirement while still eliminating all duplicate nodes in the search space. Our empirical results show that our algorithms find the exact treewidth an order of magnitude faster than the previous state-of-the-art algorithm on hard benchmark graphs.|P. Alex Dow,Richard E. Korf","66125|AAAI|2007|Transposition Tables for Constraint Satisfaction|In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.|Christophe Lecoutre,Lakhdar Sais,S√©bastien Tabary,Vincent Vidal","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","65619|AAAI|2005|Semantic Scene Concept Learning by an Autonomous Agent|Scene understanding addresses the issue of \"what a scene contains\". Existing research on scene understanding is typically focused on classifying a scene into classes that are of the same category type. These approaches, although they solve some scene-understanding tasks successfully, in general fail to address the semantics in scene understanding. For example, how does an agent learn the concept label \"red\" and \"ball\" without being told that it is a color or a shape label in advance To cope with this problem, we have proposed a novel research called semantic scene concept learning. Our proposed approach models the task of scene understanding as a \"multi-labeling\" classification problem. Each scene instance perceived by the agent may receive multiple labels coming from different concept categories, where the goal of learning is to let the agent discover the semantic meanings, i.e., the set of relevant visual features, of the scene labels received. Our preliminary experiments have shown the effectiveness of our proposed approach in solving this special intra- and inter-category mixing learning task.|Weiyu Zhu","80492|VLDB|2005|MDL Summarization with Holes|Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S &Theta H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H. We first characterize the problem by showing that solving the -d problem can be done in time linear to the size of hierarchy, but solving the -d problem is NP-hard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.|Shaofeng Bu,Laks V. S. Lakshmanan,Raymond T. Ng","66044|AAAI|2007|A Qualitative Approach to Multiple Fault Isolation in Continuous Systems|The multiple fault diagnosis problem is important, since the single fault assumption can lead to incorrect or failed diagnoses when multiple faults occur. It is challenging for continuous systems, because faults can mask or compensate each other's effects, and the solution space grows exponentially with the number of possible faults. We present a qualitative approach to multiple fault isolation in dynamic systems based on analysis of fault transient behavior. Our approach uses the observed measurement deviations and their temporal orderings to generate multiple fault hypotheses. The approach has polynomial space requirements and prunes diagnoses, resulting in an efficient online fault isolation scheme.|Matthew J. Daigle,Xenofon D. Koutsoukos,Gautam Biswas","65571|AAAI|2005|Spotting Subsequences Matching an HMM Using the Average Observation Probability Criteria with Application to Keyword Spotting|This paper addresses the problem of detecting keywords in unconstrained speech. The proposed algorithms search for the speech segment maximizing the average observation probability along the most likely path in the hypothesized keyword model. As known, this approach (sometimes referred to as sliding model method) requires a relaxation of the beginendpoints of the Viterbi matching, as well as a time normalization of the resulting score. This makes solutions complex (i.e., LN basic operations for keyword HMM models with L states and utterances with N frames). We present here two altemative (quite simple and efficient) solutions to this problem. a) First we provide a method that finds the optimal segmentation according to the criteria of maximizing the average observation probability. It uses Dynamic Programming as a step, but does not require scoring for all possible beginendpoints. While the worst case remains O(LN), this technique converged in at most (L+)N basic operations in each experiment for two very different applications. b) The second proposed algorithm does not provide a segmentation but can be used for the decision problem of whether the utterance should be classified as containing the keyword or not (provided a predefined threshold on the acceptable average observation probability). This allows the algorithm to be even faster, with fix cost of (L+)N.|Marius-Calin Silaghi","80574|VLDB|2005|A Trajectory Splitting Model for Efficient Spatio-Temporal Indexing|This paper addresses the problem of splitting trajectories optimally for the purpose of efficiently supporting spatio-temporal range queries using index structures (e.g., R-trees) that use minimum bounding hyper-rectangles as trajectory approximations. We derive a formal cost model for estimating the number of IOs required to evaluate a spatio-temporal range query with respect to a given query size and an arbitrary split of a trajectory. Based on the proposed model, we introduce a dynamic programming algorithm for splitting a set of trajectories that minimizes the number of expected disk IOs with respect to an average query size. In addition, we develop a linear time, near optimal solution for this problem to be used in a dynamic case where trajectory points are continuously updated. Our experimental evaluation confirms the effectiveness and efficiency of our proposed splitting policies when embedded into an R-tree structure.|Slobodan Rasetic,J√∂rg Sander,James Elding,Mario A. Nascimento","65957|AAAI|2007|Computing Pure Nash Equilibria in Symmetric Action Graph Games|We analyze the problem of computing pure Nash equilibria in action graph games (AGGs), which are a compact game-theoretic representation. While the problem is NP-complete in general, for certain classes of AGGs there exist polynomial time algorithms. We propose a dynamic-programming approach that constructs equilibria of the game from equilibria of restricted games played on subgraphs of the action graph. In particular, if the game is symmetric and the action graph has bounded treewidth, our algorithm determines the existence of pure Nash equilibrium in polynomial time.|Albert Xin Jiang,Kevin Leyton-Brown","80813|VLDB|2007|Efficient Computation of Reverse Skyline Queries|In this paper, for the first time, we introduce the concept of Reverse Skyline Queries. At first, we consider for a multidimensional data set P the problem of dynamic skyline queries according to a query point q. This kind of dynamic skyline corresponds to the skyline of a transformed data space where point q becomes the origin and all points of P are represented by their distance vector to q. The reverse skyline query returns the objects whose dynamic skyline contains the query object q. In order to compute the reverse skyline of an arbitrary query point, we first propose a Branch and Bound algorithm (called BBRS), which is an improved customization of the original BBS algorithm. Furthermore, we identify a super set of the reverse skyline that is used to bound the search space while computing the reverse skyline. To further reduce the computational cost of determining if a point belongs to the reverse skyline, we propose an enhanced algorithm (called RSSA) that is based on accurate pre-computed approximations of the skylines. These approximations are used to identify whether a point belongs to the reverse skyline or not. Through extensive experiments with both real-world and synthetic datasets, we show that our algorithms can efficiently support reverse skyline queries. Our enhanced approach improves reversed skyline processing by up to an order of magnitude compared to the algorithm without the usage of pre-computed approximations.|Evangelos Dellis,Bernhard Seeger","66057|AAAI|2007|Mining Sequential Patterns and Tree Patterns to Detect Erroneous Sentences|An important application area of detecting erroneous sentences is to provide feedback for writers of English as a Second Language. This problem is difficult since both erroneous and correct sentences are diversified. In this paper, we propose a novel approach to identifying erroneous sentences. We first mine labeled tree patterns and sequential patterns to characterize both erroneous and correct sentences. Then the discovered patterns are utilized in two ways to distinguish correct sentences from erroneous sentences () the patterns are transformed into sentence features for existing classification models, e.g, SVM () the patterns are used to build a rule-based classification model. Experimental results show that both techniques are promising while the second technique outperforms the first approach. Moreover, the classification model in the second proposal is easy to understand, and we can provide intuitive explanation for classification results.|Guihua Sun,Gao Cong,Xiaohua Liu,Chin-Yew Lin,Ming Zhou","80810|VLDB|2007|Tracing Lineage Beyond Relational Operators|Tracing the lineage of data is an important requirement for establishing the quality and validity of data. Recently, the problem of data provenance has been increasingly addressed in database research. Earlier work has been limited to the lineage of data as it is manipulated using relational operations within an RDBMS. While this captures a very important aspect of scientific data processing, the existing work is incapable of handling the equally important, and prevalent, cases where the data is processed by non-relational operations. This is particularly common in scientific data where sophisticated processing is achieved by programs that are not part of a DBMS. The problem of tracking lineage when non-relational operators are used to process the data is particularly challenging since there is potentially no constraint on the nature of the processing. In this paper we propose a novel technique that overcomes this significant barrier and enables the tracing of lineage of data generated by an arbitrary function. Our technique works directly with the executable code of the function and does not require any high-level description of the function or even the source code. We establish the feasibility of our approach on a typical application and demonstrate that the technique is able to discern the correct lineage. Furthermore, it is shown that the method can help identify limitations in the function itself.|Mingwu Zhang,Xiangyu Zhang,Xiang Zhang,Sunil Prabhakar"],["66246|AAAI|2007|ASKNet Automatically Generating Semantic Knowledge Networks|The ASKNet project uses a combination of NLP tools and spreading activation to transform natural language text into semantic knowledge networks. Network fragments are generated from input sentences using a parser and semantic analyser, then these fragments are combined using spreading activation based algorithms. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible. We have already managed to create networks more than twice as large as any comparable resource(. million nodes, . million edges) in less than  days. This report provides a summary of the project and its current state of development.|Brian Harrington","66004|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet system is an attempt to automatically generate large scale semantic knowledge networks from natural language text. State-of-the-art language processing tools, including parsers and semantic analysers, are used to turn input sentences into fragments of semantic network. These network fragments are combined using spreading activation-based algorithms which utilise both lexical and semantic information. The emphasis of the system is on wide-coverage and speed of construction. In this paper we show how a network consisting of over . million nodes and . million edges, more than twice as large as any network currently available, can be created in less than  days. We believe that the methods proposed here will enable the construction of semantic networks on a scale never seen before, and in doing so reduce the knowledge acquisition bottleneck for AI.|Brian Harrington,Stephen Clark","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","80834|VLDB|2007|Self-Organizing Schema Mappings in the GridVine Peer Data Management System|GridVine is a Peer Data Management System based on a decentralized access structure. Built following the principle of data independence, it separates a logical layer -- where data, schemas and mappings are managed -- from a physical layer consisting of a structured Peer-to-Peer network supporting efficient routing of messages and index load-balancing. Our system is totally decentralized, yet it fosters semantic interoperability through pairwise schema mappings and query reformulation. In this demonstration, we present a set of algorithms to automatically organize the network of schema mappings. We concentrate on three key functionalities () the sharing of data, schemas and schema mappings in the network, () the dynamic creation and deprecation of mappings to foster global interoperability, and () the propagation of queries using the mappings. We illustrate these functionalities using bioinformatic schemas and data in a network running on several hundreds of peers simultaneously.|Philippe Cudr√©-Mauroux,Suchit Agarwal,Adriana Budura,Parisa Haghani,Karl Aberer","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons","80759|VLDB|2007|A STEP Towards Realizing Codds Vision of Rendezvous with the Casual User|This demonstration showcases the STEP system for natural language access to relational databases. In STEP an administrator authors a highly structured semantic grammar through coupling phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. The resulting phrasal lexicon serves as a bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is nearing completion and will soon be field tested in several domains.|Michael Minock","66065|AAAI|2007|Semantic Inference at the Lexical-Syntactic Level|Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on complex logical representations. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, but lack a principled inference framework. We propose a generic semantic inference framework that operates directly on syntactic trees. New trees are infened by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, Covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical evaluation in a Relation Extraction setting supports the validity of our approach.|Roy Bar-Haim,Ido Dagan,Iddo Greental,Eyal Shnarch","66097|AAAI|2007|Macroscopic Models of Clique Tree Growth for Bayesian Networks|In clique tree clustering, inference consists of propagation in a clique tree compiled from a Bayesian network. In this paper, we develop an analytical approach to characterizing clique tree growth as a function of increasing Bayesian network connectedness, specifically (i) the expected number of moral edges in their moral graphs or (ii) the ratio of the number of non-root nodes to the number of root nodes. In experiments, we systematically increase the connectivity of bipartite Bayesian networks, and find that clique tree size growth is well-approximated by Gompertz growth curves. This research improves the understanding of the scaling behavior of clique tree clustering, provides a foundation for benchmarking and developing improved BN inference algorithms, and presents an aid for analytical trade-off studies of tree clustering using growth curves.|Ole J. Mengshoel","66200|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet project is an attempt to automatically generate semantic knowledge networks from natural language text. NLP tools such as parsers and semantic analysers are used to turn input sentences into fragments of semantic network, and these network fragments are combined using spreading activation algorithms that utilise both lexical and semantic information. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible.|Brian Harrington","65567|AAAI|2005|A Knowledge-Based Approach to Network Security Applying Cyc in the Domain of Network Risk Assessment|CycSecure is a network risk assessment and network monitoring application that relies on knowledge-based artificial intelligence technologies to improve on traditional network vulnerability assessment. CycSecure integrates public reports of software faults from online databases, data gathered automatically from computers on a network and hand-ontologized information about computers and computer networks. This information is stored in the Cyc knowledge base (KB) and reasoned about by the Cyc inference engine and planner to provide detailed analyses of the security (and vulnerability) of networks.|Blake Shepard,Cynthia Matuszek,C. Bruce Fraser,William Wechtenhiser,David Crabbe,Zelal G√ºng√∂rd√º,John Jantos,Todd Hughes,Larry Lefkowitz,Michael J. Witbrock,Douglas B. Lenat,Erik Larson"],["65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","66069|AAAI|2007|Active Imitation Learning|Imitation learning, also called learning by watching or programming by demonstration, has emerged as a means of accelerating many reinforcement learning tasks. Previous work has shown the value of imitation in domains where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent. We consider the more general scenario of learning from mentors who are themselves agents seeking to maximize their own rewards. We propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish, not necessarily helpful, mentor. We also address the questions of when an imitative agent should request help from a mentor, and when the mentor can be expected to acknowledge a request for help. In analogy with other types of active learning, we call the proposed approach active imitation learning.|Aaron P. Shon,Deepak Verma,Rajesh P. N. Rao","65548|AAAI|2005|Representing Conditional Independence Using Decision Trees|While the representation of decision trees is fully expressive theoretically, it has been observed that traditional decision trees has the replication problem. This problem makes decision trees to be large and learnable only when sufficient training data are available. In this paper, we present a new representation model, conditional independence trees (CITrees), to tackle the replication problem from probability perspective. We propose a novel algorithm for learning CITrees. Our experiments show that CITrees outperform naive Bayes (Langley, Iba, & Thomas ), C. (Quinlan ), TAN (Friedman, Geiger, & Goldszmidt ), and AODE (Webb, Boughton, & Wang ) significantly in classification accuracy.|Jiang Su,Harry Zhang","66058|AAAI|2007|Efficient Structure Learning in Factored-State MDPs|We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.|Alexander L. Strehl,Carlos Diuk,Michael L. Littman","66139|AAAI|2007|Clustering with Local and Global Regularization|Clustering is an old research topic in data mining and machine learning communities. Most of the traditional clustering methods can be categorized local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the dataset is proposed. The method, Clustering with Local and Global Consistency (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We will show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently by some iterative methods. Finally the experimental results on several datasets are presented to show the effectiveness of our method.|Fei Wang,Changshui Zhang,Tao Li","65372|AAAI|2005|Semi-Supervised Sequence Modeling with Syntactic Topic Models|Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or content word. We then use this projection as additional input features to a linear-chain conditional random field trained on limited labeled training data. On standard part-of-speech tagging and Chinese word segmentation data sets we show as much as % error reduction due to the unlabeled data, and also statistically-significant improvements over a related semi-supervised sequence tagging method due to Miller et al.|Wei Li 0010,Andrew McCallum","65437|AAAI|2005|Transforming between Propositions and Features Bridging the Gap|It is notoriously difficult to simultaneously deal with both probabilistic and structural representations in A.I., particularly because probability necessitates a uniform representation of the training examples. In this paper, we show how to build fully-specified probabilistic models from arbitrary propositional case descriptions about terrorist activities. Our method facilitates both reasoning and learning. Our solution is to use structural analogy to build probabilistic generalizations about those cases. We use these generalizations as a framework for mapping the structural representations, which are well-suited for reasoning, into features, which are well-suited for learning, and back again. Finally, we demonstrate how probabilistic generalizations are an excellent bridge for joining reasoning and learning by using them to perform a traditional machine learning technique, Bayesian network modeling, over arbitrarily high order structural data about terrorist actions, and further, we discuss how this might be used to facilitate automatic knowledge acquisition.|Daniel T. Halstead,Kenneth D. Forbus","80598|VLDB|2005|Statistical Learning Techniques for Costing XML Queries|Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.|Ning Zhang 0002,Peter J. Haas,Vanja Josifovski,Guy M. Lohman,Chun Zhang","65361|AAAI|2005|Efficient No-Regret Multiagent Learning|We present new results on the efficiency of no-regret algorithmsin the context of multiagent learning. We use a known approach to augment a large class of no-regret algorithms to allow stochastic sampling of actions and observation of scalar reward of only the action played. We show that the average actual payoffs of the resulting learner gets () close to the best response against (eventually) stationary opponents. () close to the asymptotic optimal payoff against opponents that playa converging sequence of policies. and () close to at least a dynamic variant of minimax payoff against arbitrary opponents. with a high probability in polynomial time. In addition the polynomial bounds are shown to be significantly better than previously known bounds. Furthermore, we do not need to assume that the learner knows the game matrices and can observe the opponents' actions, unlike previous work.|Bikramjit Banerjee,Jing Peng"],["65458|AAAI|2005|Towards Model-Based Diagnosis of Coordination Failures|With increasing deployment of multi-agent and distributed systems, there is an increasing need for failure diagnosis systems. While successfully tackling key challenges in multi-agent settings, model-based diagnosis has left open the diagnosis of coordination failures, where failures often lie in the boundaries between agents, and thus the inputs to the model--with which the diagnoser simulates the system to detect discrepancies--are not known. However, it is possible to diagnose such failures using a model of the coordination between agents. This paper formalizes model-based coordination diagnosis, using two coordination primitives (concurrence and mutual exclusion). We define the consistency-based and abductive diagnosis problems within this formalization, and show that both are NP-Hard by mapping them to other known problems.|Meir Kalech,Gal A. Kaminka","65405|AAAI|2005|Merging Argumentation Systems|In this paper, we address the problem of deriving sensible information from a collection of argumentation systems coming from different agents. A general framework for merging argumentation systems from Dung's theory of argumentation is presented. Each argumentation system gives both a set of arguments and the way they interact (i.e. attack or non-attack) according to the corresponding agent. The aim is to define the argument system (or the set of argument systems) that best represents the group. Our framework is general enough to handle the case when agents do not share the same set of arguments. Merging argumentation systems is shown as a valuable approach for defining (sets of) arguments acceptable by the group.|Sylvie Coste-Marquis,Caroline Devred,S√©bastien Konieczny,Marie-Christine Lagasquie-Schiex,Pierre Marquis","65428|AAAI|2005|Agent-Organized Networks for Multi-Agent Production and Exchange|As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.|Matthew E. Gaston,Marie desJardins","66036|AAAI|2007|Centralized Distributed or Something Else Making Timely Decisions in Multi-Agent Systems|In multi-agent systems, agents need to share information in order to make good decisions. Who does what in order to achieve this matters a lot. The assignment of responsibility influences delay and consequently affects agents' abilities to make timely decisions. It is often unclear which approaches are best. We develop a model where one can easily test the impact of different assignments and information sharing protocols by focusing only on the delays caused by computation and communication. Using the model, we obtain interesting results that provide insight about the types of assignments that perform well in various domains and how slight variations in protocols can make great differences in feasibility.|Tim Harbers,Rajiv T. Maheswaran,Pedro A. Szekely","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","65453|AAAI|2005|A Discourse Planning Approach to Cinematic Camera Control for Narratives in Virtual Environments|As the complexity of narrative-based virtual environments grows, the need for effective communication of information to the users of these systems increase. Effective camera control for narrative-oriented virtual worlds involves decision making at three different levels choosing cinematic geometric composition, choosing the best camera parameters for conveying affective information, and choosing camera shots and transitions to maintain thetorical coherence. We propose a camera planning system that mirrors the film production pipeline we describe our formalization of film idioms used to communicate affective information. Our representation of idioms captures their hierarchical nature, represents the causal motivation for selection of shots, and provides a way for the system designer to specify the ranking of candidate shot sequences.|Arnav Jhala,R. Michael Young","66158|AAAI|2007|Interest-Matching Comparisons using CP-nets|The formation of internet-based social networks has revived research on traditional social network models as well as interest-matching, or match-making, systems. In order to automate or augment the process of interest-matching, we describe a method for the comparison of preference orderings represented by CP-nets, which allows one to determine a shared interest level between agents. Empirical results suggest that this distance measure for preference orderings agrees with the intuitive assessment of shared interest levels.|Andrew W. Wicker,Jon Doyle","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66212|AAAI|2007|Anytime Coordination Using Separable Bilinear Programs|Developing scalable coordination algorithms for multi-agent systems is a hard computational challenge. One useful approach, demonstrated by the Coverage Set Algorithm (CSA), exploits structured interaction to produce significant computational gains. Empirically, CSA exhibits very good anytime performance, but an error bound on the results has not been established. We reformulate the algorithm and derive both online and offline error bounds for approximate solutions. Moreover, we propose an effective way to automatically reduce the complexity of the interaction. Our experiments show that this is a promising approach to solve a broad class of decentralized decision problems. The general formulation used by the algorithm makes it both easy to implement and widely applicable to a variety of other AI problems.|Marek Petrik,Shlomo Zilberstein","66100|AAAI|2007|Topic Segmentation Algorithms for Text Summarization and Passage Retrieval An Exhaustive Evaluation|In order to solve problems of reliability of systems based on lexical repetition and problems of adaptability of language-dependent systems, we present a context-based topic segmentation system based on a new informative similarity measure based on word co-occurrence. In particular, our evaluation with the state-of-the-art in the domain i.e. the c and the TextTiling algorithms shows improved results both with and without the identification of multiword units.|Ga√´l Dias,Elsa Alves,Jos√© Gabriel Pereira Lopes"],["65459|AAAI|2005|Flexible Teamwork in Behavior-Based Robots|A key challenge in deploying teams of robots in real-world applications is to automate the control of teamwork, such that the designer can focus on the taskwork. Existing teamwork architectures seeking to address this challenge are monolithic, in that they commit to interaction protocols at the architectural level, and do not allow the designer to mix and match protocols for a given task. We present BITE, a behavior-based teamwork architecture that automates collaboration in physical robots, in a distributed fashion. BITE separates task behaviors that control a robot's interaction with its task, from interaction behaviors that control a robot's interaction with its teammates. This distinction provides for flexibility and modularity in terms of the interactions used by teammates to collaborate effectively. It also allows BITE to synthesize and significantly extend existing teamwork architectures. BITE also incorporates key lessons learned in applying multi-agent teamwork architectures in physical robot teams. We present empirical results from experiments with teams of Sony AIBO robots executing BITE, and discuss the lessons learned.|Gal A. Kaminka,Inna Frenkel","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","65452|AAAI|2005|Non-Stationary Policy Learning in -Player Zero Sum Games|A key challenge in multiagent environments is the construction of agents that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting. These domains require on-line learning methods without the benefit of repeated training examples, as well as the ability to adapt to the evolving behavior of other agents in the environment. The difficulty is further exacerbated when the agents are in an adversarial relationship, demanding that a robust (i.e. winning) non-stationary policy be rapidly learned and adapted. We propose an on-line sequence learning algorithm, ELPH, based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non-stationary policies. We demonstrate the performance of this method in a non-stationary learning environment of adversarial zero-sum matrix games.|Steven Jensen,Daniel Boley,Maria L. Gini,Paul R. Schrater","65476|AAAI|2005|Coordinating Agile Systems through the Model-based Execution of Temporal Plans|Agile autonomous systems are emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness. We address the challenge of extending plan execution to underactuated systems that are controlled indirectly through the setting of continuous state variables. Our solution is a novel model-based executive that takes as input a temporally flexible state plan, specifying intended state evolutions, and dynamically generates a near-optimal control sequence. To achieve optimality and safety, the executive plans into the future, framing planning as a disjunctive programming problem. To achieve robustness to disturbances and tractability, planning is folded within a receding horizon, continuous planning framework. Key to performance is a problem reduction method based on constraint pruning. We benchmark performance through a suite of UAV scenarios using a hardware-in-the-loop testbed.|Thomas L√©aut√©,Brian C. Williams","80763|VLDB|2007|Proof-Infused Streams Enabling Authentication of Sliding Window Queries On Streams|As computer systems are essential components of many critical commercial services, the need for secure online transactions is now becoming evident. The demand for such applications, as the market grows, exceeds the capacity of individual businesses to provide fast and reliable services, making outsourcing technologies a key player in alleviating issues of scale. Consider a stock broker that needs to provide a real-time stock trading monitoring service to clients. Since the cost of multicasting this information to a large audience might become prohibitive, the broker could outsource the stock feed to third-party providers, who are in turn responsible for forwarding the appropriate sub-feed to clients. Evidently, in critical applications the integrity of the third-party should not be taken for granted. In this work we study a variety of authentication algorithms for selection and aggregation queries over sliding windows. Our algorithms enable the end-users to prove that the results provided by the third-party are correct, i.e., equal to the results that would have been computed by the original provider. Our solutions are based on Merkle hash trees over a forest of space partitioning data structures, and try to leverage key features, like update, query, signing, and authentication costs. We present detailed theoretical analysis for our solutions and empirically evaluate the proposed techniques.|Feifei Li,Ke Yi,Marios Hadjieleftheriou,George Kollios","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","65478|AAAI|2005|Large-Scale Localization from Wireless Signal Strength|Knowledge of the physical locations of mobile devices such as laptops or PDA's is becoming increasingly important with the rise of location-based services such as specialized web search, navigation and social network applications furthermore, location information is a key foundation for high-level activity inferencing. In this paper we propose a novel technique for accurately estimating the locations of mobile devices and their wearers from wireless signal strengths. Our technique estimates time-varying device locations on a spatial connectivity graph whose outdoor edges correspond to streets and whose indoor edges represent hallways. staircases, elevators. etc. Use of a hierarchical Bayesian framework for learning a signal strength sensor model allows us not only to achieve higher accuracy than existing approaches. but to overcome many of their limitations. In particular, our technique is able to () seamlessly integrate new access points into the model, () make use of negative information (not detecting an access point), and () bootstrap a sensor model from sparse training data. Experiments demonstrate various properties of our system.|Julie Letchner,Dieter Fox,Anthony LaMarca","65995|AAAI|2007|R-CAST Integrating Team Intelligence for Human-Centered Teamwork|Developing human-centered agent architectures requires the integral consideration of architectural flexibility, teamwork adaptability, and context reasoning capability. With the integration of various forms of team intelligence including shared teamwork process and progress, dynamic context management and infomation dependency reasoning, and recognition-primed collaborative decision mechanism, R-CAST offers a flexible solution to developing cognitive aids for the support of human-centered teamwork in information and knowledge intensive domains. In this paper, we present the key features of R-CAST. As evidence of its applications in complex real-world problems, we give two experimental evaluations of R-CAST as teammates and decision aids of human Command and Control teams.|Xiaocong Fan,John Yen"]]},"title":{"entropy":6.679302544750558,"topics":["system for, for and, the web, for, for the, the, system and, description logic, natural language, framework for, the and, business processes, for application, web service, semantic web, and logic, logic for, modal logic, and reasoning, the planning","and learning, learning for, learning, reasoning about, mobile robot, and analysis, the heuristics, reinforcement learning, and evaluation, event detection, about action, measuring the, and event, and retrieval, the impact, learning games, games, the document, the functions, analysis learning","bayesian networks, algorithms for, mechanism design, for networks, solving problem, and search, with constraint, and constraint, planning domains, for planning, planning with, models and, for constraint, constraint programming, approach domains, distributed pomdps, models for, value functions, search space, and programming","for data, data, for xml, query for, and data, for database, database, for streams, word disambiguation, data management, database system, sense disambiguation, word sense, support vector, large scale, materialized views, system data, xml, for efficient, queries","for the, the and, logic for, and logic, description logic, the, for case, description for, modal logic, the with, the case, the logic, negotiation, study, complexity","business processes, for task, for virtual, for integrating, for reasoning, reasoning, with, computational, decision, collaborative, solving, complex, extending, autonomous, content, user, knowledge, for","semantic, graph, automated, multiple, selection, filtering, assessment, index, intelligent, networks, tool, team, knowledge-based, service, based, robust, for, and","and learning, learning for, learning, and analysis, reinforcement learning, for games, learning games, learning environments, analysis learning, via learning, games, dynamic, performance, equilibria, improving, efficient, prediction, control","mechanism design, estimation for, value functions, design for, and programming, using and, and estimation, using for, using, online, combinatorial, auctions, coordination, measures, protocol, approximation, global, local, solutions, from","algorithms for, for networks, bayesian networks, and algorithms, networks, and networks, new for, inference for, networks using, and structure, structure for, sensor networks, and inference, for bayesian, for and, structure, inference, tree, hybrid, markov","query for, between and, from the, for top-k, for optimization, the plans, and query, query, information, approximate, pattern, linear, temporal, privacy, exploiting, distributed, framework, sets, the, computation","for xml, for streams, for efficient, support vector, query processing, processing queries, data processing, and streams, xml, xml and, efficient queries, efficient, xml schema, and efficient, streams, matching, processing, recognition, dynamic, machines"],"ranking":[["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy L√©cu√©,Alexandre Delteil","66031|AAAI|2007|Custom DU - A Web Based Business User Driven Automated Underwriting System|Custom DU is an automated underwriting system that enables mortgage lenders to build their own business rules that facilitate assessing borrower eligibility for different mortgage products. Developed by Fannie Mae, Custom DU has been used since  by several lenders to automate the underwriting of numerous mortgage products. Custom DU uses rule specification language techniques and a web-based, user-friendly interface for implementing business rules that represent business policy. Via the user interface, lenders can also customize their underwriting findings reports, test the rules that they have defined and publish changes to business rules on a real-time basis, all without any software modifications. The user interface enforces structure and consistency, enabling business users to focus on their underwriting guidelines when converting their business policy to rules. Once a lender has created their rules, loans are routed to the appropriate rulesets and customized, but consistent results are always returned to the lender. Using Custom DU, lenders can create different rulesets for their products and assign them to different channels of the business, allowing for centralized control of underwriting policies and procedures - even if lenders have decentralized operations.|Srinivas Krovvidy,Robin Landsman,Steve Opdahl,Nancy Templeton,Sydnor Smalera","66144|AAAI|2007|Prime Implicates and Prime Implicants in Modal Logic|The purpose of this paper is to extend the notions of prime implicates and prime implicants to the basic modal logic . We consider a number of different potential definitions of clauses and terms for , which we evaluate with respect to their syntactic, semantic, and complexity-theoretic properties. We then continue our analysis by comparing the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. We provide algorithms and complexity results for the prime implicate generation and recognition tasks for the two most satisfactory definitions.|Meghyn Bienvenu","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","65612|AAAI|2005|A Unified Framework for Representing Logic Program Updates|As a promising formulation to represent and reason about agents' dynamic behavious, logic program updates have been considerably studied recently. While similarities and differences between various approaches were discussed and evaluated by researchers, there is a lack of method to represent different logic program update approaches under a common framework. In this paper, we continue our study on a general framework for logic program conflict solving based on notions of strong and weak forgettings (Zhang, Foo, & Wang ). We show that all major logic program update approaches can be transformed into our framework, under which each update approach becomes a specific conflict solving case with certain constraints. We also investigate related computational properties for these transformations.|Yan Zhang,Norman Y. Foo","66048|AAAI|2007|A Planning Approach for Message-Oriented Semantic Web Service Composition|In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm.|Zhen Liu,Anand Ranganathan,Anton Riabov","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski"],["65477|AAAI|2005|Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts|Automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content. We aim at a summarization system that can replicate the quality of summaries created by humans. In this paper we investigate the machine learning method for extracting full sentences from documents based on the document semantic graph structure. In particular, we explore how the Support Vector Machines (SVM) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations. We apply two types of linguistic analysis () a simple part-of-speech tagging of noun phrases and verbs and () full logical form analysis which identifies Subject-Predicate-Object triples, and then build the semantic graphs. We train the SVM classifier to identify summary nodes and use these nodes to extract sentences. Experiments with the DUC  and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis. In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries.|Jure Leskovec,Natasa Milic-Frayling,Marko Grobelnik","66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","65376|AAAI|2005|An Analysis of Procedure Learning by Instruction|Many useful planning tasks are handled by plan execution tools, such as PRS, that expand procedure definitions and keep track of several interacting goals and tasks. Learning by instruction is a promising approach to help users modifY the definitions of the procedures. However, the impact of the set of possible instructions on the performance of such systems is not well understood. We develop a framework in which instruction templates may be characterized in terms of syntactic transforms on task definitions, and use it to explore the properties of coverage, ambiguity and efficiency in the set of instructions that are understood by an implemented task learning system. We determine what kind of ambiguity is affected by the instruction set, and show how context-dependent interpretation can increase efficiency and coverage without increasing ambiguity.|Jim Blythe","65380|AAAI|2005|A Learning and Reasoning System for Intelligence Analysis|This paper presents a personal cognitive assistant, called Disciple-LTA, that can acquire expertise in intelligence analysis directly from intelligence analysts, can train new analysts, and can help analysts find solutions to complex problems through mixed-initiative reasoning, making possible the synergistic integration of a human's experience and creativity with an automated agent's knowledge and speed, and facilitating the collaboration with complementary experts and their agents.|Mihai Boicu,Gheorghe Tecuci,Cindy Ayers,Dorin Marcu,Cristina Boicu,Marcel Barbulescu,Bogdan Stanescu,William Wagner,Vu Le,Denitsa Apostolova,Adrian Ciubotariu","66101|AAAI|2007|Efficient Reinforcement Learning with Relocatable Action Models|Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.|Bethany R. Leffler,Michael L. Littman,Timothy Edmunds","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","65604|AAAI|2005|Software Testing by Active Learning for Commercial Games|As software systems have become larger, exhaustive testing has become increasingly onerous. This has rendered statistical software testing and machine learning techniques increasingly attractive. Drawing from both of these, we present an active learning framework for blackbox software testing. The active learning approach samples inputoutput pairs from a blackbox and learns a model of the system's behaviour. This model is then used to select new inputs for sampling. This framework has been developed in the context of commercial video games, complex virtual worlds with high-dimensional state spaces, too large for exhaustive testing. Beyond its correctness, developers need to evaluate the gameplay of a game, properties such as difficulty. We use the learned model not only to guide sampling but also to summarize the game's behaviour for the developer to evaluate. We present results from our semi-automated gameplay analysis by machine learning (SAGA-ML) tool applied to Electronics Arts' FIFA Soccer game.|Gang Xiao,Finnegan Southey,Robert C. Holte,Dana F. Wilkinson","65975|AAAI|2007|Learning Equilibrium in Resource Selection Games|We consider a resource selection game with incomplete information about the resource-cost functions. All the players know is the set of players, an upper bound on the possible costs, and that the cost functions are positive and nondecreasing. The game is played repeatedly and after every stage each player observes her cost, and the actions of all players. For every    we prove the existence of a learning -equilibrium, which is a profile of algorithms, one for each player such that a unilateral deviation of a player is, up to  not beneficial for her regardless of the actual cost functions. Furthermore, the learning eqUilibrium yields an optimal social cost.|Itai Ashlagi,Dov Monderer,Moshe Tennenholtz","65377|AAAI|2005|Optimal Efficient Learning Equilibrium Imperfect Monitoring in Symmetric Games|Efficient Learning Equilibrium (ELE) is a natural solution concept for multi-agent encounters with incomplete information. It requires the learning algorithms themselves to be in equilibrium for any game selected from a set of (initially unknown) games. In an optimal ELE, the learning algorithms would efficiently obtain the surplus the agents would obtain in an optimal Nash equilibrium of the initially unknown game which is played. The crucial part is that in an ELE deviations from the learning algorithms would become nonbeneficial after polynomial time, although the game played is initially unknown. While appealing conceptually, the main challenge for establishing learning algorithms based on this concept is to isolate general classes of games where an ELE exists. Unfortunately, it has been shown that while an ELE exists for the setting in which each agent can observe all other agents' actions and payoffs, an ELE does not exist in general when the other agents' payoffs cannot be observed. In this paper we provide the first positive results on this problem, constructively proving the existence of an optimal ELE for the class of symmetric games where an agent can not observe other agents' payoffs.|Ronen I. Brafman,Moshe Tennenholtz"],["65429|AAAI|2005|Fast Planning in Domains with Derived Predicates An Approach Based on Rule-Action Graphs and Local Search|The ability to express \"derived predicates\" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of \"Rule-Action Graphs\", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina,Paolo Toninelli","66094|AAAI|2007|Synthesis of Constraint-Based Local Search Algorithms from High-Level Models|The gap in automation between MIPSAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location.|Pascal Van Hentenryck,Laurent D. Michel","65441|AAAI|2005|Finding Diverse and Similar Solutions in Constraint Programming|It is useful in a wide range of situations to find solutions which are diverse (or similar) to each other. We therefore define a number of different classes of diversity and similarity problems. For example, what is the most diverse set of solutions of a constraint satisfaction problem with a given cardinality We first determine the computational complexity of these problems. We then propose a number of practical solution methods, some of which use global constraints for enforcing diversity (or similarity) between solutions. Empirical evaluation on a number of problems show promising results.|Emmanuel Hebrard,Brahim Hnich,Barry O'Sullivan,Toby Walsh","66116|AAAI|2007|Solving a Stochastic Queueing Design and Control Problem with Constraint Programming|A facility with front room and back room operations has the option of hiring specialized or, more expensive, cross-trained workers. Assuming stochastic customer arrival and service times, we seek a smallest-cost combination of cross-trained and specialized workers satisfying constraints on the expected customer waiting time and expected number of workers in the back room. A constraint programming approach using logic-based Benders' decomposition is presented. Experimental results demonstrate the strong performance of this approach across a wide variety of problem parameters. This paper provides one of the first links between queueing optimization problems and constraint programming.|Daria Terekhov,J. Christopher Beck,Kenneth N. Brown","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","65353|AAAI|2005|Mechanism Design for Single-Value Domains|In \"Single-Value domains\", each agent has the same private value for all desired outcomes. We formalize this notion and give new examples for such domains. including a \"SAT domain\" and a \"single-value combinatorial auctions\" domain. We study two informational models where the set of desired outcomes is public information (the \"known\" case). and where it is private information (the \"unknown\" case). Under the \"known\" assumption, we present several truthful approximation mechanisms. Additionally, we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain (where agent values are either zero or one) to a truthful mechanism, with only a small approximation loss. In contrast, we show that even positive results from the \"unknown single minded combinatorial auctions\" literature fail to extend to the \"unknown\" single-value case. We give a characterization of truthfulness in this case, demonstrating that the difference is subtle and surprising.|Moshe Babaioff,Ron Lavi,Elan Pavlov","65362|AAAI|2005|Using SAT and Logic Programming to Design Polynomial-Time Algorithms for Planning in Non-Deterministic Domains|We show that a Horn SAT and logic programming approach to obtain polynomial time algorithms for problem solving can be fruitfully applied to finding plans for various kinds of goals in a non-deterministic domain. We particularly focus on finding weak, strong, and strong cyclic plans for planning problems, as they are the most studied ones in the literature. We describe new algorithms for these problems and show how non-monotonic logic programming can be used to declaratively compute strong cyclic plans. As a further benefit, preferred plans among alternative candidate plans may be singled out this way. We give complexity results for weak. strong, and strong cyclic planning. Finally, we briefly discuss some of the kinds of goals in non-deterministic domains for which the approach in the paper can be used.|Chitta Baral,Thomas Eiter,Jicheng Zhao","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","65578|AAAI|2005|Conformant Planning for Domains with Constraints-A New Approach|The paper presents a pair of new conformant planners, CPApc and CPAph, based on recent developments in theory of action and change. As an input the planners take a domain description D in action language AL which allows state constraints (non-stratified axioms), together with a set of CNF formulae describing the initial state, and a set of literals representing the goal. We propose two approximations of the transition diagram T defined by D. Both approximations are deterministic transition functions and can be computed efficiently. Moreover they are sound (and sometimes complete) with respect to T. In its search for a plan, an approximation based planner analyses paths of an approximation instead of that of T. CPApc and CPAph are forward, best first search planners based on this idea. We compare them with two state-of-the-art conformant planners, KACMBP and Conformant-FF (CFF), over benchmarks in the literature, and over two new domains. One has large number of state constraints and another has a high degree of incompleteness. Our planners perform reasonably well in benchmark domains and outperform KACMBP and CFF in the first domain while still working well with the second one. Our experimental result shows that having an integral part of a conformant planner to deal with state constraints directly can significantly improve its performance extending a similar claim for classical planners in (Thiebaux. Hoffmann, & Nebel ).|Tran Cao Son,Phan Huy Tu,Michael Gelfond,A. Ricardo Morales"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","80572|VLDB|2005|Large Scale Data Warehouses on Grid Oracle Database g and HP ProLiant Systems|Grid computing has the potential to drastically change enterprise computing as we know it today. The main concept of grid computing is viewing computing as a utility. It should not matter where data resides, or what computer processes a task. This concept has been applied successfully to academic research. It also has many advantages for commercial data warehouse applications such as virtualization, flexible provisioning, reduced cost due to commodity hardware, high availability and high scale-out. In this paper we show how a large-scale, high-performing and scalable grid-based data warehouse can be implemented using commodity hardware (industry-standard x-based). Oracle Database g and the Linux operating system. We further demonstrate this architecture in a recently published TPC-H benchmark.|Meikel Poess,Raghunath Othayoth Nambiar","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","65393|AAAI|2005|Scaling Up Word Sense Disambiguation via Parallel Texts|A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL- English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL- English all-words task, and significantly outperforms the baseline of always choosing sense  of WordNet.|Yee Seng Chan,Hwee Tou Ng","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Korn√©l G. Mark√≥,Stefan Schulz,Udo Hahn","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80517|VLDB|2005|Maximal Vector Computation in Large Data Sets|Finding the maximals in a collection of vectors is relevant to many applications. The maximal set is related to the convex hull---and hence, linear optimization---and nearest neighbors. The maximal vector problem has resurfaced with the advent of skyline queries for relational databases and skyline algorithms that are external and relationally well behaved.The initial algorithms proposed for maximals are based on divide-and-conquer. These established good average and worst case asymptotic running times, showing it to be O(n) average-case. where n is the number of vectors. However, they are not amenable to externalizing. We prove, furthermore, that their performance is quite bad with respect to the dimensionality, k, of the problem. We demonstrate that the more recent external skyline algorithms are actually better behaved, although they do not have as good an apparent asymptotic complexity. We introduce a new external algorithm, LESS, that combines the best features of these. experimentally evaluate its effectiveness and improvement over the field, and prove its average-case running time is O(kn).|Parke Godfrey,Ryan Shipley,Jarek Gryz"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","65598|AAAI|2005|A Theory of Forgetting in Logic Programming|The study of forgetting for reasoning has attracted considerable attention in AI. However, much of the work on forgetting, and other related approaches such as independence, irrelevance and novelty, has been restricted to the classical logics. This paper describes a detailed theoretical investigation of the notion of forgetting in the context of logic programming. We first provide a semantic definition of forgetting under the answer sets for extended logic programs. We then discuss the desirable properties and some motivating examples. An important result of this study is an algorithm for computing the result of forgetting in a logic program. Furthermore, we present a modified version of the algorithm and show that the time complexity of the new algorithm is polynomial with respect to the size of the given logic program if the size of certain rules is fixed. We show how the proposed theory of forgetting can be used to characterize the logic program updates.|Kewen Wang,Abdul Sattar,Kaile Su","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","66144|AAAI|2007|Prime Implicates and Prime Implicants in Modal Logic|The purpose of this paper is to extend the notions of prime implicates and prime implicants to the basic modal logic . We consider a number of different potential definitions of clauses and terms for , which we evaluate with respect to their syntactic, semantic, and complexity-theoretic properties. We then continue our analysis by comparing the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. We provide algorithms and complexity results for the prime implicate generation and recognition tasks for the two most satisfactory definitions.|Meghyn Bienvenu","66121|AAAI|2007|A Logic of Emotions for Intelligent Agents|This paper formalizes a well-known psychological model of emotions in an agent specification language. This is done by introducing a logical language and its semantics that are used to specify an agent model in terms of mental attitudes including emotions. We show that our formalization renders a number of intuitive and plausible properties of emotions. We also show how this formalization can be used to specify the effect of emotions on an agent's decision making process. Ultimately, the emotions in this model function as heuristics as they constrain an agent's model.|Bas R. Steunebrink,Mehdi Dastani,John-Jules Ch. Meyer","66034|AAAI|2007|Generality and Equivalence Relations in Default Logic|Generality or refinement relations between different theories have important applications to generalization in inductive logic programming, refinement of ontologies, and coordination in multi-agent systems. We study generality relations in disjunctive default logic by comparing the amounts of information brought by default theories. Intuitively, a default theory is considered more general than another default theory if the former brings more information than the latter. Using techniques in domain theory, we introduce different types of generality relations over default theories. We show that generality relations based on the Smyth and Hoare orderings reflect orderings on skeptical and credulous consequences, respectively, and that two default theories are equivalent if and only if they are equally general under these orderings. These results naturally extend both generality relations over first-order theories and those for answer set programming.|Katsumi Inoue,Chiaki Sakama","65576|AAAI|2005|Discriminative Training of Markov Logic Networks|Many machine learning applications require a combination of probability and first-order logic. Markov logic networks (MLNs) accomplish this by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Model parameters (i.e., clause weights) can be learned by maximizing the likelihood of a relational database, but this can be quite costly and lead to suboptimal results for any given prediction task. In this paper we propose a discriminative approach to training MLNs, one which optimizes the conditional likelihood of the query predicates given the evidence ones, rather than the joint likelihood of all predicates. We extend Collins's () voted perceptron algorithm for HMMs to MLNs by replacing the Viterbi algorithm with a weighted satisfiability solver. Experiments on entity resolution and link prediction tasks show the advantages of this approach compared to generative MLN training, as well as compared to purely probabilistic and purely logical approaches.|Parag Singla,Pedro Domingos","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski"],["65466|AAAI|2005|Solving Everyday Physical Reasoning Problems by Analogy Using Sketches|Understanding common sense reasoning about the physical world is one of the goals of qualitative reasoning research. This paper describes how we combine qualitative mechanics and analogy to solve everyday physical reasoning problems posed as sketches. The problems are drawn from the Bennett Mechanical Comprehension Test, which is used to evaluate technician candidates. We discuss sketch annotations, which define conceptual quantities in terms of visual measurements, how modeling decisions are made by analogy, and how analogy can be used to frame comparative analysis problems. Experimental results support the plausibility of this approach.|Matthew Klenk,Kenneth D. Forbus,Emmett Tomai,Hyeonkyeong Kim,Brian Kyckelhahn","65551|AAAI|2005|DiamondHelp A Collaborative Task Guidance Framework for Complex Devices|DiamondHelp is a reusable Java framework for building collaborative task guidance systems for complex devices, such as digitally enabled home appliances. DiamondHelp combines a generic conversational interface, adapted from online chat programs, with an application-specific direct manipulation interface. DiamondHelp provides \"a things to say\" mechanism for use without spoken language understanding it also supports extensions to take advantage of speech technology. DiamondHelp's software architecture factors all application-specific content into two modular plug-ins, one of which includes Collagen and a task model.|Charles Rich,Candace L. Sidner,Neal Lesh,Andrew Garland,Shane Booth,Markus Chimani","65526|AAAI|2005|Analysis of Strategic Knowledge in Back of the Envelope Reasoning|Back of the envelope (BotE) reasoning involves generating quantitative answers in situations where exact data and models are unavailable and where available data is often incomplete andor inconsistent. A rough estimate generated quickly is more valuable and useful than a detailed analysis, which might be unnecessary, impractical, or impossible because the situation does not provide enough time, information, or other resources to perform one. Such reasoning is a key component of commonsense reasoning about everyday physical situations. We present an implemented system, BotE-Solver, that can solve about a dozen estimation questions like \"What is the annual cost of healthcare in USA\" from different domains using a library of strategies and the Cyc knowledge base. BotE-Solver is a general-purpose problem solving framework that uses strategies represented as suggestions, and keeps track of problem solving progress in an ANDOR tree. A key contribution of this paper is a knowledge level analysis Newell,  of the strategic knowledge used in BotE reasoning. We present a core collection of seven powerful estimation strategies that provides broad coverage for such problem solving. We hypothesize that this is the complete set of back of the envelope problem solving strategies. We present twofold support for this hypothesis ) an empirical analysis of all problems (n) on Force and Pressure, Rotation and Mechanics, Heat, and Astronomy from Clifford Swartz's \"Back-of-the-Envelope Physics\" Swartz, , and ) an analysis of strategies used by BotE-Solver.|Praveen K. Paritosh,Kenneth D. Forbus","66236|AAAI|2007|Optimal Regression for Reasoning about Knowledge and Actions|We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.|Hans P. van Ditmarsch,Andreas Herzig,Tiago De Lima","80806|VLDB|2007|Monitoring Business Processes with Queries|Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. These are complex, often distributed, processes. Monitoring the execution of such processes for interesting patterns is critical for enforcing business policies and meeting efficiency and reliability goals. BP-Mon (Business Processes Monitoring) is a novel query language for monitoring business processes, that allows users to visually define monitoring tasks and associated reports, using a simple intuitive interface, similar to those used for designing BPEL processes. We describe here the BP-Mon language and its underlying formal model. We also present the language implementation and describe our novel optimization techniques. An important feature of the implementation is that BP-Mon queries are translated to BPEL processes that run on the same execution engine as the monitored processes. Our experiments indicate that this approach incurs very minimal overhead, hence is a practical and efficient approach to monitoring.|Catriel Beeri,Anat Eyal,Tova Milo,Alon Pilberg","66073|AAAI|2007|Representing and Reasoning about Commitments in Business Processes|A variety of business relationships in open settings can be understood in terms of the creation and manipulation of commitments among the participants. These include BC and BB contracts and processes, as realized via Web services and other such technologies. Business protocols, an interaction-oriented approach for modeling business processes, are formulated in terms of the commitments. Commitments can support other forms of semantic service composition as well. This paper shows how to represent and reason about commitments in a general manner. Unlike previous formalizations, the proposed formalization accommodates complex and nested commitment conditions, and concurrent commitment operations. In this manner, a rich variety of open business scenarios are enabled.|Nirmit Desai,Amit K. Chopra,Munindar P. Singh","66224|AAAI|2007|PLOW A Collaborative Task Learning Agent|To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies deep natural language understanding, knowledge representation and reasoning, dialogue systems, planningagent-based systems and machine learning. A formal evaluation shows the approach has great promise.|James F. Allen,Nathanael Chambers,George Ferguson,Lucian Galescu,Hyuckchul Jung,Mary D. Swift,William Taysom","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","66234|AAAI|2007|Using More Reasoning to Improve SAT Solving|Many real-world problems, including inference in Bayes Nets, can be reduced to SAT, the problem of counting the number of models of a propositional theory. This has motivated the need for efficient SAT solvers. Currently, such solvers utilize a modified version of DPLL that employs decomposition and caching, techniques that significantly increase the time it takes to process each node in the search space. In addition, the search space is significantly larger than when solving SAT since we must continue searching even after the first solution has been found. It has previously been demonstrated that the size of a DPLL search tree can be significantly reduced by doing more reasoning at each node. However, for SAT the reductions gained are often not worth the extra time required. In this paper we verify the hypothesis that for SAT this balance changes. In particular, we show that additional reasoning can reduce the size of a SAT solver's search space, that this reduction cannot always be achieved by the already utilized technique of clause learning, and that this additional reasoning can be cost effective.|Jessica Davies,Fahiem Bacchus","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II"],["66246|AAAI|2007|ASKNet Automatically Generating Semantic Knowledge Networks|The ASKNet project uses a combination of NLP tools and spreading activation to transform natural language text into semantic knowledge networks. Network fragments are generated from input sentences using a parser and semantic analyser, then these fragments are combined using spreading activation based algorithms. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible. We have already managed to create networks more than twice as large as any comparable resource(. million nodes, . million edges) in less than  days. This report provides a summary of the project and its current state of development.|Brian Harrington","66004|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet system is an attempt to automatically generate large scale semantic knowledge networks from natural language text. State-of-the-art language processing tools, including parsers and semantic analysers, are used to turn input sentences into fragments of semantic network. These network fragments are combined using spreading activation-based algorithms which utilise both lexical and semantic information. The emphasis of the system is on wide-coverage and speed of construction. In this paper we show how a network consisting of over . million nodes and . million edges, more than twice as large as any network currently available, can be created in less than  days. We believe that the methods proposed here will enable the construction of semantic networks on a scale never seen before, and in doing so reduce the knowledge acquisition bottleneck for AI.|Brian Harrington,Stephen Clark","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","66263|AAAI|2007|Graph Partitioning Based on Link Distributions|Existing graph partitioning approaches are mainly based on optimizing edge cuts and do not take the distribution of edge weights (link distribution) into consideration. In this paper, we propose a general model to partition graphs based on link distributions. This model formulates graph partitioning under a certain distribution assumption as approximating the graph affinity matrix under the corresponding distortion measure. Under this model, we derive a novel graph partitioning algorithm to approximate a graph affinity matrix under various Bregman divergences, which correspond to a large exponential family of distributions. We also establish the connections between edge cut objectives and the proposed model to provide a unified view to graph partitioning.|Bo Long,Zhongfei (Mark) Zhang,Philip S. Yu","66039|AAAI|2007|A Framework for Ontology-Based Service Selection in Dynamic Environments|Previous approaches to service selection are mainly based on capturing and exchanging the ratings of consumers to providers. However, ratings reflect tastes of the raters. Therefore, service selection using ratings may mislead the consumers having a taste different than that of the raters. We propose to use experiences instead of the ratings. Experiences are the representation of what is requested by a consumer and what is received at the end. Unlike ratings, experiences do not reflect the opinion of the others, but the actual story between consumers and providers concerning a service demand. Using experiences, the consumer models the services of a provider for a specific service demand and selects the provider that is expected to satisfy the consumer the most. Our simulations show that proposed approach significantly increases the overall satisfaction of the service consumers.|Murat Sensoy","65978|AAAI|2007|Explanation Support for the Case-Based Reasoning Tool myCBR|Case-Based Reasoning, in short, is the process of solving new problems based on solutions of similar past problems, much like humans solve many problems. myCBR, an extension of the ontology editor Protg, provides such similarity-based retrieval functionality. Moreover, the user is supported in modelling appropriate similarity measures by forward and backward explanations.|Daniel Bahls,Thomas Roth-Berghofer","80782|VLDB|2007|Increasing Buffer-Locality for Multiple Index Based Scans through Intelligent Placement and Index Scan Speed Control|Decision support systems are characterized by large concurrent scan operations. A significant percentage of these scans are executed as index based scans of the data. This is especially true when the data is physically clustered on the index columns using the various clustering schemes employed by database engines. Common database management systems have only limited ability to reuse buffer content across multiple running queries due to their treatment of queries in isolation. Previous attempts to coordinate scans for better buffer reuse were limited to table scans only. Attempts for index based scan sharing were non existent or were less than satisfactory due to drifting between scans. In this paper, we describe a mechanism to keep scans using the same index closer together on scan position during scanning. This is achieved via intelligent placement of index scans at scan start time based on their scan ranges and speeds. This is then augmented by adaptive throttling of scan speeds based on the index scans runtime behavior during scan execution. We discuss the challenges in doing it for index scans in comparison to the more common table scan sharing. We show that this can be done with minimal changes to an existing database management system as demonstrated in our DB UDB prototype. Our experiments show significant gains in end-to-end response times and disk IO for TPC-H workloads.|Christian A. Lang,Bishwaranjan Bhattacharjee,Timothy Malkemus,Kwai Wong","66192|AAAI|2007|GRIN A Graph Based RDF Index|RDF (\"Resource Description Framework\") is now a widely used World Wide Web Consortium standard. However, methods to index large volumes of RDF data are still in their infancy. In this paper, we focus on providing a very lightweight indexing mechanism for certain kinds of RDF queries, namely graph-based queries where there is a need to traverse edges in the graph determined by an RDF database. Our approach uses the idea of drawing circles around selected \"center\" vertices in the graph where the circle would encompass those vertices in the graph that are within a given distance of the \"center\" vertex. We come up with methods of finding such \"center\" vertices and identifying the radius of the circles and then leverage this to build an index called GRIN. We compare GRIN with three existing RDF indexex Jena, Sesame. and RDFBroker. We compared (i) the time to answer graph based queries, (ii) memory needed to store the index, and (iii) the time to build the index. GRIN outperforms Jena, Sesame and RDFBroker on all three measures for graph based queries (for other types of queries, it may be worth building one of these other indexes and using it), at the expense of using a larger amount of memory when answering queries.|Octavian Udrea,Andrea Pugliese,V. S. Subrahmanian","66200|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet project is an attempt to automatically generate semantic knowledge networks from natural language text. NLP tools such as parsers and semantic analysers are used to turn input sentences into fragments of semantic network, and these network fragments are combined using spreading activation algorithms that utilise both lexical and semantic information. The ultimate goal of the project is to create a semantic resource on a scale that has never before been possible.|Brian Harrington","65567|AAAI|2005|A Knowledge-Based Approach to Network Security Applying Cyc in the Domain of Network Risk Assessment|CycSecure is a network risk assessment and network monitoring application that relies on knowledge-based artificial intelligence technologies to improve on traditional network vulnerability assessment. CycSecure integrates public reports of software faults from online databases, data gathered automatically from computers on a network and hand-ontologized information about computers and computer networks. This information is stored in the Cyc knowledge base (KB) and reasoned about by the Cyc inference engine and planner to provide detailed analyses of the security (and vulnerability) of networks.|Blake Shepard,Cynthia Matuszek,C. Bruce Fraser,William Wechtenhiser,David Crabbe,Zelal G√ºng√∂rd√º,John Jantos,Todd Hughes,Larry Lefkowitz,Michael J. Witbrock,Douglas B. Lenat,Erik Larson"],["65452|AAAI|2005|Non-Stationary Policy Learning in -Player Zero Sum Games|A key challenge in multiagent environments is the construction of agents that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting. These domains require on-line learning methods without the benefit of repeated training examples, as well as the ability to adapt to the evolving behavior of other agents in the environment. The difficulty is further exacerbated when the agents are in an adversarial relationship, demanding that a robust (i.e. winning) non-stationary policy be rapidly learned and adapted. We propose an on-line sequence learning algorithm, ELPH, based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non-stationary policies. We demonstrate the performance of this method in a non-stationary learning environment of adversarial zero-sum matrix games.|Steven Jensen,Daniel Boley,Maria L. Gini,Paul R. Schrater","65376|AAAI|2005|An Analysis of Procedure Learning by Instruction|Many useful planning tasks are handled by plan execution tools, such as PRS, that expand procedure definitions and keep track of several interacting goals and tasks. Learning by instruction is a promising approach to help users modifY the definitions of the procedures. However, the impact of the set of possible instructions on the performance of such systems is not well understood. We develop a framework in which instruction templates may be characterized in terms of syntactic transforms on task definitions, and use it to explore the properties of coverage, ambiguity and efficiency in the set of instructions that are understood by an implemented task learning system. We determine what kind of ambiguity is affected by the instruction set, and show how context-dependent interpretation can increase efficiency and coverage without increasing ambiguity.|Jim Blythe","65380|AAAI|2005|A Learning and Reasoning System for Intelligence Analysis|This paper presents a personal cognitive assistant, called Disciple-LTA, that can acquire expertise in intelligence analysis directly from intelligence analysts, can train new analysts, and can help analysts find solutions to complex problems through mixed-initiative reasoning, making possible the synergistic integration of a human's experience and creativity with an automated agent's knowledge and speed, and facilitating the collaboration with complementary experts and their agents.|Mihai Boicu,Gheorghe Tecuci,Cindy Ayers,Dorin Marcu,Cristina Boicu,Marcel Barbulescu,Bogdan Stanescu,William Wagner,Vu Le,Denitsa Apostolova,Adrian Ciubotariu","66101|AAAI|2007|Efficient Reinforcement Learning with Relocatable Action Models|Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.|Bethany R. Leffler,Michael L. Littman,Timothy Edmunds","66143|AAAI|2007|RETALIATE Learning Winning Policies in First-Person Shooter Games|In this paper we present RETALIATE, an online reinforcement learning algorithm for developing winning policies in team first-person shooter games. RETALIATE has three crucial characteristics () individual BOT behavior is fixed although not known in advance, therefore individual BOTS work as \"plugins\", () RETALIATE models the problem of learning team tactics through a simple state formulation, () discount rates commonly used in Q-Iearning are not used. As a result of these characteristics, the application of the Q-learning algorithm results in the rapid exploration towards a winning policy against an opponent team. In our empirical evaluation we demonstrate that RETALIATE adapts well when the environment changes.|Megan Smith,Stephen Lee-Urban,Hector Mu√±oz-Avila","65604|AAAI|2005|Software Testing by Active Learning for Commercial Games|As software systems have become larger, exhaustive testing has become increasingly onerous. This has rendered statistical software testing and machine learning techniques increasingly attractive. Drawing from both of these, we present an active learning framework for blackbox software testing. The active learning approach samples inputoutput pairs from a blackbox and learns a model of the system's behaviour. This model is then used to select new inputs for sampling. This framework has been developed in the context of commercial video games, complex virtual worlds with high-dimensional state spaces, too large for exhaustive testing. Beyond its correctness, developers need to evaluate the gameplay of a game, properties such as difficulty. We use the learned model not only to guide sampling but also to summarize the game's behaviour for the developer to evaluate. We present results from our semi-automated gameplay analysis by machine learning (SAGA-ML) tool applied to Electronics Arts' FIFA Soccer game.|Gang Xiao,Finnegan Southey,Robert C. Holte,Dana F. Wilkinson","65975|AAAI|2007|Learning Equilibrium in Resource Selection Games|We consider a resource selection game with incomplete information about the resource-cost functions. All the players know is the set of players, an upper bound on the possible costs, and that the cost functions are positive and nondecreasing. The game is played repeatedly and after every stage each player observes her cost, and the actions of all players. For every    we prove the existence of a learning -equilibrium, which is a profile of algorithms, one for each player such that a unilateral deviation of a player is, up to  not beneficial for her regardless of the actual cost functions. Furthermore, the learning eqUilibrium yields an optimal social cost.|Itai Ashlagi,Dov Monderer,Moshe Tennenholtz","65600|AAAI|2005|Improving Reinforcement Learning Function Approximators via Neuroevolution|Reinforcement learning problems are commonly tackled with temporal difference methods, which use dynamic programming and statistical sampling to estimate the long-term value of taking each action in each state. In most problems of real-world interest, learning this value function requires a function approximator. which represents the mapping from stateaction pairs to values via a concise, parameterized function and uses supervised learning methods to set its parameters. Function approximators make it possible to use temporal difference methods on large problems but, in practice, the feasibility of doing so depends on the ability of the human designer to select an appropriate representation for the value function. My thesis presents a new approach to function approximation that automates some of these difficult design choices by coupling temporal difference methods with policy search methods such as evolutionary computation. It also presents a particular implementation which combines NEAT, a neuroevolutionary policy search method, and Q-learning, a popular temporal difference method, to yield a new method called NEAT+Q that automatically learns effective representations for neural network function approximators. Empirical results in a server job scheduling task demonstrate that NEAT+Q can outperform both NEAT and Q-learning with manually designed neural networks.|Shimon Whiteson","65377|AAAI|2005|Optimal Efficient Learning Equilibrium Imperfect Monitoring in Symmetric Games|Efficient Learning Equilibrium (ELE) is a natural solution concept for multi-agent encounters with incomplete information. It requires the learning algorithms themselves to be in equilibrium for any game selected from a set of (initially unknown) games. In an optimal ELE, the learning algorithms would efficiently obtain the surplus the agents would obtain in an optimal Nash equilibrium of the initially unknown game which is played. The crucial part is that in an ELE deviations from the learning algorithms would become nonbeneficial after polynomial time, although the game played is initially unknown. While appealing conceptually, the main challenge for establishing learning algorithms based on this concept is to isolate general classes of games where an ELE exists. Unfortunately, it has been shown that while an ELE exists for the setting in which each agent can observe all other agents' actions and payoffs, an ELE does not exist in general when the other agents' payoffs cannot be observed. In this paper we provide the first positive results on this problem, constructively proving the existence of an optimal ELE for the class of symmetric games where an agent can not observe other agents' payoffs.|Ronen I. Brafman,Moshe Tennenholtz","65361|AAAI|2005|Efficient No-Regret Multiagent Learning|We present new results on the efficiency of no-regret algorithmsin the context of multiagent learning. We use a known approach to augment a large class of no-regret algorithms to allow stochastic sampling of actions and observation of scalar reward of only the action played. We show that the average actual payoffs of the resulting learner gets () close to the best response against (eventually) stationary opponents. () close to the asymptotic optimal payoff against opponents that playa converging sequence of policies. and () close to at least a dynamic variant of minimax payoff against arbitrary opponents. with a high probability in polynomial time. In addition the polynomial bounds are shown to be significantly better than previously known bounds. Furthermore, we do not need to assume that the learner knows the game matrices and can observe the opponents' actions, unlike previous work.|Bikramjit Banerjee,Jing Peng"],["66244|AAAI|2007|Automated Online Mechanism Design and Prophet Inequalities|Recent work on online auctions for digital goods has explored the role of optimal stopping theory -- particularly secretary problems -- in the design of approximately optimal online mechanisms. This work generally assumes that the size of the market (number of bidders) is known a priori, but that the mechanism designer has no knowledge of the distribution of bid values. However, in many real-world applications (such as online ticket sales), the opposite is true the seller has distributional knowledge of the bid values (e.g., via the history of past transactions in the market), but there is uncertainty about market size. Adopting the perspective of automated mechanism design, introduced by Conitzer and Sandholm, we develop algorithms that compute an optimal, or approximately optimal, online auction mechanism given access to this distributional knowledge. Our main results are twofold. First, we show that when the seller does not know the market size, no constant-approximation to the optimum efficiency or revenue is achievable in the worst case, even under the very strong assumption that bid values are i.i.d. samples from a distribution known to the seller. Second, we show that when the seller has distributional knowledge of the market size as well as the bid values, one can do well in several senses. Perhaps most interestingly, by combining dynamic programming with prophet inequalities (a technique from optimal stopping theory) we are able to design and analyze online mechanisms which are temporally strategyproof (even with respect to arrival and departure times) and approximately efficiency (revenue)-maximizing. In exploring the interplay between automated mechanism design and prophet inequalities, we prove new prophet inequalities motivated by the auction setting.|Mohammad Taghi Hajiaghayi,Robert D. Kleinberg,Tuomas Sandholm","66253|AAAI|2007|Compact Spectral Bases for Value Function Approximation Using Kronecker Factorization|A new spectral approach to value function approximation has recently been proposed to automatically construct basis functions from samples. Global basis functions called proto-value functions are generated by diagonalizing a diffusion operator, such as a reversible random walk or the Laplacian, on a graph formed from connecting nearby samples. This paper addresses the challenge of scaling this approach to large domains. We propose using Kronecker factorization coupled with the Metropolis-Hastings algorithm to decompose reversible transition matrices. The result is that the basis functions can be computed on much smaller matrices and combined to form the overall bases. We demonstrate that in several continuous Markov decision processes, compact basis functions can be constructed without significant loss in performance. In one domain, basis functions were compressed by a factor of . A theoretical analysis relates the quality of the approximation to the spectral gap. Our approach generalizes to other basis constructions as well.|Jeffrey Johns,Sridhar Mahadevan,Chang Wang","65353|AAAI|2005|Mechanism Design for Single-Value Domains|In \"Single-Value domains\", each agent has the same private value for all desired outcomes. We formalize this notion and give new examples for such domains. including a \"SAT domain\" and a \"single-value combinatorial auctions\" domain. We study two informational models where the set of desired outcomes is public information (the \"known\" case). and where it is private information (the \"unknown\" case). Under the \"known\" assumption, we present several truthful approximation mechanisms. Additionally, we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain (where agent values are either zero or one) to a truthful mechanism, with only a small approximation loss. In contrast, we show that even positive results from the \"unknown single minded combinatorial auctions\" literature fail to extend to the \"unknown\" single-value case. We give a characterization of truthfulness in this case, demonstrating that the difference is subtle and surprising.|Moshe Babaioff,Ron Lavi,Elan Pavlov","65362|AAAI|2005|Using SAT and Logic Programming to Design Polynomial-Time Algorithms for Planning in Non-Deterministic Domains|We show that a Horn SAT and logic programming approach to obtain polynomial time algorithms for problem solving can be fruitfully applied to finding plans for various kinds of goals in a non-deterministic domain. We particularly focus on finding weak, strong, and strong cyclic plans for planning problems, as they are the most studied ones in the literature. We describe new algorithms for these problems and show how non-monotonic logic programming can be used to declaratively compute strong cyclic plans. As a further benefit, preferred plans among alternative candidate plans may be singled out this way. We give complexity results for weak. strong, and strong cyclic planning. Finally, we briefly discuss some of the kinds of goals in non-deterministic domains for which the approach in the paper can be used.|Chitta Baral,Thomas Eiter,Jicheng Zhao","65964|AAAI|2007|Partial Revelation Automated Mechanism Design|In most mechanism design settings, optimal general-purpose mechanisms are not known. Thus the automated design of mechanisms tailored to specific instances of a decision scenario is an important problem. Existing techniques for automated mechanism design (AMD) require the revelation of full utility information from agents, which can be very difficult in practice. In this work, we study the automated design of mechanisms that only require partial revelation of utilities. Each agent's type space is partitioned into a finite set of partial types, and agents (should) report the partial type within which their full type lies. We provide a set of optimization routines that can be combined to address the trade-offs between the amount of communication, approximation of incentive properties, and objective value achieved by a mechanism. This allows for the automated design of partial revelation mechanisms with worst-case guarantees on incentive properties for any objective function (revenue, social welfare, etc.).|Nathanael Hyafil,Craig Boutilier","66212|AAAI|2007|Anytime Coordination Using Separable Bilinear Programs|Developing scalable coordination algorithms for multi-agent systems is a hard computational challenge. One useful approach, demonstrated by the Coverage Set Algorithm (CSA), exploits structured interaction to produce significant computational gains. Empirically, CSA exhibits very good anytime performance, but an error bound on the results has not been established. We reformulate the algorithm and derive both online and offline error bounds for approximate solutions. Moreover, we propose an effective way to automatically reduce the complexity of the interaction. Our experiments show that this is a promising approach to solve a broad class of decentralized decision problems. The general formulation used by the algorithm makes it both easy to implement and widely applicable to a variety of other AI problems.|Marek Petrik,Shlomo Zilberstein","66211|AAAI|2007|An Ironing-Based Approach to Adaptive Online Mechanism Design in Single-Valued Domains|Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method (\"ironing\") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.|David C. Parkes,Quang Duong","66081|AAAI|2007|Discovering Multivariate Motifs using Subsequence Density Estimation and Greedy Mixture Learning|The problem of locating motifs in real-valued, multivariate time series data involves the discovery of sets of recurring patterns embedded in the time series. Each set is composed of several non-overlapping subsequences and constitutes a motif because all of the included subsequences are similar. The ability to automatically discover such motifs allows intelligent systems to form endogenously meaningful representations of their environment through unsupervised sensor analysis. In this paper, we formulate a unifying view of motif discovery as a problem of locating regions of high density in the space of all time series subsequences. Our approach is efficient (sub-quadratic in the length of the data), requires fewer user-specified parameters than previous methods, and naturally allows variable length motif occurrences and non-linear temporal warping. We evaluate the performance of our approach using four data sets from different domains including on-body inertial sensors and speech.|David Minnen,Charles Lee Isbell Jr.,Irfan A. Essa,Thad Starner","65499|AAAI|2005|Samuel Meets Amarel Automating Value Function Approximation Using Global State Space Analysis|Most work on value function approximation adheres to Samuel's original design agents learn a task-specific value function using parameter estimation, where the approximation architecture (e.g, polynomials) is specified by a human designer. This paper proposes a novel framework generalizing Samuel's paradigm using a coordinate-free approach to value function approximation. Agents learn both representations and value functions by constructing geometrically customized task-independent basis functions that form an orthonormal set for the Hilbert space of smooth functions on the underlying state space manifold. The approach rests on a technical result showing that the space of smooth functions on a (compact) Riemanian manifold has a discrete spectrum associated with the Laplace-Beltrami operator. In the discrete setting, spectral analysis of the graph Laplacian yields a set of geometrically customized basis functions for approximating and decomposing value functions. The proposed framework generalizes Samuel's value function approximation paradigm by combining it with a formalization of Saul Amarel's paradigm of representation learning through global state space analysis.|Sridhar Mahadevan","66156|AAAI|2007|Counting CSP Solutions Using Generalized XOR Constraints|We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.|Carla P. Gomes,Willem Jan van Hoeve,Ashish Sabharwal,Bart Selman"],["80534|VLDB|2005|BATON A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.|H. V. Jagadish,Beng Chin Ooi,Quang Hieu Vu","65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","66259|AAAI|2007|SUNNY A New Algorithm for Trust Inference in Social Networks Using Probabilistic Confidence Models|In many computing systems, information is produced and processed by many people. Knowing how much a user trusts a source can be very useful for aggregating, filtering, and ordering of information. Furthermore, if trust is used to support decision making, it is important to have an accurate estimate of trust when it is not directly available, as well as a measure of confidence in that estimate. This paper describes a new approach that gives an explicit probabilistic interpretation for confidence in social networks. We describe SUNNY, a new trust inference algorithm that uses a probabilistic sampling technique to estimate our confidence in the trust information from some designated sources. SUNNY computes an estimate of trust based on only those information sources with high confidence estimates. In our experiments, SUNNY produced more accurate trust estimates than the well known trust inference algorithm TIDALTRUST (Golbeck ), demonstrating its effectiveness.|Ugur Kuter,Jennifer Golbeck","65576|AAAI|2005|Discriminative Training of Markov Logic Networks|Many machine learning applications require a combination of probability and first-order logic. Markov logic networks (MLNs) accomplish this by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Model parameters (i.e., clause weights) can be learned by maximizing the likelihood of a relational database, but this can be quite costly and lead to suboptimal results for any given prediction task. In this paper we propose a discriminative approach to training MLNs, one which optimizes the conditional likelihood of the query predicates given the evidence ones, rather than the joint likelihood of all predicates. We extend Collins's () voted perceptron algorithm for HMMs to MLNs by replacing the Viterbi algorithm with a weighted satisfiability solver. Experiments on entity resolution and link prediction tasks show the advantages of this approach compared to generative MLN training, as well as compared to purely probabilistic and purely logical approaches.|Parag Singla,Pedro Domingos","65591|AAAI|2005|Approximate Inference of Bayesian Networks through Edge Deletion|In this paper, we introduce two new algorithms for approximate inference of Bayesian networks that use edge deletion techniques. The first reduces a network to its maximal weight spanning tree using the Kullback-Leibler information divergence as edge weights, and then runs Pearl's algorithm on the resulting tree for linear-time inference. The second algorithm deletes edges from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound, thus placing a polynomial time bound on inference. When tested for efficiency, these two algorithms perform up to , times faster than exact techniques. See www.cis.ksu.edujasresearch.html for more information.|Julie Thornton","66040|AAAI|2007|Generalized Evidence Pre-propagated Importance Sampling for Hybrid Bayesian Networks|In this paper, we first provide a new theoretical understanding of the Evidence Pre-propagated Importance Sampling algorithm (EPIS-BN) (Yuan & Druzdzel  b) and show that its importance function minimizes the KL-divergence between the function itself and the exact posterior probability distribution in Polytrees. We then generalize the method to deal with inference in general hybrid Bayesian networks consisting of deterministic equations and arbitrary probability distributions. Using a novel technique called soft arc reversal, the new algorithm can also handle evidential reasoning with observed deterministic variables.|Changhe Yuan,Marek J. Druzdzel","65363|AAAI|2005|Hybrid Possibilistic Networks|Possibilistic networks are important tools for dealing with uncertain pieces of information. For multiply-connected networks, it is well known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of standard propagation algorithm.|Salem Benferhat,Salma Smaoui","66181|AAAI|2007|Adaptive Traitor Tracing with Bayesian Networks|The practical success of broadcast encryption hinges on the ability to () revoke the access of compromised keys and () determine which keys have been compromised. In this work we focus on the latter, the so-called traitor tracing problem. We present an adaptive tracing algorithm that selects forensic tests according to the information gain criteria. The results of the tests refine an explicit, Bayesian model of our beliefs that certain keys are compromised. In choosing tests based on this criteria, we significantly reduce the number of tests, as compared to the state-of-the-art techniques, required to identify compromised keys. As part of the work we developed an efficient, distributable inference algorithm that is suitable for our application and also give an efficient heuristic for choosing the optimal test.|Philip Zigoris,Hongxia Jin","66097|AAAI|2007|Macroscopic Models of Clique Tree Growth for Bayesian Networks|In clique tree clustering, inference consists of propagation in a clique tree compiled from a Bayesian network. In this paper, we develop an analytical approach to characterizing clique tree growth as a function of increasing Bayesian network connectedness, specifically (i) the expected number of moral edges in their moral graphs or (ii) the ratio of the number of non-root nodes to the number of root nodes. In experiments, we systematically increase the connectivity of bipartite Bayesian networks, and find that clique tree size growth is well-approximated by Gompertz growth curves. This research improves the understanding of the scaling behavior of clique tree clustering, provides a foundation for benchmarking and developing improved BN inference algorithms, and presents an aid for analytical trade-off studies of tree clustering using growth curves.|Ole J. Mengshoel","66106|AAAI|2007|Unscented Message Passing for Arbitrary Continuous Variables in Bayesian Networks|Since Bayesian network (BN) was introduced in the field of artificial intelligence in s, a number of inference algorithms have been developed for probabilistic reasoning. However, when continuous variables are present in Bayesian networks, their dependence relationships could be nonlinear and their probability distributions could be arbitrary. So far no efficient inference algorithm could deal with this case except Monte Carlo simulation methods such as Likelihood Weighting. But with unlikely evidence, simulation methods could be very slow to converge. In this paper, we propose an efficient approximate inference algorithm called Unscented Message Passing (UMP-BN) for Bayesian networks with arbitrary continuous variables. UMP-BN combines unscented transformation - a deterministic sampling method, and Pearl's message passing algorithm to provide the estimates of the first two moments of the posterior distributions. We test this algorithm with several networks including the ones with nonlinear andor non-Gaussian variables. The numerical experiments show that UMP-BN converges very fast and produces promising results.|Wei Sun,Kuo-Chu Chang"],["80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","80503|VLDB|2005|Sketching Streams Through the Net Distributed Approximate Query Tracking|Emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Effective solutions have to be simultaneously spacetime efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and spacetime-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.|Graham Cormode,Minos N. Garofalakis","80805|VLDB|2007|Depth Estimation for Ranking Query Optimization|A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just \"enough\" of the input in order to generate just \"enough\" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.|Karl Schnaitter,Joshua Spiegel,Neoklis Polyzotis","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80767|VLDB|2007|Materialized Views in Probabilistic Databases for Information Exchange and Query Optimization|Views over probabilistic data contain correlations between tuples, and the current approach is to capture these correlations using explicit lineage. In this paper we propose an alternative approach to materializing probabilistic views, by giving conditions under which a view can be represented by a block-independent disjoint (BID) table. Not all views can be represented as BID tables and so we propose a novel partial representation that can represent all views but may not define a unique probability distribution. We then give conditions on when a query's value on a partial representation will be uniquely defined. We apply our theory to two applications query processing using views and information exchange using views. In query processing on probabilistic data, we can ignore the lineage and use materialized views to more efficiently answer queries. By contrast, if the view has explicit lineage, the query evaluation must reprocess the lineage to compute the query resulting in dramatically slower execution. The second application is information exchange when we do not wish to disclose the entire lineage, which otherwise may result in shipping the entire database. The paper contains several theoretical results that completely solve the problem of deciding whether a conjunctive view can be represented as a BID and whether a query on a partial representation is uniquely determined. We validate our approach experimentally showing that representable views exist in real and synthetic workloads and show over three magnitudes of improvement in query processing versus a lineage based approach.|Christopher Re,Dan Suciu","80743|VLDB|2007|SQLB A Query Allocation Framework for Autonomous Consumers and Providers|In large-scale distributed information systems, where participants are autonomous and have special interests for some queries, query allocation is a challenge. Much work in this context has focused on distributing queries among providers in a way that maximizes overall performance (typically throughput and response time). However, preserving the participants\" interests is also important. In this paper, we make two main contributions. First, we provide a model to define participants' perception of the system w.r.t. their interests and propose metrics to evaluate the quality of query allocation methods. This model facilitates the design and evaluation of new query allocation methods that take into account the participants' interests. Second, we propose a framework for query allocation called Satisfaction-based Query Load Balancing (SQLB). To be fair, SQLB dynamically trades consumers' interests for providers' interests. And it continuously adapts to changes in participants' interests and to the workload. We implemented SQLB and compared it, through experimentation, to two important baseline query allocation methods, namely Capacity based and Mariposa-like. The results demonstrate that SQLB yields high efficiency while satisfying the participants' interests and significantly outperforms the baseline methods.|Jorge-Arnulfo Quian√©-Ruiz,Philippe Lamarre,Patrick Valduriez","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","65977|AAAI|2007|Approximate Query Answering in Locally Closed Databases|The Closed-World Assumption (CWA) on databases expresses that an atom not in the database is false. A more appropriate assumption for databases that are sound but partially incomplete, is the Local Closed-World Assumption (LCWA), which is a local form of the CWA, expressing that the database is complete in a certain area, called the 'window of expertise'. Databases consisting of a standard database instance augmented with a collection of LCWA's are called locally closed databases. In this paper, we investigate the complexity of certain and possible query answering in such databases. As it tums out that these problems are intractlble, we develop efficient approximate methods to underestimate certain answers and overestimate possible answers. We prove that under certain conditions, our methods produce complete answers.|Alvaro Cort√©s-Calabuig,Marc Denecker,Ofer Arieli,Maurice Bruynooghe","80859|VLDB|2007|Query language support for incomplete information in the MayBMS system|MayBMS , , ,  is a data management system for incomplete information developed at Saarland University. Its main features are a simple and compact representation system for incomplete information and a language called I-SQL with explicit operations for handling uncertainty. MayBMS is currently an extension of PostgreSQL and manages both complete and incomplete data and evaluates I-SQL queries.|Lyublena Antova,Christoph Koch,Dan Olteanu"],["80544|VLDB|2005|StreamGlobe Processing and Sharing Data Streams in Grid-Based PP Infrastructures|Data stream processing is currently gaining importance due to the developments in novel application areas like e-science, e-health, and e-business (considering RFID, for example). Focusing on e-science, it can be observed that scientific experiments and observations in many fields, e. g., in physics and astronomy, create huge volumes of data which have to be interchanged and processed. With experimental and observational data coming in particular from sensors, online simulations, etc., the data has an inherently streaming nature. Furthermore, continuing advances will result in even higher data volumes, rendering storing all of the delivered data prior to processing increasingly impractical. Hence, in such e-science scenarios, processing and sharing of data streams will play a decisive role. It will enable new possibilities for researchers, since they will be able to subscribe to interesting data streams of other scientists without having to set up their own devices or experiments. This results in much better utilization of expensive equipment such as telescopes, satellites, etc. Further, processing and sharing data streams on-the-fly in the network helps to reduce network traffic and to avoid network congestion. Thus, even huge streams of data can be handled efficiently by removing unnecessary parts early on, e. g., by early filtering and aggregation, and by sharing previously generated data streams and processing results.|Richard Kuntschke,Bernhard Stegmaier,Alfons Kemper,Angelika Reiser","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80737|VLDB|2007|Staying FIT Efficient Load Shedding Techniques for Distributed Stream Processing|In distributed stream processing environments, large numbers of continuous queries are distributed onto multiple servers. When one or more of these servers become overloaded due to bursty data arrival, excessive load needs to be shed in order to preserve low latency for the query results. Because of the load dependencies among the servers, load shedding decisions on these servers must be well-coordinated to achieve end-to-end control on the output quality. In this paper, we model the distributed load shedding problem as a linear optimization problem, for which we propose two alternative solution approaches a solver-based centralized approach, and a distributed approach based on metadata aggregation and propagation, whose centralized implementation is also available. Both of our solutions are based on generating a series of load shedding plans in advance, to be used under certain input load conditions. We have implemented our techniques as part of the Borealis distributed stream processing system. We present experimental results from our prototype implementation showing the performance of these techniques under different input and query workloads.|Nesime Tatbul,Ugur √\u2021etintemel,Stanley B. Zdonik","80828|VLDB|2007|Processing Forecasting Queries|Forecasting future events based on historic data is useful in many domains like system management, adaptive query processing, environmental monitoring, and financial planning. We describe the Fa system where users and applications can pose declarative forecasting queries---both one-time queries and continuous queries---and get forecasts in real-time along with accuracy estimates. Fa supports efficient algorithms to generate execution plans automatically for forecasting queries from a novel plan space comprising operators for transforming data, learning statistical models from data, and doing inference using the learned models. In addition, Fa supports adaptive query-processing algorithms that adapt plans for continuous forecasting queries to the time-varying properties of input data streams. We report an extensive experimental evaluation of Fa using synthetic datasets, datasets collected on a testbed, and two real datasets from production settings. Our experiments give interesting insights on plans for forecasting queries, and demonstrate the effectiveness and scalability of our plan-selection algorithms.|Songyun Duan,Shivnath Babu","80734|VLDB|2007|Example-driven design of efficient record matching queries|Record matching is the task of identifying records that match the same real world entity. This is a problem of great significance for a variety of business intelligence applications. Implementations of record matching rely on exact as well as approximate string matching (e.g., edit distances) and use of external reference data sources. Record matching can be viewed as a query composed of a small set of primitive operators. However, formulating such record matching queries is difficult and depends on the specific application scenario. Specifically, the number of options both in terms of string matching operations as well as the choice of external sources can be daunting. In this paper, we exploit the availability of positive and negative examples to search through this space and suggest an initial record matching query. Such queries can be subsequently modified by the programmer as needed. We ensure that the record matching queries our approach produces are () efficient these queries can be run on large datasets by leveraging operations that are well-supported by RDBMSs, and () explainable the queries are easy to understand so that they may be modified by the programmer with relative ease. We demonstrate the effectiveness of our approach on several real-world datasets.|Surajit Chaudhuri,Bee-Chung Chen,Venkatesh Ganti,Raghav Kaushik","80553|VLDB|2005|From Region Encoding To Extended Dewey On Efficient Processing of XML Twig Pattern Matching|Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.|Jiaheng Lu,Tok Wang Ling,Chee Yong Chan,Ting Chen","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis","80769|VLDB|2007|Structured Materialized Views for XML Queries|The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad  prototype and we present a performance analysis.|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Yannis Papakonstantinou"]]}}