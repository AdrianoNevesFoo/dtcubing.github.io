{"abstract":{"entropy":6.727364715889393,"topics":["constraint satisfaction, artificial intelligence, constraint problem, heuristic search, satisfaction problem, solving problem, theorem proving, recent years, real world, theorem prover, search, search problem, widely used, constraint, search space, last years, arc consistency, spanning tree, received attention, local search","genetic algorithm, evolutionary algorithm, genetic programming, algorithm, particle swarm, optimization problem, presents algorithm, neural networks, optimization algorithm, evolutionary computation, presents approach, algorithm problem, estimation distribution, search algorithm, evolutionary optimization, optimization, swarm optimization, particle optimization, presents novel, multi-objective optimization","machine learning, learning, learning system, learning classifier, reinforcement learning, play role, classifier system, mobile robot, problem learning, data mining, consider problem, solving problem, autonomous agents, markov decision, planning problem, presents learning, scheduling problem, dimensionality reduction, partially observable, learning domain","natural language, system, describes system, expert system, knowledge base, language, knowledge representation, knowledge, describes, description logic, knowledge system, programming language, presents system, program, logic programming, knowledge acquisition, pattern recognition, natural processing, system based, language processing","theorem prover, spanning tree, situation calculus, consider problem, decision tree, effects actions, minimum spanning, difficult problem, general problem, general framework, predicate calculus, modal logic, form genetic, spanning problem, tree problem, theory problem, cooperative coevolutionary, tree, truth maintenance, logic formulas","artificial intelligence, recent years, last years, research area, recent research, received attention, becoming increasingly, research intelligence, time series, problem artificial, recent, last decades, research artificial, directed graph, important problem, horn clause, applications field, problem various, problem area, become popular","neural networks, paper algorithm, bayesian networks, networks, ant colony, crossover operators, algorithm networks, efficient algorithm, problem networks, bayesian algorithm, approach based, paper, model based, algorithm based, paper based, paper approach, algorithm solve, probabilistic model, colony aco, solve problem","presents approach, presents algorithm, presents novel, novel approach, presents, markov processes, approach based, presents problem, fitness function, presents based, approach problem, approach, presents model, work algorithm, presents genetic, fitness landscape, presents evolutionary, presents called, novel algorithm, introduce approach","play role, consider problem, markov decision, autonomous agents, solving problem, partially observable, combinatorial problem, agents, problem agents, combinatorial auctions, environment agents, problem considered, deals problem, complex problem, resources allocation, large number, play important, observable markov, partially markov, important role","planning problem, model-based diagnosis, planning plan, scheduling problem, planning, problem multiple, approach planning, planning domain, xcs system, presents planning, conceptual clustering, problem domain, problem consists, robot planning, challenging problem, decision uncertainty, planning task, traveling salesman, problem plan, important issue","question answering, multi-agent system, system designed, question system, system implemented, equality elimination, image objects, current system, dynamical system, automated reasoning, answer question, presents overview, program designed, describes designed, human system, automated verification, interpretation system, modeling system, tool system, understanding system","logic programming, description logic, system described, computer program, program, describes program, vision system, logic program, programming program, system presented, recognition objects, production system, computer system, program system, system logic, intelligent system, computer vision, computer, described, first-order logic"],"ranking":[["15032|IJCAI|1993|Exploiting Interchangeabilities in Constraint-Satisfaction Problems|Constraint satisfaction - a method for representing and solving many AI problems in a very elegant manner - is a well-studied research area of recent years. Freuder observed that some constraint satisfaction problems are fashioned so that certain domain values of constraint variables are interchangeable. The use of such knowledge can increase search efficiency drastically by reducing the problem. In this paper we carry on these considerations and give a formal foundation of interchangeabilities by the notion of domain partitions induced by equivalence relations. We show how these domain partitions can be used in a very accurate manner by the majority of existing constraint propagation algorithms and introduce a novel backtrack procedure exploiting such interchangeabilities of domain values. Both theoretical analysis and experiments indicate that our proposed approach is an improvement of Freuder's use of neighborhood interchangeability and has very good behavior for certain problem types.|Alois Haselböck","15491|IJCAI|1999|Solving Strategies for Highly Symmetric CSPs|Symmetry often appears in real-world constraint satisfaction problems, but strategies for exploiting it are only beginning to be developed. Here, a rationale for exploiting symmetry within depth-first search is proposed, leading to an heuristic for variable selection and a domain pruning procedure. These strategies are then applied to a highly symmetric combinatorial problem, namely the generation of balanced incomplete block designs. Experimental results show that these strategies achieve a reduction of up to two orders of magnitude in computational effort. Interestingly, two previously developed strategies are shown to be particular instances of this approach.|Pedro Meseguer,Carme Torras","14677|IJCAI|1989|A Comparison of ATMS and CSP Techniques|A fundamental problem for most AI problem solvers is how to control search to avoid searching subspaces which previously have been determined to be inconsistent. Two of the standard approaches to this problem are embodied in the constraint satisfaction problem (CSP) techniques which evolved from vision tasks and assumption-based truth maintenance system (ATMS) techniques which evolved from applying constraint propagation techniques to reasoning about the physical world. This paper argues that both approaches embody similar intuitions for avoiding thrashing and shows how CSPs can be mapped to ATMS problems and vice versa. In particular, Mackworth's notions of node, arc, and path consistency, Freuder's notion of it-consistency, and Dechter and Pearl's notion of directed K-consistency have precise analogs in the ATMS framework.|Johan de Kleer","15786|IJCAI|2003|Propagate the Right Thing How Preferences Can Speed-Up Constraint Solving|We present an algorithm Pref-AC that limits arc consistency (AC) to the preferred choices of a tree search procedure and that makes constraint solving more efficient without changing the pruning and shape of the search tree. Arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problems such as configuration and scheduling. Moreover, Pref-AC directly computes a preferred solution for treelike constraint satisfaction problems.|Christian Bessière,Anaïs Fabre,Ulrich Junker","15903|IJCAI|2003|In the quest of the best form of local consistency for Weighted CSP|The weighted CSP (WCSP) framework is a soft constraint framework with a wide range of applications. In this paper, we consider the problem of maintaining local consistency during search for solving WCSP. We first refine the notions of directional arc consistency (DAC) and full directional arc consistency (FDAC) introduced in Cooper,  for binary WCSP, define algorithms that enforce these properties and study their complexities. We then consider algorithms that maintain either arc consistency (AC), DAC or FDAC during search. The efficiency of these algorithms is empirically studied. It appears that despite its high theoretical cost, the strongest FDAC property is the best choice.|Javier Larrosa,Thomas Schiex","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan","16361|IJCAI|2007|Arc Consistency during Search|Enforcing arc consistency (AC) during search has proven to be a very effective method in solving Constraint Satisfaction Problems and it has been widely-used in many Constraint Programming systems. Although much effort has been made to design efficient standalone AC algorithms, there is no systematic study on how to efficiently enforce AC during search, as far as we know. The significance of the latter is clear given the fact that AC will be enforced millions of times in solving hard problems. In this paper, we propose a framework for enforcing AC during search (ACS) and complexity measurements of ACS algorithms. Based on this framework, several ACS algorithms are designed to take advantage of the residual data left in the data structures by the previous invocation(s) of ACS. The algorithms vary in the worst-case time and space complexity and other complexity measurements. Empirical study shows that some of the new ACS algorithms perform better than the conventional implementation of AC algorithms in a search procedure.|Chavalit Likitvivatanavong,Yuanlin Zhang,Scott Shannon,James Bowen,Eugene C. Freuder","16468|IJCAI|2007|Probabilistic Consistency Boosts MAC and SAC|Constraint Satisfaction Problems (CSPs) are ubiquitous in Artificial Intelligence. The backtrack algorithms that maintain some local consistency during search have become the de facto standard to solve CSPs. Maintaining higher levels of consistency, generally, reduces the search effort. However, due to ineffective constraint propagation, it often penalises the search algorithm in terms of time. If we can reduce ineffective constraint propagation, then the effectiveness of a search algorithm can be enhanced significantly. In order to do so, we use a probabilistic approach to resolve when to propagate and when not to. The idea is to perform only the useful consistency checking by not seeking a support when there is a high probability that a support exists. The idea of probabilistic support inference is general and can be applied to any kind of local consistency algorithm. However, we shall study its impact with respect to arc consistency and singleton arc consistency (SAC). Experimental results demonstrate that enforcing probabilistic SAC almost always enforces SAC, but it requires significantly less time than SAC. Likewise, maintaining probabilistic arc consistency and maintaining probabilistic SAC require significantly less time than maintaining arc consistency and maintaining SAC.|Deepak Mehta,Marc R. C. van Dongen","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder"],["57331|GECCO|2005|An efficient evolutionary algorithm applied to the design of two-dimensional IIR filters|This paper presents an efficient technique of designing two-dimensional IIR digital filters using a new algorithm involving the tightly coupled synergism of particle swarm optimization and differential evolution. The design task is reformulated as a constrained minimization problem and is solved by our newly developed PSO-DV (Particle Swarm Optimizer with Differentially perturbed Velocity) algorithm. Numerical results are presented. The paper also demonstrates the superiority of the proposed design method by comparing it with two recently published filter design methods.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57240|GECCO|2003|Optimization Using Particle Swarms with Near Neighbor Interactions|This paper presents a modification of the particle swarm optimization algorithm (PSO) intended to combat the problem of premature convergence observed in many applications of PSO. In the new algorithm, each particle is attracted towards the best previous positions visited by its neighbors, in addition to the other aspects of particle dynamics in PSO. This is accomplished by using the ratio of the relative fitness and the distance of other particles to determine the direction in which each component of the particle position needs to be changed. The resulting algorithm, known as Fitness-Distance-Ratio based PSO (FDR-PSO), is shown to perform significantly better than the original PSO algorithm and several of its variants, on many different benchmark optimization problems. Avoiding premature convergence allows FDR-PSO to continue search for global optima in difficult multimodal optimization problems, reaching better solutions than PSO and several of its variants.|Kalyan Veeramachaneni,Thanmaya Peram,Chilukuri K. Mohan,Lisa Ann Osadciw","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58430|GECCO|2008|A new quantum behaved particle swarm optimization|This paper presents a variant of Quantum behaved Particle Swarm Optimization (QPSO) named Q-QPSO for solving global optimization problems. The Q-QPSO algorithm is based on the characteristics of QPSO, and uses interpolation based recombination operator for generating a new solution vector in the search space. The performance of Q-QPSO is compared with Basic Particle Swarm Optimization (BPSO), QPSO and two other variants of QPSO taken from literature on six standard unconstrained, scalable benchmark problems. The experimental results show that the proposed algorithm outperforms the other algorithms quite significantly.|Millie Pant,Radha Thangaraj,Ajith Abraham","58657|GECCO|2009|Particle swarm optimization with oscillation control|Particle Swarm Optimization (PSO) is a metaheuristic that has been successfully applied to linear and non-linear optimization problems in functions with discrete and continuous domains. This paper presents a new variation of this algorithm - called oscPSO - that improves the inherent search capacity of the original (canonical) version of the PSO algorithm. This version uses a deterministic local search method whose use depends on the movement patterns of the particles in each dimension of the problem. The method proposed was assessed by means of a set of complex test functions, and the performance of this version was compared with that of the original version of the PSO algorithm. In all cases, the oscPSO variation equaled or surpassed the performance of the canonical version of the algorithm.|Javier H. López,Laura Lanzarini,Armando De Giusti","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule","58990|GECCO|2010|Biogeography-based optimization with blended migration for constrained optimization problems|Biogeography-based optimization (BBO) is a new evolutionary algorithm based on the science of biogeography. We propose two extensions to BBO. First, we propose blended migration. Second, we modify BBO to solve constrained optimization problems. The constrained BBO algorithm is compared with solutions based on a genetic algorithm (GA) and particle swarm optimization (PSO). Numerical results indicate that BBO generally performs better than GA and PSO in handling constrained single-objective optimization problems.|Haiping Ma,Dan Simon","58942|GECCO|2010|A robust estimation of distribution algorithm for power electronic circuits design|The automated synthesis and optimization of power electronic circuits (PECs) is a significant and challenging task in the field of power electronics. Traditional methods such as the gradient-based methods, the hill-climbing techniques and the genetic algorithms (GA), are either prone to local optima or not efficient enough to find highly accurate solutions for this problem. To better optimize the design of PECs, this paper presents an extended histogram-based estimation of distribution algorithm with an adaptive refinement process (EDAa-r). In the EDAa-r, the histogram-based estimation of distribution algorithm is used to roughly locate the global optimum, while the adaptive refinement process is used to improve the accuracy of solutions. The adaptive refinement process, with its search radius adjusted adaptively during the evolution, is executed to search the surrounding region of the best-so-far solution in every generation. To maintain the diversity, a historic learning strategy is used in constructing the probabilistic model and a mutation strategy is hybridized in the sampling operation. The proposed EDAa-r has been successfully used to optimize the design of a buck regulator. Experimental results show that compared with the GA and the particle swarm optimization (PSO), the EDAa-r can obtain much better mean solution quality and is less likely to be trapped into local optima.|Jinghui Zhong,Jun Zhang"],["16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","14467|IJCAI|1987|Combining Weak Learning Heuristics in General Problem Solvers|This paper is concerned with state space problem solvers that achieve generality by learning strong heuristics through experience in a particular domain. We specifically consider two ways of learning by analysing past solutions that can improve future problem solving creating macros and the chunks. A method of learning search heuristics is specified which is related to 'chunking' but which complements the use of macros within a goal directed system. An example of the creation and combined use of macros and chunks, taken from an implemented system, is described.|T. L. McCluskey","16879|IJCAI|2009|Predictive Projections|This paper addresses the problem of learning control policies in very high dimensional state spaces. We propose a linear dimensionality reduction algorithm that discovers predictive projections projections in which accurate predictions of future states can be made using simple nearest neighbor style learning. The goal of this work is to extend the reach of existing reinforcement learning algorithms to domains where they would otherwise be inapplicable without extensive engineering of features. The approach is demonstrated on a synthetic pendulum balancing domain, as well as on a robot domain requiring visually guided control.|Nathan Sprague","17086|IJCAI|2009|Autonomously Learning an Action Hierarchy Using a Learned Qualitative State Representation|There has been intense interest in hierarchical reinforcement learning as a way to make Markov decision process planning more tractable, but there has been relatively little work on autonomously learning the hierarchy, especially in continuous domains. In this paper we present a method for learning a hierarchy of actions in a continuous environment. Our approach is to learn a qualitative representation of the continuous environment and then to define actions to reach qualitative states. Our method learns one or more options to perform each action. Each option is learned by first learning a dynamic Bayesian network (DBN). We approach this problem from a developmental robotics perspective. The agent receives no extrinsic reward and has no external direction for what to learn. We evaluate our work using a simulation with realistic physics that consists of a robot playing with blocks at a table.|Jonathan Mugan,Benjamin Kuipers","15217|IJCAI|1995|Determining What to Learn Through Component-Task Modeling|Research in machine learning has typically addressed the problem of how and when to learn, and ignored the problem of formulating learning tasks in the first place. This paper addresses this issue in the context of the CASTLE system, that dynamically formulates learning tasks for a given situation. Our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. CASTLE can then focus the learning process on the issues involved in improving the performance of the particular component.|Bruce Krulwich,Lawrence Birnbaum,Gregg Collins","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf","15204|IJCAI|1995|A Model for Hormonal Modulation of Learning|Recent studies in neurobiology have discovered that many hormones exist in the brain, and play key roles in learning and memorization. In this paper, we discuss the possible role of hormones in learning, and propose a new learning model which incorporates hormonal effects on learning. The model is a variant of reinforcement learning with modulation on learning rate and the frequency of mental rehearsal. The modulation enables the system to focus its learning on data which are evaluated as important for the system's overall performance. The experiment demonstrates that the incorporation of hormonal modulation improves behavior learning performance, and such an evaluation network can be acquired through the evolutionary mechanism.|Hiroaki Kitano"],["13365|IJCAI|1973|A Gobal View of Automatic Programming|This paper presents a framework for characterizing automatic programming systems In terms of how a task Is communicated to the system, the method and time at which the system acquires the knowledge to perform the task, and the characteristics of the resulting program to perform that task. It describes one approach In which both tasks and knowledge about the task domain are stated in natural language In the terms of that domain. All knowledge of computer science necessary to Implement the task Is Internalized Inside the system.|Robert Balzer","14014|IJCAI|1983|Towards Knowledge Acquisition From Natural Language Documents - Automatic Model Construction From Hardware Manual|In this paper, we explore automatic model construction by analyzing natural language documents. The extracted model will be utilized by a CAD system. A system called hmU, in the course of development, is designed to allow knowledge on very complex hardware module like LSI or VLSI to be incorporated into its knowledge base. The acquired knowledge will be utilized for helping human designer understand the component from various levels of abstraction. The focus of this paper is attentioned more to issues on knowledge representation and model inference than that on natural language analysis. Hierarchical model is employed. In particular, cause-effect representation is used to make it clear how actions of each module and events are related to each other. A brief description is given to illustrate our approach.|Toyoaki Nishida,Akira Kosaka,Shuji Doshita","14929|IJCAI|1991|Representing Diagnostic Knowledge for Probabilistic Horn Abduction|This paper presents a simple logical framework for abduction, with probabilities associated with hypotheses. The language is an extension to pure Prolog, and it has straight-forward implementations using branch and bound search with either logic-programming technology or ATMS technology. The main focus of this paper is arguing for a form of representational adequacy of this very simple system for diagnostic reasoning. It is shown how it can represent model-based knowledge, with and without faults, and with and without nonintermittency assumptions. It is also shown how this representation can represent any probabilistic knowledge representable in a Bayesian belief network.|David Poole","14443|IJCAI|1987|Amalgamating Multiple Programming Paradigms in PROLOG|This paper discusses the issues in amalgamating multiple programming paradigms in the logic programming language, Prolog. It is shown that multiple paradigms can be incorporated without disturbing logic programming language features and efficiency. It also introduces a new programming paradigm called the relation-oriented paradigm. The research results are reflected in the implementation of the Prolog-based knowledge programming system PEACE, which is used to realize an expert system in a diagnostic domain. PEACE provides a relation-oriented programming paradigm, as well as previously discussed paradigms, such as object-oriented, data-oriented, and rule-oriented paradigms. These paradigms are nicely amalgamated in Prolog language and can be used intermixedly.|Yoshiyuki Koseki","13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","13862|IJCAI|1981|A Knowledge-Based Approach to Language Processing A Progress Report|We present a model of natural language use meant to encompass the language-specific aspects of understanding and production. The model is motivated by the pervasiveness of nongenerative language, by the desirability of a language analyzer ana a language production mechanism to share their knowledge, and the advantages of knowledge engineering features such as ease of extention and modification. This model has been used as the basis for PHRAN, a language analyzer, and PHRED, a language production mechanism. We have implemented both these systems using a common knowledge base we have produced versions of PHRAN that understand Spanish and Chinese with only changing the knowledge base and not modifying the program and we have implemented PHRAN using the query language of a conventional relational data base system, and compared the performance of this system to a conventional LISP implementation.|Robert Wilensky","15058|IJCAI|1993|Using Classification as a Programming Language|Our experience in the IDAS natural language generation project has shown us that IDAS'S KLONE-like classifier, originally built solely to hold a domain knowledge base, could also be used to perform many of the computations required by a natural-language generation system in fact it seems possible to use the classifier to encode and execute arbitrary programs. We discuss IDAS'S classification system and how it differs from other such systems (perhaps most notably in the presence of template' constructs that enable recursion to be encoded) give examples of program fragments encoded in the classification system and compare the classification approach to other AI programming paradigms (e.g., logic programming).|Chris Mellish,Ehud Reiter","14703|IJCAI|1989|Approximation of Indistinct Concepts|This theory on semi-equivalence relations is an important and useful tool for investigating classification, pattern recognition, polling and inference etc. Based on it, this paper presents a new framework, in which an indistinct concept, that is one with incomplete or undetermined information about the objects, can be represented approximately. Such an approximate representation will reflect deep structures of concepts which are meaningful for the system. Clearly, the work we present here is to a great extent inspired by general discussions of knowledge engineering research. The theory developed here seems to be of interest in knowledge representation and natural language processing. From the implementation point of view, this theory can be realized by various AI techniques.|Zhang Mingyi","14140|IJCAI|1985|SAPHIR  RESEDA A New Approach to Intelligent Data Base Access|This paper describes a transportable natural language interface to databases, augmented with a knowledge base and inference techniques. The inference mechanism, based on a classical expert system's type of approach, allows, when needed, to automatically convert an Input query into another one which is \"semantically close\". According to RESEDAS theory, \"semantically close\" means that the answer to the transformed query Implies what could have been the answer to the original question. The presented system Integrates natural language processing, expert system and knowledge representation technology to provide a cooperative database access.|Bernard Euzenat,Bernard Normier,Antoine Ogonowski,Gian Piero Zarri"],["58348|GECCO|2008|Greedy heuristics and evolutionary algorithms for the bounded minimum-label spanning tree problem|Given an edge-labeled, connected, undirected graph G and a bound r, the bounded minimum-label spanning tree problem seeks a spanning tree on G whose edges carry the fewest possible labels and in which no label appears more than r times. Two greedy heuristics for the unbounded version of the problem are adapted to the bounded version. Two genetic algorithms for the problem encode labeled spanning trees as permutations of G's edges. A simple GA performs poorly, but the addition of local search enables consistently good results.|Arindam Khaled,Bryant A. Julstrom","58174|GECCO|2007|On quality performance of heuristic and evolutionary algorithms for biobjective minimum spanning trees|In this paper, we consider a biobjective minimum spanning tree problem (MOST) and minimize two objectives - tree cost and diameter - in terms of Pareto-optimality. We assess the quality of obtained MOEA solutions in comparison to well-known diameter-constrained minimum spanning tree (dc-MST) algorithms and further improve MOEA solutions using problem-specific knowledge.|Rajeev Kumar,Pramod Kumar Singh","58677|GECCO|2009|Multiobjective genetic programming approach to evolving heuristics for the bounded diameter minimum spanning tree problem MOGP for BDMST|The bounded-diameter (or diameter-constrained) minimum spanning tree (BDMST) problem is a well-studied combinatorial optimization problem for which several heuristics such as one-time tree construction (OTTC), center based tree construction(CBTC), iterative refinement (IR) and randomized greedy heuristics (RGH) have been developed. Very little work, however, has been done on producing heuristics for BDMSTs using evolutionary algorithms. In this paper we have used multiobjective genetic programming (MOGP) to explore the evolution of a set of heuristics for the BDMST problem. The quality of the Pareto fronts obtained from the MOGP-evolved heuristics is comparable to, or in some cases better than, the Pareto fronts generated by established, hand-crafted heuristics. MOGP is thus a very promising technique for finding heuristics for BDMSTs and more general multiobjective combinatorial optimizations.|Rajeev Kumar,Bipul Kumar Bal,Peter Rockett","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","58368|GECCO|2008|Orientation matters how to efficiently solve ocst problems with problem-specific EAs|The optimal communication spanning tree (OCST) problem is a well known $mathcalNP$-hard combinatorial optimization problem which seeks a spanning tree that satisfies all given communication requirements for minimal total costs. It has been shown that optimal solutions of OCST problems are biased towards the much simpler minimum spanning tree (MST) problem. Therefore, problem-specific representations for EAs like heuristic variants of edge-sets that are biased towards MSTs show high performance. In this paper, additional properties of optimal solutions for Euclidean variants of OCST problems are studied. Experimental results show that not only edges in optimal trees are biased towards low-distance weights but also edges which are directed towards the graph's center are overrepresented in optimal solutions. Therefore, efficient heuristic search algorithms for OCST should be biased towards edges with low distance weight emphand edges that point towards the center of the graph. Consequently, we extend the recombination operator of edge-sets such that the orientation of the edges is considered for constructing offspring solutions. Experimental results show a higher search performance in comparison to EAs using existing crossover strategies of edge-sets. As a result, we suggest to consider not only the distance weights but also the orientation of edges in heuristic solution approaches for the OCST problem.|Wolfgang Steitz,Franz Rothlauf","58747|GECCO|2009|New heuristic and hybrid genetic algorithm for solving the bounded diameter minimum spanning tree problem|In this paper, we propose a new heuristic, called Center-Based Recursive Clustering - CBRC, for solving the bounded diameter minimum spanning tree (BDMST) problem. Our proposed hybrid genetic algorithm  is also extended to include the new heuristic and a multi-parent crossover operator. We test the new heuristic and genetic algorithm on two sets of benchmark problem instances for the Euclidean and Non-Euclidean cases. Experimental results show the effectiveness of the proposed heuristic and genetic algorithm.|Huynh Thi Thanh Binh,Robert I. McKay,Nguyen Xuan Hoai,Nguyen Duc Nghia","58664|GECCO|2009|Exploiting hierarchical clustering for finding bounded diameter minimum spanning trees on euclidean instances|The bounded diameter minimum spanning tree problem is an NP-hard combinatorial optimization problem arising, for example, in network design when quality of service is of concern. There exist various exact and metaheuristic approaches addressing this problem, whereas fast construction heuristics are primarily based on Prim's minimum spanning tree algorithm and fail to produce reasonable solutions in particular on large Euclidean instances. A method based on hierarchical clustering to guide the construction process of a diameter constrained tree is presented. Solutions obtained are further refined using a greedy randomized adaptive search procedure. Based on the idea of clustering we also designed a new neighborhood search for this problem. Especially on large Euclidean instances with a tight diameter bound the results are excellent. In this case the solution quality can also compete with that of a leading metaheuristic, whereas the computation only needs a fraction of the time.|Martin Gruber,Günther R. Raidl","14027|IJCAI|1983|Characterizing Search Spaces|A heuristic is good on a search space to the extent that it allows the prediction of which states are near to the goal. In this paper heuristics are investigated on several different search spaces. A measure is proposed for assessing the predictive accuracy of a given heuristic on a given search space. The measure sheds light on characteristics of the Traveling Salesman Problem that make it computationally more difficult to solve than the Minimum Spanning-Tree Problem.|Roy Rada","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","57390|GECCO|2005|The blob code is competitive with edge-sets in genetic algorithms for the minimum routing cost spanning tree problem|Among the many codings of spanning trees for evolutionary search are those based on bijections between Prfer strings---strings of n- vertex labels---and spanning trees on the labeled vertices. One of these bijections, called the Blob Code, showed promise as an evolutionary coding, but EAs that use it to represent spanning trees have not performed well. Here, a genetic algorithm that represents spanning trees via the Blob Code is faster than, and returns results competitive with those of, a GA that encodes spanning trees as edge-sets on Euclidean instances of the minimum routing cost spanning tree problem. On instances whose edge weights have been chosen at random, the Blob-coded GA maintains its time advantage, but its results are inferior to those of the edge-set-coded GA, and both GAs are hard pressed to keep up with a simple stochastic hill-climber on all the test instances.|Bryant A. Julstrom"],["16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","13908|IJCAI|1983|The Use of Meta-Level Control for Coordination in a Distributed Problem Solving Network|Distributed problem solving networks provide an interesting application area for meta-level control through the use of organizational structuring. We describe a decentralized approach to network coordination that relies on each node making sophisticated local decisions that balance its own perceptions of appropriate problem solving activity with activities deemed important by other nodes. Each node is guided by a high-level strategic plan for cooperation among the nodes in the network. The high-level strategic plan, which is a form of meta-level control, is represented as a network organizational structure that specifies in a general way the information and control relationships among the nodes. An implementation of these ideas is briefly described along with the results of preliminary experiments with various network problem solving strategies specified via organizational structuring. In addition to its application to Distributed Artificial Intelligence, this research has implications for organizing and controlling complex knowledge-based systems that involve semi-autonomous problem solving agents.|Daniel D. Corkill,Victor R. Lesser","13386|IJCAI|1973|Forecasting and Assessing the Impact of Artificial Intelligence on Society|At the present stage of research in artificial intelligence, machines are still remote from achieving a level of intelligence comparable in complexity to human thought. As computer applications become more sophisticated, however, and thus more influential in human affairs, it becomes increasingly important to understand both the capabilities and limitations of machine Intelligence and its potential impact on society. To this end, the artificial intelligence field was examined in a systematic manner. The study was divided into two parts () Delineation of areas of artificial intelli gence, and postulatio\" of hypothetical products resulting from progress in the field, and () A judgmental portion, which involved applications and implications of the products to society. For the latter purpose, a Delphi study was conducted among experts in the artificial intelligence field to solicit their opinion concerning prototype and commercial dates for the products, and the possibility and desirability of their applications and implications.|Oscar Firschein,Martin A. Fischler,L. Stephen Coles,Jay M. Tenenbaum","13663|IJCAI|1977|Artificial Intelligence Systems That Understand|From its beginnings, artificial intelligence has borrowed freely from the vocabulary of psychology. The use of the word \"intelligence\" to label our area of research is a case in point. Other terms referring originally to human mental processes that have consider able currency in AI are \"thinking,\" \"comprehending,\" and, with increasing frequency in the past five years, \"understanding.\" Infact, these terms are probably used more freely in AI than in experimental psychology, where a deep suspicion of \"mentalistic\" terminology still lingers as a heritage of behaviorism.|Herbert A. Simon","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","15640|IJCAI|2001|Backjumping for Quantified Boolean Logic Satisfiability|The implementation of effective reasoning tools for deciding the satisfiability of Quantified Boolean Formulas (QBFs) is an important research issue in Artificial Intelligence. Many decision procedures have been proposed in the last few years, most of them based on the Davis, Logemann, Loveland procedure (DLL) for propositional satisfiability (SAT). In this paper we show how it is possible to extend the conflict-directed backjumping schema for SAT to QBF when applicable, it allows to jump over existentially quantified literals while backtracking. We introduce solution-directed backjumping, which allows the same for universally quantified literals. Then, we show how it is possible to incorporate both conflict-directed and solution-directed backjumping in a DLL-based decision procedure for QBF satisfiability. We also implement and test the procedure The experimental analysis shows that, because of backjumping, significant speed-ups can be obtained. While there have been several proposals for backjumping in SAT, this is the first time - as far as we know - this idea has been proposed, implemented and experimented for QBFs.|Enrico Giunchiglia,Massimo Narizzano,Armando Tacchella","14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue","14891|IJCAI|1991|Commitment and Effectiveness of Situated Agents|Recent research in real-time Artificial Intelligence has focussed upon the design of situated agents and, in particular, how to achieve effective and robust behaviour with limited computational resources. A range of architectures and design principles has been proposed to solve this problem. This has led to the development of simulated worlds that can serve as testbeds in which the effectiveness of different agents can be evaluated. We report here an experimental program that aimed to investigate how commitment to goals contributes to effective behaviour and to compare the properties of different strategies for reacting to change. Our results demonstrate the feasibility of developing systems for empirical measurement of agent performance that are stable, sensitive, and capable of revealing the effect of \"high-level\" agent characteristics such as commitment.|David Kinny,Michael P. Georgeff","57465|GECCO|2005|The application of antigenic search techniques to time series forecasting|Time series have been a major topic of interest and analysis for hundreds of years, with forecasting a central problem. A large body of analysis techniques has been developed, particularly from methods in statistics and signal processing. Evolutionary techniques have only recently have been applied to time series problems. To date, applications of artificial immune system (AIS) techniques have been in the area of anomaly detection. In this paper we apply AIS techniques to the forecasting problem. We characterize a class of search algorithms we call antigenic search and show their ability to give a good forecast of next elements in series generated from Mackey-Glass and Lorenz equations.|Ian Nunn,Tony White","14826|IJCAI|1991|Intelligence Without Reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.|Rodney A. Brooks"],["15521|IJCAI|1999|PEBM A Probabilistic Exemplar Based Model|A central problem in case based reasoning (CBR) is how to store and retrieve cases. One approach to this problem is to use exemplar based models, where only the prototypical cases are stored. However, the development of an exemplar based model (EBM) requires the solution of several problems (i) how can a EBM be represented (ii) given a new case, how can a suitable exemplar be retrieved (iii) what makes a good exemplar (iv) how can an EBM be learned incrementally This paper develops a new model, called a probabilistic exemplar based model, that addresses these questions. The model utilizes Bayesian networks to develop a suitable representation and uses probabilistic propagation for assessing and retrieving exemplars when a new case is presented. The model learns incrementally by revising the exemplars retained and by updating the conditional probabilities required by the Bayesian network. The paper also presents the results of evaluating the model on three datasets.|Andres F. Rodriguez,Sunil Vadera","58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","58865|GECCO|2009|On the evolution of neural networks for pairwise classification using gene expression programming|Neural networks are a common choice for solving classification problems, but require experimental adjustments of the topology, weights and thresholds to be effective. Success has been seen in the development of neural networks with evolutionary algorithms, making the extension of this work to classification problems a logical step. This paper presents the first known use of the Gene Expression Programming-based GEP-NN algorithm to design neural networks for classification purposes. The system uses pairwise decomposition to produce a series of binary classifiers for a given multi-class problem, with the results of the classifier set being combined by majority vote.|Stephen Johns,Marcus V. dos Santos","57234|GECCO|2003|Spatial Operators for Evolving Dynamic Bayesian Networks from Spatio-temporal Data|Learning Bayesian networks from data has been studied extensively in the evolutionary algorithm communities Larranaga, Wong. We have previously explored extending some of these search methods to temporal Bayesian networks Tucker. A characteristic of many datasets from medical to geographical data is the spatial arrangement of variables. In this paper we investigate a set of operators that have been designed to exploit the spatial nature of such data in order to learn dynamic Bayesian networks more efficiently. We test these operators on synthetic data generated from a Gaussian network where the architecture is based upon a Cartesian coordinate system, and real-world medical data taken from visual field tests of patients suffering from ocular hypertension.|Allan Tucker,Xiaohui Liu,David Garway-Heath","14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine","15772|IJCAI|2003|Optimal Time-Space Tradeoff in Probabilistic Inference|Recursive Conditioning, RC, is an any-space algorithm for exact inference in Bayesian networks, which can trade space for time in increments of the size of a floating point number. This smooth trade-off is possible by varying the algorithm's cache size. When RC is run with a constrained cache size, an important problem arises Which specific results should be cached in order to minimize the running time of the algorithm RC is driven by a structure known as a dtree, and many such dtrees exist for a given Bayesian network. In this paper, we examine the problem of searching for an optimal caching scheme for a given dtree, and present some optimal time-space tradeoff curves for given dtrees of several published Bayesian networks. We also compare these curves to the memory requirements of state-of-the-art algorithms based on join-trees. Our results show that the memory requirements of these networks can be significantly reduced with only a minimal cost in time, allowing for exact inference in situations previously impractical. They also show that probabilistic reasoning systems can be efficiently designed to run under varying amounts of memory.|David Allen,Adnan Darwiche","58494|GECCO|2008|iBOA the incremental bayesian optimization algorithm|This paper proposes the incremental Bayesian optimization algorithm (iBOA), which modifies standard BOA by removing the population of solutions and using incremental updates of the Bayesian network. iBOA is shown to be able to learn and exploit unrestricted Bayesian networks using incremental techniques for updating both the structure as well as the parameters of the probabilistic model. This represents an important step toward the design of competent incremental estimation of distribution algorithms that can solve difficult nearly decomposable problems scalably and reliably.|Martin Pelikan,Kumara Sastry,David E. Goldberg","58312|GECCO|2008|Scaling ant colony optimization with hierarchical reinforcement learning partitioning|This paper merges hierarchical reinforcement learning (HRL) with ant colony optimization (ACO) to produce a HRL ACO algorithm capable of generating solutions for large domains. This paper describes two specific implementations of the new algorithm the first a modification to Dietterich's MAXQ-Q HRL algorithm, the second a hierarchical ant colony system algorithm. These implementations generate faster results, with little to no significant change in the quality of solutions for the tested problem domains. The application of ACO to the MAXQ-Q algorithm replaces the reinforcement learning, Q-learning, with the modified ant colony optimization method, Ant-Q. This algorithm, MAXQ-AntQ, converges to solutions not significantly different from MAXQ-Q in % of the time. This paper then transfers HRL techniques to the ACO domain and traveling salesman problem (TSP). To apply HRL to ACO, a hierarchy must be created for the TSP. A data clustering algorithm creates these subtasks, with an ACO algorithm to solve the individual and complete problems. This paper tests two clustering algorithms, k-means and G-means. The results demonstrate the algorithm with data clustering produces solutions  times faster with -% decrease in solution quality due to the effects of clustering.|Erik J. Dries,Gilbert L. Peterson","15762|IJCAI|2001|Genetic Algorithm based Selective Neural Network Ensemble|Neural network ensemble is a learning paradigm where several neural networks are jointly used to solve a problem. In this paper, the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural networks is analyzed, which reveals that ensembling a selective subset of individual networks is superior to ensembling all the individual networks in some cases. Therefore an approach named GASEN is proposed, which trains several individual neural networks and then employs genetic algorithm to select an optimum subset of individual networks to constitute an ensemble. Experimental results show that, comparing with a popular ensemble approach, i.e. averaging all, and a theoretically optimum selective ensemble approach, i.e. enumerating, GASEN has preferable performance in generating ensembles with strong generalization ability in relatively small computational cost.|Zhi-Hua Zhou,Jianxin Wu,Yuan Jiang,Shifu Chen","57046|GECCO|2003|Pruning Neural Networks with Distribution Estimation Algorithms|This paper describes the application of four evolutionary algorithms to the pruning of neural networks used in classification problems. Besides of a simple genetic algorithm (GA), the paper considers three distribution estimation algorithms (DEAs) a compact GA, an extended compact GA, and the Bayesian Optimization Algorithm. The objective is to determine if the DEAs present advantages over the simple GA in terms of accuracy or speed in this problem. The experiments considered a feedforward neural network trained with standard backpropagation and  public-domain and artificial data sets. In most cases, the pruned networks seemed to have better or equal accuracy than the original fully-connected networks. We found few differences in the accuracy of the networks pruned by the four EAs, but found large differences in the execution time. The results suggest that a simple GA with a small population might be the best algorithm for pruning networks on the data sets we tested.|Erick Cantú-Paz"],["59062|GECCO|2010|Generalized crowding for genetic algorithms|Crowding is a technique used in genetic algorithms to preserve diversity in the population and to prevent premature convergence to local optima. It consists of pairing each offspring with a similar individual in the current population (pairing phase) and deciding which of the two will remain in the population (replacement phase). The present work focuses on the replacement phase of crowding, which usually has been carried out by one of the following three approaches Deterministic, Probabilistic, and Simulated Annealing. These approaches present some limitations regarding the way replacement is conducted. On the one hand, the first two apply the same selective pressure regardless of the problem being solved or the stage of the genetic algorithm. On the other hand, the third does not apply a uniform selective pressure over all the individuals in the population, which makes the control of selective pressure over the generations somewhat difficult. This work presents a Generalized Crowding approach that allows selective pressure to be controlled in a simple way in the replacement phase of crowding, thus overcoming limitations of the other approaches. Furthermore, the understanding of existing approaches is greatly improved, since both Deterministic and Probabilistic Crowding turn out to be special cases of Generalized Crowding. In addition, the temperature parameter used in Simulated Annealing is replaced by a parameter called scaling factor that controls the selective pressure applied. Theoretical analysis using Markov chains and empirical evaluation using Bayesian networks demonstrate the potential of this novel Generalized Crowding approach.|Severino F. Galán,Ole J. Mengshoel","58876|GECCO|2009|A memetic algorithm and a parallel hyperheuristic island-based model for a D packing problem|This work presents several approaches used to deal with the D packing problem proposed in the GECCO  contest session. A memetic algorithm, together with the specifically designed local search and variation operators, are presented. A novel parallel model was used to parallelize the approach. The model is a hybrid algorithm which combines a parallel island-based scheme with a hyperheuristic approach. An adaptive behavior is added to the island-based model by applying the hyperheuristic procedure. The main operation of the island-based model is kept, but the configurations of the memetic algorithms executed on each island are dynamically mapped. The model grants more computational resources to those configurations that show a more promising behavior. For this purpose a specific criterion was designed in order to select the configurations with better success expectations. Computational results obtained for the contest problem demonstrate the validity of the proposed model. The best reported solutions for the problem contest instance have been achieved by using the here presented approaches.|Coromoto León,Gara Miranda,Carlos Segura","58257|GECCO|2008|Hybridizing surrogate techniques rough sets and evolutionary algorithms to efficiently solve multi-objective optimization problems|This paper presents an approach in which a multi-objective evolutionary algorithm (MOEA) is coupled to a surrogate method in order to explore the search space in an efficient manner. A small comparative study among three surrogate methods is conducted an artificial neural network (ANN), a radial basis function (RBF) and a support vector machine (SVM). The winner in this comparative study was the SVM. However, our results indicated that the spread of solutions achieved by our surrogate-based MOEA was poor. Thus, we decided to introduce a second phase to the algorithm in which it is hybridized with the rough sets in order to improve the spread of solutions and help to reach the true Pareto front. We show that our proposed hybrid approach only requires , fitness function evaluations in order to solve test problems with up to  decision variables.|Luis V. Santana-Quintero,Carlos A. Coello Coello,Alfredo García Hernández-Díaz","58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|?ngel M. Pérez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ortíz-García,Antonio Portilla-Figueras","58152|GECCO|2007|Estimation of fitness landscape contours in EAs|Evolutionary algorithms applied in real domain should profit from information about the local fitness function curvature. This paper presents an initial study of an evolutionary strategy with a novel approach for learning the covariance matrix of a Gaussian distribution. The learning method is based one stimation of the fitness landscape contour line between the selected and discarded individuals. The distribution learned this way is then used to generate new population members. The algorithm presented here is the first attempt to construct the Gaussian distribution this way and should beconsidered only a proof of concept nevertheless, the empirical comparison on low-dimensional quadratic functions shows that our approach is viable and with respect to the number of evaluations needed to find a solution of certain quality, it is comparable to the state-of-the-art CMA-ES incase of sphere function and outperforms the CMA-ES in case of elliptical function.|Petr Posík,Vojtech Franc","15938|IJCAI|2003|Taming Decentralized POMDPs Towards Efficient Policy Computation for Multiagent Settings|The problem of deriving joint policies for a group of agents that maximize some joint reward function can be modeled as a decentralized partially observable Markov decision process (POMDP). Yet, despite the growing importance and applications of decentralized POMDP models in the multiagents arena, few algorithms have been developed for efficiently deriving joint policies for these models. This paper presents a new class of locally optimal algorithms called \"Joint Equilibrium-based search for policies (JESP)\". We first describe an exhaustive version of JESP and subsequently a novel dynamic programming approach to JESP. Our complexity analysis reveals the potential for exponential speedups due to the dynamic programming approach. These theoretical results are verified via empirical comparisons of the two JESP versions with each other and with a globally optimal brute-force search algorithm. Finally, we prove piece-wise linear and convexity (PWLC) properties, thus taking steps towards developing algorithms for continuous belief states.|Ranjit Nair,Milind Tambe,Makoto Yokoo,David V. Pynadath,Stacy Marsella","57297|GECCO|2005|Solving geometric TSP with ants|This paper presents an ant-based approach for solving the Traveling Salesman Problem (TSP). Novel concepts of this algorithm that distinguish it from the other heuristics are the inclusion of a preprocessing stage and the use of a modified version of an ant-based approach with local optimization in multi stages. Experimental results show that this algorithm outperforms ACS  and is comparable to MMAS  for Euclidean TSP instances. Of the  instances of Euclidean TSP from TSPLIB  that were tested, this algorithm found the optimal solution for  instances. For the remaining instances, this algorithm returned solutions that were within .% of the optimum.|Thang Nguyen Bui,Mufit Colpan","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","16364|IJCAI|2007|Correlation Clustering for Crosslingual Link Detection|The crosslingual link detection problem calls for identifying news articles in multiple languages that report on the same news event. This paper presents a novel approach based on constrained clustering. We discuss a general way for constrained clustering using a recent, graph-based clustering framework called correlation clustering. We introduce a correlation clustering implementation that features linear program chunking to allow processing larger datasets. We show how to apply the correlation clustering algorithm to the crosslingual link detection problem and present experimental results that show correlation clustering improves upon the hierarchical clustering approaches commonly used in link detection, and, hierarchical clustering approaches that take constraints into account.|Jurgen Van Gael,Xiaojin Zhu","59058|GECCO|2010|ConBreO a music performance rendering system using hybrid approach of IEC and automated evolution|This paper presents an IEC (Interactive Evolutionary Computation) system named ConBreO to render expressive music performance using Genetic Programming. The central problem of IEC is the limitation of number of fitness evaluations because of user fatigue. In the system, we introduce two support techniques for IEC. The first one is a hybrid approach of IEC and automated evolution which allows the system to evolve both of IEC and automated evolution. The second one is the selective presentation which selects a new individual to be evaluated by the user based on its expected improvement of fitness. Using the system, obtained expression rule won an award at a performance rendering contest which evaluates computer systems generating expressive musical performances. Our experiment shows that the selective presentation reduces the number of fitness evaluations required to construct the fitness prediction model and prevents the system evaluating unfruitful individuals.|Makoto Tanji,Hitoshi Iba"],["16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","16534|IJCAI|2007|An Efficient Protocol for Negotiation over Multiple Indivisible Resources|We study the problem of autonomous agents negotiating the allocation of multiple indivisible resources. It is difficult to reach optimal outcomes in bilateral or multi-lateral negotiations over multiple resources when the agents' preferences for the resources are not common knowledge. Self-interested agents often end up negotiating inefficient agreements in such situations. We present a protocol for negotiation over multiple indivisible resources which can be used by rational agents to reach efficient outcomes. Our proposed protocol enables the negotiating agents to identify efficient solutions using systematic distributed search that visits only a subspace of the whole solution space.|Sabyasachi Saha,Sandip Sen","16147|IJCAI|2005|Solving POMDPs with Continuous or Large Discrete Observation Spaces|We describe methods to solve partially observable Markov decision processes (POMDPs) with continuous or large discrete observation spaces. Realistic problems often have rich observation spaces, posing significant problems for standard POMDP algorithms that require explicit enumeration of the observations. This problem is usually approached by imposing an a priori discretisation on the observation space, which can be sub-optimal for the decision making task. However, since only those observations that would change the policy need to be distinguished, the decision problem itself induces a lossless partitioning of the observation space. This paper demonstrates how to find this partition while computing a policy, and how the resulting discretisation of the observation space reveals the relevant features of the application domain. The algorithms are demonstrated on a toy example and on a realistic assisted living task.|Jesse Hoey,Pascal Poupart","17055|IJCAI|2009|A Distributed Control Loop for Autonomous Recovery in a Multi-Agent Plan|This paper considers the execution of a Multi-Agent Plan in a partially observable environment, and faces the problem of recovering from action failures. The paper formalizes a local plan repair strategy, where each agent in the system is responsible for controlling (monitoring and diagnosing) the actions it executes, and for autonomously repairing its own plan when an action failure is detected. The paper describes also how to mitigate the impact of an action failure on the plans of other agents when the local recovery strategy fails.|Roberto Micalizio","15875|IJCAI|2003|A Planning Algorithm for Predictive State Representations|We address the problem of optimally controlling stochastic environments that are partially observable. The standard method for tackling such problems is to define and solve a Partially Observable Markov Decision Process (POMDP). However, it is well known that exactly solving POMDPs is very costly computationally. Recently, Littman, Sutton and Singh () have proposed an alternative representation of partially observable environments, called predictive state representations (PSRs). PSRs are grounded in the sequence of actions and observations of the agent, and hence relate the state representation directly to the agent's experience. In this paper, we present a policy iteration algorithm for finding policies using PSRs. In preliminary experiments, our algorithm produced good solutions.|Masoumeh T. Izadi,Doina Precup","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","15938|IJCAI|2003|Taming Decentralized POMDPs Towards Efficient Policy Computation for Multiagent Settings|The problem of deriving joint policies for a group of agents that maximize some joint reward function can be modeled as a decentralized partially observable Markov decision process (POMDP). Yet, despite the growing importance and applications of decentralized POMDP models in the multiagents arena, few algorithms have been developed for efficiently deriving joint policies for these models. This paper presents a new class of locally optimal algorithms called \"Joint Equilibrium-based search for policies (JESP)\". We first describe an exhaustive version of JESP and subsequently a novel dynamic programming approach to JESP. Our complexity analysis reveals the potential for exponential speedups due to the dynamic programming approach. These theoretical results are verified via empirical comparisons of the two JESP versions with each other and with a globally optimal brute-force search algorithm. Finally, we prove piece-wise linear and convexity (PWLC) properties, thus taking steps towards developing algorithms for continuous belief states.|Ranjit Nair,Milind Tambe,Makoto Yokoo,David V. Pynadath,Stacy Marsella","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell"],["16820|IJCAI|2009|Plan Recognition as Planning|In this work we aim to narrow the gap between plan recognition and planning by exploiting the power and generality of recent planning algorithms for recognizing the set G* of goals G that explain a sequence of observations given a domain theory. After providing a crisp definition of this set, we show by means of a suitable problem transformation that a goal G belongs to G* if there is an action sequence  that is an optimal plan for both the goal G and the goal G extended with extra goals representing the observations. Exploiting this result, we show how the set G* can be computed exactly and approximately by minor modifications of existing optimal and suboptimal planning algorithms, and existing polynomial heuristics. Experiments over several domains show that the suboptimal planning algorithms and the polynomial heuristics provide good approximations of the optimal goal set G* while scaling up as well as state-of-the-art planning algorithms and heuristics.|Miquel Ramírez,Hector Geffner","14405|IJCAI|1987|Explaining and Repairing Plans that Fail|A persistent problem in machine planning is that of repairing plans that fail. Two solutions have been suggested to deal with this problem planning critics and met a-planning techniques. Unfortunately, both of these suggestions suffer from lack of flexibility due to an extremely restricted view of how to describe planning failures. This paper presents an alternative approach in which plan failures are described in terms of causal explanations of why they occurred. These explanations are used to access different abstract replanning strategies, which are then turned into specific changes to the faulty plans. The approach is demonstrated using examples from CHEF, a case-based planner that creates and debugs plans in the domain of Szee hwan cooking.|Kristian J. Hammond","58335|GECCO|2008|Strategic positioning in tactical scenario planning|Capability planning problems are pervasive throughout many areas of human interest with prominent examples found in defense and security. Planning provides a unique context for optimization that has not been explored in great detail and involves a number of interesting challenges which are distinct from traditional optimization research. Planning problems demand solutions that can satisfy a number of competing objectives on multiple scales related to robustness, adaptiveness, risk, etc. The scenario method is a key approach for planning. Scenarios can be defined for long-term as well as short-term plans. This paper introduces computational scenario-based planning problems and proposes ways to accommodate strategic positioning within the tactical planning domain. We demonstrate the methodology in a resource planning problem that is solved with a multi-objective evolutionary algorithm. Our discussion and results highlight the fact that scenario-based planning is naturally framed within a multi-objective setting. However, the conflicting objectives occur on different system levels rather than within a single system alone. This paper also contends that planning problems are of vital interest in many human endeavors and that Evolutionary Computation may be well positioned for this problem domain.|James M. Whitacre,Hussein A. Abbass,Ruhul A. Sarker,Axel Bender,Stephen Baker","15635|IJCAI|2001|Hybrid STAN Identifying and Managing Combinatorial Optimisation Sub- problems in Planning|It is well-known that planning is hard but it is less well-known how to approach the hard parts of a problem instance effectively. Using static domain analysis techniques we can identify and abstract certain combinatorial sub-problems from a planning instance, and deploy specialised technology to solve these sub-problems in a way that is integrated with the broader planning activities. We have developed a hybrid planning system (STAN) which brings together alternative planning strategies and specialised algorithms and selects between them according to the structure of the planning domain. STAN participated successfully in the AIPS- planning competition. We describe how sub-problem abstraction is done, with particular reference to route-planning abstraction, and present some of the competition data to demonstrate the potential power of the hybrid approach.|Maria Fox,Derek Long","15469|IJCAI|1999|Unifying SAT-based and Graph-based Planning|The Blackbox planning system unifies the planning as satisfiability framework (Kautz and Selman , ) with the plan graph approach to STRIPS planning (Blum and Furst ). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new randomized systematic solvers. For certain computationally challenging benchmark problems this unified approach outperforms both SATPLAN and Graphplan alone. We also demonstrate that polynomialtime SAT simplification algorithms applied to the encoded problem instances are a powerful complement to the \"mutex\" propagation algorithm that works directly on the plan graph.|Henry A. Kautz,Bart Selman","14844|IJCAI|1991|The Base Selection Task in Analogical Planning|Analogical planning provides a means of solving problems where other machine learning methods fail, because it does not require numerous previous examples or a rich domain theory. Given a problem in an unfamiliar domain (the target case), an analogical planning system locates a successful plan in a similar domain (the bast case), and uses the similarities to generate the target plan. Unfortunately, the analogical planning process is expensive and inflexible Many of the limiting factors reside in the base selection step, which drives the analogy formation process. This paper describes two ways of increasing the effectiveness and efficiency of analogical planning. First, a parallel graph-match base selection algorithm is presented. A parallel implementation on the Connection Machine is described and shown to substantially decrease the complexity of base selection. Second, a base-case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefits of several base cases when no single plan contributes enough information to the analogy. The effectiveness of this approach is demonstrated with examples from the domain of automatic programming.|Diane J. Cook","16574|IJCAI|2007|Planning for Temporally Extended Goals as Propositional Satisfiability|Planning for temporally extended goals (TEGs) expressed as formulae of Linear-time Temporal Logic (LTL) is a proper generalization of classical planning, not only allowing to specify properties of a goal state but of the whole plan execution. Additionally, LTL formulae can be used to represent domain-specific control knowledge to speed up planning. In this paper we extend SATbased planning for LTL goals (akin to bounded LTL model-checking in verification) to partially ordered plans, thus significantly increasing planning efficiency compared to purely sequential SAT planning. We consider a very relaxed notion of partial ordering and show how planning for LTL goals (without the next-time operator) can be translated into a SAT problem and solved very efficiently. The results extend the practical applicability of SATbased planning to a wider class of planning problems. In addition, they could be applied to solving problems in bounded LTL model-checking more efficiently.|Robert Mattmüller,Jussi Rintanen","15910|IJCAI|2003|Generalizing GraphPlan by Formulating Planning as a CSP|We examine the approach of encoding planning problems as CSPs more closely. First we present a simple CSP encoding for planning problems and then a set of transformations that can be used to eliminate variables and add new constraints to the encoding. We show that our transformations uncover additional structure in the planning problem, structure that subsumes the structure uncovered by GRAPHPLAN planning graphs. We solve the CSP encoded planning problem by using standard CSP algorithms. Empirical evidence is presented to validate the effectiveness of this approach to solving planning problems, and to show that even a prototype implementation is more effective than standard GRAPHPLAN. Our prototype is even competitive with far more optimized planning graph based implementations. We also demonstrate that this approach can be more easily lifted to more complex types of planning than can planning graphs. In particular, we show that the approach can be easily extended to planning with resources.|Adriana Lopez,Fahiem Bacchus","15318|IJCAI|1997|Par-KAP a Knowledge Acquisition Tool for Building Practical Planning Systems|Recently, attention has been focused on providing Knowledge Acquisition (KA) support for building practical planning systems. Such support is needed to guide a knowledge engineer in selecting planning methods, as well as for building and validating the planning knowledge-base for a given practical domain. Following current practice in knowledge acquisition, developing KA tools for planning requires that a number of planning knowledge components are made explicit. This includes explicating (i) a planning domain ontology, (ii) a library of problem-solving methods (PSMs) used in planning, and (iii) a set of domain requirements that are used to select a suitable PSM. In this paper, we summarize the planning knowledge components which we have identified in previous work, and, based on these, present an implementation (Par-KAP) that can exploit these models to aid knowledge engineers in constructing practical planning systems.|Leliane Nunes de Barros,James A. Hendler,V. Richard Benjamins","15358|IJCAI|1997|Development of Iterative Real-time Scheduler to Planner Feedback|Planning for real-time applications involves decisions not only about what actions to take in what states to progress toward achieving goals (the traditional decision problem faced by AI planning systems), but also about how to realize those actions within hard real-time deadlines given the inherent limitations of an execution platform. Determining how to arrange actions in a sequence such that timely execution is guaranteed within constraints is a manifestation of the scheduling problem. All cases of the scheduling problem in any domain of nontrivial complexity are difficult to solve (NP-Hard). To more efficiently solve the real-time plan scheduling problem, we propose and analyze an iterative feedbackconstraint relaxation method in which a scheduler and planner iteratively interact to efficiently develop a well-utilized schedule which includes as many planned actions as possible. This method has been successfully implemented within the Cooperative Intelligent Real-time Control Architecture (CIRCA).|Charles B. McVey,Ella M. Atkins,Edmund H. Durfee,Kang G. Shin"],["14465|IJCAI|1987|DANTES An Expert System for Real-Time Network Troubleshooting|Today's computer networks are large and complex. Their day-to-day operation and maintenance can benefit from the support of an expert system, mainly as an aid in troubleshooting. Network troubleshooting has characteristics, like incomplete data, high rate of events, simultaneous presence of several problems, which raise interesting problems in the development of an expert system. DANTES is an expert system designed to provide real-time assistance to network operators. This paper presents the system and stresses the development issues that are peculiar to network troubleshooting. Of particular importance are performance of inference in real-time, multi-problem handling, and consideration of time in reasoning and revision of belief Dealing with such issues and especially with real-time efficiency is primarily a question of system design. This has implications for the knowledge base organization, reasoning mechanism, and recording of deductions.|Robert Mathonet,Herwig Van Cotthem,Leon Vanryckeghem","13451|IJCAI|1975|QUESTION-ANSWER - A Multipurpose Information System|We describe a \"QUESTION-ANSWER\" information system implemented on computer BESM- in the time-shared system. The \"QUESTION-ANSWER\" system is capable of deducing facts that have not been explicitly given to it by using a large data base and interpreting some of data as rules of inference. The system employs a special procedure which allows not to use contradictory information even if it is contained in the data base.|Y. Buchstab,S. Kamynin","16389|IJCAI|2007|On the Automatic Scoring of Handwritten Essays|Automating the task of scoring short handwritten student essays is considered. The goal is to assign scores which are comparable to those of human scorers by coupling two AI technologies optical handwriting recognition and automated essay scoring. The test-bed is that of essays written by children in reading comprehension tests. The process involves several image-level operations removal of pre-printed matter, segmentation of handwritten text lines and extraction of words. Recognition constraints are provided by the reading passage, the question and the answer rubric. Scoring is based on using a vector space model and machine learning of parameters from a set of human-scored samples. System performance is comparable to that of scoring based on perfect manual transcription.|Sargur N. Srihari,Rohini K. Srihari,Pavithra Babu,Harish Srinivasan","13368|IJCAI|1973|Semantic Modeling for Deductive Question-Answering|A description of techniques used for semantic modeling in a deductive question-answering system is given. The system maintains a dialog and is able to understand situations which can be expressed as a series of sequential time-frames. Specific relevant questions are asked by the system when it is unable to succeed in a given task. It can also provide reasons for its previous actions.|Kenneth Biss,Robert T. Chien,Fred Stahl,Steven J. Weissman","13372|IJCAI|1973|Steps Toward Automatic Theory Formation|This paper describes a theory formation system which can discover a partial axiomization of a data base represented as extensionally defined binary relations.- The system first discovers all possible intensional definitions of each binary relation in terms of the others. It then determines a minimal set of these relations from which the others can be defined. It then attempts to discover all the ways the relations of this minimal set can interact with each other, thus generating a set of inference rules. Although the system was originally designed to explore automatic techniques for theory construction for question-answering systems, it is currently being expanded to function as a symbiotic system to help social scientists explore certain kinds of data bases.|John Seely Brown","13733|IJCAI|1981|Integration Unification Reconstruction Modification An Eternal Parsing Braid|BORIS is an integrated natural language understanding system for narratives. In an integrated system, processes of event assimilation, inference, and episodic memory search occur on a word-by-word basis as parsing proceeds. \"Parsing\" here refers to the task of building a conceptual representation for each natural language expression. In addition to being integrated, the BORIS parser is also a unified parser. The same parser is used both at story understanding time and question answering time. This paper explores some of the consequences which arise when the same parser serves both tasks. For instance, one such consequence is that BORIS often knows the answer to a question before it has completely understood the question.|Michael C. Dyer","13614|IJCAI|1977|Ghosts in the Machine An AI Treatment of Medieval History|This paper gives a generalized overview of RESEDA, an interactive question answering system designed primarily for use by historians. Its data base consists of historical information, which attemps to describe the attitudes, political, religious and interpersonal, of the chief characters of the period. Question answering is done by search of the data base and by inference on the information therein. The difficulties of representing this type of data and of formulating inference rules dealing with human motivations and attitudes is also discussed.|Margaret King,Monique Ornato,Gian Piero Zarri,L. Zarri-Baldi,A. Zwiebel","16555|IJCAI|2007|Cluster-Based Selection of Statistical Answering Strategies|Question answering (QA) is a highly complex task that brings together classification, clustering, retrieval, and extraction. Question answering systems include various statistical and rule-based components that combine and form multiple strategies for finding answers. However, in real-life scenarios efficiency constraints make it infeasible to simultaneously use all available strategies in a QA system. To address this issue, we present an approach for carefully selecting answering strategies that are likely to benefit individual questions, without significantly reducing performance. We evaluate the impact of strategy selection on question answering performance at several important QA stages document retrieval, answer extraction, and answer merging. We present strategy selection experiments using a statistical question answering system, and we show significant efficiency improvements. By selecting % of the available answering strategies, we obtained similar performance when compared to using all of the strategies combined.|Lucian Vlad Lita,Jaime G. Carbonell","13349|IJCAI|1971|A Net Structure for Semantic Information Storage Deduction and Retrieval|This paper describes a data structure, MENS (MEmory Net Structure), that is useful for storing semantic information stemming from a natural language, and a system, MENTAL (MEmory Net That Answers and Learns) that interacts with a user (human or program), stores information into and retrieves information from MENS and interprets some information in MENS as rules telling it how to deduce new information from what is already stored. MENTAL can be used as a guestion-answering system with formatted input output, as a vehicle for experimenting with various theories of semantic structures or as the memory management portion of a natural language question-answering system.|Stuart C. Shapiro","15935|IJCAI|2003|A Logic Prover for Text Processing|This paper demonstrates the applicability of automated reasoning to text processing, specifically to Question Answering. It is shown that the approach is feasible, effective, and scalable. A Logic Prover has been implemented and integrated into a state-of-the-art Question Answering System.|Dan I. Moldovan,Christine Clark"],["13373|IJCAI|1973|Case Structure Systems|A logic for case structure systems is presented which allows variations in the order and number of terms in atomic formulas. This logic is used to describe and characterize four existing case systems. A computer program which allows flexible case structures is then described. Applications of the program to medical record analysis and disease modeling are used to illustrate important concepts about case systems. Several open problems are also discussed.|Bertram C. Bruce","15037|IJCAI|1993|Nonmonotonic Model Inference-A Formalization of Student Modeling|A student model description language and its synthesis method are presented. The language called SMDL is based on a logic programming language taking  truth values such as true, false, unknown and fail. A modeling method called HSMIS is a new nonmonotonic model inference system and has the following major characteristics () Model inference of logic program taking  truth values, () Treatment of nonmonotonicity of both student's belief and inference process itself. HSMIS incorporates de Kleer's ATMS as a vehicle for formulating the nonmonotonicity. Both SMDL interpreter and HSMIS have been implemented in Common ESP(Extended Self-contained Prolog) and incorporated into a framework for ITS, called FITS.|Mitsuru Ikeda,Yasuyuki Kono,Riichiro Mizoguchi","14039|IJCAI|1983|Automated Reasoning Real Uses and Potential Uses|An automated reasoning program has provided invaluable assistance in answering certain previously open questions in mathematics and in formal logic. These questions would not have been answered, at least by those who obtained the results, were it not for the program's contribution. Others have used such a program to design logic circuits, many of which proved superior (with respect to transistor count) to the existing designs, and to validate the design of other circuits. These successes establish the value of an automated reasoning program for research and suggest the value for practical applications. We thus conclude that the field of automated reasoning is on the verge of becoming one of the more significant branches of computer science. Further, we conclude that the field has already advanced from stage , that of potential usefulness, to stage , that of actual usefulness. To pass to stage , that of wide acceptance and use, requires, among other things, easy access to an automated reasoning program and an understanding of the various aspects of automated reasoning. In fact, an automated reasoning program is available that is portable and can be run on relatively inexpensive machines. Moreover, a system exists for producing a reasoning program tailored to given specifications. As for the requirement of understanding the aspects of automated reasoning, much research remains--research aided by access to a reasoning program. A large obstacle has thus been removed, permitting many to experiment with and find uses for a computer program that can be relied upon as a most valuable automated reasoning assistant.|Larry Wos","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","13435|IJCAI|1973|PAS-II An Interactive Task-Free Version of an Automatic Protocol Analysis System|PAS-II, a computer program which represents a generalized version of an automatic protocol system (PAS-I) is described. PAS-II is a task-free, interactive, modular data analysis sis system for inferring the information processes used by a human from his verbal behavior while solving a problem. The output of the program is a problem behavior graph a description of the subject's changing knowledge state during problem solving. As an example of system operation the PAS-II analysis of a short cryptarithmetic protocol is presented.|Donald A. Waterman,Allen Newell","13586|IJCAI|1977|A Random Access Picture Digitizer Display and Memory System|The JPL Robotics Research Program is developing techniques that might be applicable in the future to planetary missions, to the assembly of large structures in Earth orbit, or to free-swimming underwater vehicles where there is a need for the integration of a computer vision system with mechanical effectors. In each of these applications there Is a necessity for real-time processing and a size limit on the on-board processor. To meet these objectives, a random access picture digitizer and memory system (RAPID) has been developed which provides, in effect, real-time random access television cameras to the computer. This short report describes the impact of RAPID on the Robotics Program, both in terms of the hardware design and software organization.|R. Eskenazi,R. Cunningham","15800|IJCAI|2003|Logic Programs for Consistently Querying Data Integration Systems|We solve the problem of obtaining answers to queries posed to a mediated integration system under the local-as-view paradigm that are consistent wrt to certain global integrity constraints. For this, the query program is combined with logic programming specifications under the stable model semantics of the class of minimal global instances, and of the class of their repairs.|Loreto Bravo,Leopoldo E. Bertossi","13562|IJCAI|1977|Image Segmentation Technique for Locating Automotive Parts on Belt Conveyors|A simple, model free computer vision program to determine the locations of non-overlapping parts on belt conveyors is described. This program illustrates a simple and effective procedure for segmenting objects from background in instances where simple thresholding of a gray-level image does not suffice. The procedure consists of a unique sequence of standard image enhancement processes. The resultant image exhibits silhouettes of the objects, which contain sufficient information for locating those parts whose orientation can be determined without observation of internal features. The technique has been implemented on a large research computer, as well as a mini-computer coupled to a prototype belt conveyor-robot arm part transfer system. The technique has been validated for a large variety of parts and belt surfaces. It can meet production rates and has the potential for actual production use.|M. L. Baird","13395|IJCAI|1973|Artificial Intelligence and Automatic Programming in CAI|This paper discusses generative computer-assisted instruction (CAI) and its relationship to Artificial Intelligence Research. Systems which have a limited capability for natural language communication are described. In addition, potential areas in which Artificial Intelligence could be applied are outlined. These include individualization of instruction, determining the degree of accuracy of a student response, and problem-solving. A CAI system which is capable of writing computer programs is described in detail. Techniques are given for generating meaningful programming problems. These problems are represented as a sequence of primitive tasks each of which can be coded in several ways. The manner in which the system designs its own solution program and monitors the student solution is also described.|Elliot B. Koffman,Sumner E. Blount","14466|IJCAI|1987|A Parsing System Based on Logic Programming|The paper presents a practical parsing system based on logic programming. A restricted Definite Clause Grammar is assumed as grammar description and the grammar is translated into a parsing program written in Prolog. The system employs a bottom-up parsing strategy with top-down prediction. The major advantages of our system are that the system works in a bottom-up manner so that the left-recursive rules do not cause difficulties, the parsing process does not involve backtracking, and there is no duplicated construction of same syntactic structures. Experiments are shown to estimate the efficiency of the system.|Yuji Matsumoto,Ryôichi Sugimura"]]},"title":{"entropy":6.200717735374085,"topics":["learning for, learning, model for, reinforcement learning, and learning, theorem proving, classifier system, model, for recognition, from, and image, learning classifier, first order, pattern recognition, information extraction, mobile robot, sense disambiguation, support vector, learning with, learning from","the problem, algorithm for, for problem, for the, solving problem, for, constraint satisfaction, algorithm problem, and problem, search for, search, the algorithm, local search, and search, the, heuristic search, the and, the search, for and, genetic for","genetic algorithm, genetic programming, particle swarm, genetic for, evolutionary algorithm, using genetic, algorithm for, neural networks, using algorithm, for optimization, swarm optimization, particle optimization, algorithm with, and genetic, estimation distribution, evolutionary for, using, algorithm optimization, genetic with, programming for","natural language, the and, for system, and, for and, the system, reasoning about, expert system, system, for logic, for language, reasoning, logic and, artificial intelligence, logic, and system, logic programming, knowledge for, for reasoning, description logic","model for, model and, model, classifier system, sense disambiguation, using model, word sense, word disambiguation, for disambiguation, web page, extracting from, web mining, based model, semantic for, for from, markov model, hidden markov, motion from, from, the model","theorem proving, mobile robot, object recognition, feature for, structure and, for object, building blocks, for structure, feature selection, for text, using feature, for building, and object, for robot, matrix factorization, theorem for, active for, nonnegative factorization, hierarchical boa, nonnegative matrix","techniques for, for tree, decision tree, for graph, spanning tree, for domains, the tree, search space, minimum spanning, for space, algorithm for, for decision, for minimum, for spanning, and tree, the graph, for the, the spanning, the space, minimum tree","for problem, the problem, solving problem, algorithm problem, constraint satisfaction, and problem, problem, for solving, for constraint, optimization problem, constraint problem, problem with, satisfaction problem, constraint and, evolutionary problem, the constraint, the solving, genetic problem, approach problem, method for","neural networks, for networks, estimation distribution, estimation algorithm, distribution algorithm, using networks, algorithm with, genetic networks, networks, algorithm networks, neural for, the networks, for with, genetic with, and networks, networks with, estimation and, algorithm for, with, for classification","the effects, differential evolution, using evolution, evolution and, dynamic environments, the evolution, evolution strategies, evolution for, evolution genetic, population size, the population, population for, population and, population genetic, evolution, analysis the, the and, adaptive for, evaluation for, performance algorithm","for and, the and, and, logic programs, for programs, complexity and, logic and, programs, nonmonotonic logic, the complexity, preliminary report, automatic programs, model-based diagnosis, for robot, with and, and programs, the programs, propositional logic, and agents, programs with","for reasoning, reasoning and, reasoning about, and action, for planning, reasoning with, the calculus, framework for, planning and, planning with, reasoning, with and, planning, temporal reasoning, qualitative reasoning, situation calculus, and plans, reasoning knowledge, time and, temporal and"],"ranking":[["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","16592|IJCAI|2007|Learning from Partial Observations|We present a general machine learning framework for modelling the phenomenon of missing information in data. We propose a masking process model to capture the stochastic nature of information loss. Learning in this context is employed as a means to recover as much of the missing information as is recoverable. We extend the Probably Approximately Correct semantics to the case of learning from partial observations with arbitrarily hidden attributes. We establish that simply requiring learned hypotheses to be consistent with observed values suffices to guarantee that hidden values are recoverable to a certain accuracy we also show that, in some sense, this is an optimal strategy for achieving accurate recovery. We then establish that a number of natural concept classes, including all the classes of monotone formulas that are PAC learnable by monotone formulas, and the classes of conjunctions, disjunctions, k-CNF, k-DNF, and linear thresholds, are consistently learnable from partial observations. We finally show that the concept classes of parities and monotone term -decision lists are not properly consistently learnable from partial observations, if RP  NP. This implies a separation of what is consistently learnable from partial observations versus what is learnable in the complete or noisy setting.|Loizos Michael","13295|IJCAI|1969|The Learning of Parameters for Generating Compound Characterizers for Pattern Recognition|This paper presents and describes a pattern recognition program with a relatively simple and general basic structure upon which has been superimposed a rather wide variety of techniques for learning, or self-organization. The program attempts to generalize n-tuple approaches to pattern recognition, in which an n-tuple is a set of individual cells or small pieces of patterns, and each n-tuple is said to characterize an input pattern when these pieces match it, as specified. The program allows n-tuples to match when only some of their parts match, and it allows these parts to match even though they are not precisely positioned (See Uhr, b, for some simple example programs). It further learns, in a variety of ways It searches for good weights on its characterizers' implications, byre-weighting as a function of feedback. It generates and discovers new characterizers (and can therefore begin with no characterizers at all), and discards characterizers that prove to be poor (See Uhr and Vossler, , and Prather and Uhr, ). It also uses a set of characterizers of characterizers, to search for good parameter values that newly-generated characterizers should have. A detailed flow-chart-like \"precis\" description of the program is given, along with an actual listing. It is thus possible to examine exactly what the program does, and how it does it, and therefore to see how a wide variety of learning mechanisms have been implemented in a single pattern recognition program. But because it was coded in a \"high-level\" pattern-matching and list-processing language the program runs too slowly for extensive tests to be practicable. Therefore only a brief listing of output is given, to show that the program, works and begins to learn.|Leonard Uhr,Sara Jordan","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","57159|GECCO|2003|Learning Features for Object Recognition|Features represent the characteristics of objects and selecting or synthesizing effective composite features are the key factors to the performance of object recognition. In this paper, we propose a co-evolutionary genetic programming (CGP) approach to learn composite features for object recognition. The motivation for using CGP is to overcome the limitations of human experts who consider only a small number of conventional combinations of primitive features during synthesis. On the other hand, CGP can try a very large number of unconventional combinations and these unconventional combinations may yield exceptionally good results in some cases. Our experimental results with real synthetic aperture radar (SAR) images show that CGP can learn good composite features. We show results to distinguish objects from clutter and to distinguish objects that belong to several classes.|Yingqiang Lin,Bir Bhanu","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","58397|GECCO|2008|An analysis of matching in learning classifier systems|We investigate rule matching in learning classifier systems for problems involving binary and real inputs. We consider three rule encodings the widely used character-based encoding, a specificity-based encoding, and a binary encoding used in Alecsys. We compare the performance of the three algorithms both on matching alone and on typical test problems. The results on matching alone show that the population generality influences the performance of the matching algorithms based on string representations in different ways. Character-based encoding becomes slower and slower as generality increases, specificity-based encoding becomes faster and faster as generality increases. The results on typical test problems show that the specificity-based representation can halve the time required for matching but also that binary encoding is about ten times faster on the most difficult problems. Moreover, we extend specificity-based encoding to real-inputs and propose an algorithm that can halve the time require for matching real inputs using an interval-based representation.|Martin V. Butz,Pier Luca Lanzi,Xavier Llorà,Daniele Loiacono"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57683|GECCO|2006|Search--based approaches to the component selection and prioritization problem|This poster paper addresses the problem of choosing sets of software components to combine in component--based software engineering. It formulates both ranking and selection problems as feature subset selection problems to which search based software engineering can be applied. We will consider selection and ranking of elements from a set of software components from the component base of a large telecommunications organisation.|Mark Harman,Alexandros Skaliotis,Kathleen Steinhöfel,Paul Baker","58959|GECCO|2010|Efficient stochastic local search algorithm for solving the shortest common supersequence problem|The Shortest Common Supersequence (SCS) problem is a well-known hard combinatorial optimization problem that formalizes many real world problems. Recently, an application of the iterative optimization method called Prototype Optimization with Evolved Improvement Steps (POEMS) to the SCS problem has been proposed. The POEMS seeks the best variation of the current solution in each iteration. The variations, considered as structured hypermutations, are evolved by means of an evolutionary algorithm. This approach has been shown to work very well on synthetic as well as real biological data. However, the approach exhibited rather low scalability which is caused by very time demanding evaluation function. This paper proposes a new time efficient evaluation procedure and a new moving-window strategy for constructing and refining the supersequence. These two enhancements significantly improve an efficiency of the approach. Series of experiments with the modified POEMS method have been carried out. Results presented in this paper show that the method is competitive with current state-of-the-art algorithms for solving the SCS problem. Moreover, there is a potential for further improvement as discussed in the conclusions.|Jirí Kubalík","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","58316|GECCO|2008|Genetic algorithms with local search optimization for protein structure prediction problem|This paper presents a new Genetic Algorithm for Protein Structure Prediction problem in both D and D hydrophobic-hydrophilic lattice models, introduced in . Our algorithm evolves a new local-search genetic operation (called Pull-Move and well described in ), into the standard GA (,). The experiments show that performing a set of Pull-Moves in addition to standard genetic operations in GA (such as crossover and mutation) leads to significant energy improvements. The paper also introduces the Global Energy as fitness function and explains the advantages of utilizing it rather than the standard Free Energy. The experimental results are even more impressive when using the Global Energy as fitness function in GA.|Igor Berenboym,Mireille Avigal","16229|IJCAI|2005|Combination of Local Search Strategies for Rotating Workforce Scheduling Problem|Rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work places (e.g. industrial plants). Solving this problem is of a high practical relevance. In this paper we propose the combination of tabu search with random walk and min conflicts strategy to solve this problem. Computational results for benchmark examples in literature and other real life examples show that combination of tabu search with random walk and min conflicts strategy improves the performance of tabu search for this problem. The methods proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedules.|Nysret Musliu","57233|GECCO|2003|Exploration of a Two Sided Rendezvous Search Problem Using Genetic Algorithms|The problem of searching for a walker that wants to be found, when the walker moves toward the helicopter when it can hear it, is an example of a two sided search problem which is intrinsically difficult to solve. Thomas et al  considered the effectiveness of three standard NATO search paths  for this type of problem. In this paper a genetic algorithm is used to show that more effective search paths exist. In addition it is shown that genetic algorithms can be effective in finding a near optimal path of length  when searching a  cell area, that is a search space of ....|T. Q. S. Truong,A. Stacey","59006|GECCO|2010|Solving OCST problems with problem-specific guided local search|This paper considers the Euclidean variant of the optimal communication spanning tree (OCST) problem. Previous work analyzed features of high-quality solutions and found that edges in optimal solutions have low weight and point towards the center of a tree. Consequently, integrating this problem-specific knowledge into a metaheuristic increases its performance. In this paper, we present an approach to dynamically change the objective function to guide the search process into promising areas. Our approach is based on guided local search. The resulting problem-specific guided local search method considering weight and orientation of edges outperforms standard variants considering only edge weights as well as state-of-the-art evolutionary algorithms using edge-sets for larger problems.|Wolfgang Steitz,Franz Rothlauf","13695|IJCAI|1981|Tuning of Search of the Problem Space for Geometry Proofs|In planning a proof, a student searches through a space of inferences leading forward from the givens of the problem and backward from the to-be-proven statement. One dimension of growth of expertise is that students become more tuned in the search of this problem space. This can be shown to result from the application of various learning operators to production embodiments of the inference rules. Rules are evaluated after the solution of a problem according to whether they led to or led away from the solution. Rules that contributed to a solution are strengthened and an attempt is made to formulate general versions of these rules that will apply in other situations. Rules that led away from the solution are weakened and a discrimination process is evoked to try to add features to the rules that will try to restrict them to the correct circumstances of application. Composition is a learning process that collapses successful sequences of rule operations into single macro-rule productions. There is also a process that converts the backward reasoning rules formed by composition into forward reasoning rules. The effect of these learning processes is to put into production conditions tests for problem features that are heunstically predictive of the rule's success.|John R. Anderson","14053|IJCAI|1983|Flexible Learning of Problem Solving Heuristics Through Adaptive Search|Noting that the methods employed by existing learning systems are often bound to the intended task domain and have little applicability outside that domain, this paper considers an alternative learning system design that offers greater flexibility without sacrificing performance. An operational prototype, constructed around a powerful adaptive search technique, is presented and applied to the problem of acquiring problem solving heuristics through experience. Some performance results obtained with the system in a poker betting domain are reported and compared with those of a previously investigated learning system in the same domain. It is seen that comparable levels of performance are achieved by the two systems, despite the latter's dependence on a considerable amount of domain specific knowledge for effective operation.|Stephen F. Smith"],["57126|GECCO|2003|Genetic Algorithm Optimization of a Filament Winding Process|This paper describes a research effort to improve the efficiency of a filament winding process by combining the simulation capabilities of the WITNESS modeling program with the search capabilities of a genetic algorithm. Results show that the genetic algorithm is able to reduce the cost of producing filament wound mandrels used in the defense industry.|Charles L. Karr,Eric L. Wilson,Sherri L. Messimer","58639|GECCO|2009|Optimization of the trading rule in foreign exchange using genetic algorithm|The generation of profitable trading rules for Foreign Exchange (FX) investments is a difficult but popular problem. The use of Machine Learning in this problem allows us to obtain objective results by using information of the past market behavior. In this paper, we propose a Genetic Algorithm (GA) system to automatically generate trading rules based on Technical Indexes. Unlike related researches in the area, our work focuses on calculating the most appropriate trade timing, instead of predicting the trading prices.|Akinori Hirabayashi,Claus de Castro Aranha,Hitoshi Iba","57060|GECCO|2003|Parameter Optimization by a Genetic Algorithm for a Pitch Tracking System|The emergence of multimedia data in databases requires adequate methods for information retrieval. In a music data retrieval system by humming, the first stage is to extract exact pitch periods from a flow of signals. Due to the complexity of speech signals, it is difficult to make a robust and practical pitch tracking system. We adopt genetic algorithm in optimizing the control parameters for note segmentation and pitch determination. We applied the results to HumSearch, a commercialized product, as a pitch tracking engine. Experimental results showed that the proposed engine notably improved the performance of the existing engine in HumSearch.|Yoon-Seok Choi,Byung Ro Moon","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","59100|GECCO|2010|Optimization of the hlder image descriptor using a genetic algorithm|Local image features can provide the basis for robust and invariant recognition of objects and scenes. Therefore, compact and distinctive representations of local shape and appearance has become invaluable in modern computer vision. In this work, we study a local descriptor based on the Hlder exponent, a measure of signal regularity. The proposal is to find an optimal number of dimensions for the descriptor using a genetic algorithm (GA). To guide the GA search, fitness is computed based on the performance of the descriptor when applied to standard region matching problems. This criterion is quantified using the F-Measure, derived from recall and precision analysis. Results show that it is possible to reduce the size of the canonical Hlder descriptor without degrading the quality of its performance. In fact, the best descriptor found through the GA search is nearly % smaller and achieves similar performance on standard tests.|Leonardo Trujillo,Pierrick Legrand,Gustavo Olague,Cynthia B. Pérez","58433|GECCO|2008|Combining cartesian genetic programming with an estimation of distribution algorithm|This paper describes initial testing of a novel idea to combine a CGP with an EDA. In recent work a new improved crossover technique was successfully applied to a CGP. To implement the new method meant changing the traditional CGP representation. The new representation developed in that work lends itself very nicely to some probability distribution being implemented. The work in this paper has investigated this idea of incoporating estimated probability distributions into the new CGP method with crossover.|Janet Clegg","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["15035|IJCAI|1993|A Parameterised Module System for Constructing Typed Logic Programs|The paper is concerned with the design of a module system for logic programming so as to satisfy many of the requirements of software engineering. The design is based on the language Godel which is a logic programming language which already has a simple type and module system. The module system described here extends the Godel module system so as to include parameterised modules. In particular, this extended system allows general purpose predicates that depend on facts and rules for specific applications to be defined in modules that are independent of their applications.|Patricia M. Hill","16872|IJCAI|2009|Conjunctive Query Answering in the Description Logic EL Using a Relational Database System|Conjunctive queries (CQ) are fundamental for accessing description logic (DL) knowledge bases. We study CQ answering in (extensions of) the DL EL, which is popular for large-scale ontologies and underlies the designated OWL-EL profile of OWL. Our main contribution is a novel approach to CQ answering that enables the use of standard relational database systems as the basis for query execution. We evaluate our approach using the IBM DB system, with encouraging results.|Carsten Lutz,David Toman,Frank Wolter","14429|IJCAI|1987|Extending Logic Programming to Object Programming The System Lap|Object oriented programming aims at code lisibility and conciseness via abstract data type declarations and takes advantage from setting the objects (ie data) in the center of the application, instead of procedures or demonstrations. Just as in Logic programming, where describing the properties of the solution of some problem is (theoreticaly) enough to allow the program to compute the solution, Object programming proceeds from the fact that purely declarative information leads to more procedural behaviour. It appears that Logic programming and Object programming are complementary in the sense that the first is very suitable for expressing subtle reasoning, but rather weak as a formalism for describing complex structures, and conversely for the second. These considerations led us, at the IIRIAM, to combine Logic and Object programming in order to use the advantages of both. Our research effort resulted In the system LAP, an extension of Prolog to object programming, giving a new dimension to the latter, through logic programming concepts processing of partially known data and non determinism applied at the object level. The first part of this paper introduces the main characteristics of LAP, faced with the requirements of AI application development. The second part illustrates some of these capabilities through simple examples.|Herman Iline,Henry Kanoui","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","57445|GECCO|2005|A first order logic classifier system|Motivated by the intention to increase the expressive power of learning classifier systems, we developed a new Xcs derivative, Fox-cs, where the classifier and observation languages are a subset of first order logic. We found that Fox-cs was viable at tasks in two relational task domains, poker and blocks world, which cannot be represented easily using traditional bit-string classifiers and inputs. We also found that for these tasks, the level of generality obtained by Fox-cs in the portion of population that produces optimal behaviour is consistent with Wilson's generality hypothesis.|Drew Mellor","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16866|IJCAI|2009|A Logic for Reasoning about Counterfactual Emotions|The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. An example of this kind of emotions is regret. In order to meet this objective, we exploit the well-known STIT logic Belnap et al.,  Horty, . STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable. In this work we study a decidable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions.|Emiliano Lorini,François Schwarzentruber","14109|IJCAI|1985|A Logic Programming and Verification System for Recursive Quantificational Logic|In this paper, we describe a logic programming and program verification system which is based on quantifier elimination techniques and axiomatization rather than on more common method of doing logic programming using the Herbrand-Prawitz-Robinson unification algorithm without occur-check. This system is shown to have interesting properties for logic programming and includes a number of advanced features. Among these features are user-defined data objects, user-defined recursive relations and functions, either of which may involve quantifiers in the body of their definitions, and automatic termination and consistency checking for recursively defined concept. In addition, it has a correct implementation of negation in contrast to PROLOG implementation of negation as failure, a smooth interaction between LISP-like functions and PROLOG-like relations, and a smooth interaction between specifications and programs. Finally, it provides a method of mathematical induction applicable to recursive definitions involving quantifiers.|Frank M. Brown,Peiya Liu","14466|IJCAI|1987|A Parsing System Based on Logic Programming|The paper presents a practical parsing system based on logic programming. A restricted Definite Clause Grammar is assumed as grammar description and the grammar is translated into a parsing program written in Prolog. The system employs a bottom-up parsing strategy with top-down prediction. The major advantages of our system are that the system works in a bottom-up manner so that the left-recursive rules do not cause difficulties, the parsing process does not involve backtracking, and there is no duplicated construction of same syntactic structures. Experiments are shown to estimate the efficiency of the system.|Yuji Matsumoto,Ryôichi Sugimura"],["16469|IJCAI|2007|Dynamically Weighted Hidden Markov Model for Spam Deobfuscation|Spam deobfuscation is a processing to detect obfuscated words appeared in spam emails and to convert them back to the original words for correct recognition. Lexicon tree hidden Markov model (LTHMM) was recently shown to be useful in spam deobfuscation. However, LT-HMM suffers from a huge number of states, which is not desirable for practical applications. In this paper we present a complexity-reduced HMM, referred to as dynamically weighted HMM (DW-HMM) where the states involving the same emission probability are grouped into super-states, while preserving state transition probabilities of the original HMM. DWHMM dramatically reduces the number of states and its state transition probabilities are determined in the decoding phase. We illustrate how we convert a LT-HMM to its associated DW-HMM. We confirm the useful behavior of DW-HMM in the task of spam deobfuscation, showing that it significantly reduces the number of states while maintaining the high accuracy.|Seunghak Lee,Iryoung Jeong,Seungjin Choi","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","15850|IJCAI|2003|Improving Word Sense Disambiguation in Lexical Chaining|Previous algorithms to compute lexical chains suffer either from a lack of accuracy in word sense disambiguation (WSD) or from computational inefficiency. In this paper, we present a new linear-time algorithm for lexical chaining that adopts the assumption of one sense per discourse. Our results show an improvement over previous algorithms when evaluated on a WSD task.|Michel Galley,Kathleen McKeown","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin","15818|IJCAI|2003|Hierarchical Semantic Classification Word Sense Disambiguation with World Knowledge|We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \"world knowledge\". The model compiles knowledge contained in a dictionary-ontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \"hierarchical classifier\" outperforms a state-of-the-art standard \"flat\" one.|Massimiliano Ciaramita,Thomas Hofmann,Mark Johnson","57076|GECCO|2003|Extracting Test Sequences from a Markov Software Usage Model by ACO|The aim of the paper is to investigate methods for deriving a suitable set of test paths for a software system. The design and the possible uses of the software system are modelled by a Markov Usage Model which reflects the operational distribution of the software system and is enriched by estimates of failure probabilities, losses in case of failure and testing costs. Exploiting this information, we consider the tradeoff between coverage and testing costs and try to find an optimal compromise between both. For that purpose, we use a heuristic optimization procedure inspired by nature, Ant Colony Optimization, which seems to fit very well to the problem structure under consideration. A real world software system is studied to demonstrate the applicability of our approach and to obtain first experimental results.|Karl Doerner,Walter J. Gutjahr","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","16625|IJCAI|2007|From Sampling to Model Counting|We introduce a new technique for counting models of Boolean satisfiability problems. Our approach incorporates information obtained from sampling the solution space. Unlike previous approaches, our method does not require uniform or near-uniform samples. It instead converts local search sampling without any guarantees into very good bounds on the model count with guarantees. We give a formal analysis and provide experimental results showing the effectiveness of our approach.|Carla P. Gomes,Jörg Hoffmann,Ashish Sabharwal,Bart Selman","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["14088|IJCAI|1985|Object Recognition Using Vision and Touch|A system is described that integrates vision and tactile sensing in a robotics environment to perform object recognition tasks. It uses multiple sensor systems (active touch and passive stereo vision) to compute three dimensional primitives that can be matched against a model data base of complex curved surface objects containing holes and cavities. The low level sensing elements provide local surface and feature matches which are constrained by relational criteria embedded in the models. Once a model has been invoked, a verification procedure establishes confidence measures for a correct recognition. The three dimen* sional nature of the sensed data makes the matching process more robust as does the system's ability to sense visually occluded areas with touch. The model is hierarchic in nature and allows matching at different levels to provide support or inhibition for recognition.|Peter K. Allen,Ruzena Bajcsy","16122|IJCAI|2005|Feature Generation for Text Categorization Using World Knowledge|We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.|Evgeniy Gabrilovich,Shaul Markovitch","16981|IJCAI|2009|Locality Preserving Nonnegative Matrix Factorization|Matrix factorization techniques have been frequently applied in information processing tasks. Among them, Non-negative Matrix Factorization (NMF) have received considerable attentions due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in human brain. On the other hand, from geometric perspective the data is usually sampled from a low dimensional manifold embedded in high dimensional ambient space. One hopes then to find a compact representation which uncovers the hidden topics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Locality Preserving Non-negative Matrix Factorization (LPNMF), for this purpose. For two data points, we use KL-divergence to evaluate their similarity on the hidden topics. The optimal maps are obtained such that the feature values on hidden topics are restricted to be non-negative and vary smoothly along the geodesics of the data manifold. Our empirical study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art algorithms on two large high-dimensional databases.|Deng Cai,Xiaofei He,Xuanhui Wang,Hujun Bao,Jiawei Han","16418|IJCAI|2007|Detect and Track Latent Factors with Online Nonnegative Matrix Factorization|Detecting and tracking latent factors from temporal data is an important task. Most existing algorithms for latent topic detection such as Nonnegative Matrix Factorization (NMF) have been designed for static data. These algorithms are unable to capture the dynamic nature of temporally changing data streams. In this paper, we put forward an online NMF (ONMF) algorithm to detect latent factors and track their evolution while the data evolve. By leveraging the already detected latent factors and the newly arriving data, the latent factors are automatically and incrementally updated to reflect the change of factors. Furthermore, by imposing orthogonality on the detected latent factors, we can not only guarantee the unique solution of NMF but also alleviate the partial-data problem, which may cause NMF to fail when the data are scarce or the distribution is incomplete. Experiments on both synthesized data and real data validate the efficiency and effectiveness of our ONMF algorithm.|Bin Cao,Dou Shen,Jian-Tao Sun,Xuanhui Wang,Qiang Yang,Zheng Chen","17106|IJCAI|2009|Local Learning Regularized Nonnegative Matrix Factorization|Nonnegative Matrix Factorization (NMF) has been widely used in machine learning and data mining. It aims to find two nonnegative matrices whose product can well approximate the nonnegative data matrix, which naturally lead to parts-based representation. In this paper, we present a local learning regularized nonnegative matrix factorization (LLNMF) for clustering. It imposes an additional constraint on NMF that the cluster label of each point can be predicted by the points in its neighborhood. This constraint encodes both the discriminative information and the geometric structure, and is good at clustering data on manifold. An iterative multiplicative updating algorithm is proposed to optimize the objective, and its convergence is guaranteed theoretically. Experiments on many benchmark data sets demonstrate that the proposed method outperforms NMF as well as many state of the art clustering methods.|Quanquan Gu,Jie Zhou","16900|IJCAI|2009|An Efficient Nonnegative Matrix Factorization Approach in Flexible Kernel Space|In this paper, we propose a general formulation for kernel nonnegative matrix factorization with flexible kernels. Specifically, we propose the Gaussian nonnegative matrix factorization (GNMF) algorithm by using the Gaussian kernel in the framework. Different from a recently developed polynomial NMF (PNMF), GNMF finds basis vectors in the kernel-induced feature space and the computational cost is independent of input dimensions. Furthermore, we prove the convergence and nonnegativity of decomposition of our method. Extensive experiments compared with PNMF and other NMF algorithms on several face databases, validate the effectiveness of the proposed method.|Daoqiang Zhang,Wanquan Liu","58994|GECCO|2010|Using feature construction to avoid large feature spaces in text classification|Feature space design is a critical part of machine learning. This is an especially difficult challenge in the field of text classification, where an arbitrary number of features of varying complexity can be extracted from documents as a preprocessing step. A challenge for researchers has consistently been to balance expressiveness of features with the size of the corresponding feature space, due to issues with data sparsity that arise as feature spaces grow larger. Drawing on past successes utilizing genetic programming in similar problems outside of text classification, we propose and implement a technique for constructing complex features from simpler features, and adding these more complex features into a combined feature space which can then be utilized by more sophisticated machine learning classifiers. Applying this technique to a sentiment analysis problem, we show encouraging improvement in classification accuracy, with a small and constant increase in feature space size. We also show that the features we generate carry far more predictive power than any of the simple features they contain.|Elijah Mayfield,Carolyn Penstein Rosé","17005|IJCAI|2009|Relation Regularized Matrix Factorization|In many applications, the data, such as web pages and research papers, contain relation (link) structure among entities in addition to textual content information. Matrix factorization (MF) methods, such as latent semantic indexing (LSI), have been successfully used to map either content information or relation information into a lower-dimensional latent space for subsequent processing. However, how to simultaneously model both the relation information and the content information effectively with an MF framework is still an open research problem. In this paper, we propose a novel MF method called relation regularized matrix factorization (RRMF) for relational data analysis. By using relation information to regularize the content MF procedure, RRMF seamlessly integrates both the relation information and the content information into a principled framework. We propose a linear-time learning algorithm with convergence guarantee to learn the parameters of RRMF. Extensive experiments on real data sets show that RRMF can achieve state-of-the-art performance.|Wu-Jun Li,Dit-Yan Yeung","15736|IJCAI|2001|A Simple Feature Selection Method for Text Classification|In text classification most techniques use bag-of-words to represent documents. The main problem is to identify what words are best suited to classify the documents in such a way as to discriminate between them. Feature selection techniques are then needed to identify these words. The feature selection method presented in this paper is rather simple and computationally efficient. It combines a well known feature selection criterion, the information gain, and a new algorithm that selects and adds a feature to a bag-of-words if it does not occur too often with the features already in a small set composed of the best features selected so far for their high information gain. In brief, it tries to avoid considering features whose discrimination capability is sufficiently covered by already selected features, reducing in size the set of the features used to characterize the document set. This paper presents this feature selection method and its results, and how we have predetermined some of its parameters through experimentation.|Pascal Soucy,Guy W. Mineau","14556|IJCAI|1989|Building a World Model for a Mobile Robot Using Dynamic Semantic Constraints|We are developing a new paradigm for a world model construction system which interprets a scene and builds a world model for a mobile robot using dynamic semantic constraints. The system represents a world model in hierarchical form sensor-based maps to a global map with both numerical and symbolic descriptions. At the beginning of interpretation, sensory data (video and range images) are analyzed in bottom-up fashion. A range image is transformed into a height map, and analyzed for the purpose of generating a geometrical property list for both obstacle and traversable regions that is used as the initial input to the interpretation process. At each step of the scene interpretation process, the most reliable feature of an object is selected in the region property list to propagate semantic constraints on other objects close to it. Geometrical modeling for individual objects in the scene is performed, and parameters of each model are dynamically refined by the scene interpretation process. These model parameters and their interrelationships make spatial reasoning robust. Preliminary results with video and range images are shown.|Minoru Asada,Yoshiaki Shirai"],["58348|GECCO|2008|Greedy heuristics and evolutionary algorithms for the bounded minimum-label spanning tree problem|Given an edge-labeled, connected, undirected graph G and a bound r, the bounded minimum-label spanning tree problem seeks a spanning tree on G whose edges carry the fewest possible labels and in which no label appears more than r times. Two greedy heuristics for the unbounded version of the problem are adapted to the bounded version. Two genetic algorithms for the problem encode labeled spanning trees as permutations of G's edges. A simple GA performs poorly, but the addition of local search enables consistently good results.|Arindam Khaled,Bryant A. Julstrom","58327|GECCO|2008|The node-depth encoding analysis and application to the bounded-diameter minimum spanning tree problem|The node-depth encoding has elements from direct and indirect encoding for trees which encodes trees by storing the depth of nodes in a list. Node-depth encoding applies specific search operators that is a typical characteristic for direct encodings. An investigation into the bias of the initialization process and the mutation operators of the node-depth encoding shows that the initialization process has a bias to solutions with small depths and diameters, and a bias towards stars. This investigation, also, shows that the mutation operators are unbiased. The performance of node-depth encoding is investigated for the bounded-diameter minimum spanning tree problem. The results are presented for Euclidean instances presented in the literature. In contrast with the expectation, the evolutionary algorithm using the biased initialization operator does not allow evolutionary algorithms to find better solutions compared to an unbiased initialization. In comparison to other evolutionary algorithms for the bounded-diameter minimum spanning tree evolutionary algorithms using the node-depth encoding have a good performance.|Telma Woerle de Lima,Franz Rothlauf,Alexandre C. B. Delbem","58677|GECCO|2009|Multiobjective genetic programming approach to evolving heuristics for the bounded diameter minimum spanning tree problem MOGP for BDMST|The bounded-diameter (or diameter-constrained) minimum spanning tree (BDMST) problem is a well-studied combinatorial optimization problem for which several heuristics such as one-time tree construction (OTTC), center based tree construction(CBTC), iterative refinement (IR) and randomized greedy heuristics (RGH) have been developed. Very little work, however, has been done on producing heuristics for BDMSTs using evolutionary algorithms. In this paper we have used multiobjective genetic programming (MOGP) to explore the evolution of a set of heuristics for the BDMST problem. The quality of the Pareto fronts obtained from the MOGP-evolved heuristics is comparable to, or in some cases better than, the Pareto fronts generated by established, hand-crafted heuristics. MOGP is thus a very promising technique for finding heuristics for BDMSTs and more general multiobjective combinatorial optimizations.|Rajeev Kumar,Bipul Kumar Bal,Peter Rockett","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","59108|GECCO|2010|An efficient algorithm for generalized minimum spanning tree problem|The Generalized Minimum Spanning Tree problem (GMST) has attracted much attention during the last few years. Since it is in-tractable, many heuristic algorithms have been proposed to solve large GMST instances. Motivated by the effectiveness and effi-ciency of the muscle (the union of all optimal solutions) for solv-ing other NP-hard problems, we investigate how to incorporate the muscle into heuristic design for GMST. Firstly, we demon-strate that it's NP-hard to obtain the muscle for GMST. Then we show that the muscle can be well approximated by the principle and subordinate candidate sets, which can be calculated on a re-duced version of GMST. Therefore, a Dynamic cAndidate set based Search Algorithm (DASA) is presented in this paper for GMST. In contrast to existing heuristics, DASA employs those candidate sets to initialize and optimize solutions. During the search process, those candidate sets are dynamically adjusted to include in new features provided by good solutions. Since those candidate sets cover almost all optimal solutions, the search space of DASA can be dramatically reduced so that elite solutions can be easily found in a short time. Extensive experiments demon-strate that our new algorithm slightly outperforms existing heuris-tic algorithms in terms of solution quality.|He Jiang,Yudong Chen","58747|GECCO|2009|New heuristic and hybrid genetic algorithm for solving the bounded diameter minimum spanning tree problem|In this paper, we propose a new heuristic, called Center-Based Recursive Clustering - CBRC, for solving the bounded diameter minimum spanning tree (BDMST) problem. Our proposed hybrid genetic algorithm  is also extended to include the new heuristic and a multi-parent crossover operator. We test the new heuristic and genetic algorithm on two sets of benchmark problem instances for the Euclidean and Non-Euclidean cases. Experimental results show the effectiveness of the proposed heuristic and genetic algorithm.|Huynh Thi Thanh Binh,Robert I. McKay,Nguyen Xuan Hoai,Nguyen Duc Nghia","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","57677|GECCO|2006|Neighbourhood searches for the bounded diameter minimum spanning tree problem embedded in a VNS EA and ACO|We consider the Bounded Diameter Minimum Spanning Tree problem and describe four neighbourhood searches for it. They are used as local improvement strategies within a variable neighbourhood search (VNS), an evolutionary algorithm (EA) utilising a new encoding of solutions, and an ant colony optimisation (ACO). We compare the performance in terms of effectiveness between these three hybrid methods on a suite of popular benchmark instances, which contains instances too large to solve by current exact methods. Our results show that the EA and the ACO outperform the VNS on almost all used benchmark instances. Furthermore, the ACO yields most of the time better solutions than the EA in long-term runs, whereas the EA dominates when the computation time is strongly restricted.|Martin Gruber,Jano I. van Hemert,Günther R. Raidl","57390|GECCO|2005|The blob code is competitive with edge-sets in genetic algorithms for the minimum routing cost spanning tree problem|Among the many codings of spanning trees for evolutionary search are those based on bijections between Prfer strings---strings of n- vertex labels---and spanning trees on the labeled vertices. One of these bijections, called the Blob Code, showed promise as an evolutionary coding, but EAs that use it to represent spanning trees have not performed well. Here, a genetic algorithm that represents spanning trees via the Blob Code is faster than, and returns results competitive with those of, a GA that encodes spanning trees as edge-sets on Euclidean instances of the minimum routing cost spanning tree problem. On instances whose edge weights have been chosen at random, the Blob-coded GA maintains its time advantage, but its results are inferior to those of the edge-set-coded GA, and both GAs are hard pressed to keep up with a simple stochastic hill-climber on all the test instances.|Bryant A. Julstrom","58676|GECCO|2009|Extending evolutionary algorithms to discover tri-criterion and non-supported solutions for the minimum spanning tree problem|The study of multi-criterion minimum spanning trees is important as many optimization problems in networks, such as communication, transport and utilities can be represented by this model. Conventional evolutionary approaches struggle to discover near-optimal solutions due to the combinatorial search space, and the difficulty in discovering the non-supported solutions. Recently, a knowledge-based evolutionary approach, KEA, has been developed that overcomes some of the problems of the earlier algorithms as it is not restricted to the bi-criterion case, finds non-supported solutions and scales well to larger problems however, the mid-point of its Pareto front is often dominated by alternative algorithms where they are applicable. Novel extensions to KEA, increasing the knowledge of the mid-point, termed KEA-W are examined, eliminating the mid-point deficiencies at the cost of computational time.|Madeleine Davis-Moradkhan,Will N. Browne,Peter Grindrod"],["57388|GECCO|2005|On the complexity of hierarchical problem solving|Competent Genetic Algorithms can efficiently address problems in which the linkage between variables is limited to a small order k. Problems with higher order dependencies can only be addressed efficiently if further problem properties exist that can be exploited. An important class of problems for which this occurs is that of hierarchical problems. Hierarchical problems can contain dependencies between all variables (kn) while being solvable in polynomial time.An open question so far is what precise properties a hierarchical problem must possess in order to be solvable efficiently. We study this question by investigating several features of hierarchical problems and determining their effect on computational complexity, both analytically and empirically. The analyses are based on the Hierarchical Genetic Algorithm (HGA), which is developed as part of this work. The HGA is tested on ranges of hierarchical problems, produced by a generator for hierarchical problems.|Edwin D. de Jong,Richard A. Watson,Dirk Thierens","58756|GECCO|2009|Cheating for problem solving a genetic algorithm with social interactions|We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, i.e. animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.|Rafael Lahoz-Beltra,Gabriela Ochoa,Uwe Aickelin","13478|IJCAI|1975|Problem Solving Approach In Data Management|This paper looks at an artificial intelligence control program required for a system which accepts queries to a data base which consists of both data and program modules. Using a problem solving approach, the data base structure and the program modules specify the initial world model. The operators consist of functions which look through sets in the data base and the actual program modules themselves. The goal is the query which requests data andor processes. The control program determines the path which maps through the data base and the program modules in a manner which satisfies the predicate calculus constraints and the goal. Real examples of how this system is being used for a large scale data base is included.|William D. Haseman,Andrew B. Whinston","14407|IJCAI|1987|Performance in Practical Problem Solving|The quantity of resources that an agent expends in solving problems in a given domain is determined by the representations and search control strategies that it employs. The value of individual representations or strategies to the agent is determined by their contribution to the resource expenditure. We argue here that in order to choose the component representations and strategies appropriate for a particular problem domain it is necessary to measure their contribution to the resource expenditure on the actual problems the agent faces. This is as true for a system designer making such choices as it is for an autonomous mechanical agent. We present one way to measure this contribution and give an example in which the measure is used to improve problem solving performance.|Leo B. Hartman,Josh D. Tenenberg","15623|IJCAI|2001|Visual Analogy in Problem Solving|Computational models of analogical problem solving have traditionally described source and target domains in terms of their causal structure. But psychological research shows that visual reasoning plays a part for many kinds of analogies. This paper describes a model that transfers a solution from a source analog to a new target problem using only visual knowledge represented symbolically. The knowledge representation is based on a language of primitive visual elements and transformations. We found that visual knowledge is sufficient for transfer, but that causal knowledge is needed to determine if the transferred solution is appropriate.|Jim Davies,Ashok K. Goel","13429|IJCAI|1973|On a Local Approach to Representation in Problem Solving|The common feature of all of the problems discussed in this paper is that, in their initial formulation, they have a large number of elements with different names, and for this reason the problems seem to be difficult. However, the first attempts to solve the problems give the necessary heuristic information for simplification of the problem description and a reduction in the number of different names for the elements. We could say that the desire to cut down the number of names is a \"global idea\", but the choice of names and the algorithm of naming are contained in the first attempts. It is important to stress that the reduced description obtained is not always useful for the solution of the original problem, but it turns out to be useful for some of such problems.|V. L. Stefanuk","16689|IJCAI|2007|On Modeling Multiagent Task Scheduling as a Distributed Constraint Optimization Problem|This paper investigates how to represent and solve multiagent task scheduling as a Distributed Constraint Optimization Problem (DCOP). Recently multiagent researchers have adopted the CTMS language as a standard for multiagent task scheduling. We contribute an automated mapping that transforms CTMS into a DCOP. Further, we propose a set of representational compromises for CTMS that allow existing distributed algorithms for DCOP to be immediately brought to bear on CTMS problems. Next, we demonstrate a key advantage of a constraint based representation is the ability to leverage the representation to do efficient solving. We contribute a set of pre-processing algorithms that leverage existing constraint propagation techniques to do variable domain pruning on the DCOP. We show that these algorithms can result in % reduction in state space size for a given set of CTMS problems. Finally, we demonstrate up to a % increase in the ability to optimally solve CTMS problems in a reasonable amount of time and in a distributed manner as a result of applying our mapping and domain pruning algorithms.|Evan Sultanik,Pragnesh Jay Modi,William C. Regli","14779|IJCAI|1989|Abstraction in Problem Solving and Learning|Abstraction has proven to be a powerful tool for controlling the combinatorics of a problemsolving search. It is also of critical importance for learning systems. In this article we present, and evaluate experimentally, a general abstraction method -- impasse-driven abstraction - which is able to provide necessary assistance to both problem solving and learning. It reduces the amount of time required to solve problems, and the time required to learn new rules. In addition, it results in the acquisition of rules that are more general than would have otherwise been learned.|Amy Unruh,Paul S. Rosenbloom","13270|IJCAI|1969|Application of Theorem Proving to Problem Solving|This paper shows how an extension of the resolution proof procedure can be used to construct problem solutions. The extended proof procedure can solve problems involving state transformations. The paper explores several alternate problem representations and provides a discussion of solutions to sample problems including the \"Monkey and Bananas\" puzzle and the 'Tower of Hanoi\" puzzle. The paper exhibits solutions to these problems obtained by QA, a computer program bused on these theorem-proving methods. In addition, the paper shows how QA can write simple computer programs and can solve practical problems for a simple robot.|C. Cordell Green","15023|IJCAI|1993|Perception and Experience in Problem Solving|Whilst much emphasis in AI has been placed on the use of goals in problem solving, less emphasis has been placed on the role of perception and experience. In this paper we show that in the domain that may be considered the most abstract, namely mathematics, that perception and experience play an important role. The mathematician has a vast amount of mathematical knowledge, and yet is able to utilise the appropriate knowledge without difficulty. We argue that it is essential to model how well the knowledge is grasped, so that mathematical knowledge can grow from partial knowledge to important results that are easily accessed. Not all knowledge is equal in its importance, and we argue that perception and experience play a key role in ordering our knowledge. Features play a role in both representing the information from the environment, and indexing the knowledge of our memories, but a key requirement is that the features should be dynamic and not be built in. This research is implemented in the program MU, the Mathematics Understander, which utilises the CMS, Contextual Memory System. MU has sucessfully \"read\" university level texts in pure mathematics, checking the proofs and solving the simple problems.|Edmund Furse,Rod Nicolson"],["57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","14705|IJCAI|1989|Training Feedforward Neural Networks Using Genetic Algorithms|Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both ) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and ) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it.|David J. Montana,Lawrence Davis","57796|GECCO|2006|Distributed genetic algorithm for energy-efficient resource management in sensor networks|In this work we consider energy-efficient resource management in an environment monitoring and hazard detection sensor network. Our goal is to allocate different detection methods to different sensor nodes in the way such that the required detection probability can be achieved while the network lifetime is maximized. The optimization algorithm is designed based on the Island multi-deme genetic algorithm (GA). The experimental results show that our algorithm increases the network lifetime by approximately .% in average compared with the heuristic approaches. We also investigate the effect of the configuration parameters on the searching quality of the proposed distributed GA. A regression model is derived empirically that estimates the runtime of the distributed GA given the configuration parameters such as the sub-population size, parallelism, and migration rate. Once the model has been fit to a group of data, it can be utilized to find the efficient configurations of the proposed algorithm.|Qinru Qiu,Qing Wu,Daniel J. Burns,Douglas Holzhauer","58585|GECCO|2009|Agglomerative genetic algorithm for clustering in social networks|Size and complexity of data repositories collaboratively created by Web users generate a need for new processing approaches. In this paper, we study the problem of detection of fine-grained communities of users in social networks, which can be defined as clustering with a large number of clusters. The practical size of social networks makes the traditional evolutionary based clustering approaches, which represent the entire clustering solution as one individual, hard to apply. We propose an Agglomerative Clustering Genetic Algorithm (ACGA) a population of clusters evolves from the initial state in which each cluster represents one user to a high quality clustering solution. Each step of the evolutionary process is performed locally, engaging only a small part of the social network limited to two clusters and their direct neighborhood. This makes the algorithm practically useful independently of the size of the network. Evaluation on two social network models indicates that ACGA is potentially able to detect communities with accuracy comparable or better than two typical centralized clustering algorithms even though ACGA works under much stricter conditions.|Marek Lipczak,Evangelos E. Milios","58548|GECCO|2008|Improved EDNA estimation of dependency networks algorithm using combining function with bivariate probability distributions|One of the key points in Estimation of Distribution Algorithms (EDAs) is the learning of the probabilistic graphical model used to guide the search the richer the model the more complex the learning task. Dependency networks-based EDAs have been recently introduced. On the contrary of Bayesian networks, dependency networks allow the presence of directed cycles in their structure. In a previous work the authors proposed EDNA, an EDA algorithm in which a multivariate dependency network is used but approximating its structure learning by considering only bivariate statistics. EDNA was compared with other models from the literature with the same computational complexity (e.g., univariate and bivariate models). In this work we propose a modified version of EDNA in which not only the structural learning phase is limited to bivariate statistics, but also the simulation and the parameter learning task. Now, we extend the comparison employing multivariate models based on Bayesian networks (EBNA and hBOA). Our experiments show that the modified EDNA is more accurate than the original one, being its accuracy comparable to EBNA and hBOA, but with the advantage of being faster specially in the more complex cases.|José A. Gámez,Juan L. Mateo,Jose Miguel Puerta","15374|IJCAI|1997|Law Discovery using Neural Networks|This paper proposes a new connectionist approach to numeric law discovery i.e., neural networks (law-candidates) are trained by using a newly invented second-order learning algor ithm based on a quasi-Newton method, called BPQ, and the Minimum Description Length criterion selects the most suitable from lawcandidates. The main advantage of our method over previous work of symbolic or connectionist approach is that it can efficiently discover numeric laws whose power values are not restricted to integers. Experiments showed that the proposed method works well in discovering such laws even from data containing irrelevant variables or a small amount of noise.|Kazumi Saito,Ryohei Nakano","16518|IJCAI|2007|A Call Admission Control Scheme Using NeuroEvolution Algorithm in Cellular Networks|This paper proposes an approach for learning call admission control (CAC) policies in a cellular network that handles several classes of traffic with different resource requirements. The performance measures in cellular networks are long term revenue, utility, call blocking rate (CBR) and handoff failure rate (CDR). Reinforcement Learning (RL) can be used to provide the optimal solution, however such method fails when the state space and action space are huge. We apply a form of NeuroEvolution (NE) algorithm to inductively learn the CAC policies, which is called CN (Call Admission Control scheme using NE). Comparing with the Q-Learning based CAC scheme in the constant traffic load shows that CN can not only approximate the optimal solution very well but also optimize the CBR and CDR in a more flexibility way. Additionally the simulation results demonstrate that the proposed scheme is capable of keeping the handoff dropping rate below a pre-specified value while still maintaining an acceptable CBR in the presence of smoothly varying arrival rates of traffic, in which the state space is too large for practical deployment of the other learning scheme.|Xu Yang,John Bigham","15336|IJCAI|1997|On the Efficient Classification of Data Structures by Neural Networks|In the last few years it has been shown that recurrent neural networks are adequate for processing general data structures like trees and graphs, which opens the doors to a number of new interesting applications previously unexplored. In this paper, we analyze the efficiency of learning the membership of DO AGs (Directed Ordered Acyclic Graphs) in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning. We give sufficient conditions under which the error surface is local minima free. Specifically, we define a topological index associated with a collection of DOAGs that makes it possible to design the architecture so as to avoid local minima.|Paolo Frasconi,Marco Gori,Alessandro Sperduti","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","57046|GECCO|2003|Pruning Neural Networks with Distribution Estimation Algorithms|This paper describes the application of four evolutionary algorithms to the pruning of neural networks used in classification problems. Besides of a simple genetic algorithm (GA), the paper considers three distribution estimation algorithms (DEAs) a compact GA, an extended compact GA, and the Bayesian Optimization Algorithm. The objective is to determine if the DEAs present advantages over the simple GA in terms of accuracy or speed in this problem. The experiments considered a feedforward neural network trained with standard backpropagation and  public-domain and artificial data sets. In most cases, the pruned networks seemed to have better or equal accuracy than the original fully-connected networks. We found few differences in the accuracy of the networks pruned by the four EAs, but found large differences in the execution time. The results suggest that a simple GA with a small population might be the best algorithm for pruning networks on the data sets we tested.|Erick Cantú-Paz"],["57155|GECCO|2003|Finite Population Models of Co-evolution and Their Application to Haploidy versus Diploidy|In order to study genetic algorithms in co-evolutionary environments, we construct a Markov model of co-evolution of populations with fixed, finite population sizes. In this combined Markov model, the behavior toward the limit can be utilized to study the relative performance of the algorithms. As an application of the model, we perform an analysis of the relative performance of haploid versus diploid genetic algorithms in the co-evolutionary setup, under several parameter settings. Because of the use of Markov chains, this paper provides exact stochastic results on the expected performance of haploid and diploid algorithms in the proposed co-evolutionary model.|Anthony M. L. Liekens,Huub M. M. ten Eikelder,Peter A. J. Hilbers","58770|GECCO|2009|Efficient natural evolution strategies|Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.|Yi Sun,Daan Wierstra,Tom Schaul,Jürgen Schmidhuber","58987|GECCO|2010|Adaptive strategy selection in differential evolution|Differential evolution (DE) is a simple yet powerful evolutionary algorithm for global numerical optimization. Different strategies have been proposed for the offspring generation but the selection of which of them should be applied is critical for the DE performance, besides being problem-dependent. In this paper, the probability matching technique is employed in DE to autonomously select the most suitable strategy while solving the problem. Four credit assignment methods, that update the known performance of each strategy based on the relative fitness improvement achieved by its recent applications, are analyzed. To evaluate the performance of our approach, thirteen widely used benchmark functions are used. Experimental results confirm that our approach is able to adaptively choose the suitable strategy for different problems. Compared to classical DE algorithms and to a recently proposed adaptive scheme (SaDE), it obtains better results in most of the functions, in terms of the quality of the final results and convergence speed.|Wenyin Gong,?lvaro Fialho,Zhihua Cai","57081|GECCO|2003|Performance Evaluation and Population Reduction for a Self Adaptive Hybrid Genetic Algorithm SAHGA|This paper examines the effects of local search on hybrid genetic algorithm performance and population sizing. It compares the performance of a self-adaptive hybrid genetic algorithm (SAHGA) to a non-adaptive hybrid genetic algorithm (NAHGA) and the simple genetic algorithm (SGA) on eight different test functions, including unimodal, multimodal and constrained optimization problems. The results show that the hybrid genetic algorithm substantially reduces required population sizes because of the reduction in population variance. The adaptive nature of the SAHGA algorithm together with the reduction in population size allow for faster solution of the test problems without sacrificing solution quality.|Felipe P. Espinoza,Barbara S. Minsker,David E. Goldberg","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,Jörn Mehnen,Thomas Michelitsch","58696|GECCO|2009|Geometric differential evolution|Geometric Particle Swarm Optimization (GPSO) is a recently introduced formal generalization of traditional Particle Swarm Optimization (PSO) that applies naturally to both continuous and combinatorial spaces. Differential Evolution (DE) is similar to PSO but it uses different equations governing the motion of the particles. This paper generalizes the DE algorithm to combinatorial search spaces extending its geometric interpretation to these spaces, analogously as what was done for the traditional PSO algorithm. Using this formal algorithm, Geometric Differential Evolution (GDE), we formally derive the specific GDE for the Hamming space associated with binary strings and present experimental results on a standard benchmark of problems.|Alberto Moraglio,Julian Togelius","58337|GECCO|2008|Non-monotone differential evolution|The Differential Evolution algorithm uses an elitist selection, constantly pushing the population in a strict downhill search, in an attempt to guarantee the conservation of the best individuals. However, when this operator is combined with an exploitive mutation operator can lead to premature convergence to an undesired region of attraction. To alleviate this problem, we propose the Non-Monotone Differential Evolution algorithm. To this end, we allow the best individual to perform some uphill movements, greatly enhancing the exploration of the search space. This approach further aids algorithm's ability to escape undesired regions of the search space and improves its performance. The proposed approach utilizes already computed pieces of information and does not require extra function evaluations. Experimental results indicate that the proposed approach provides stable and reliable convergence.|Michael G. Epitropakis,Vassilis P. Plagianakos,Michael N. Vrahatis","57525|GECCO|2005|Niching in evolution strategies|EAs have the tendency to converge quickly into a single solution. Niching methods, the extension of EAs to address this issue, have been investigated up to date mainly within the field of Genetic Algorithms (GAs). In our study we investigate the basis for niching methods within Evolution Strategies (ES), and propose the first ES niching method. Results show that this method can reliably find and maintain multiple niches even for high-dimensional problems.|Ofer M. Shir,Thomas Bäck","57979|GECCO|2007|Using evolution strategies for automatic extraction of parameters for stellar population synthesis of galaxy spectra from sdss|In this work we employ Evolution Strategies (ES) to automatically extract a set of physical parameters (ages, metallicities, reddening and contributions) from a sample of galaxy spectra taken from Sloan Digital Sky Survey (SDSS) for stellar populations studies. We pose this parameter extraction as an optimization problem and then solve it using ES. The idea is to reconstruct each galactic spectrum from the sample by means of a linear combination of three different theoretical models of stellar population synthesis. This combination produces a model spectrum that is compared with the original spectrum using a difference function. The goal is to find a model that minimizes this difference, using ES as the algorithm to explore the parameter space.|Juan Carlos Gomez,Olac Fuentes","58961|GECCO|2010|Fast parallelization of differential evolution algorithm using MapReduce|MapReduce is a promising programming model for developing distributed applications due to its superb simplicity, scalability and fault tolerance. This paper demonstrates how to apply MapReduce and the open source Hadoop framework for a quick and easy parallelization of the Differential Evolution algorithm. Instead of parallelizing the whole evolution process, our simple solution is to only apply the MR model to the fitness evaluation part, which usually consumes most of the running time. Two alternative approaches are investigated, i.e., population based and data based. Experimental results reveal that even though the population based approach is a better way, the extra cost of Hadoop DFS IO operations and system bookkeeping overhead significantly reduces the benefits of parallelism.|Chi Zhou"],["14824|IJCAI|1991|An Architecture for Visualizing the Execution of Parallel Logic Programs|This paper describes the development of an architecture and implementation of a graphical tracing system for the parallel logic programming language PARLOG. Novel features of the architecture include a graphical execution model of PARLOG a range of representational techniques that allow the user a choice of perspective and granularity of analysis and ongoing work on graphical tools that provide user-defined visualisations of their programs, either before the program is run, or afterwards by demonstration from a textual trace. The aims of the architecture are threefold () to aid program construction and debugging by providing an informative graphical trace of the program's execution () to provide the user with a choice of representational techniques, at a preferred level of granularity and () to allow users to define their own visualisations, that more truly map onto their conception of the problem, and which support the way they wish to view the execution information.|Mike Brayshaw","15917|IJCAI|2003|On Tight Logic Programs and Yet Another Translation from Normal Logic Programs to Propositional Logic|Fages showed that if a program is tight, then every propositional model of its completion is also its stable model. Recently, Babovich, Erdem, and Lifschitz generalized Fages' result, and showed that this is also true if the program is tight on the given model of the completion. As it turned out, this is quite a general result. Among the commonly known benchmark domains, only Niemelii's normal logic program encoding of the Hamiltonian Circuit (HC) problem does not have this property. In this paper, we propose a new normal logic program for solving the HC problem, and show that the program is tight on every model of its completion. Experimental results showed that for many graphs, this new encoding improves the performance of both SMODELS and ASSAT(Chaff), especially of the latter system which is based on the SAT solver Chaff. We also propose a notion of inherently tight logic programs and show that for any program, it is inherently tight iff all its completion models are stable models. We then propose a polynomial transformation from a logic programs to one that is inherently tight, thus providing a reduction of stable model semantics to program completion semantics and SAT.|Fangzhen Lin,Jicheng Zhao","14544|IJCAI|1987|A Network of Communicating Logic Programs and Its Semantics|In this paper, a network of communicating loqic programs is proposed as a model for parallolconcrurrent programming based on logic programs. This network is regarded as an extension of Kahn's pure dataflow in the sense that nodes are logic programs which have atoms for receiving and sending messages as well as queues to accept and memorize them. The nodes' behaviour is unboundedly nondeterministic. On the assumption that the channels' denotations should be defined by using sequence domains, the main concern of the present paper is whether or not mathematical semantics is always well-defined for a given network even when it is of unbounded nondeterminisrn. This paper will show that the proposed network is reduced to a dataflow when a kind of 'fairness' is asked for nondeterminisrn in which logic programs receive inputs and produce outputs based on their computations. The network, then, has a (least) fixpoint semantics it is regarded as one of mathematical semantics of the network, since it can satisfy the recursive relations among the channels' denotations. It is also stated that the network with fair merge operators is applicable to the realization of a computational mechanism for sequential ligic programs.|Susumu Yamasaki","15728|IJCAI|2001|A Comparative Study of Logic Programs with Preference|We are interested in semantical underpinnings for existing approaches to preference handling in extended logic programming (within the framework of answer set programming). As a starting point, we explore three different approaches that have been recently proposed in the literature. Because these approaches use rather different formal means, we furnish a series of uniform characterizations that allow us to gain insights into the relationships among these approaches. To be more precise, we provide different characterizations in terms of (i) fixpoints, (ii) order preservation, and (iii) translations into standard logic programs. While the two former provide semantics for logic programming with preference information, the latter furnishes implementation techniques for these approaches.|Torsten Schaub,Kewen Wang","13879|IJCAI|1983|Integrating Logic Programs and Schemata|(orn clauseSchema Representation Language) is the result of an athor to combine the- tools of logic program-niinp. and schema based knowledge represention into a single hybrid system. knowledge, compressed in schemala can be accessed during the execution of logic programs, and the retrieval of the values of a SLOT in a schema can involve- the execution of logic programs that attempt. to declare the- values prior to presenting- to inheritence, should the slot be empty. ISRI. supports the implementation of programs that take advantage of the best controles. of the logical and objec oriented approaches to knowledge represenlation.|Bradley P. Allen,J. Mark Wright","16739|IJCAI|2007|Complexity Results for Checking Equivalence of Stratified Logic Programs|Recent research in nonmonotonic logic programming under the answer-set semantics focuses on different notions of program equivalence. However, previous results do not address the important classes of stratified programs and its subclass of acyclic (i.e., recursion-free) programs, although they are recognized as important tools for knowledge representation and reasoning. In this paper, we consider such programs, possibly augmented with constraints. Our results show that in the propositional setting, where reasoning is well-known to be polynomial, deciding strong and uniform equivalence is as hard as for arbitrary normal logic programs (and thus coNP-complete), but is polynomial in some restricted cases. Nonground programs behave similarly. However, exponential lower bounds already hold for small programs (i.e., with constantly many rules). In particular, uniform equivalence is undecidable even for small Horn programs plus a single negative constraint.|Thomas Eiter,Michael Fink,Hans Tompits,Stefan Woltran","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","16108|IJCAI|2005|Strong Equivalence for Logic Programs with Preferences|Recently, strong equivalence for Answer Set Programming has been studied intensively, and was shown to be beneficial for modular programming and automated optimization. In this paper we define the novel notion of strong equivalence for logic programs with preferences. Based on this definition we give, for several semantics for preference handling, necessary and sufficient conditions for programs to be strongly equivalent. These results provide a clear picture of the relationship of these semantics with respect to strong equivalence, which differs considerably from their relationship with respect to answer sets. Finally, based on these results, we present for the first time simplification methods for logic programs with preferences.|Wolfgang Faber,Kathrin Konczak","15000|IJCAI|1993|An Abductive Framework for General Logic Programs and other Nonmonotonic Systems|We present an abductive semantics for general propositional logic programs which defines the meaning of a logic program in terms of its extensions. This approach extends the stable model semantics for normal logic programs in a natural way. The new semantics is equivalent to stable semantics for a logic program P whenever P is normal and has a stable model. The abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner. Our approach is based on an idea recently proposed by Konolige for causal reasoning. Instead of maximizing the set of hypotheses alone we maximize the union of the hypotheses, along with possible hypotheses that are excused or refuted by the theory.|Gerhard Brewka,Kurt Konolige","13907|IJCAI|1983|AND Parallelism in Logic Programs|An interpreter for logic programs is defined which executes some goals in parallel. OR parallelism exploits the parallelism defined from nondeterministic choices, and is essentially a replacement for backtracking. AND parallelism comes from solving goals in the body of a single clause in parallel, and is the only way to exploit parallelism in deterministic functions written as logic programs. A unique feature of our model is that it allows both forms of parallelism for the same computation.|John S. Conery,Dennis F. Kibler"],["15367|IJCAI|1997|Qualitative Temporal Reasoning with Points and Durations|We present here a qualitative temporal reasoning system that takes both points and durations as primitive objects and allows relative and indefinite information. We formaly define a point duration network, as a structure formed by two point algebra (PA) networks separately but not independently, since ternary constraints are introduced for relating point and duration information. We adapt some of the concepts and reasoning techniques developed for the point algebra networks, such as consistency and minimality. We prove that the problem of determining consistency in a point duration network is NP-complete. A simpler and polynomial-time decision problem is introduced for a restricted kind of point duration networks. Finally we suggest how to determine consistency and find minimal point duration network in the general case.|Isabel Navarrete,Roque Marín","16132|IJCAI|2005|Integrating Planning and Temporal Reasoning for Domains with Durations and Time Windows|The treatment of exogenous events in planning is practically important in many domains. In this paper we focus on planning with exogenous events that happen at known times, and affect the plan actions by imposing that the execution of certain plan actions must be during some time windows. When actions have durations, handling such constraints adds an extra difficulty to planning, which we address by integrating temporal reasoning into planning. We propose a new approach to planning in domains with durations and time windows, combining graph-based planning and disjunctive constraint-based temporal reasoning. Our techniques are implemented in a planner that took part in the th International Planning Competition showing very good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina","15112|IJCAI|1995|Reasoning about Noisy Sensors in the Situation Calculus|Agents interacting with an incompletely known dynamic world need to be able to reason about the effects of their actions, and to gain further information about that world using sensors of some sort. Unfortunately, sensor information is inherently noisy, and in general serves only to increase the agent's degree of confidence in various propositions. Building on a general logical theory of action formalized in the situation calculus developed by Reiter and others, we propose a simple axiomatization of the effect on an agent's state of belief of taking a reading from a noisy sensor. By exploiting Reiter's solution to the frame problem, we automatically obtain that these sensor actions leave the rest of the world unaffected, and further, that non-sensor actions change the state of belief of the agent in appropriate ways.|Fahiem Bacchus,Joseph Y. Halpern,Hector J. Levesque","15536|IJCAI|1999|Temporal Planning with Mutual Exclusion Reasoning|Many planning domains require a richer notion of time in which actions can overlap and have different durations. The key to fast performance in classical planners (e.g., Graphplan, IPP, and Blackbox) has been the use of a disjunctive representation with powerful mutual exclusion reasoning. This paper presents TGP, a new algorithm for temporal planning. TGP operates by incrementally expanding a compact planning graph representation that handles actions of differing duration. The key to TGP performance is tight mutual exclusion reasoning which is based on an expressive language for bounding mutexes and includes mutexes between actions and propositions. Our experiments demonstrate that mutual exclusion reasoning remains valuable in a rich temporal setting.|David E. Smith,Daniel S. Weld","15971|IJCAI|2003|Reasoning about the Interaction of Knowledge Time and Concurrent Actions in the Situation Calculus|A formal framework for specifying and developing agentsrobots must handle not only knowledge and sensing actions, but also time and concurrency. Researchers have extended the situation calculus to handle knowledge and sensing actions. Other researchers have addressed the issue of adding time and concurrent actions. Here both of these features are combined into a united logical theory of knowledge, sensing, time, and concurrency. The result preserves the solution to the frame problem of previous work, maintains the distinction between indexical and objective knowledge of time, and is capable of representing the various ways in which concurrency interacts with time and knowledge. Furthermore, a method based on regression is developed for solving the projection problem for theories specified in this version of the situation calculus.|Richard B. Scherl","13637|IJCAI|1977|Reasoning About Knowledge and Action|This paper discusses the problems of representing and reasoning with information about knowledge and action. The first section discusses the importance of having systems that understand the concept of knowledge, and how knowledge is related to action. Section  points out some of the special problems that are involved in reasoning about knowledge, and section S presents a logic of knowledge based on the idea of possible worlds. Section  integrates this with a logic of actions and gives an example of reasoning in the combined system. Section  makes some concluding comments.|Robert C. Moore","15854|IJCAI|2003|Incremental Tractable Reasoning about Qualitative Temporal Constraints|In many applications of temporal reasoning we are interested in reasoning incrementally In particular, given a CSP of temporal constrains and a new constraint, we want to maintain certain properties in the extended CSP (e.g., a solution), rather than recomputing them from scratch. The Point Algebra (PA) and the Interval Algebra (IA) are two well-known frameworks for qualitative temporal reasoning. Most of the existing algorithms for PA and the known tractable fragments of IA, such as ORD-Horn, has been designed for \"static\" reasoning. In this paper we study the incremental version of some fundamental problems of temporal reasoning, proposing new algorithms that amortize their complexity when processing a sequence of input constraints. After analyzing the role of path-consistency for incremental satisfiability, we propose algorithms for maintaining a solution of a CSP over either PA or ORD-Horn, and the minimal labels of a CSP over PA. Our algorithms improve the complexity of using existing techniques by a factor of O(n) or O(n) where n is the number of variables involved in the CSP.|Alfonso Gerevini","16348|IJCAI|2007|Decidable Reasoning in a Modified Situation Calculus|We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter's version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.|Yilan Gu,Mikhail Soutchanski","16523|IJCAI|2007|Qualitative Temporal Reasoning about Vague Events|The temporal boundaries of many real-world events are inherently vague. In this paper, we discuss the problem of qualitative temporal reasoning about such vague events. We show that several interesting reasoning tasks, such as checking satisfiability, checking entailment, and calculating the best truth value bound, can be reduced to reasoning tasks in a well-known point algebra with disjunctions. Furthermore, we identify a maximal tractable subset of qualitative relations to support efficient reasoning.|Steven Schockaert,Martine De Cock,Etienne E. Kerre","14926|IJCAI|1991|Planning Robot Control Parameter Values with Qualitative Reasoning|A qualitative reasoning planner for determining robot control parameters to drive manipulation actions has been developed, integrated into a telerobot system, and demonstrated for a match striking task. The planner consists of a qualitative reasoner and a numerical execution history which interact to jointly direct and narrow the search for reliable numerical control parameter values. The planner algorithm, implementation, and an execution example are described. The relationship to previous qualitative reasoning work is also discussed.|Stephen F. Peters,Shigeoki Hirai,Toru Omata,Tomomasa Sato"]]}}