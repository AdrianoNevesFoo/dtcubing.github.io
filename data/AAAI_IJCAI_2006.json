{"abstract":{"entropy":6.009866690763375,"topics":["markov decision, artificial intelligence, markov processes, decision processes, preferences voting, general voting, multiple voting, distributed systems, agents voting, preferences agents, agents, goal agents, robots, agents users, general, important agents, multiple agents, users, software, autonomous","semantic web, problem stochastic, agents environments, web, use large, web become, information, systems complex, agents information, web information, use, complex, environments, intelligent, utility, world, need, virtual, research, involving","algorithm problem, algorithm, present algorithm, search, search problem, constraint problem, commonly problem, search algorithm, constraint, game, problem, logic, efficient, ontology, theory, solving, programs, variables, practical, optimal","partially observable, learning, present novel, actions effects, present learning, machine learning, bayesian networks, problem reinforcement, machine data, learning data, present, learning task, algorithm learning, reinforcement learning, present algorithm, learning domains, learning techniques, paper present, present problem, problem networks","multiple voting, agents voting, preferences agents, general voting, preferences voting, multiple agents, reasoning, multi-agent, report","artificial intelligence, goal agents, agents, robots, important, architecture, cognitive, aaai, describe","information, agents information, need, study, objects, involving, multiple, representation, automated","web, semantic web, social, paper, algorithm","constraint problem, constraint, recent, various, design, variables, memory, practical, feature","commonly problem, ontology, used, solving, bound, applications","partially observable, actions effects, actions, algorithm, planning, uncertainty","bayesian networks, problem networks, networks, ability, data, probabilistic, causal, addresses"],"ranking":[["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","65695|AAAI|2006|Traffic Intersections of the Future|Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.|Kurt M. Dresner,Peter Stone","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aurélie Beynier,Abdel-Illah Mouaddib"],["65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","65784|AAAI|2006|Predicting Task-Specific Webpages for Revisiting|With the increased use of the web has come a corresponding increase in information overload that users face when trying to locate specific webpages, especially as a majority of visits to webpages are revisits. While automatically created browsing history lists offer a potential low-cost solution to re-locating webpages, even short browsing sessions generate a glut of webpages that do not relate to the user's information need or have no revisit value. We address how we can better support web users who want to return to information on a webpage that they have previously visited by building more useful history lists. The paper reports on a combination technique that semi-automatically segments the webpage browsing history list into tasks, applies heuristics to remove webpages that carry no intrinsic revisit value, and uses a learning model, sensitive to individual users and tasks, that predicts which webpages are likely to be revisited again. We present results from an empirical evaluation that report the likely revisit need of users and that show that adequate overall prediction accuracy can be achieved. This approach can be used to increase utility of history lists by removing information overload to users when revisiting webpages.|Arwen Twinkle Lettkeman,Simone Stumpf,Jed Irvine,Jonathan L. Herlocker","65668|AAAI|2006|Using Semantics to Identify Web Objects|Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.|Nathanael Chambers,James F. Allen,Lucian Galescu,Hyuckchul Jung,William Taysom","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65852|AAAI|2006|Sensor-Based Understanding of Daily Life via Large-Scale Use of Common Sense|The use of large quantities of common sense has long been thought to be critical to the automated understanding of the world. To this end, various groups have collected repositories of common sense in machine-readable form. However, efforts to apply these large bodies of knowledge to enable correspondingly large-scale sensor-based understanding of the world have been few. Challenges have included semantic gaps between facts in the repositories and phenomena detected by sensors, fragility of reasoning in the face of noise, incompleteness of repositories, and slowness of reasoning with these large repositories. We show how to address these problems with a combination of novel sensors, probabilistic representation, web-scale information retrieval and approximate reasoning. In particular, we show how to use the ,-fact hand-entered Open-Mind Indoor Common Sense database to interpret sensor traces of day-to-day activities with % accuracy (which is easy) and % precisionrecall (which is not).|William Pentney,Ana-Maria Popescu,Shiaokai Wang,Henry A. Kautz,Matthai Philipose","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang","65745|AAAI|2006|Regret-based Incremental Partial Revelation Mechanisms|Classic direct mechanisms suffer from the drawback of requiring full type (or utility function) revelation from participating agents. In complex settings with multi-attribute utility, assessing utility functions can be very difficult, a problem addressed by recent work on preference elicitation. In this work we propose a framework for incremental, partial revelation mechanisms and study the use of minimax regret as an optimization criterion for allocation determination with type uncertainty. We examine the incentive properties of incremental mechanisms when minimax regret is used to determine allocations with no additional elicitation of payment information, and when additional payment information is obtained. We argue that elicitation effort can be focused simultaneously on reducing allocation and payment uncertainty.|Nathanael Hyafil,Craig Boutilier"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65908|AAAI|2006|An Efficient Algorithm for Scatter Chart Labeling|This paper presents an efficient algorithm for a new variation of the point feature labeling problem. The goal is to position the largest number of point labels such that they do not intersect each other or their points. First we present an algorithm using a greedy algorithm with limited lookahead. We then present an algorithm that iteratively regroups labels, calling the first algorithm on each group, thereby identifying a close to optimal labeling order. The presented algorithm is being used in a commercial product to label charts, and our evaluation shows that it produces results far superior to those of other labeling algorithms.|Sebastian Theophil,Arno Schödl","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65641|AAAI|2006|A Value Theory of Meta-Learning Algorithms|We use game theory to analyze meta-learning algorithms. The objective of meta-learning is to determine which algorithm to apply on a given task. This is an instance of a more general problem that consists of allocating knowledge consumers to learning producers. Solving this general problem in the field of meta-learning yields solutions for related fields such as information retrieval and recommender systems.|Abraham Bagherjeiran","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","65830|AAAI|2006|DD Lite Efficient Incremental Search with State Dominance|This paper presents DD* Lite, an efficient incremental search algorithm for problems that can capitalize on state dominance. Dominance relationships between nodes are used to prune graphs in search algorithms. Thus, exploiting state dominance relationships can considerably speed up search problems in large state spaces, such as mobile robot path planning considering uncertainty, time, or energy constraints. Incremental search techniques are useful when changes can occur in the search graph, such as when re-planning paths for mobile robots in partially known environments. While algorithms such as D* and D* Lite are very efficient incremental search algorithms, they cannot be applied as formulated to search problems in which state dominance is used to prune the graph. DD* Lite extends D* Lite to seamlessly support reasoning about state dominance. It maintains the algorithmic simplicity and incremental search capability of D* Lite, while resulting in orders of magnitude increase in search efficiency in large state spaces with dominance. We illustrate the efficiency of DD* Lite with simulation results from applying the algorithm to a path planning problem with time and energy constraints. We also prove that DD* Lite is sound, complete, optimal, and efficient.|G. Ayorkor Mills-Tettey,Anthony Stentz,M. Bernardine Dias","65762|AAAI|2006|Estimating Search Tree Size|We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the unexplored part of the search tree will be similar to the part we have so far explored. We compare these methods against an old method due to Knuth based on random probing. We show that these methods can reliably estimate the size of search trees explored by both optimization and decision procedures. We also demonstrate that these methods for estimating search tree size can be used to select the algorithm likely to perform best on a particular problem instance.|Philip Kilby,John K. Slaney,Sylvie Thiébaux,Toby Walsh","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","65864|AAAI|2006|Targeting Specific Distributions of Trajectories in MDPs|We define TTD-MDPs, a novel class of Markov decision processes where the traditional goal of an agent is changed from finding an optimal trajectory through a state space to realizing a specified distribution of trajectories through the space. After motivating this formulation, we show how to convert a traditional MDP into a TTD-MDP. We derive an algorithm for finding non-deterministic policies by constructing a trajectory tree that allows us to compute locally-consistent policies. We specify the necessary conditions for solving the problem exactly and present a heuristic algorithm for constructing policies when an exact answer is impossible or impractical. We present empirical results for our algorithm in two domains a synthetic grid world and stories in an interactive drama or game.|David L. Roberts,Mark J. Nelson,Charles Lee Isbell Jr.,Michael Mateas,Michael L. Littman"],["65905|AAAI|2006|Cross-Domain Knowledge Transfer Using Structured Representations|Previous work in knowledge transfer in machine learning has been restricted to tasks in a single domain. However, evidence from psychology and neuroscience suggests that humans are capable of transferring knowledge across domains. We present here a novel learning method, based on neuroevolution, for transferring knowledge across domains. We use many-layered, sparsely-connected neural networks in order to learn a structural representation of tasks. Then we mine frequent sub-graphs in order to discover sub-networks that are useful for multiple tasks. These sub-networks are then used as primitives for speeding up the learning of subsequent related tasks, which may be in different domains.|Samarth Swarup,Sylvian R. Ray","65718|AAAI|2006|Incremental Least-Squares Temporal Difference Learning|Approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning, usually solved using temporal difference (TD) algorithms. In this paper we introduce a new variant of linear TD learning, called incremental least-squares TD learning, or iLSTD. This method is more data efficient than conventional TD algorithms such as TD() and is more computationally efficient than non-incremental least-squares TD methods such as LSTD (Bradtke & Barto  Boyan ). In particular, we show that the per-time-step complexities of iLSTD and TD() are O(n), where n is the number of features, whereas that of LSTD is O(n). This difference can be decisive in modern applications of reinforcement learning where the use of a large number features has proven to be an effective solution strategy. We present empirical comparisons, using the test problem introduced by Boyan (), in which iLSTD converges faster than TD() and almost as fast as LSTD.|Alborz Geramifard,Michael H. Bowling,Richard S. Sutton","65866|AAAI|2006|A Fast Decision Tree Learning Algorithm|There is growing interest in scaling up the widely-used decision-tree learning algorithms to very large data sets. Although numerous diverse techniques have been proposed, a fast tree-growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential. In this paper, we present a novel, fast decision-tree learning algorithm that is based on a conditional independence assumption. The new algorithm has a time complexity of O(m  n), where m is the size of the training data and n is the number of attributes. This is a significant asymptotic improvement over the time complexity O(m  n) of the standard decision-tree learning algorithm C., with an additional space increase of only O(n). Experiments show that our algorithm performs competitively with C. in accuracy on a large number of UCI benchmark data sets, and performs even better and significantly faster than C. on a large number of text classification data sets. The time complexity of our algorithm is as low as naive Bayes'. Indeed, it is as fast as naive Bayes but outperforms naive Bayes in accuracy according to our experiments. Our algorithm is a core tree-growing algorithm that can be combined with other scaling-up techniques to achieve further speedup.|Jiang Su,Harry Zhang","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","65657|AAAI|2006|Fast Hierarchical Goal Schema Recognition|We present our work on using statistical, corpus-based machine learning techniques to simultaneously recognize an agent's current goal schemas at various levels of a hierarchical plan. Our recognizer is based on a novel type of graphical model, a Cascading Hidden Markov Model, which allows the algorithm to do exact inference and make predictions at each level of the hierarchy in time quadratic to the number of possible goal schemas. We also report results of our recognizer's performance on a plan corpus.|Nate Blaylock,James F. Allen","65666|AAAI|2006|B-ROC Curves for the Assessment of Classifiers over Imbalanced Data Sets|The class imbalance problem appears to be ubiquitous to a large portion of the machine learning and data mining communities. One of the key questions in this setting is how to evaluate the learning algorithms in the case of class imbalances. In this paper we introduce the Bayesian Receiver Operating Characteristic (B-ROC) curves, as a set of tradeoff curves that combine in an intuitive way, the variables that are more relevant to the evaluation of classifiers over imbalanced data sets. This presentation is based on section  of (Crdenas, Baras, & Seamon ).|Alvaro A. Cárdenas,John S. Baras","65884|AAAI|2006|Learning Partially Observable Action Schemas|We present an algorithm that derives actions' effects and preconditions in partially observable, relational domains. Our algorithm has two unique features an expressive relational language, and an exact tractable computation. An action-schema language that we present permits learning of preconditions and effects that include implicit objects and unstated relationships between objects. For example, we can learn that replacing a blown fuse turns on all the lights whose switch is set to on. The algorithm maintains and outputs a relational-logical representation of all possible action-schema models after a sequence of executed actions and partial observations. Importantly, our algorithm takes polynomial time in the number of time steps and predicates. Time dependence on other domain parameters varies with the action-schema language. Our experiments show that the relational structure speeds up both learning and generalization, and outperforms propositional learning methods. It also allows establishing apriori-unknown connections between objects (e.g. light bulbs and their switches), and permits learning conditional effects in realistic and complex situations. Our algorithm takes advantage of a DAG structure that can be updated efficiently and preserves compactness of representation.|Dafna Shahaf,Eyal Amir","65862|AAAI|2006|Boosting Expert Ensembles for Rapid Concept Recall|Many learning tasks in adversarial domains tend to be highly dependent on the opponent. Predefined strategies optimized for play against a specific opponent are not likely to succeed when employed against another opponent. Learning a strategy for each new opponent from scratch, though, is inefficient as one is likely to encounter the same or similar opponents again. We call this particular variant of inductive transfer a concept recall problem. We present an extension to AdaBoost called ExpBoost that is especially designed for such a sequential learning tasks. It automatically balances between an ensemble of experts each trained on one known opponent and learning the concept of the new opponent. We present and compare results of Exp-Boost and other algorithms on both synthetic data and in a simulated robot soccer task. ExpBoost can rapidly adjust to new concepts and achieve performance comparable to a classifier trained exclusively on a particular opponent with far more data.|Achim Rettinger,Martin Zinkevich,Michael H. Bowling","65885|AAAI|2006|Learning Partially Observable Action Models Efficient Algorithms|We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 's of features (more than  states).|Dafna Shahaf,Allen Chang,Eyal Amir","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65627|AAAI|2006|QUICR-Learning for Multi-Agent Coordination|Coordinating multiple agents that need to perform a sequence of actions to maximize a system level reward requires solving two distinct credit assignment problems. First, credit must be assigned for an action taken at time step t that results in a reward at time step t  t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first credit assignment problem is typically addressed with temporal difference methods such as Q-learning. The second credit assignment problem is typically addressed by creating custom reward functions. To address both credit assignment problems simultaneously, we propose the \"Q Updates with Immediate Counterfactual Rewards-learning\" (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods.|Adrian K. Agogino,Kagan Tumer","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","65705|AAAI|2006|The Complexity of Bribery in Elections|We study the complexity of influencing elections through bribery How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified candidate can be made the election's winner We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.|Piotr Faliszewski,Edith Hemaspaandra,Lane A. Hemaspaandra","65640|AAAI|2006|Impersonation-Based Mechanisms|In this paper we present a general scheme to create mechanisms that approximate the social welfare in the presence of selfish (but rational) behavior of agents. The usual approach is to design truthful mechanisms in which an agent can only lose by impersonating as another agent. In contrast, our approach is to allow an agent to impersonate several different agents. We design the mechanisms such that only a limited set of impersonations are reasonable to rational agents. Our mechanisms make sure that for any choice of such impersonations by the agents, an approximation to the social welfare is achieved. We demonstrate our results on the well studied domain of Combinatorial Auctions (CA). Our mechanisms are algorithmic implementations, a notion recently suggested in (Babaioff, Lavi, & Pavlov ).|Moshe Babaioff,Ron Lavi,Elan Pavlov","65820|AAAI|2006|Strong Mediated Equilibrium|Stability against potential deviations by sets of agents is a most desired property in the design and analysis of multi-agent systems. However, unfortunately, this property is typically not satisfied. In game-theoretic terms, a strong equilibrium, which is a strategy profile immune to deviations by coalition, rarely exists. This paper suggests the use of mediators in order to enrich the set of situations where we can obtain stability against deviations by coalitions. A mediator is defined to be a reliable entity, which can ask the agents for the right to play on their behalf, and is guaranteed to behave in a pre-specified way based on messages received from the agents. However, a mediator cannot enforce behavior that is, agents can play in the game directly, without the mediator's help. A mediator generates a new game for the players, the mediated game. We prove some general results about mediators, and mainly concentrate on the notion of strong mediated equilibrium, which is just a strong equilibrium at the mediated game. We show that desired behaviors, which are stable against deviations by coalitions, can be obtained using mediators in several classes of settings.|Dov Monderer,Moshe Tennenholtz","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm"],["65680|AAAI|2006|Building Explainable Artificial Intelligence Systems|As artificial intelligence (AI) systems and behavior models in military simulations become increasingly complex, it has been difficult for users to understand the activities of computer-controlled entities. Prototype explanation systems have been added to simulators, but designers have not heeded the lessons learned from work in explaining expert system behavior. These new explanation systems are not modular and not portable they are tied to a particular AI system. In this paper, we present a modular and generic architecture for explaining the behavior of simulated entities. We describe its application to the Virtual Humans, a simulation designed to teach soft skills such as negotiation and cultural awareness.|Mark G. Core,H. Chad Lane,Michael van Lent,Dave Gomboc,Steve Solomon,Milton Rosenberg","65669|AAAI|2006|Multimodal Cognitive Architecture Making Perception More Central to Intelligent Behavior|I propose that the notion of cognitive state be broadened from the current predicate-symbolic, Language-of-Thought framework to a multi-modal one, where perception and kinesthetic modalities participate in thinking. In contrast to the roles assigned to perception and motor activities as modules external to central cognition in the currently dominant theories in AI and Cognitive Science, in the proposed approach, central cognition incorporates parts of the perceptual machinery. I motivate and describe the proposal schematically, and describe the implementation of a bi-modal version in which a diagrammatic representation component is added to the cognitive state. The proposal explains our rich multimodal internal experience, and can be a key step in the realization of embodied agents. The proposed multimodal cognitive state can significantly enhance the agent's problem solving.|B. Chandrasekaran","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","65671|AAAI|2006|Evaluating Critiquing-based Recommender Agents|We describe a user study evaluating two critiquing-based recommender agents based on three criteria decision accuracy. decision effort, and user confidence. Results show that user-motivated critiques were more frequently applied and the example critiquing system employing only this type of critiques achieved the best results. In particular, the example critiquing agent significantly improves users' decision accuracy with less cognitive effort consumed than the dynamic critiquing recommender with system-proposed critiques. Additionally, the former is more likely to inspire users' confidence of their choice and promote their intention to purchase and return to the agent for future use.|Li Chen,Pearl Pu","65713|AAAI|2006|Constraints The Ties that Bind|Constraints can serve as a unifying force in artificial intelligence.|Eugene C. Freuder","65896|AAAI|2006|Real-Time Interactive Learning in the NERO Video Game|In the NeuroEvolving Robotic Operatives (NERO) video game, the player trains a team of virtual robots for combat against other players' teams. The virtual robots learn in real time through interacting with the player. Since NERO was originally released in June, , it has been downloaded over , times, appeared on Slashdot, and won several honors. The real-time NeuroEvolution of Augmenting Topologies (rt-NEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, drives the robots' learning, making possible this entirely new genre of video game. The live demo will show how agents in NERO adapt in real time as they interact with the player. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.|Kenneth O. Stanley,Igor Karpov,Risto Miikkulainen,Aliza Gold","65695|AAAI|2006|Traffic Intersections of the Future|Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.|Kurt M. Dresner,Peter Stone","65647|AAAI|2006|Goal Specification Non-Determinism and Quantifying over Policies|One important aspect in directing cognitive robots or agents is to formally specify what is expected of them. This is often referred to as goal specification. Temporal logics such as LTL, and CTL* have been used to specify goals of cognitive robots and agents when their actions have deterministic consequences. It has been suggested that in domains where actions have non-deterministic effects, temporal logics may not be able to express many intuitive and useful goals. In this paper we first show that this is indeed true with respect to existing temporal logics such as LTL, CTL*, and -CTL*. We then propose the language, P-CTL*, which includes the quantifiers, exist a policy and for all policies. We show that this language allows for the specification of richer goals, including many intuitive and useful goals mentioned in the literature which cannot be expressed in existing temporal languages. We generalize our approach of showing the limitations of -CTL* to develop a framework to compare expressiveness of goal languages.|Chitta Baral,Jicheng Zhao","65890|AAAI|2006|Multiagent Coalition Formation for Computer-Supported Cooperative Learning|In this paper, we describe a computer-supported cooperative learning system in education and the results of its deployment. The system, called I-MINDS, consists of a set of teacher agents, group agents, and student agents. While the agents possess individual intelligent capabilities, the novel invention of I-MINDS lies in multiagent intelligence and coalition formation. I-MINDS supports student participation and collaboration and helps the instructor manage large, distance classrooms. Specifically, it uses a Vickrey auction-based and learning-enabled algorithm called VALCAM to form student groups in a structured cooperative learning setting. We have deployed I-MINDS in an introductory computer science course (CS) and conducted experiments in the Spring and Fall semesters of  to study how I-MINOS-supported collaboration fares against traditional, face-to-face collaboration. Results showed that students using I-MINDS performed (and outperformed in some aspects) as well as students in traditional settings.|Leen-Kiat Soh,Nobel Khandaker,Hong Jiang","65777|AAAI|2006|A Unified Cognitive Architecture for Physical Agents|In this paper we describe ICARUS, a cognitive architecture for physical agents that integrates ideas from a number of traditions, but that has been especially influenced by results from cognitive psychology. We review ICARUS' commitments to memories and representations, then present its basic processes for performance and learning. We illustrate the architecture's behavior on a task from in-city driving that requires interaction among its various components. In addition, we discuss ICARUS' consistency with qualitative findings about the nature of human cognition. In closing, we consider the framework's relation to other cognitive architectures that have been proposed in the literature.|Pat Langley,Dongkyu Choi"],["65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","65915|AAAI|2006|Classification Spanning Private Databases|In this paper, we study the classification problem involving information spanning multiple private databases. The privacy challenges lie in the facts that data cannot be collected in one place and the classifier itself may disclose private information. We present a novel solution that builds the same decision tree classifier as if data are collected in a central place, but preserves the privacy of participating sites.|Ke Wang,Yabo Xu,Rong She,Philip S. Yu","65912|AAAI|2006|Contract Enactment in Virtual Organizations A Commitment-Based Approach|A virtual organization (VO) is a dynamic collection of entities (individuals, enterprises, and information resources) collaborating on some computational activity. VOs are an emerging means to model, enact, and manage large-scale computations. VOs consist of autonomous, heterogeneous members, often dynamic exhibiting complex behaviors. Thus, VOs are best modeled via multiagent systems. An agent can be an individual such as a person, business partner, or a resource. An agent may also be a VO. A VO is an agent that comprises other agents. Contracts provide a natural arms-length abstraction for modeling interaction among autonomous and heterogeneous agents. The interplay of contracts and VOs is the subject of this paper. The core of this paper is an approach to formalize VOs and contracts based on commitments. Our main contributions are () a formalization of VOs, () a discussion of certain key properties of VOs, and () an identification of a variety of VO structures and an analysis of how they support contract enactment. We evaluate our approach with an analysis of several scenarios involving the handling of exceptions and conflicts in contracts.|Yathiraj B. Udupi,Munindar P. Singh","65668|AAAI|2006|Using Semantics to Identify Web Objects|Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.|Nathanael Chambers,James F. Allen,Lucian Galescu,Hyuckchul Jung,William Taysom","65940|AAAI|2006|Responsive Information Architect Enabling Context-Sensitive Information Seeking|Information seeking is an important but often difficult task especially when involving large and complex data sets. We hypothesize that a context-sensitive interaction paradigm can greatly assist users in their information seeking. Such a paradigm allows a system to both understand user data requests and present the requested information in context. Driven by this hypothesis, we have developed a suite of intelligent user interaction technologies and integrated them in a full-fledged, context-sensitive information system. In this paper, we review two sets of key technologies context-sensitive multimodal input interpretation and automated multimedia output generation. We also share our evaluation results, which indicate that our approaches are capable of supporting context-sensitive information seeking for practical applications.|Michelle X. Zhou,Keith Houck,Shimei Pan,James Shaw,Vikram Aggarwal,Zhen Wen","65766|AAAI|2006|Lessons on Applying Automated Recommender Systems to Information-Seeking Tasks|Automated recommender systems predict user preferences by applying machine learning techniques to data on products, users, and past user preferences for products. Such systems have become increasingly popular in entertainment and e-commerce domains, but have thus far had little success in information-seeking domains such as identifying published research of interest. We report on several recent publications that show how recommenders can be extended to more effectively address information-seeking tasks by expanding the focus from accurate prediction of user preferences to identifying a useful set of items to recommend in response to the user's specific information need. Specific research demonstrates the value of diversity in recommendation lists, shows how users value lists of recommendations as something different from the sum of the individual recommendations within, and presents an analytic model for customizing a recommender to match user information-seeking needs.|Joseph A. Konstan,Sean M. McNee,Cai-Nicolas Ziegler,Roberto Torres,Nishikant Kapoor,John Riedl","65709|AAAI|2006|Mining and Re-ranking for Answering Biographical Queries on the Web|The rapid growth of the Web has made itself a huge and valuable knowledge base. Among them, biographical information is of great interest to society. However, there has not been an efficient and complete approach to automated biography creation by querying the web. This paper describes an automatic web-based question answering system for biographical queries. Ad-hoc improvements on pattern learning approaches are proposed for mining biographical knowledge. Using bootstrapping, our approach learns surface text patterns from the web, and applies the learned patterns to extract relevant information. To reduce human labeling cost, we propose a new IDF-inspired reranking approach and compare it with pattern's precision-based re-ranking approach. A comparative study of the two re-ranking models is conducted. The tested system produces promising results for answering biographical queries.|Donghui Feng,Deepak Ravichandran,Eduard H. Hovy","65745|AAAI|2006|Regret-based Incremental Partial Revelation Mechanisms|Classic direct mechanisms suffer from the drawback of requiring full type (or utility function) revelation from participating agents. In complex settings with multi-attribute utility, assessing utility functions can be very difficult, a problem addressed by recent work on preference elicitation. In this work we propose a framework for incremental, partial revelation mechanisms and study the use of minimax regret as an optimization criterion for allocation determination with type uncertainty. We examine the incentive properties of incremental mechanisms when minimax regret is used to determine allocations with no additional elicitation of payment information, and when additional payment information is obtained. We argue that elicitation effort can be focused simultaneously on reducing allocation and payment uncertainty.|Nathanael Hyafil,Craig Boutilier"],["65710|AAAI|2006|Inconsistencies Negations and Changes in Ontologies|Ontology management and maintenance are considered cornerstone issues in current Semantic Web applications in which semantic integration and ontological reasoning play a fundamental role. The ability to deal with inconsistency and to accommodate change is of utmost importance in real-world applications of ontological reasoning and management, wherein the need for expressing negated assertions also arises naturally. For this purpose, precise, formal definitions of the the different types of inconsistency and negation in ontologies are required. Unfortunately, ontology languages based on Description Logics (DLs) do not provide enough expressive power to represent axiom negations. Furthermore, there is no single, well-accepted notion of inconsistency and negation in the Semantic Web community, due to the lack of a common and solid foundational framework. In this paper, we propose a general framework accounting for inconsistency, negation and change in ontologies. Different levels of negation and inconsistency in DL-based ontologies are distinguished. We demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontologies.|Giorgos Flouris,Zhisheng Huang,Jeff Z. Pan,Dimitris Plexousakis,Holger Wache","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","65730|AAAI|2006|Large Scale Knowledge Base Systems An Empirical Evaluation Perspective|In this paper, we discuss how our work on evaluating Semantic Web knowledge base systems (KBSs) contributes to address some broader AI problems. First, we show how our apprcach provides a benchmarking solution to the Semantic Web, a new application area of AI. Second, we discuss how the approach is also beneficial in a more traditional AI context. We focus on issues such as scalability, performance tradeoffs, and the comparison of different classes of systems.|Yuanbo Guo,Abir Qasem,Jeff Heflin","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65807|AAAI|2006|Bookmark Hierarchies and Collaborative Recommendation|GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely. The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web. GiveALink does not use tags, content, or links in the submitted Web pages. Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users. In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network. Search results can be personalized using the bookmarks submitted by a user. We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.|Benjamin Markines,Lubomira Stoilova,Filippo Menczer","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65835|AAAI|2006|Constraint-Based Random Stimuli Generation for Hardware Verification|We report on random stimuli generation for hardware verification in IBM as a major application of various artificial intelligence technologies, including knowledge representation, expert systems, and constraint satisfaction. The application has been developed for almost a decade, with huge payoffs. Research and development around this application is still thriving, as we continue to cope with the ever-increasing complexity of modern hardware systems and demanding business environments.|Yehuda Naveh,Michal Rimon,Itai Jaeger,Yoav Katz,Michael Vinov,Eitan Marcus,Gil Shurek","65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","65631|AAAI|2006|The Impact of Balancing on Problem Hardness in a Highly Structured Domain|Random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider random problem distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCPQWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness.|Carlos Ansótegui,Ramón Béjar,Cèsar Fernández,Carla P. Gomes,Carles Mateu","65724|AAAI|2006|Model Counting A New Strategy for Obtaining Good Bounds|Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained.|Carla P. Gomes,Ashish Sabharwal,Bart Selman","65728|AAAI|2006|Analysis of Privacy Loss in Distributed Constraint Optimization|Distributed Constraint Optimization (DCOP) is rapidly emerging as a prominent technique for multi agent coordination. However, despite agent privacy being a key motivation for applying DCOPs in many applications, rigorous quantitative evaluations of privacy loss in DCOP algorithms have been lacking. Recently, Maheswaran et al.  introduced a framework for quantitative evaluations of privacy in DCOP algorithms, showing that some DCOP algorithms lose more privacy than purely centralized approaches and questioning the motivation for applying DCOPs. This paper addresses the question of whether state-of-the art DCOP algorithms suffer from a similar shortcoming by investigating several of the most efficient DCOP algorithms, including both DPOP and ADOPT. Furthermore, while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions (e.g. constraint-graph topology, asynchrony, message-contents), this paper examines the privacy aspect of such decisions, providing an improved understanding of privacy-efficiency tradeoffs.|Rachel Greenstadt,Jonathan P. Pearce,Milind Tambe","65857|AAAI|2006|A Quadratic Propagator for the Inter-Distance Constraint|We present a new propagator achieving bound consistency for the INTER-DISTANCE constraint. This constraint ensures that, among a set of variables X,..., Xn, the difference between two variables is at least p. This restriction models, in particular scheduling problems in which tasks require p contiguous units of a resource to be completed. Until now, the best known propagator for bound consistency had time complexity O(n). In this work we propose a quadratic propagator for the same level of consistency. We then show that this theoretical gain gives savings of an order of magnitude in our benchmark of scheduling problems.|Claude-Guy Quimper,Alejandro López-Ortiz,Gilles Pesant","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65687|AAAI|2006|Identifying and Generating Easy Sets of Constraints for Clustering|Clustering under constraints is a recent innovation in the artificial intelligence community that has yielded significant practical benefit. However, recent work has shown that for some negative forms of constraints the associated subproblem of just finding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufficient conditions from graph theory to identify a priori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.|Ian Davidson,S. S. Ravi"],["65710|AAAI|2006|Inconsistencies Negations and Changes in Ontologies|Ontology management and maintenance are considered cornerstone issues in current Semantic Web applications in which semantic integration and ontological reasoning play a fundamental role. The ability to deal with inconsistency and to accommodate change is of utmost importance in real-world applications of ontological reasoning and management, wherein the need for expressing negated assertions also arises naturally. For this purpose, precise, formal definitions of the the different types of inconsistency and negation in ontologies are required. Unfortunately, ontology languages based on Description Logics (DLs) do not provide enough expressive power to represent axiom negations. Furthermore, there is no single, well-accepted notion of inconsistency and negation in the Semantic Web community, due to the lack of a common and solid foundational framework. In this paper, we propose a general framework accounting for inconsistency, negation and change in ontologies. Different levels of negation and inconsistency in DL-based ontologies are distinguished. We demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontologies.|Giorgos Flouris,Zhisheng Huang,Jeff Z. Pan,Dimitris Plexousakis,Holger Wache","65694|AAAI|2006|Inexact Matching of Ontology Graphs Using Expectation-Maximization|We present a new method for mapping ontology schemas that address similar domains. The problem of ontology mapping is crucial since we are witnessing a decentralized development and publication of ontological data. We formulate the problem of inferring a match between two ontologies as a maximum likelihood problem, and solve it using the technique of expectation-maximization (EM). Specifically, we adopt directed graphs as our model for ontologies and use a generalized version of EM to arrive at a mapping between the nodes of the graphs. We exploit the structural and lexical similarity between the graphs, and improve on previous approaches by generating a many-one correspondence between the concept nodes. We provide preliminary experimental results in support of our method and outline its limitations.|Prashant Doshi,Christopher Thomas","65843|AAAI|2006|Strategy Variations in Analogical Problem Solving|While it is commonly agreed that analogy is useful in human problem solving, exactly how analogy can and should be used remains an intriguing problem. VanLehn () for instance argues that there are differences in how novices and experts use analogy, but the VanLehn and Jones () Cascade model does not implement these differences. This paper analyzes several variations in strategies for using analogy to explore possible sources of noviceexpert differences. We describe a series of ablation experiments on an expert model to examine the effects of strategy variations in using analogy in problem solving. We provide evidence that failing to use qualitative reasoning when encoding problems, being careless in validating analogical inferences, and not using multiple retrievals can degrade the efficiency of problem-solving.|Tom Y. Ouyang,Kenneth D. Forbus","65718|AAAI|2006|Incremental Least-Squares Temporal Difference Learning|Approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning, usually solved using temporal difference (TD) algorithms. In this paper we introduce a new variant of linear TD learning, called incremental least-squares TD learning, or iLSTD. This method is more data efficient than conventional TD algorithms such as TD() and is more computationally efficient than non-incremental least-squares TD methods such as LSTD (Bradtke & Barto  Boyan ). In particular, we show that the per-time-step complexities of iLSTD and TD() are O(n), where n is the number of features, whereas that of LSTD is O(n). This difference can be decisive in modern applications of reinforcement learning where the use of a large number features has proven to be an effective solution strategy. We present empirical comparisons, using the test problem introduced by Boyan (), in which iLSTD converges faster than TD() and almost as fast as LSTD.|Alborz Geramifard,Michael H. Bowling,Richard S. Sutton","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65930|AAAI|2006|Dual Search in Permutation State Spaces|Geometrical symmetries are commonly exploited to improve the efficiency of search algorithms. We introduce a new logical symmetry in permutation state spaces which we call duality. We show that each state has a dual state. Both states share important attributes and these properties can be used to improve search efficiency. We also present a new search algorithm, dual search, which switches between the original state and the dual state when it seems likely that the switch will improve the chances of a cutoff. The decision of when to switch is very important and several policies for doing this are investigated. Experimental results show significant improvements for a number of applications.|Uzi Zahavi,Ariel Felner,Robert Holte,Jonathan Schaeffer","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","65631|AAAI|2006|The Impact of Balancing on Problem Hardness in a Highly Structured Domain|Random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider random problem distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCPQWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness.|Carlos Ansótegui,Ramón Béjar,Cèsar Fernández,Carla P. Gomes,Carles Mateu","65842|AAAI|2006|Characterizing Data Complexity for Conjunctive Query Answering in Expressive Description Logics|Description Logics (DLs) are the formal foundations of the standard web ontology languages OWL-DL and OWL-Lite. In the Semantic Web and other domains, ontologies are increasingly seen also as a mechanism to access and query data repositories. This novel context poses an original combination of challenges that has not been addressed before (i) sufficient expressive power of the DL to capture common data modeling constructs (ii) well established and flexible query mechanisms such as Conjunctive Queries (CQs) (iii) optimization of inference techniques with respect to data size, which typically dominates the size of ontologies. This calls for investigating data complexity of query answering in expressive DLs. While the complexity of DLs has been studied extensively, data complexity has been characterized only for answering atomic queries, and was still open for answering CQs in expressive DLs. We tackle this issue and prove a tight CONP upper bound for the problem in SHIQ, as long as no transitive roles occur in the query. We thus establish that for a whole range of DLs from AL to SHIQ, answering CQs with no transitive roles has CONP-complete data complexity. We obtain our result by a novel tableaux-based algorithm for checking query entailment, inspired by the one in , but which manages the technical challenges of simultaneous inverse roles and number restrictions (which leads to a DL lacking the finite model property).|Magdalena Ortiz,Diego Calvanese,Thomas Eiter","65889|AAAI|2006|Focused Real-Time Dynamic Programming for MDPs Squeezing More Out of a Heuristic|Real-time dynamic programming (RTDP) is a heuristic search algorithm for solving MDPs. We present a modified algorithm called Focused RTDP with several improvements. While RTDP maintains only an upper bound on the long-term reward function, FRTDP maintains two-sided bounds and bases the output policy on the lower bound. FRTDP guides search with a new rule for outcome selection, focusing on parts of the search graph that contribute most to uncertainty about the values of good policies. FRTDP has modified trial termination criteria that should allow it to solve some problems (within ) that RTDP cannot. Experiments show that for all the problems we studied, FRTDP significantly outperforms RTDP and LRTDP, and converges with up to six times fewer backups than the state-of-the-art HDP algorithm.|Trey Smith,Reid G. Simmons"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","65746|AAAI|2006|Improving Approximate Value Iteration Using Memories and Predictive State Representations|Planning in partially-observable dynamical systems is a challenging problem, and recent developments in point-based techniques, such as Perseus significantly improve performance as compared to exact techniques. In this paper, we show how to apply these techniques to new models for non-Markovian dynamical systems called Predictive State Representatiolls (PSRs) and Memory-PSRs (mPSRs). PSRs and mPSRs are models of non-Markovian decision processes that differ from latent-variable models (e.g. HMMs, POMDPs) by representing state using only observable quantities. Further, mPSRs explicitly represent certain structural properties of the dynamical system that are also relevant to planning. We show how planning techniques can be adapted to leverage this structure to improve performance both in terms of execution time as well as quality of the resulting policy.|Michael R. James,Ton Wessling,Nikos A. Vlassis","65879|AAAI|2006|Identification of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models|This paper is concerned with estimating the effects of actions from causal assumptions, represented concisely as a directed graph, and statistical knowledge, given as a probability distribution. We provide a necessary and sufficient graphical condition for the cases when the causal effect of an arbitrary set of variables on another arbitrary set can be determined uniquely from the available information, as well as an algorithm which computes the effect whenever this condition holds. Furthermore, we use our results to prove completeness of do-calculus Pearl, , and a version of an identification algorithm in Tian,  for the same identification problem. Finally, we derive a complete characterization of semi-Markovian models in which all causal effects are identifiable.|Ilya Shpitser,Judea Pearl","65836|AAAI|2006|Reasoning about Partially Observed Actions|Partially observed actions are observations of action executions in which we are uncertain about the identity of objects, agents, or locations involved in the actions (e.g., we know that action move(o, x, y) occurred, but do not know o, y). Observed-Action Reasoning is the problem of reasoning about the world state after a sequence of partial observations of actions and states. In this paper we formalize Observed-Action Reasoning, prove intractability results for current techniques, and find tractable algorithms for STRIPS and other actions. Our new algorithms update a representation of all possible world states (the belief state) in logic using new logical constants for unknown objects. A straightforward application of this idea is incorrect, and we identify and add two key amendments. We also present successful experimental results for our algorithm in Blocks-world domains of varying sizes and in Kriegspiel (partially observable chess). These results are promising for relating sensors with symbols, partial-knowledge games, multi-agent decision making, and AI planning.|Megan Nance,Adam Vogel,Eyal Amir","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65711|AAAI|2006|Techniques for Generating Optimal Robust Plans when Temporal Uncertainty is Present|Planning under uncertainty has been well studied, but usually the uncertainty is in action outcomes. This work instead investigates uncertainty in the amount of time that actions require to execute. In addition to this temporal uncertainty, the problems being studied must have robust solution plans that are optimized based on an objective function. This thesis summary details two iterative approaches that have been used to solve these type of problems and discusses future work, including MDP approaches.|Janae N. Foss","65884|AAAI|2006|Learning Partially Observable Action Schemas|We present an algorithm that derives actions' effects and preconditions in partially observable, relational domains. Our algorithm has two unique features an expressive relational language, and an exact tractable computation. An action-schema language that we present permits learning of preconditions and effects that include implicit objects and unstated relationships between objects. For example, we can learn that replacing a blown fuse turns on all the lights whose switch is set to on. The algorithm maintains and outputs a relational-logical representation of all possible action-schema models after a sequence of executed actions and partial observations. Importantly, our algorithm takes polynomial time in the number of time steps and predicates. Time dependence on other domain parameters varies with the action-schema language. Our experiments show that the relational structure speeds up both learning and generalization, and outperforms propositional learning methods. It also allows establishing apriori-unknown connections between objects (e.g. light bulbs and their switches), and permits learning conditional effects in realistic and complex situations. Our algorithm takes advantage of a DAG structure that can be updated efficiently and preserves compactness of representation.|Dafna Shahaf,Eyal Amir","65830|AAAI|2006|DD Lite Efficient Incremental Search with State Dominance|This paper presents DD* Lite, an efficient incremental search algorithm for problems that can capitalize on state dominance. Dominance relationships between nodes are used to prune graphs in search algorithms. Thus, exploiting state dominance relationships can considerably speed up search problems in large state spaces, such as mobile robot path planning considering uncertainty, time, or energy constraints. Incremental search techniques are useful when changes can occur in the search graph, such as when re-planning paths for mobile robots in partially known environments. While algorithms such as D* and D* Lite are very efficient incremental search algorithms, they cannot be applied as formulated to search problems in which state dominance is used to prune the graph. DD* Lite extends D* Lite to seamlessly support reasoning about state dominance. It maintains the algorithmic simplicity and incremental search capability of D* Lite, while resulting in orders of magnitude increase in search efficiency in large state spaces with dominance. We illustrate the efficiency of DD* Lite with simulation results from applying the algorithm to a path planning problem with time and energy constraints. We also prove that DD* Lite is sound, complete, optimal, and efficient.|G. Ayorkor Mills-Tettey,Anthony Stentz,M. Bernardine Dias","65885|AAAI|2006|Learning Partially Observable Action Models Efficient Algorithms|We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 's of features (more than  states).|Dafna Shahaf,Allen Chang,Eyal Amir"],["65869|AAAI|2006|Identification and Evaluation of Weak Community Structures in Networks|Identifying intrinsic structures in large networks is a fundamental problem in many fields, such as engineering, social science and biology. In this paper, we are concerned with communities, which are densely connected sub-graphs in a network, and address two critical issues for finding community structures from large experimental data. First, most existing network clustering methods assume sparse networks and networks with strong community structures. In contrast, we consider sparse and dense networks with weak community structures. We introduce a set of simple operations that capture local neighborhood information of a node to identify weak communities. Second, we consider the issue of automatically determining the most appropriate number of communities, a crucial problem for all clustering methods. This requires to properly evaluate the quality of community structures. Built atop a function for network cluster evaluation by Newman and Girvan, we extend their work to weighted graphs. We have evaluated our methods on many networks of known structures, and applied them to analyze a collaboration network and a genetic network. The results showed that our methods can find superb community structures and correct numbers of communities. Comparing to the existing approaches, our methods performed significantly better on networks with weak community structures and equally well on networks with strong community structures.|Jianhua Ruan,Weixiong Zhang","65739|AAAI|2006|Solving MAP Exactly by Searching on Compiled Arithmetic Circuits|The MAP (maximum a posteriori hypothesis) problem in Bayesian networks is to find the most likely states of a set of variabls given partial evidence on the complement of that set. Standard structure-based inference methods for finding exact solutions to MAP, such as variable elimination and join-tree algorithms, have complexities that are exponential in the constrained treewidth of the network. A more recent algorithm, proposed by Park and Darwiche, is exponential only in the treewidth and has been shown to handle networks whose constrained treewidth is quite high. In this paper we present a new algorithm for exact MAP that is not necessarily limited in scalability even by the treewidth. This is achieved by leveraging recent advances in compilation of Bayesian networks into arithmetic circuits, which can circumvent treewidth-imposed limits by exploiting the local structure present in the network. Specifically, we implement a branch-and-bound search where the bounds are computed using linear-time operations on the compiled arithmetic circuit. On networks with local structure, we observe orders-of-magnitude improvements over the algorithm of Park and Darwiche. In particular, we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high treewidth.|Jinbo Huang,Mark Chavira,Adnan Darwiche","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","65832|AAAI|2006|Probabilistic Goal Recognition in Interactive Narrative Environments|Recent years have witnessed a growing interest in interactive narrative-centered virtual environments for education, training, and entertainment. Narrative environments dynamically craft engaging story-based experiences for users, who are themselves active participants in unfolding stories. A key challenge posed by interactive narrative is recognizing users' goals so that narrative planners can dynamically orchestrate plot elements and character actions to create rich, customized stories. In this paper we present an inductive approach to predicting users' goals by learning probabilistic goal recognition models. This approach has been evaluated in a narrative environment for the domain of microbiology in which the user plays the role of a medical detective solving a science mystery. An empirical evaluation of goal recognition based on n-gram models and Bayesian networks suggests that the models offer significant predictive power.|Bradford W. Mott,Sunyoung Lee,James C. Lester","65703|AAAI|2006|CUI Networks A Graphical Representation for Conditional Utility Independence|We introduce CUI networks, a compact graphical representation of utility functions over multiple attributes. CUI networks model multiattribute utility functions using the well-studied and widely applicable utility independence concept. We show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes can be used to calculate joint utility. We discuss aspects of elicitation, network construction, and optimization, and contrast our new representation with previous graphical preference modeling.|Yagil Engel,Michael P. Wellman","65741|AAAI|2006|Identifiability in Causal Bayesian Networks A Sound and Complete Algorithm|This paper addresses the problem of identifying causal effects from nonexperimental data in a causal Bayesian network, i.e., a directed acyclic graph that represents causal relationships. The identifiability question asks whether it is possible to compute the probability of some set of (effect) variables given intervention on another set of (intervention) variables, in the presence of non-observable (i.e., hidden or latent) variables. It is well known that the answer to the question depends on the structure of the causal Bayesian network, the set of observable variables, the set of effect variables, and the set of intervention variables. Our work is based on the work of Tian, Pearl, Huang, and Valtorta (Tian & Pearl a b  Huang & Valtorta a) and extends it. We show that the identify algorithm that Tian and Pearl define and prove sound for semi-Markovian models can be transfered to general causal graphs and is not only sound, but also complete. This result effectively solves the identifiability question for causal Bayesian networks that Pearl posed in  (Pearl ), by providing a sound and complete algorithm for identifiability.|Yimin Huang,Marco Valtorta","65785|AAAI|2006|Performing Incremental Bayesian Inference by Dynamic Model Counting|The ability to update the structure of a Bayesian network when new data becomes available is crucial for building adaptive systems. Recent work by Sang, Beame, and Kautz (AAAI ) demonstrates that the well-known Davis-Putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in Bayesian networks with high density and width. In this paper, we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple runs on a series of problems with similar structure. This allows us to perform Bayesian inference incrementally as the structure of the network changes. Experimental results show that our approach yields significant improvements over the previous model counting approaches on multiple challenging Bayesian network instances.|Wei Li 0002,Peter van Beek,Pascal Poupart","65750|AAAI|2006|A Bayesian Network for Outbreak Detection and Prediction|Health care officials are increasingly concerned with knowing early whether an outbreak of a particular disease is unfolding. We often have daily counts of some variable that are indicative of the number of individuals in a given community becoming sick each day with a particular disease. By monitoring these daily counts we can possibly detect an outbreak in an early stage. A number of classical time-series methods have been applied to outbreak detection based on monitoring daily counts of some variables. These classical methods only give us an alert as to whether there may be an outbreak. They do not predict properties of the outbreak such as its size, duration, and how far we are into the outbreak. Knowing the probable values of these variables can help guide us to a cost-effective decision that maximizes expected utility. Bayesian networks have become one of the most prominent architectures for reasoning under uncertainty in artificial intelligence. We present an intelligent system, implemented using a Bayesian network, which not only detects an outbreak, but predicts its size and duration, and estimates how far we are into the outbreak. We show results of investigating the performance of the system using simulated outbreaks based on real outbreak data. These results indicate that the system shows promise of being able to predict properties of an outbreak.|Xia Jiang,Garrick L. Wallstrom","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65854|AAAI|2006|Sound and Efficient Inference with Probabilistic and Deterministic Dependencies|Reasoning with both probabilistic and deterministic dependencies is important for many real-world problems, and in particular for the emerging field of statistical relational learning. However, probabilistic inference methods like MCMC or belief propagation tend to give poor results when deterministic or near-deterministic dependencies are present, and logical ones like satisfiability testing are inapplicable to probabilistic ones. In this paper we propose MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. MC-SAT is based on Markov logic, which defines Markov networks using weighted clauses in first-order logic. From the point of view of MCMC, MC-SAT is a slice sampler with an auxiliary variable per clause, and with a satisfiability-based method for sampling the original variables given the auxiliary ones. From the point of view of satisfiability, MCSAT wraps a procedure around the SampleSAT uniform sampler that enables it to sample from highly non-uniform distributions over satisfying assignments. Experiments on entity resolution and collective classification problems show that MC-SAT greatly outperforms Gibbs sampling and simulated tempering over a broad range of problem sizes and degrees of determinism.|Hoifung Poon,Pedro Domingos"]]},"title":{"entropy":5.576342776117046,"topics":["algorithm for, and tree, for data, for, optimal for, for sets, constraint, for constraint, for information, for performance, and constraint, for the, tree, for and, distributed, partial, logic, csp, elicitation, weighted","planning with, methods for, probabilistic and, and with, learning with, and methods, efficient for, efficient with, for and, search with, with, search, search and, efficient, action, for with, and, preferences, goal, networks","the web, semantic web, the and, the, using web, for web, for the, for semantic, using semantic, web services, semantic and, the semantic, extraction the, and web, language and, the services, and systems, knowledge and, using the, from","reinforcement learning, human-robot interaction, for learning, systems for, learning, for reinforcement, learning with, for models, models, transfer, towards, human, relational, computing, incremental, representation, solving, decision, modeling, function","for information, for the, logic, when, elicitation, description, programming, partial","for data, for, csp","and","search, support, machine, planning, for","from, agent, sense, extraction, for, and","the, interactive, real-time, games, text","for learning, reinforcement learning, for reinforcement, learning with, learning, relational, modeling, function","systems for, value, iteration, for, computing"],"ranking":[["65676|AAAI|2006|Constraint Symmetry and Solution Symmetry|Symmetry in constraint satisfaction problems (CSPs) has been considered in two fundamentally different ways as an operation preserving the solutions of a CSP instance, or as an operation preserving the constraints. To reflect these two views, we define solution symmetry and constraint symmetry. We discuss how these concepts are related and show that some CSP instances have many more solution symmetries than constraint symmetries.|David A. Cohen,Peter Jeavons,Christopher Jefferson,Karen E. Petrie,Barbara M. Smith","65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","65728|AAAI|2006|Analysis of Privacy Loss in Distributed Constraint Optimization|Distributed Constraint Optimization (DCOP) is rapidly emerging as a prominent technique for multi agent coordination. However, despite agent privacy being a key motivation for applying DCOPs in many applications, rigorous quantitative evaluations of privacy loss in DCOP algorithms have been lacking. Recently, Maheswaran et al.  introduced a framework for quantitative evaluations of privacy in DCOP algorithms, showing that some DCOP algorithms lose more privacy than purely centralized approaches and questioning the motivation for applying DCOPs. This paper addresses the question of whether state-of-the art DCOP algorithms suffer from a similar shortcoming by investigating several of the most efficient DCOP algorithms, including both DPOP and ADOPT. Furthermore, while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions (e.g. constraint-graph topology, asynchrony, message-contents), this paper examines the privacy aspect of such decisions, providing an improved understanding of privacy-efficiency tradeoffs.|Rachel Greenstadt,Jonathan P. Pearce,Milind Tambe","65943|AAAI|2006|Mechanisms for Partial Information Elicitation The Truth but Not the Whole Truth|We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios.|Aviv Zohar,Jeffrey S. Rosenschein","65853|AAAI|2006|ODPOP An Algorithm for OpenDistributed Constraint Optimization|We propose ODPOP, a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domains (Faltings & Macho-Gonzalez ). The ODPOP algorithm explores the same search space as the dynamic programming algorithm DPOP (Petcu & Faltings b) or ADOPT (Modi et at. ). but does so in an incremental, best-first fashion suitable for open problems. ODPOP has several advantages over DPOP. First, it uses messages whose size only grows linearly with the treewidth of the problem. Second, by letting agents explore values in a best-first order, it avoids incurring always the worst case complexity as DPOP, and on average it saves a significant amount of computation and information exchange. To show the merits of our approach, we report on experiments with practically sized distributed meeting scheduling problems on a multiagent system.|Adrian Petcu,Boi Faltings","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65891|AAAI|2006|Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms|In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs. The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms. We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.|Tran Cao Son,Enrico Pontelli,Phan Huy Tu"],["65931|AAAI|2006|An Efficient Algorithm for Local Distance Metric Learning|Learning application-specific distance metrics from labeled data is critical for both statistical classification and information retrieval. Most of the earlier work in this area has focused on finding metrics that simultaneously optimize compactness and separability in a global sense. Specifically, such distance metrics attempt to keep all of the data points in each class close together while ensuring that data points from different classes are separated. However, particularly when classes exhibit multimodal data distributions, these goals conflict and thus cannot be simultaneously satisfied. This paper proposes a Local Distance Metric (LDM) that aims to optimize local compactness and local separability. We present an efficient algorithm that employs eigenvector analysis, and bound optimization to learn the LDM from training data in a probabilistic framework. We demonstrate that LDM achieves significant improvements in both classification and retrieval accuracy compared to global distance learning and kernel-based KNN.|Liu Yang,Rong Jin,Rahul Sukthankar,Yi Liu","65945|AAAI|2006|Heuristic Search and Information Visualization Methods for School Redistricting|We describe an application of AI search and information visualization techniques to the problem of school redistricting, in which students are assigned to home schools within a county or school district. This is a multicriteria optimization problem in which competing objectives must be considered, such as school capacity, busing costs, and socioeconomic distribution. Because of the complexity of the decision-making problem, tools are needed to help end users generate, evaluate, and compare alternative school assignment plans. A key goal of our research is to aid users in finding multiple qualitatively different redistricting plans that represent different tradeoffs in the decision space. We present heuristic search methods that can be used to find a set of qualitatively different plans, and give empirical results of these search methods on population data from the school district of Howard County, Maryland. We show the resulting plans using novel visualization methods that we have developed for summarizing and comparing alternative plans.|Marie desJardins,Blazej Bulka,Ryan Carr,Andrew Hunt,Priyang Rathod,Penny Rheingans","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","65792|AAAI|2006|PPCP Efficient Probabilistic Planning with Clear Preferences in Partially-Known Environments|For most real-world problems the agent operates in only partially-known environments. Probabilistic planners can reason over the missing information and produce plans that take into account the uncertainty about the environment. Unfortunately though, they can rarely scale up to the problems that are of interest in real-world. In this paper, however, we show that for a certain subset of problems we can develop a very efficient probabilistic planner. The proposed planner, called PPCP, is applicable to the problems for which it is clear what values of the missing information would result in the best plan. In other words, there exists a clear preference for the actual values of the missing information. For example, in the problem of robot navigation in partially-known environments it is always preferred to find out that an initially unknown location is traversable rather than not. The planner we propose exploits this property by using a series of deterministic A*-like searches to construct and refine a policy in anytime fashion. On the theoretical side, we show that once converged, the policy is guaranteed to be optimal under certain conditions. On the experimental side, we show the power of PPCP on the problem of robot navigation in partially-known terrains. The planner can scale up to very large environments with thousands of initially unknown locations. We believe that this is several orders of magnitude more unknowns than what the current probabilistic planners developed for the same problem can handle. Also, despite the fact that the problem we experimented on in general does not satisfy the conditions for the solution optimality, PPCP still produces the solutions that are nearly always optimal.|Maxim Likhachev,Anthony Stentz","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65939|AAAI|2006|A Breadth-First Approach to Memory-Efficient Graph Search|Recent work shows that the memory requirements of A* and related graph-search algorithms can be reduced substantially by only storing nodes that are on or near the search frontier, using special techniques to prevent node regeneration, and recovering the solution path by a divide-and-conquer technique. When this approach is used to solve graph-search problems with unit edge costs, we have shown that a breadth-first search strategy can be more memory-efficient than a best-first strategy. We provide an overview of our work using this approach, which we call breadth-first heuristic search.|Rong Zhou,Eric A. Hansen","65830|AAAI|2006|DD Lite Efficient Incremental Search with State Dominance|This paper presents DD* Lite, an efficient incremental search algorithm for problems that can capitalize on state dominance. Dominance relationships between nodes are used to prune graphs in search algorithms. Thus, exploiting state dominance relationships can considerably speed up search problems in large state spaces, such as mobile robot path planning considering uncertainty, time, or energy constraints. Incremental search techniques are useful when changes can occur in the search graph, such as when re-planning paths for mobile robots in partially known environments. While algorithms such as D* and D* Lite are very efficient incremental search algorithms, they cannot be applied as formulated to search problems in which state dominance is used to prune the graph. DD* Lite extends D* Lite to seamlessly support reasoning about state dominance. It maintains the algorithmic simplicity and incremental search capability of D* Lite, while resulting in orders of magnitude increase in search efficiency in large state spaces with dominance. We illustrate the efficiency of DD* Lite with simulation results from applying the algorithm to a path planning problem with time and energy constraints. We also prove that DD* Lite is sound, complete, optimal, and efficient.|G. Ayorkor Mills-Tettey,Anthony Stentz,M. Bernardine Dias","65885|AAAI|2006|Learning Partially Observable Action Models Efficient Algorithms|We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 's of features (more than  states).|Dafna Shahaf,Allen Chang,Eyal Amir","65854|AAAI|2006|Sound and Efficient Inference with Probabilistic and Deterministic Dependencies|Reasoning with both probabilistic and deterministic dependencies is important for many real-world problems, and in particular for the emerging field of statistical relational learning. However, probabilistic inference methods like MCMC or belief propagation tend to give poor results when deterministic or near-deterministic dependencies are present, and logical ones like satisfiability testing are inapplicable to probabilistic ones. In this paper we propose MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. MC-SAT is based on Markov logic, which defines Markov networks using weighted clauses in first-order logic. From the point of view of MCMC, MC-SAT is a slice sampler with an auxiliary variable per clause, and with a satisfiability-based method for sampling the original variables given the auxiliary ones. From the point of view of satisfiability, MCSAT wraps a procedure around the SampleSAT uniform sampler that enables it to sample from highly non-uniform distributions over satisfying assignments. Experiments on entity resolution and collective classification problems show that MC-SAT greatly outperforms Gibbs sampling and simulated tempering over a broad range of problem sizes and degrees of determinism.|Hoifung Poon,Pedro Domingos","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings"],["65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65668|AAAI|2006|Using Semantics to Identify Web Objects|Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.|Nathanael Chambers,James F. Allen,Lucian Galescu,Hyuckchul Jung,William Taysom","65767|AAAI|2006|Novel Relationship Discovery Using Opinions Mined from the Web|This paper proposes relationship discovery models using opinions mined from the Web instead of only conventional collocations. Web opinion mining extracts subjective information from the Web for specific targets, summarizes the polarity and the degree of the information, and tracks the development over time. Targets which gain similar opinionated tendencies within a period of time may be correlated. This paper detects event bursts from the tracking plots of opinions, and decides the strength of the relationship using the coverage of the plots. Companies are selected as the experimental targets. A total of ,, economics-related documents are collected from  Web sources between August  and May  for experiments. Models that discover relations are then proposed and compared on the basis of their performance. There are three types of models, collocation-based, opinion-based, and integration models, and respectively, four, two and two variants of each type. For evaluation, company pairs which demonstrate similar oscillation of stock prices are considered correlated and are selected as the gold standard. The results show that collocation-based models and opinion-based models are complementary, and the integration models perform the best. The top ,  and  answers discovered by the best integration model achieve precision rates of , . and ., respectively.|Lun-Wei Ku,Hsiu-Wei Ho,Hsin-Hsi Chen","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","65882|AAAI|2006|RL-CD Dealing with Non-Stationarity in Reinforcement Learning|This student abstract describes ongoing investigations regarding an approach for dealing with non-stationarity in reinforcement learning (RL) problems. We briefly propose and describe a method for managing multiple partial models of the environment and comment previous results which show that the proposed mechanism has better convergence times comparing to standard RL algorithms. Current efforts include the development of a more robust approach, capable of dealing with noisy environments, and also investigations regarding the possibility of using partial models in order to aliviate learning problems in systems with an explosive number of states.|Bruno Castro da Silva,Eduardo W. Basso,Ana L. C. Bazzan,Paulo Martins Engel","65761|AAAI|2006|Learning Systems of Concepts with an Infinite Relational Model|Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.|Charles Kemp,Joshua B. Tenenbaum,Thomas L. Griffiths,Takeshi Yamada,Naonori Ueda","65885|AAAI|2006|Learning Partially Observable Action Models Efficient Algorithms|We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 's of features (more than  states).|Dafna Shahaf,Allen Chang,Eyal Amir","65909|AAAI|2006|Reinforcement Learning with Human Teachers Evidence of Feedback and Guidance with Implications for Learning Performance|As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacherrobot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance.|Andrea Lockerd Thomaz,Cynthia Breazeal","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["65701|AAAI|2006|Forgetting and Conflict Resolving in Disjunctive Logic Programming|We establish a declarative theory of forgetting for disjunctive logic programs. The suitability of this theory is justified by a number of desirable properties. In particular, one of our results shows that our notion of forgetting is completely captured by the classical forgetting. A transformation-based algorithm is also developed for computing the result of forgetting. We also provide an analysis of computational complexity. As an application of our approach, a fairly general framework for resolving conflicts in inconsistent knowledge bases represented by disjunctive logic programs is defined. The basic idea of our framework is to weaken the preferences of each agent by forgetting certain knowledge that causes inconsistency. In particular, we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming.|Thomas Eiter,Kewen Wang","65822|AAAI|2006|Finding Maximally Satisfiable Terminologies for the Description Logic ALC|For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.|Thomas Andreas Meyer,Kevin Lee,Richard Booth,Jeff Z. Pan","65720|AAAI|2006|On the Update of Description Logic Ontologies at the Instance Level|We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.|Giuseppe De Giacomo,Maurizio Lenzerini,Antonella Poggi,Riccardo Rosati","65659|AAAI|2006|Factored Planning How When and When Not|Automated domain factoring, and planning methods that utilize them, have long been of interest to planning researchers. Recent work in this area yielded new theoretical insight and algorithms, but left many questions open How to decompose a domain into factors How to work with these factors And whether and when decomposition-based methods are useful This paper provides theoretical analysis that answers many of these questions it proposes a novel approach to factored planning proves its theoretical superiority over previous methods provides insight into how to factor domains and uses its novel complexity results to analyze when factored planning is likely to perform well, and when not. It also establishes the key role played by the domain's causal graph in the complexity analysis of planning algorithms.|Ronen I. Brafman,Carmel Domshlak","65791|AAAI|2006|A Modular Action Description Language|\"Toy worlds\" involving actions, such as the blocks world and the Missionaries and Cannibals puzzle, are often used by researchers in the areas of commonsense reasoning and planning to illustrate and test their ideas. We would like to create a datahase of general-purpose knowledge about actions that encodes common features of many action domains of this kind. in the same way as abstract algebra and topology represent common features of specific number systems. This paper is a report on the first stage of this project--the design of an action description language in which this database will be written The new language is an extension of the action language C+. Its main distinctive feature is the possibility of referring to other action descriptions in the definition of a new action domain.|Vladimir Lifschitz,Wanwan Ren","65653|AAAI|2006|On the Difficulty of Modular Reinforcement Learning for Real-World Partial Programming|In recent years there has been a great deal of interest in \"modular reinforcement learning\" (MRL). Typically, problems are decomposed into concurrent subgoals, allowing increased scalability and state abstraction. An arbitrator combines the subagents' preferences to select an action. In this work, we contrast treating an MRL agent as a set of subagents with the same goal with treating an MRL agent as a set of subagents who may have different, possibly conflicting goals. We argue that the latter is a more realistic description of real-world problems, especially when building partial programs. We address a range of algorithms for single-goal MRL, and leveraging social choice theory, we present an impossibility result for applications of such algorithms to multigoal MRL. We suggest an alternative formulation of arbitration as scheduling that avoids the assumptions of comparability of preference that are implicit in single-goal MRL. A notable feature of this formulation is the explicit codification of the tradeoffs between the subproblems. Finally, we introduce ABL, a language that encapsulates many of these ideas.|Sooraj Bhat,Charles Lee Isbell Jr.,Michael Mateas","65943|AAAI|2006|Mechanisms for Partial Information Elicitation The Truth but Not the Whole Truth|We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios.|Aviv Zohar,Jeffrey S. Rosenschein","65663|AAAI|2006|Preference Elicitation and Generalized Additive Utility|Any automated decision support software must tailor its actions or recommendations to the preferences of different users. Thus it requires some representation of user preferences as well as a means of eliciting or otherwise learning the preferences of the specific user on whose behalf it is acting. While additive preference models offer a compact representation of multiattribute utility functions, and ease of elicitation, they are often overly restrictive. The more flexible generalized additive independence (GAI) model maintains much of the intuitive nature of additive models, but comes at the cost of much more complex elicitation. In this article, we summarize the key contributions of our earlier paper (UAI ) (a) the first elaboration of the semantic foundations of GAI models that allows one to engage in preference elicitation using local queries over small subsets of attributes rather than global queries over full outcomes and (b) specific procedures for Bayesian preference elicitation of the parameters of a GAI model using such local queries.|Darius Braziunas,Craig Boutilier","65944|AAAI|2006|Robust Mechanisms for Information Elicitation|We study information elicitation mechanisms in which a principal agent attempts to elicit the private information of other agents using a carefully selected payment scheme based on proper scoring rules. Scoring rules, like many other mechanisms set in a probabilistic environment, assume that all participating agents share some common belief about the underlying probability of events. In real-life situations however, underlying distributions are not known precisely, and small differences in beliefs about these distributions may alter agent behavior under the prescribed mechanism.We propose designing elicitation mechanisms in a manner that will be robust to small changes in belief. We show how to algorithmically design such mechanisms in polynomial time using tools of stochastic programming and convex programming, and discuss implementation issues for multiagent scenarios.|Aviv Zohar,Jeffrey S. Rosenschein","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith"],["65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65937|AAAI|2006|A Direct Evolutionary Feature Extraction Algorithm for Classifying High Dimensional Data|Among various feature extraction algorithms, those based on genetic algorithms are promising owing to their potential parallelizability and possible applications in large scale and high dimensional data classification. However, existing genetic algorithm based feature extraction algorithms are either limited in searching optimal projection basis vectors or costly in both time and space complexities and thus not directly applicable to high dimensional data. In this paper, a direct evolutionary feature extraction algorithm is proposed for classifying high-dimensional data. It constructs projection basis vectors using the linear combination of the basis of the search space and the technique of orthogonal complement. It also constrains the search space when seeking for the optimal projection basis vectors. It evaluates individuals according to the classification performance on a subset of the training samples and the generalization ability Df the projection basis vectors represented by the individuals. We compared the proposed algorithm with some representative feature extraction algorithms in face recognition, including the evolutionary pursuit algorithm, Eigenfaces, and Fisherfaces. The results on the widely-used Yale and ORL face databases show that the proposed algorithm has an excellent performance in classification while reducing the space complexity by an order of magnitude.|Qijun Zhao,David Zhang,Hongtao Lu","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","65666|AAAI|2006|B-ROC Curves for the Assessment of Classifiers over Imbalanced Data Sets|The class imbalance problem appears to be ubiquitous to a large portion of the machine learning and data mining communities. One of the key questions in this setting is how to evaluate the learning algorithms in the case of class imbalances. In this paper we introduce the Bayesian Receiver Operating Characteristic (B-ROC) curves, as a set of tradeoff curves that combine in an intuitive way, the variables that are more relevant to the evaluation of classifiers over imbalanced data sets. This presentation is based on section  of (Crdenas, Baras, & Seamon ).|Alvaro A. Cárdenas,John S. Baras","65642|AAAI|2006|Classifying Learner Engagement through Integration of Multiple Data Sources|Intelligent tutoring systems (ITS) can provide effective instruction, but learners do not always use such systems effectively. In the present study, high school students' action sequences with a mathematics ITS were machine-classified into five finite-state machines indicating guessing strategies, appropriate help use, and independent problem solving over % of problem events were categorized. Students were grouped via cluster analyses based on self reports of motivation. Motivation grouping predicted ITS strategic approach better than prior math achievement (as rated by classroom teachers). Learners who reported being disengaged in math were most likely to exhibit appropriate help use while working with the ITS, relative to average and high motivation learners. The results indicate that learners can readily report their motivation state and that these data predict how learners interact with the ITS.|Carole R. Beal,Lei Qu,Hyokyeong Lee","65867|AAAI|2006|Closest Pairs Data Selection for Support Vector Machines|This paper presents data selection procedures for support vector machines (SVM). The purpose of data selection is to reduce the dataset by eliminating as many non support vectors (non-SVs) as possible. Based on the fact that support vectors (SVs) are those vectors close to the decision boundary, data selection keeps only the closest pair vectors of opposite classes. The selected dataset will replace the full dataset as the training component for any standard SVM algorithm.|Chaofan Sun","65826|AAAI|2006|Phoebus A System for Extracting and Integrating Data from Unstructured and Ungrammatical Sources|With the proliferation of online classifieds and auctions comes a new need to meaningfully search and organize the items for sale. However, since the seller's item descriptions are not structured and do not conform to a standard set of values (think \"Chevy\" versus \"Chevrolet\"), searching and organizing this data is difficult. This paper describes a working demonstration of the Phoebus system which uses both record linkage and information extraction to parse out the meaningful attributes of an item description and assign them standard values. This allows the data to be sorted, searched and linked to other data sources where standard values for the attributes are required to link the sources together.|Matthew Michelson,Craig A. Knoblock","65842|AAAI|2006|Characterizing Data Complexity for Conjunctive Query Answering in Expressive Description Logics|Description Logics (DLs) are the formal foundations of the standard web ontology languages OWL-DL and OWL-Lite. In the Semantic Web and other domains, ontologies are increasingly seen also as a mechanism to access and query data repositories. This novel context poses an original combination of challenges that has not been addressed before (i) sufficient expressive power of the DL to capture common data modeling constructs (ii) well established and flexible query mechanisms such as Conjunctive Queries (CQs) (iii) optimization of inference techniques with respect to data size, which typically dominates the size of ontologies. This calls for investigating data complexity of query answering in expressive DLs. While the complexity of DLs has been studied extensively, data complexity has been characterized only for answering atomic queries, and was still open for answering CQs in expressive DLs. We tackle this issue and prove a tight CONP upper bound for the problem in SHIQ, as long as no transitive roles occur in the query. We thus establish that for a whole range of DLs from AL to SHIQ, answering CQs with no transitive roles has CONP-complete data complexity. We obtain our result by a novel tableaux-based algorithm for checking query entailment, inspired by the one in , but which manages the technical challenges of simultaneous inverse roles and number restrictions (which leads to a DL lacking the finite model property).|Magdalena Ortiz,Diego Calvanese,Thomas Eiter","65723|AAAI|2006|Embedding Heterogeneous Data Using Statistical Models|Embedding algorithms are a method for revealing low dimensional structure in complex data. Most embedding algorithms are designed to handle objects of a single type for which pairwise distances are specified. Here we describe a method for embedding objects of different types (such as authors and terms) into a single common Euclidean space based on their co-occurrence statistics. The joint distributions of the heterogenous objects are modeled as exponentials of squared Euclidean distances in a low-dimensional embedding space. This construction links the problem to convex optimization over positive semidefinite matrices. We quantify the performance of our method on two text datasets, and show that it consistently and significantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling and correspondence analysis.|Amir Globerson,Gal Chechik,Fernando Pereira,Naftali Tishby","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings"],[],["65765|AAAI|2006|Detecting Spam Blogs A Machine Learning Approach|Weblogs or blogs are an important new way to publish information, engage in discussions, and form communities on the Internet. The Blogosphere has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from splogs - false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features can be used to detect splogs. We present an evaluation of learned models and their utility to blog search engines systems that employ techniques differing from those of conventional web search engines.|Pranam Kolari,Akshay Java,Tim Finin,Tim Oates,Anupam Joshi","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65772|AAAI|2006|Controlled Search over Compact State Representations in Nondeterministic Planning Domains and Beyond|Two of the most efficient planners for planning in nondeterministic domains are MBP and ND-SHOP. MBP achieves its efficiency by using Binary Decision Diagrams (BDDs) to represent sets of states that share some common properties, so it can plan for all of these states simultaneously. ND-SHOP achieves its efficiency by using HTN task decomposition to focus the search. In some environments, ND-SHOP runs exponentially faster than MBP, and in others the reverse is true. In this paper, we discuss the following  We describe how to combine ND-SHOP's HTNs with MBP's BDDs. Our new planning algorithm, YoYo, performs task decompositions over classes of states that are represented as BDDs. In our experiments, YoYo easily outperformed both MBP and ND-SHOP, often by several orders of magnitude.  HTNs are just one of several techniques that are originally developed for classical planning domains and that can be adapted to work in nondeterministic domains. By combining those techniques with a BDD representation, it should be possible to get great speedups just as we did here. We discuss how these same ideas can be generalized for use in several other research areas, such as planning with Markov Decision Processes, synthesizing controllers for hybrid systems, and composing Semantic Web Services.|Ugur Kuter,Dana S. Nau","65670|AAAI|2006|Case-Based Reasoning for General Electric Appliance Customer Support|A case-based reasoning system was created to support customers who purchased appliances from General Electric. When a customer calls General Electric for help, a call-taker uses the system to diagnose the problem and step the customer through its solution. The system has been in use by  call-takers since . It has resulted in a  percent increase in the probability the customer's problem can be solved over the phone. This has greatly improved customer satisfaction and saved GE $. million between  and  from reduced cost of visits of field service technician to customer's homes.|William Cheetham","65867|AAAI|2006|Closest Pairs Data Selection for Support Vector Machines|This paper presents data selection procedures for support vector machines (SVM). The purpose of data selection is to reduce the dataset by eliminating as many non support vectors (non-SVs) as possible. Based on the fact that support vectors (SVs) are those vectors close to the decision boundary, data selection keeps only the closest pair vectors of opposite classes. The selected dataset will replace the full dataset as the training component for any standard SVM algorithm.|Chaofan Sun","65729|AAAI|2006|Predicting Electricity Distribution Feeder Failures Using Machine Learning Susceptibility Analysis|A Machine Learning (ML) System known as ROAMS (Ranker for Open-Auto Maintenance Scheduling) was developed to create failure-susceptibility rankings for almost one thousand .kV-kV energy distribution feeder cables that supply electricity to the boroughs of New York City. In Manhattan, rankings are updated every  minutes and displayed on distribution system operators' screens. Additionally, a separate system makes seasonal predictions of failure susceptibility. These feeder failures, known as \"Open Autos\" or \"OAs,\" are a significant maintenance problem. A year's sustained research has led to a system that demonstrates high accuracy % of the feeders that actually failed over the summer of  were in the % of feeders ranked as most at-risk. By the end of the summer, the  most susceptible feeders as ranked by the ML system were accounting for up to % of all OAs that subsequently occurred each day. The system's algorithm also identifies the factors underlying failures which change over time and with varying conditions (especially temperature), providing insights into the operating properties and failure causes in the feeder system.|Philip Gross,Albert Boulanger,Marta Arias,David L. Waltz,Philip M. Long,Charles Lawson,Roger Anderson,Matthew Koenig,Mark Mastrocinque,William Fairechio,John A. Johnson,Serena Lee,Frank Doherty,Arthur Kressner","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","65637|AAAI|2006|AI Support for Building Cognitive Models|Cognitive modeling techniques provide a way of evaluating user interface designs, based on what is known about human cognitive strengths and limitations. Cognitive modelers face a tradeoff, however more detailed models require disproportionately more time and effort to develop than coarser models. In this paper we describe a system, GA, that automatically produces translations from abstract GOMS models into more detailed ACT-R models. GA demonstrates how even simple AI techniques can facilitate the construction of cognitive models and suggests new directions for improving modeling tools.|Robert St. Amant,Sean P. McBride,Frank E. Ritter","65698|AAAI|2006|Cost-Optimal External Planning|This paper considers strategies for external memory based optimal planning. An external breadth-first search exploration algorithm is devised that is guaranteed to find the costoptimal solution. We contribute a procedure for finding the upper bound on the locality of the search in planning graphs that dictates the number of layers that have to be kept to avoid re-openings. We also discuss an external variant of Enforced Hill Climbing. Using relaxed-plan heuristic without helpful-action pruning we have been able to perform large explorations on metric planning problems, providing better plan lengths than have been reported earlier. A novel approach to plan reconstruction in external setting with linear IO complexity is proposed. We provide external exploration results on some recently proposed planning domains.|Stefan Edelkamp,Shahid Jabbar","65928|AAAI|2006|Robust Support Vector Machine Training via Convex Outlier Ablation|One of the well known risks of large margin training methods, such as boosting and support vector machines (SVMs), is their sensitivity to outliers. These risks are normally mitigated by using a soft margin criterion, such as hinge loss, to reduce outlier sensitivity. In this paper, we present a more direct approach that explicitly incorporates outlier suppression in the training process. In particular, we show how outlier detection can be encoded in the large margin training principle of support vector machines. By expressing a convex relaxation of the joint training problem as a semide finite program, one can use this approach to robustly train a support vector machine while suppressing outliers. We demonstrate that our approach can yield superior results to the standard soft margin approach in the presence of outliers.|Linli Xu,Koby Crammer,Dale Schuurmans"],["65627|AAAI|2006|QUICR-Learning for Multi-Agent Coordination|Coordinating multiple agents that need to perform a sequence of actions to maximize a system level reward requires solving two distinct credit assignment problems. First, credit must be assigned for an action taken at time step t that results in a reward at time step t  t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first credit assignment problem is typically addressed with temporal difference methods such as Q-learning. The second credit assignment problem is typically addressed by creating custom reward functions. To address both credit assignment problems simultaneously, we propose the \"Q Updates with Immediate Counterfactual Rewards-learning\" (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods.|Adrian K. Agogino,Kagan Tumer","65937|AAAI|2006|A Direct Evolutionary Feature Extraction Algorithm for Classifying High Dimensional Data|Among various feature extraction algorithms, those based on genetic algorithms are promising owing to their potential parallelizability and possible applications in large scale and high dimensional data classification. However, existing genetic algorithm based feature extraction algorithms are either limited in searching optimal projection basis vectors or costly in both time and space complexities and thus not directly applicable to high dimensional data. In this paper, a direct evolutionary feature extraction algorithm is proposed for classifying high-dimensional data. It constructs projection basis vectors using the linear combination of the basis of the search space and the technique of orthogonal complement. It also constrains the search space when seeking for the optimal projection basis vectors. It evaluates individuals according to the classification performance on a subset of the training samples and the generalization ability Df the projection basis vectors represented by the individuals. We compared the proposed algorithm with some representative feature extraction algorithms in face recognition, including the evolutionary pursuit algorithm, Eigenfaces, and Fisherfaces. The results on the widely-used Yale and ORL face databases show that the proposed algorithm has an excellent performance in classification while reducing the space complexity by an order of magnitude.|Qijun Zhao,David Zhang,Hongtao Lu","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","65648|AAAI|2006|Opinion Extraction and Summarization on the Web|The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sources containing such opinions, e.g., product reviews, forums, discussion groups. and blogs. Techniques are now being developed to exploit these sources to help organizations and individuals to gain such important information easily and quickly. In this paper, we first discuss several aspects of the problem in the AI context, and then present some results of our existing work published in KDD- and WWW-.|Minqing Hu,Bing Liu","65754|AAAI|2006|An End-to-End Supervised Target-Word Sense Disambiguation System|We present an extensible supervised Target-Word Sense Disambiguation system that leverages upon GATE (General Architecture for Text Engineering), NSP (Ngram Statistics Package) and WEKA (Waikato Environment for Knowledge Analysis) to present an end-to-end solution that integrates feature identification, feature extraction, preprocessing and classification.|Mahesh Joshi,Serguei V. S. Pakhomov,Ted Pedersen,Richard Maclin,Christopher G. Chute","65932|AAAI|2006|A Unified Knowledge Based Approach for Sense Disambiguation and Semantic Role Labeling|In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our approach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.|Peter Z. Yeh,Bruce W. Porter,Ken Barker","65651|AAAI|2006|Performance Evaluation Methods for the Trading Agent Competition|This paper proposes a novel method to characterize the performance of autonomous agents in the Trading Agent Competition for Supply Chain Management (TAC-SCM). We create benchmarking tools that manipulate market environments to control the conditions and provide guidelines to test trading agents. Using these tools, we show how developers can inspect their agents and unveil behaviors that might otherwise have gone undiscovered.|Brett Borghetti,Eric Sodomka","65902|AAAI|2006|Simultaneous Team Assignment and Behavior Recognition from Spatio-Temporal Agent Traces|This paper addresses the problem of activity recognition for physically-embodied agent teams. We define team activity recognition as the process of identifying team behaviors from traces of agent positions over time for many physical domains, military or athletic, coordinated team behaviors create distinctive spatio-temporal patterns that can be used to identify low-level action sequences. This paper focuses on the novel problem of recovering agent-to-team assignments for complex team tasks where team composition, the mapping of agents into teams, changes over time. Without a priori knowledge of current team assignments, the behavior recognition problem is challenging since behaviors are characterized by the aggregate motion of the entire team and cannot generally be determined by observing the movements of a single agent in isolation. To handle this problem, we introduce a new algorithm, Simultaneous Team Assignment and Behavior Recognition (STABR), that generates behavior annotations from spatio-temporal agent traces. The proposed approach is able to perform accurate team behavior recognition without an exhaustive search over the combinatorial space of potential team assignments, as demonstrated on several scenarios of simulated military maneuvers.|Gita Sukthankar,Katia P. Sycara","65732|AAAI|2006|From Centralized to Distributed Selective Overhearing|Overhearing is an approach for monitoring open, distributed, multi-agent systems by listening to the routine communications taking place within them. Previous investigations of overhearing assumed that all inter-agent communications are accessible to a single overhearing agent. However, as multi-agent systems grow both in size and distribution two problems arise. First, in large-scale settings, an overhearing agent cannot monitor all agents and their conversations, and must therefore be selective in carefully choosing its targets. Second, a single overhearer would encounter difficulties overhearing agents acting in a geographically-distributed environment. This paper tackles these challenges by addressing distributed teams of overhearing agents involved in selective overhearing. Building on prior work on centralized selective overhearing, we consider the consequences of transitioning from overhearing teams working in a centrally-coordinated manner to distributed overhearing teams. In doing so, we distinguish the various factors influencing the level of distribution within these teams and determine their importance in terms of effective selective overhearing.|Gery Gutnik,Gal A. Kaminka"],["65683|AAAI|2006|ScriptEase - Motivational Behaviors for Interactive Characters in Computer Role-Playing Games|ScriptEase is a tool that allows authors with no programming experience to create interactive stories for computer role-playing games. Instead of writing scripting code manually, game authors select design patterns that encapsulate frequent game scenarios, creating stories at a higher level of abstraction and being shielded from the underlying scripting language. ScriptEase has been extended to support behavior patterns that generate ambient behaviors for non-player characters. This demonstration shows how ScriptEase creates intricate non-player character scripts to generate compelling and engaging character behaviors. We demonstrate our ScriptEase motivational ambient and PC-interactive behaviors for a guard character using BioWare Corp.'s Neverwinter Nights game.|Maria Cutumisu,Duane Szafron,Jonathan Schaeffer,Kevin Waugh,Curtis Onuczko,Jeff Siegel,Allan Schumacher","65721|AAAI|2006|A Competitive Texas Holdem Poker Player via Automated Abstraction and Real-Time Equilibrium Computation|We present a game theory-based heads-up Texas Hold'em poker player, GS. To overcome the computational obstacles stemming from Texas Hold'em's gigantic game tree, the player employs our automated abstraction techniques to reduce the complexity of the strategy computations. Texas Hold'em consists of four betting rounds. Our player solves a large linear program (offline) to compute strategies for the abstracted first and second rounds. After the second betting round, our player updates the probability of each possible hand based on the observed betting actions in the first two rounds as well as the revealed cards. Using these updated probabilities, our player computes in real-time an equilibrium approximation for the last two abstracted rounds. We demonstrate that our player, which incorporates very little poker-specific knowledge, is competitive with leading poker-playing programs which incorporate extensive domain knowledge, as well as with advanced human players.|Andrew Gilpin,Tuomas Sandholm","65693|AAAI|2006|On the Difficulty of Achieving Equilibrium in Interactive POMDPs|We analyze the asymptotic behavior of agents engaged in an infinite horizon partially observable stochastic game as formalized by the interactive POMDP framework. We show that when agents' initial beliefs satisfy a truth compatibility condition, their behavior converges to a subjective -equilibrium in a finite time, and subjective equilibrium in the limit. This result is a generalization of a similar result in repeated games, to partially observable stochastic games. However, it turns out that the equilibrating process is difficult to demonstrate computationally because of the difficulty in coming up with initial beliefs that are both natural and satisfy the truth compatibility condition. Our results, therefore, shed some negative light on using equilibria as a solution concept for decision making in partially observable stochastic games.|Prashant Doshi,Piotr J. Gmytrasiewicz","65895|AAAI|2006|Real-Time Evolution of Neural Networks in the NERO Video Game|A major goal for AI is to allow users to interact with agents that learn in real time, making new kinds of interactive simulations, training applications, and digital entertainment possible. This paper describes such a learning technology, called real-time NeuroEvolution of Augmenting Topologies (rtNEAT), and describes how rtNEAT was used to build the NeuroEvolving Robotic Operatives (NERO) video game. This game represents a new genre of machine learning games where the player trains agents in real time to perform challenging tasks in a virtual environment. Providing laymen the capability to effectively train agents in real time with no prior knowledge of AI or machine learning has broad implications, both in promoting the field of AI and making its achievements accessible to the public at large.|Kenneth O. Stanley,Bobby D. Bryant,Igor Karpov,Risto Miikkulainen","65896|AAAI|2006|Real-Time Interactive Learning in the NERO Video Game|In the NeuroEvolving Robotic Operatives (NERO) video game, the player trains a team of virtual robots for combat against other players' teams. The virtual robots learn in real time through interacting with the player. Since NERO was originally released in June, , it has been downloaded over , times, appeared on Slashdot, and won several honors. The real-time NeuroEvolution of Augmenting Topologies (rt-NEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, drives the robots' learning, making possible this entirely new genre of video game. The live demo will show how agents in NERO adapt in real time as they interact with the player. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.|Kenneth O. Stanley,Igor Karpov,Risto Miikkulainen,Aliza Gold","65861|AAAI|2006|Demonstration of Music Plus One - A Real-Time System for Automatic Orchestral Accompaniment|We demonstrate a system that creates a real-time accompaniment for a live musician performing a nonimprovisatory piece of music. The system listens to the live player by performing a hidden Markov model analysis of the player's acoustic signal. A belief network uses this information, a musical score, and past rehearsals, to create a sequence of evolving predictions for future note-onsets in the soloist and accompaniment. These predictions are used to guide the time-stretched resynthesis of prerecorded orchestral audio using a phase vocoder.|Christopher Raphael","65661|AAAI|2006|On Strictly Competitive Multi-Player Games|We embark on an initial study of a new class of strategic (normal-form) games, so-called ranking games, in which the payoff to each agent solely depends on his position in a ranking of the agents induced by their actions. This definition is motivated by the observation that in many strategic situations such as parlor games, competitive economic scenarios, and some social choice settings, players are merely interested in performing optimal relative to their opponents rather than in absolute measures. A simple but important subclass of ranking games are single-winner games where in any outcome one agent wins and all others lose. We investigate the computational complexity of a variety of common game-theoretic solution concepts in ranking games and deliver hardness results for iterated weak dominance and mixed Nash equilibria when there are more than two players and pure Nash equilibria when the number of players is unbounded. This dashes hope that multi-player ranking games can be solved efficiently, despite the structural restrictions of these games.|Felix Brandt,Felix A. Fischer,Yoav Shoham","65748|AAAI|2006|A Polynomial-Time Algorithm for Action Graph Games|Action-Graph Games (AGGs) (Bhat & Leyton-Brown ) are a fully expressive game representation which can compactly express strict and context-specific independence and anonymity structure in players' utility functions. We present an efficient algorithm for computing expected payoffs under mixed strategy profiles. This algorithm runs in time polynomial in the size of the AGG representation (which is itself polynomial in the number of players when the in-degree of the action graph is bounded). We also present an extension to the AGG representation which allows us to compactly represent a wider variety of structured utility functions.|Albert Xin Jiang,Kevin Leyton-Brown","65889|AAAI|2006|Focused Real-Time Dynamic Programming for MDPs Squeezing More Out of a Heuristic|Real-time dynamic programming (RTDP) is a heuristic search algorithm for solving MDPs. We present a modified algorithm called Focused RTDP with several improvements. While RTDP maintains only an upper bound on the long-term reward function, FRTDP maintains two-sided bounds and bases the output policy on the lower bound. FRTDP guides search with a new rule for outcome selection, focusing on parts of the search graph that contribute most to uncertainty about the values of good policies. FRTDP has modified trial termination criteria that should allow it to solve some problems (within ) that RTDP cannot. Experiments show that for all the problems we studied, FRTDP significantly outperforms RTDP and LRTDP, and converges with up to six times fewer backups than the state-of-the-art HDP algorithm.|Trey Smith,Reid G. Simmons","65734|AAAI|2006|Negation Contrast and Contradiction in Text Processing|This paper describes a framework for recognizing contradictions between multiple text sources by relying on three forms of linguistic information (a) negation (b) antonymy and (c) semantic and pragmatic information associated with the discourse relations. Two views of contradictions are considered, in which a novel method of recognizing contrast and of finding antonymies are described. Contradictions are used for informing fusion operators in question answering. Our experiments show promising results for the detection of contradictions.|Sanda M. Harabagiu,Andrew Hickl,V. Finley Lacatusu"],["65776|AAAI|2006|kFOIL Learning Simple Relational Kernels|A novel and simple combination of inductive logic programming with kernel methods is presented. The kFOIL algorithm integrates the well-known inductive logic programming system FOIL with kernel methods. The feature space is constructed by leveraging FOIL search for a set of relevant clauses. The search is driven by the performance obtained by a support vector machine based on the resulting kernel. In this way, kFOIL implements a dynamic propositionalization approach. Both classification and regression tasks can be naturally handled. Experiments in applying kFOIL to well-known benchmarks in chemoinformatics show the promise of the approach.|Niels Landwehr,Andrea Passerini,Luc De Raedt,Paolo Frasconi","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","65911|AAAI|2006|From Pigeons to Humans Grounding Relational Learning in Concrete Examples|We present a cognitive model that bridges work in analogy and category learning. The model, Building Relations through Instance Driven Gradient Error Shifting (BRIDGES), extends ALCOVE, an exemplar-based connectionist model of human category learning (Kruschke, ). Unlike ALCOVE which is limited to featural or spatial representations, BRIDGES can appreciate analogical relationships between stimuli and stored predicate representations of exemplars. Like ALCOVE, BRIDGES learns to shift attention over the course of learning to reduce error and, in the process, alters its notion of similarity. A shift toward relational sources of similarity allows BRIDGES to display what appears to be an understanding of abstract domains, when in fact performance is driven by similarity-based structural alignment (i.e., analogy) to stored exemplars. Supportive simulations of animal, infant, and adult learning are provided. We end by considering possible extensions of BRIDGES suitable for computationally demanding applications.|Marc T. Tomlinson,Bradley C. Love","65653|AAAI|2006|On the Difficulty of Modular Reinforcement Learning for Real-World Partial Programming|In recent years there has been a great deal of interest in \"modular reinforcement learning\" (MRL). Typically, problems are decomposed into concurrent subgoals, allowing increased scalability and state abstraction. An arbitrator combines the subagents' preferences to select an action. In this work, we contrast treating an MRL agent as a set of subagents with the same goal with treating an MRL agent as a set of subagents who may have different, possibly conflicting goals. We argue that the latter is a more realistic description of real-world problems, especially when building partial programs. We address a range of algorithms for single-goal MRL, and leveraging social choice theory, we present an impossibility result for applications of such algorithms to multigoal MRL. We suggest an alternative formulation of arbitration as scheduling that avoids the assumptions of comparability of preference that are implicit in single-goal MRL. A notable feature of this formulation is the explicit codification of the tradeoffs between the subproblems. Finally, we introduce ABL, a language that encapsulates many of these ideas.|Sooraj Bhat,Charles Lee Isbell Jr.,Michael Mateas","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","65882|AAAI|2006|RL-CD Dealing with Non-Stationarity in Reinforcement Learning|This student abstract describes ongoing investigations regarding an approach for dealing with non-stationarity in reinforcement learning (RL) problems. We briefly propose and describe a method for managing multiple partial models of the environment and comment previous results which show that the proposed mechanism has better convergence times comparing to standard RL algorithms. Current efforts include the development of a more robust approach, capable of dealing with noisy environments, and also investigations regarding the possibility of using partial models in order to aliviate learning problems in systems with an explosive number of states.|Bruno Castro da Silva,Eduardo W. Basso,Ana L. C. Bazzan,Paulo Martins Engel","65761|AAAI|2006|Learning Systems of Concepts with an Infinite Relational Model|Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.|Charles Kemp,Joshua B. Tenenbaum,Thomas L. Griffiths,Takeshi Yamada,Naonori Ueda","65909|AAAI|2006|Reinforcement Learning with Human Teachers Evidence of Feedback and Guidance with Implications for Learning Performance|As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacherrobot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance.|Andrea Lockerd Thomaz,Cynthia Breazeal","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65746|AAAI|2006|Improving Approximate Value Iteration Using Memories and Predictive State Representations|Planning in partially-observable dynamical systems is a challenging problem, and recent developments in point-based techniques, such as Perseus significantly improve performance as compared to exact techniques. In this paper, we show how to apply these techniques to new models for non-Markovian dynamical systems called Predictive State Representatiolls (PSRs) and Memory-PSRs (mPSRs). PSRs and mPSRs are models of non-Markovian decision processes that differ from latent-variable models (e.g. HMMs, POMDPs) by representing state using only observable quantities. Further, mPSRs explicitly represent certain structural properties of the dynamical system that are also relevant to planning. We show how planning techniques can be adapted to leverage this structure to improve performance both in terms of execution time as well as quality of the resulting policy.|Michael R. James,Ton Wessling,Nikos A. Vlassis","65641|AAAI|2006|A Value Theory of Meta-Learning Algorithms|We use game theory to analyze meta-learning algorithms. The objective of meta-learning is to determine which algorithm to apply on a given task. This is an instance of a more general problem that consists of allocating knowledge consumers to learning producers. Solving this general problem in the field of meta-learning yields solutions for related fields such as information retrieval and recommender systems.|Abraham Bagherjeiran","65788|AAAI|2006|Detecting Disjoint Inconsistent Subformulas for Computing Lower Bounds for Max-SAT|Many lower bound computation methods for branch and bound Max-SAT solvers can be explained as procedures that search for disjoint inconsistent subformulas in the Max-SAT instance under consideration. The difference among them is the technique used to detect inconsistencies. In this paper, we define five new lower bound computation methods two of them are based on detecting inconsistencies via a unit propagation procedure that propagates unit clauses using an original ordering the other three add an additional level of forward look-ahead based on detecting failed literals. Finally, we provide empirical evidence that the new lower bounds are of better quality than the existing lower bounds, as well as that a solver with our new lower bounds greatly outperforms some of the best performing state-of-the-art Max-SAT solvers on Max-SAT, Max-SAT, and Max-Cut instances.|Chu Min Li,Felip Manyà,Jordi Planes","65786|AAAI|2006|Incremental Least Squares Policy Iteration for POMDPs|We present a new algorithm, called incremental least squares policy iteration (ILSPI), for finding the infinite-horizon stationary policy for partially observable Markov decision processes (POMDPs). The ILSPI algorithm computes a basis representation of the infinite-horizon value function by minirnizing the square of Bellman residual and performs policy improvenent in reachable belief states. A number of optimal basis functions are determined by the algorithm to minimize the Bellman residual incrementally, via efficient computations. We show that, by using optimally determined basis functions, the policy can be improved successively on a set of most probable belief points sampled from the reachable belief set. As the ILSPI is based on belief sample points, it represents a point-based policy iteration method. The results on four benchmark problems show that the ILSPI compares competitively to its value-iteration counterparts in terms of both performance and computational efficiency.|Hui Li,Xuejun Liao,Lawrence Carin","65743|AAAI|2006|Representing Systems with Hidden State|We discuss the problem of finding a good state representation in stochastic systems with observations. We develop a duality theory that generalizes existing work in predictive state representations as well as automata theory. We discuss how this theoretical framework can be used to build learning algorithms, approximate planning algorithms as well as to deal with continuous observations.|Christopher Hundt,Prakash Panangaden,Joelle Pineau,Doina Precup","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","65855|AAAI|2006|An Efficient Way of Breaking Value Symmetries|Several methods for breaking value symmetries have been proposed recently in the constraint programming community. They can be used in conjunction with variable symmetry breaking methods. However, this combination does not break all symmetries in general. We present a combination of lex constraints and element constrants that can be used to break all combinations of variable and value symmetries. It is the first time to our knowledge that it is possible to break all combinations of value and variable symmetries by adding constraints. This method is quite efficient when the number of symmetries is not too large, as shown by experiments using graceful graph problems. We also present a new global constraint that deals with the case where there are too many value symmetries. Experiments show that this is highly effective.|Jean-François Puget","65795|AAAI|2006|Functional Value Iteration for Decision-Theoretic Planning with General Utility Functions|We study how to find plans that maximize the expected total utility for a given MDP, a planning objective that is important for decision making in high-stakes domains. The optimal actions can now depend on the total reward that has been accumulated so far in addition to the current state. We extend our previous work on functional value iteration from one-switch utility functions to all utility functions that can be approximated with piecewise linear utility functions (with and without exponential tails) by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function. Functional value iteration does not maintain a value for every state but a value function that maps the total reward that has been accumulated so far into a value. We describe how functional value iteration represents these value functions in finite form, how it performs dynamic programming by manipulating these representations and what kinds of approximation guarantees it is able to make. We also apply it to a probabilistic blocksworld problem, a standard test domain for decision-theoretic planners.|Yaxin Liu,Sven Koenig","65899|AAAI|2006|WikiRelate Computing Semantic Relatedness Using Wikipedia|Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by integrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the performance of an NLP application processing naturally occurring texts.|Michael Strube,Simone Paolo Ponzetto"]]}}