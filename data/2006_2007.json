{"abstract":{"entropy":6.884470943839964,"topics":["machine learning, recent years, speech recognition, artificial intelligence, widely used, web page, support vector, natural language, time series, semantic web, learning, speech enhancement, data mining, web mining, reinforcement learning, learning system, learning classifier, search engine, classifier system, data management","markov decision, plays role, markov processes, decision processes, description logic, partially observable, agents, multi-agent system, constraint satisfaction, knowledge base, real world, autonomous agents, schema mapping, longest common, social network, decision making, wide range, observable pomdps, satisfaction csp, observable markov","genetic algorithm, algorithm, evolutionary algorithm, genetic programming, present algorithm, optimization problem, particle swarm, novel approach, neural network, algorithm problem, algorithm optimization, search algorithm, present approach, solving problem, problem, evolutionary computation, paper algorithm, search heuristic, consider problem, adaptive algorithm","orthogonal division, division multiplexing, frequency division, orthogonal frequency, orthogonal multiplexing, wireless network, frequency multiplexing, scheme, paper system, low-density parity-check, zero-correlation zone, present novel, sequence zone, parity-check codes, paper based, motion estimation, sensor network, class zero-correlation, proposes, sequence zero-correlation","widely used, natural language, time series, data, data management, present data, data integration, petri nets, large data, data system, model data, management system, algorithm data, series data, data time, simulation model, efficient parsing, present corpus-based, data relational, becoming increasingly","machine learning, classifier system, learning system, reinforcement learning, learning, quality software, learning classifier, present learning, learning data, learning task, learning model, classification semi-supervised, improve performance, supervised learning, learning improve, data classification, unsupervised semi-supervised, systolic array, interactive configuration, software","markov decision, markov processes, decision processes, partially observable, decision making, reinforcement learning, observable pomdps, observable markov, decision support, partially pomdps, partially markov, markov model, observable decision, agents actions, actions uncertainty, markov pomdps, markov mdps, observable processes, processes mdps, partially decision","plays role, social network, important applications, important role, important task, important problem, important, query data, data items, addresses problem, real-world problem, multidimensional data, plays important, top-k query, data points, large social, query function, query scoring, consider online, allows users","solving problem, games player, combinatorial problem, search problem, deals problem, problem, solution problem, search space, optimization problem, problem number, algorithm problem, consider problem, combinatorial optimization, function approximation, study problem, design problem, present problem, problem optimal, games playing, learning problem","neural network, problem graph, graph vertices, graph edges, genetic programming, consider problem, bayesian network, problem network, spanning tree, graph, genetic cartesian, given vertices, spanning vertex, graph vertex, hierarchical bayesian, directed graph, programming cartesian, paper graph, problem programming, problem given","present novel, image based, proposes based, paper present, letter scheme, present system, present scheme, proposes novel, letter proposes, image, scheme image, novel image, paper image, jpeg image, proposes, watermarking image, letter, identification based, letter present, present image","low-density parity-check, parity-check codes, paper system, ultra wideband, low-density codes, ldpc decoder, ultra uwb, parity-check matrix, wideband uwb, ldpc codes, sense disambiguation, cancer detection, speech noise, decoding codes, noise environment, early cancer, schedule decoder, speech coder, feedback system, paper decoder"],"ranking":[["57927|GECCO|2007|Improving the human readability of features constructed by genetic programming|The use of machine learning techniques to automatically analyse data for information is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to pre-process data before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract more information about the domain.|Matthew Smith,Larry Bull","44120|IEICE Transations|2007|A Machine Learning Approach for an Indonesian-English Cross Language Question Answering System|We have built a CLQA (Cross Language Question Answering) system for a source language with limited data resources (e.g. Indonesian) using a machine learning approach. The CLQA system consists of four modules question analyzer, keyword translator, passage retriever and answer finder. We used machine learning in two modules, the question classifier (part of the question analyzer) and the answer finder. In the question classifier, we classify the EAT (Expected Answer Type) of a question by using SVM (Support Vector Machine) method. Features for the classification module are basically the output of our shallow question parsing module. To improve the classification score, we use statistical information extracted from our Indonesian corpus. In the answer finder module, using an approach different from the common approach in which answer is located by matching the named entity of the word corpus with the EAT of question, we locate the answer by text chunking the word corpus. The features for the SVM based text chunking process consist of question features, word corpus features and similarity scores between the word corpus and the question keyword. In this way, we eliminate the named entity tagging process for the target document. As for the keyword translator module, we use an Indonesian-English dictionary to translate Indonesian keywords into English. We also use some simple patterns to transform some borrowed English words. The keywords are then combined in boolean queries in order to retrieve relevant passages using IDF scores. We first conducted an experiment using , questions (about % are used as the test data) obtained from  Indonesian college students. We next conducted a similar experiment using the NTCIR (NII Test Collection for IR Systems)  CLQA task by translating the English questions into Indonesian. Compared to the Japanese-English and Chinese-English CLQA results in the NTCIR , we found that our system is superior to others except for one system that uses a high data resource employing  dictionaries. Further, a rough comparison with two other Indonesian-English CLQA systems revealed that our system achieved higher accuracy score.|Ayu Purwarianti,Masatoshi Tsuchiya,Seiichi Nakagawa","58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","43694|IEICE Transations|2006|An Anomaly Intrusion Detection System Based on Vector Quantization|Machine learning and data mining algorithms are increasingly being used in the intrusion detection systems (IDS), but their performances are laggard to some extent especially applied in network based intrusion detection the larger load of network traffic monitoring requires more efficient algorithm in practice. In this paper, we propose and design an anomaly intrusion detection (AID) system based on the vector quantization (VQ) which is widely used for data compression and high-dimension multimedia data index. The design procedure optimizes the performance of intrusion detection by jointly accounting for accurate usage profile modeling by the VQ codebook and fast similarity measures between feature vectors to reduce the computational cost. The former is just the key of getting high detection rate and the later is the footstone of guaranteeing efficiency and real-time style of intrusion detection. Experiment comparisons to other related researches show that the performance of intrusion detection is improved greatly.|Jun Zheng,Mingzeng Hu","44230|IEICE Transations|2007|Zero-Anaphora Resolution in Chinese Using Maximum Entropy|In this paper, we propose a learning classifier based on maximum entropy (ME) for resolving zero-anaphora in Chinese text. Besides regular grammatical, lexical, positional and semantic features motivated by previous research on anaphora resolution, we develop two innovative Web-based features for extracting additional semantic information from the Web. The values of the two features can be obtained easily by querying the Web using some patterns. Our study shows that our machine learning approach is able to achieve an accuracy comparable to that of state-of-the-art systems. The Web as a knowledge source can be incorporated effectively into the ME learning framework and significantly improves the performance of our approach.|Jing Peng,Kenji Araki","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","65682|AAAI|2006|Comparative Experiments on Sentiment Classification for Online Product Reviews|Evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc. This paper looks at a simplified version of the problem classifying online product reviews into positive and negative classes. We discuss a series of experiments with different machine learning algorithms in order to experimentally evaluate various trade-offs, using approximately K product reviews from the web.|Hang Cui,Vibhu O. Mittal,Mayur Datar","44334|IEICE Transations|2007|Incremental Language Modeling for Automatic Transcription of Broadcast News|In this paper, we address the task of incremental language modeling for automatic transcription of broadcast news speech. Daily broadcast news naturally contains new words that are not in the lexicon of the speech recognition system but are important for downstream applications such as information retrieval or machine translation. To recognize those new words, the lexicon and the language model of the speech recognition system need to be updated periodically. We propose a method of estimating a list of words to be added to the lexicon based on some time-series text data. The experimental results on the RT Broadcast News data and other TV audio data showed that this method provided an impressive and stable reduction in both out-of-vocabulary rates and speech recognition word error rates.|Katsutoshi Ohtsuki,Long Nguyen","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka"],["16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","16658|IJCAI|2007|AEMS An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs|Solving large Partially Observable Markov Decision Processes (POMDPs) is a complex task which is often intractable. A lot of effort has been made to develop approximate offline algorithms to solve ever larger POMDPs. However, even state-of-the-art approaches fail to solve large POMDPs in reasonable time. Recent developments in online POMDP search suggest that combining offline computations with online computations is often more efficient and can also considerably reduce the error made by approximate policies computed offline. In the same vein, we propose a new anytime online search algorithm which seeks to minimize, as efficiently as possible, the error made by an approximate value function computed offline. In addition, we show how previous online computations can be reused in following time steps in order to prevent redundant computations. Our preliminary results indicate that our approach is able to tackle large state space and observation space efficiently and under real-time constraints.|St√©phane Ross,Brahim Chaib-draa","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,Fran√ßois Charpillet","66182|AAAI|2007|Situated Conversational Agents|A Situated Conversational Agent (SCA) is an agent that engages in dialog about the context within which it is embedded. Situated dialog is characterized by its deep connection to the embedding context, and the precise cross-timing of linguistic and non-linguistic actions. This paper describes initial research into the construction of an SCA that engages in dialog about collaborative physical tasks, in which agents engage in dialog with the joint goal of manipulating the physical context in some manner. Constructing an SCA that can interact naturally in such tasks requires an agent with the ability to interleave planning, action, and observation while operating in a partially observable environment. Consequently, I propose to model an SCA as a Partially Observable Markov Decision Process (POMDP).|William Thompson","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|√?ngel M. P√©rez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ort√≠z-Garc√≠a,Antonio Portilla-Figueras","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","57666|GECCO|2006|A new approach for shortest path routing problem by random key-based GA|In this paper, we propose a Genetic Algorithm (GA) approach using a new paths growth procedure by the random key-based encoding for solving Shortest Path Routing (SPR) problem. And we also develop a combined algorithm by arithmetical crossover, swap mutation, and immigration operator as genetic operators. Numerical analysis for various scales of SPR problems shows the proposed random key-based genetic algorithm (rkGA) approach has a higher search capability that enhanced rate of reaching optimal solutions and improve computation time than other GA approaches using different genetic representation methods.|Mitsuo Gen,Lin Lin","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","58240|GECCO|2007|Symbiotic tabu search|Recombination in the Genetic Algorithm (GA) is supposed to extract the component characteristics from two parents and reassemble them in different combinations hopefully producing an offspring that has the good characteristics of both parents. Symbiotic Combination is formerly introduced as an alternative for sexual recombination operator to overcome the need of explicit design of recombination operators in GA all. This paper presents an optimization algorithm based on using this operator in Tabu Search. The algorithm is benchmarked on two problem sets and is compared with standard genetic algorithm and symbiotic evolutionary adaptation model, showing success rates higher than both cited algorithms.|Ramin Halavati,Saeed Bagheri Shouraki,Bahareh Jafari Jashmi,Mojdeh Jalali Heravi","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius"],["43211|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Construction Using a Cyclic Hadamard Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone (hereinafter binary ZCA sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCA sequence set by using a cyclic difference set and a collection of mutually orthogonal complementary sets.|Takafumi Hayashi","43950|IEICE Transations|2007|Double Window Cancellation and Combining for OFDM in Time-Invariant Large Delay Spread Channel|In a time-invariant wireless channel, the multipath that exceeds the cyclic prefix (CP) or the guard interval (GI) causes orthogonal frequency division multiplexing (OFDM) systems to hardly achieve high data rate transmission due to the inter-symbol interference (ISI) and the inter-carrier interference (ICI). In this paper the new canceller scheme, named as Double Window Cancellation and Combining (DWCC) is proposed. It includes the entire symbol interval, delayed by multipath as a signal processing window and intends to improve the performance by combining the double windows that can be formed by the pre-and post-ISI cancellation and reconstruction to the received OFDM symbol interfered by the multipath exceeding the guard interval. The proposed scheme has two algorithm structures of the DWCC-I and -II which are distinguished by the operational sequence (Symbol-wise or Group-wise) to the OFDM symbols of the received packet and by the selection of the processing window in the iterative decision feedback processing. Since the performance of the canceller is dependant on the equalization, particularly on the initial equalization, the proposed schemes operate with the time and frequency domain equalizer in the initial and the iterative symbol detection, respectively. For the verification of the proposed schemes, each scheme is evaluated in the turbo coded OFDM for low (QPSK) and high level modulation systems (QAM, QAM), and compared with the conventional canceller with respect to the performance and computational complexity. As a result, the proposed schemes do not have an error floor even for QAM in a severe frequency selective channel.|JunHwan Lee,Yoshihisa Kishiyama,Tomoaki Ohtsuki,Masao Nakagawa","43816|IEICE Transations|2007|MLSE Detection with Blind Linear Prediction and Subcarriers Interpolation for DSTBC-OFDM Systems|This paper proposes low-complexity blind detection for orthogonal frequency division multiplexing (OFDM) systems with the differential space-time block code (DSTBC) under time-varying frequency-selective Rayleigh fading. The detector employs the maximum likelihood sequence estimation (MLSE) in cooperation with the blind linear prediction (BLP), of which prediction coefficients are determined by the method of Lagrange multipliers. Interpolation of channel frequency responses is also applied to the detector in order to reduce the complexity. A complexity analysis and computer simulations demonstrate that the proposed detector can reduce the complexity to about a half, and that the complexity reduction causes only a loss of  dB in average EbN at BER of - when the prediction order and the degree of polynomial approximation are  and , respectively.|Seree Wanichpakdeedecha,Kazuhiko Fukawa,Hiroshi Suzuki,Satoshi Suyama","43974|IEICE Transations|2007|Zero-Correlation Zone Sequence Set Construction Using an Even-Perfect Sequence and an Odd-Perfect Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone sequence set. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary zcz sequence set by using an arbitrary pair of an even-perfect sequence and an odd-perfect sequence. The proposed zcz sequence set reaches the theoretical upper bound of the member size of the sequence set.|Takafumi Hayashi","43874|IEICE Transations|2007|An Integrated Sequence Construction of Binary Zero-Correlation Zone Sequences|The present paper introduces an integrated construction of binary sequences having a zero-correlation zone. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The proposed method enables more flexible design of the binary zero-correlation zone sequence set with respect to its member size, length, and width of zero-correlation zone. Several previously reported sequence construction methods of binary zero-correlation zone sequence sets can be explained as special cases of the proposed method.|Takafumi Hayashi","43279|IEICE Transations|2006|Families of Sequence Pairs with Zero Correlation Zone|A family of sequences with zero correlation zone, which is shortly called a ZCZ set, can provide CDMA system without co-channel interference nor influence of multipath. This paper presents two types of ZCZ sets of non-binary sequence pairs, which achieve the upper bound of family size for length and zero correlation zone. One, which is produced by use of a perfect complementary pair and an orthogonal code, can change zero correlation zone, while the upper bound is kept. The other, which is generated by use of a newly defined orthogonal pair and an orthogonal code, can offer such CDMA system as a binary ZCZ set seems to be used.|Shinya Matsufuji","43043|IEICE Transations|2006|Joint Estimation of Frequency Offset and Channel Frequency Response Using EM Algorithm for OFDM Systems|Orthogonal Frequency Division Multiplexing (OFDM) systems are very sensitive to the frequency offset of the local oscillator at the receiver while the symbol timing offset can be absorbed in the guard interval. For the same reason, estimation of the frequency characteristics, needed for OFDM to be adapted to the frequency selective fading, can only be carried out conventionally after the frequency offset has been compensated. And accurate estimation of large frequency offset certainly requires high precision estimate of the frequency characteristics. In this paper, we propose a new joint estimation method of the frequency offset and the channel frequency response using an Expectation-Maximization (EM) algorithm for OFDM systems. The proposed algorithm overcomes the limitation of the thus far proposed algorithm. By computer simulations, we show the proposed algorithm provides estimation accuracy close to its lower bound in a wide range of the frequency offset.|Masahiro Fujii,Makoto Itami,Kohji Itoh","43376|IEICE Transations|2006|IQ Imbalance Compensation Scheme for MB-OFDM Using Transmission Diversity|Currently, multiband orthogonal frequency division multiplexing (MB-OFDM) is considered to be one of the modulation schemes of UWB and is being actively investigated. It is necessary to provide low-cost receivers for consumers to receive wide support for the MB-OFDM system. Such receivers can be achieved by utilizing direct-conversion architecture. Direct-conversion architecture suffers from IQ imbalance. IQ imbalance causes intercarrier interference (ICI) in the demodulated signals. In this paper, a new scheme of IQ imbalance compensation using transmit diversity is proposed. This scheme enables the system to achieve frequency diversity and simultaneously compensates for the influence of IQ imbalance. It is shown that the performance of the proposed scheme is better than that of the conventional IQ imbalance compensation scheme.|Yohei Kato,Tsuyoshi Ikuno,Yukitoshi Sanada","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","44046|IEICE Transations|2007|Reduced-Complexity Detection for DPC-OFTDMA System Enhanced by Multi-Layer MIMO-OFDM in Wireless Multimedia Communications|During these years, we have been focusing on developing ultra high-data-rate wireless access systems for future wireless multimedia communications. One of such kind of systems is called DPC-OFTDMA (dynamic parameter controlled orthogonal frequency and time division multiple access) which targets at beyond  Mbps data rate. In order to support higher data rates, e.g., several hundreds of Mbps or even Gbps for future wireless multimedia applications (e.g., streaming video and file transfer), it is necessary to enhance DPC-OFTDMA system based on MIMO-OFDM (multiple-input multiple-output orthogonal frequency division multiplexing) platform. In this paper, we propose an enhanced DPC-OFTDMA system based on Multi-Layer MIMO-OFDM scheme which combines both diversity and multiplexing in order to exploit potentials of both techniques. The performance investigation shows the proposed scheme has better performance than its counterpart based on full-multiplexing MIMO-OFDM scheme. In addition to the Exhaustive Detection (EXD) scheme which applies the same detection algorithm on each subcarrier independently, we propose the Reduced-Complexity Detection (RCD) scheme. The complexity reduction is achieved by exploiting the suboptimal Layer Detection Order and subcarrier correlation. The simulation results show that huge complexity can be reduced with very small performance loss, by using the proposed detection scheme. For example, .% complexity can be cut off with only . dB performance loss for the    enhanced DPC-OFTDMA system.|Ming Lei,Hiroshi Harada"],["80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80628|VLDB|2006|Efficiently Linking Text Documents with Relevant Structured Information|Faced with growing knowledge management needs, enterprises are increasingly realizing the importance of interlinking critical business information distributed across structured and unstructured data sources. We present a novel system, called EROCS, for linking a given text document with relevant structured data. EROCS views the structured data as a predefined set of \"entities\" and identifies the entities that best match the given document. EROCS also embeds the identified entities in the document, effectively creating links between the structured data and segments within the document. Unlike prior approaches, EROCS identifies such links even when the relevant entity is not explicitly mentioned in the document. EROCS uses an efficient algorithm that performs this task keeping the amount of information retrieved from the database at a minimum. Our evaluation shows that EROCS achieves high accuracy with reasonable overheads.|Venkatesan T. Chakaravarthy,Himanshu Gupta,Prasan Roy,Mukesh K. Mohania","80638|VLDB|2006|iDM A Unified and Versatile Data Model for Personal Dataspace Management|Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML . Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace  of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) , , . This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles","80797|VLDB|2007|SOR A Practical System for Ontology Storage Reasoning and Search|Ontology, an explicit specification of shared conceptualization, has been increasingly used to define formal data semantics and improve data reusability and interoperability in enterprise information systems. In this paper, we present and demonstrate SOR (Scalable Ontology Repository), a practical system for ontology storage, reasoning, and search. SOR uses Relational DBMS to store ontologies, performs inference over them, and supports SPARQL language for query. Furthermore, a faceted search with relationship navigation is designed and implemented for ontology search. This demonstration shows how to efficiently solve three key problems in practical ontology management in RDBMS, namely storage, reasoning, and search. Moreover, we show how the SOR system is used for semantic master data management.|Jing Lu,Li Ma,Lei Zhang,Jean-S√©bastien Brunner,Chen Wang,Yue Pan,Yong Yu","80747|VLDB|2007|Model Management and Schema Mappings Theory and Practice|We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.|Philip A. Bernstein,Howard Ho","80728|VLDB|2007|Mining Approximate Top-K Subspace Anomalies in Multi-Dimensional Time-Series Data|Market analysis is a representative data analysis process with many applications. In such an analysis, critical numerical measures, such as profit and sales, fluctuate over time and form time-series data. Moreover, the time series data correspond to market segments, which are described by a set of attributes, such as age, gender, education, income level, and product-category, that form a multi-dimensional structure. To better understand market dynamics and predict future trends, it is crucial to study the dynamics of time-series in multi-dimensional market segments. This is a topic that has been largely ignored in time series and data cube research. In this study, we examine the issues of anomaly detection in multi-dimensional time-series data. We propose time-series data cube to capture the multi-dimensional space formed by the attribute structure. This facilitates the detection of anomalies based on expected values derived from higher level, \"more general\" time-series. Anomaly detection in a time-series data cube poses computational challenges, especially for high-dimensional, large data sets. To this end, we also propose an efficient search algorithm to iteratively select subspaces in the original high-dimensional space and detect anomalies within each one. Our experiments with both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed solution.|Xiaolei Li,Jiawei Han","44334|IEICE Transations|2007|Incremental Language Modeling for Automatic Transcription of Broadcast News|In this paper, we address the task of incremental language modeling for automatic transcription of broadcast news speech. Daily broadcast news naturally contains new words that are not in the lexicon of the speech recognition system but are important for downstream applications such as information retrieval or machine translation. To recognize those new words, the lexicon and the language model of the speech recognition system need to be updated periodically. We propose a method of estimating a list of words to be added to the lexicon based on some time-series text data. The experimental results on the RT Broadcast News data and other TV audio data showed that this method provided an impressive and stable reduction in both out-of-vocabulary rates and speech recognition word error rates.|Katsutoshi Ohtsuki,Long Nguyen","80846|VLDB|2007|iTrails Pay-as-you-go Information Integration in Dataspaces|Dataspace management has been recently identified as a new agenda for information management ,  and information integration . In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration , as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.|Marcos Antonio Vaz Salles,Jens-Peter Dittrich,Shant Kirakos Karakashian,Olivier Ren√© Girard,Lukas Blunschi","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao","58225|GECCO|2007|Nonlinearity linkage detection for financial time series analysis|Standard detection algorithms for nonlinearity linkage fail when applied to typical problems in the analysis of financial time-series data. We explain how this failure arises when standard algorithms are applied navely, how linkage detection needs to be applied directly to the observed data samples, and how this raises problems that are not addressedby current techniques. We extend the existing DSMDGA linkage detection algorithm and present a new system that can determine the required nonlinearity linkage in observed data samples for financial time series. The new system has been evaluated on synthetic datasets and experimental results are provided. The sensitivity of the system to changes in both the problem and the algorithm parameters has also been explored and we discuss the results. We present evidence of the success of the new system and identify areas for further work.|Theodore Chiotis,Christopher D. Clack"],["16660|IJCAI|2007|Selective Supervision Guiding Supervised Learning with Decision-Theoretic Active Learning|An inescapable bottleneck with learning from large data sets is the high cost of labeling training data. Unsupervised learning methods have promised to lower the cost of tagging by leveraging notions of similarity among data points to assign tags. However, unsupervised and semi-supervised learning techniques often provide poor results due to errors in estimation. We look at methods that guide the allocation of human effort for labeling data so as to get the greatest boosts in discriminatory power with increasing amounts of work. We focus on the application of value of information to Gaussian Process classifiers and explore the effectiveness of the method on the task of classifying voice messages.|Ashish Kapoor,Eric Horvitz,Sumit Basu","66166|AAAI|2007|Learning Large Scale Common Sense Models of Everyday Life|Recent work has shown promise in using large, publicly available, hand-contributed commonsense databases as joint models that can be used to infer human state from day-to-day sensor data. The parameters of these models are mined from the web. We show in this paper that learning these parameters using sensor data (with the mined parameters as priors) can improve performance of the models significantly. The primary challenge in learning is scale. Since the model comprises roughly , irregularly connected nodes in each time slice, it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice. We show how to solve the resulting semi-supervised learning problem by combining a variety of conventional approximation techniques and a novel technique for simplifying the model called context-based pruning. We show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various techniques contribute to the performance.|William Pentney,Matthai Philipose,Jeff A. Bilmes,Henry A. Kautz","66164|AAAI|2007|Transferring Naive Bayes Classifiers for Text Classification|A basic assumption in traditional machine learning is that the training and test data distributions should be identical. This assumption may not hold in many situations in practice, but we may be forced to rely on a different-distribution data to learn a prediction model. For example, this may be the case when it is expensive to label the data in a domain of interest, although in a related but different domain there may be plenty of labeled data available. In this paper, we propose a novel transfer-learning algorithm for text classification based on an EM-based Naive Bayes classifiers. Our solution is to first estimate the initial probabilities under a distribution Dl of one labeled data set, and then use an EM algorithm to revise the model for a different distribution Du of the test data which are unlabeled. We show that our algorithm is very effective in several different pairs of domains, where the distances between the different distributions are measured using the Kullback-Leibler (KL) divergence. Moreover, KL-divergence is used to decide the trade-off parameters in our algorithm. In the experiment, our algorithm outperforms the traditional supervised and semi-supervised learning algorithms when the distributions of the training and test sets are increasingly different.|Wenyuan Dai,Gui-Rong Xue,Qiang Yang,Yong Yu","57680|GECCO|2006|On semi-supervised clustering via multiobjective optimization|Semi-supervised classification uses aspects of both unsupervised and supervised learning to improve upon the performance of traditional classification methods. Semi-supervised clustering, in particular, explicitly integrates both information about the data distribution and about class memberships into the clustering process. In this paper, the potential of a multiobjective formulation of the semi-supervised clustering problem is explored, and two evolutionary multiobjective approaches to the problem are outlined. Experimental results demonstrate practical performance benefits of this methodology, including an improved classification performance and an increased robustness towards annotation errors.|Julia Handl,Joshua D. Knowles","58139|GECCO|2007|Ensemble learning for free with evolutionary algorithms|Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.|Christian Gagn√©,Mich√®le Sebag,Marc Schoenauer,Marco Tomassini","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","66153|AAAI|2007|Humans Perform Semi-Supervised Classification Too|We explore the connections between machine learning and human learning in one form of semi-supervised classification.  human subjects completed a novel -class categorization task in which they were first taught to categorize a single labeled example from each category, and subsequently were asked to categorize, without feedback, a large set of additional items. Stimuli were visually complex and unrecognizable shapes. The unlabeled examples were sampled from a bimodal distribution with modes appearing either to the left (left-shift condition) or right (right-shift condition) of the two labeled examples. Results showed that, although initial decision boundaries were near the middle of the two labeled examples, after exposure to the unlabeled examples, they shifted in different directions in the two groups. In this respect, the human behavior conformed well to the predictions of a Gaussian mixture model for semi-supervised learning. The human behavior differed from model predictions in other interesting respects, suggesting some fruitful avenues for future inquiry.|Xiaojin Zhu,Timothy J. Rogers,Ruichen Qian,Chuck Kalish","65717|AAAI|2006|Overview of AutoFeed An Unsupervised Learning System for Generating Webfeeds|The AutoFeed system automatically extracts data from semistructured web sites. Previously, researchers have developed two types of supervised learning approaches for extracting web data methods that create precise, site-specific extraction rules and methods that learn less-precise site-independent extraction rules. In either case, significant training is required. AutoFeed follows a third, more ambitious approach, in which unsupervised learning is used to analyze sites and discover their structure. Our method relies on a set of heterogeneous \"experts\", each of which is capable of identifying certain types of generic structure. Each expert represents its discoveries as \"hints\". Based on these hints, our system clusters the pages and identifies semi-structured data that can be extracted. To identify a good clustering, we use a probabilistic model of the hint-generation process. This paper summarizes our formulation of the fully-automatic web-extraction problem, our clustering approach, and our results on a set of experiments.|Bora Gazen,Steven Minton","66201|AAAI|2007|Improving Learning in Networked Data by Combining Explicit and Mined Links|This paper is about using multiple types of information for classification of networked data in a semi-supervised setting given a fully described network (nodes and edges) with known labels for some of the nodes, predict the labels of the remaining nodes. One method recently developed for doing such inference is a guilt-by-association model. This method has been independently developed in two different settings-relational learning and semi-supervised learning. In relational learning, the setting assumes that the networked data has explicit links such as hyperlinks between webpages or citations between research papers. The semi-supervised setting assumes a corpus of non-relational data and creates links based on similarity measures between the instances. Both use only the known labels in the network to predict the remaining labels but use very different information sources. The thesis of this paper is that if we combine these two types of links, the resulting network will carry more information than either type of link by itself. We test this thesis on six benchmark data sets, using a within-network learning algorithm, where we show that we gain significant improvements in predictive performance by combining the links. We describe a principled way of combining mUltiple types of edges with different edge-weights and semantics using an objective graph measure called node-based assortativity. We investigate the use of this measure to combine text-mined links with explicit links and show that using our approach significantly improves performance of our classifier over naively combining these two types of links.|Sofus A. Macskassy","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","66123|AAAI|2007|Continuous State POMDPs for Object Manipulation Tasks|My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. ) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.|Emma Brunskill","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,Fran√ßois Charpillet","65786|AAAI|2006|Incremental Least Squares Policy Iteration for POMDPs|We present a new algorithm, called incremental least squares policy iteration (ILSPI), for finding the infinite-horizon stationary policy for partially observable Markov decision processes (POMDPs). The ILSPI algorithm computes a basis representation of the infinite-horizon value function by minirnizing the square of Bellman residual and performs policy improvenent in reachable belief states. A number of optimal basis functions are determined by the algorithm to minimize the Bellman residual incrementally, via efficient computations. We show that, by using optimally determined basis functions, the policy can be improved successively on a set of most probable belief points sampled from the reachable belief set. As the ILSPI is based on belief sample points, it represents a point-based policy iteration method. The results on four benchmark problems show that the ILSPI compares competitively to its value-iteration counterparts in terms of both performance and computational efficiency.|Hui Li,Xuejun Liao,Lawrence Carin","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein"],["16682|IJCAI|2007|Conjunctive Query Answering for the Description Logic SHIQ|Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, it was an open problem whether conjunctive query answering over DL knowledge bases is decidable if transitive roles are admitted in the query. In this paper, we consider conjunctive queries over knowledge bases formulated in the popular DL SHIQ and allow transitive roles in both the query and the knowledge base. We show that query answering is decidable and establish the following complexity bounds regarding combined complexity, we devise a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query. Regarding data complexity, we prove co-NP-completeness.|Birte Glimm,Ian Horrocks,Carsten Lutz,Ulrike Sattler","80607|VLDB|2006|On Biased Reservoir Sampling in the Presence of Stream Evolution|The method of reservoir based sampling is often used to pick an unbiased sample from a data stream. A large portion of the unbiased sample may become less relevant over time because of evolution. An analytical or mining task (eg. query estimation) which is specific to only the sample points from a recent time-horizon may provide a very inaccurate result. This is because the size of the relevant sample reduces with the horizon itself. On the other hand, this is precisely the most important case for data stream algorithms, since recent history is frequently analyzed. In such cases, we show that an effective solution is to bias the sample with the use of temporal bias functions. The maintenance of such a sample is non-trivial, since it needs to be dynamically maintained, without knowing the total number of points in advance. We prove some interesting theoretical properties of a large class of memory-less bias functions, which allow for an efficient implementation of the sampling algorithm. We also show that the inclusion of bias in the sampling process introduces a maximum requirement on the reservoir size. This is a nice property since it shows that it may often be possible to maintain the maximum relevant sample with limited storage requirements. We not only illustrate the advantages of the method for the problem of query estimation, but also show that the approach has applicability to broader data mining problems such as evolution analysis and classification.|Charu C. Aggarwal","80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","80833|VLDB|2007|Efficient Bulk Deletes for Multi Dimensionally Clustered Tables in DB|In data warehousing applications, the ability to efficiently delete large chunks of data from a table is very important. This feature is also known as Rollout or Bulk Deletes. Rollout is generally carried out periodically and is often done on more than one dimension or attribute. The ability to efficiently handle the updates of RID indexes while doing Rollouts is a well known problem for database engines and its solution is very important for data warehousing applications. DB UDB V. introduced a new physical clustering scheme called Multi Dimensional Clustering (MDC) which allows users to cluster data in a table on multiple attributes or dimensions. This is very useful for query processing and maintenance activities including deletes. Subsequently, an enhancement was incorporated in DB UDB Viper  which allows for very efficient online rollout of data on dimensional boundaries even when there are a lot of secondary RID indexes defined on the table. This is done by the asynchronous updates of these RID indexes in the background while allowing the delete to commit and the table to be accessed. This paper details the design of MDC Rollout and the challenges that were encountered. It discusses some performance results which show order of magnitude improvements using it and the lessons learnt.|Bishwaranjan Bhattacharjee,Timothy Malkemus,Sherman Lau,Sean Mckeough,Jo-Anne Kirton,Robin Von Boeschoten,John Kennedy","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","80693|VLDB|2006|Compact Histograms for Hierarchical Identifiers|Distributed monitoring applications often involve streams of unique identifiers (UIDs) such as IP addresses or RFID tag IDs. An important class of query for such applications involves partitioning the UIDs into groups using a large lookup table the query then performs aggregation over the groups. We propose using histograms to reduce bandwidth utilization in such settings, using a histogram partitioning function as a compact representation of the lookup table. We investigate methods for constructing histogram partitioning functions for lookup tables over unique identifiers that form a hierarchy of contiguous groups, as is the case with network addresses and several other types of UID. Each bucket in our histograms corresponds to a subtree of the hierarchy. We develop three novel classes of partitioning functions for this domain, which vary in their structure, construction time, and estimation accuracy.Our approach provides several advantages over previous work. We show that optimal instances of our partitioning functions can be constructed efficiently from large lookup tables. The partitioning functions are also compact, with each partition represented by a single identifier. Finally, our algorithms support minimizing any error metric that can be expressed as a distributive aggregate and they extend naturally to multiple hierarchical dimensions. In experiments on real-world network monitoring data, we show that our histograms provide significantly higher accuracy per bit than existing techniques.|Frederick Reiss,Minos N. Garofalakis,Joseph M. Hellerstein","80805|VLDB|2007|Depth Estimation for Ranking Query Optimization|A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just \"enough\" of the input in order to generate just \"enough\" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.|Karl Schnaitter,Joshua Spiegel,Neoklis Polyzotis","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Cal√¨,Michael Kifer","80790|VLDB|2007|A General Framework for Modeling and Processing Optimization Queries|An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function andor a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that IO optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.|Michael Gibas,Ning Zheng,Hakan Ferhatosmanoglu","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","66255|AAAI|2007|Extracting Influential Nodes for Information Diffusion on a Social Network|We consider the combinatorial optimization problem of finding the most influential nodes on a large-scale social network for two widely-used fundamental stochastic diffusion models. It was shown that a natural greedy strategy can give a good approximate solution to this optimization problem. However, a conventional method under the greedy algorithm needs a large amount of computation, since it estimates the marginal gains for the expected number of nodes influenced by a set of nodes by simulating the random process of each model many times. In this paper, we propose a method of efficiently estimating all those quantities on the basis of bond percolation and graph theory, and apply it to approximately solving the optimization problem under the greedy algorithm. Using real-world large-scale networks including blog networks, we experimentally demonstrate that the proposed method can outperform the conventional method, and achieve a large reduction in computational cost.|Masahiro Kimura,Kazumi Saito,Ryohei Nakano","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","57698|GECCO|2006|Multi-objective genetic algorithms for pipe arrangement design|This paper presents an automatic design method for piping arrangement. A pipe arrangement design problem is proposed for a space in which many pipes and objects co-exist. This problem includes large-scale numerical optimization and combinatorial optimization problems, as well as two criteria. For these reasons, it is difficult to optimize the problem using usual optimization techniques such as Random Search. Therefore, multi-objective genetic algorithms suitable for this problem are developed. The proposed method for optimizing a pipe arrangement efficiently is demonstrated through several experiments.|Satoshi Ikehira,Hajime Kimura","57920|GECCO|2007|A study of mutational robustness as the product of evolutionary computation|This paper investigates the ability of a tournament selection based genetic algorithm to find mutationally robust solutions to a simple combinatorial optimization problem. Two distinct algorithms (a stochastic hill climber and a tournament selection based GA) were used to search for optimal walks in several variants of the self avoiding walk problem. The robustness of the solutions obtained by the algorithms were compared, both with each other and with solutions obtained by a random sampling of the optimal solution space. The solutions found by the GA were, for most of the problem variants, significantly more robust than those found by either the hill climbing algorithm or random sampling. The solutions found by the hill climbing algorithm were often significantly less robust than those obtained through random sampling. .|Justin Schonfeld","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","57918|GECCO|2007|Analyzing heuristic performance with response surface models prediction optimization and robustness|This research uses a Design of Experiments (DOE) approach to build a predictive model of the performance of a combinatorial optimization heuristic over a range of heuristic tuning parameter settings and problem instance characteristics. The heuristic is Ant Colony System (ACS) for the Travelling Salesperson Problem.  heurstic tuning parameters and  problem characteristics are considered. Response Surface Models (RSM) of the solution quality and solution time predicted ACS performance on both new instances from a publicly available problem generator and new real-world instances from the TSPLIB benchmark library. A numerical optimisation of the RSMs is used to find the tuning parameter settings that yield optimal performance in terms of solution quality and solution time. This paper is the first use of desirability functions, a well-established technique in DOE, to simultaneously optimise these conflicting goals. Finally, overlay plots are used to examine the robustness of the performance of the optimised heuristic across a range of problem instance characteristics. These plots give predictions on the range of problem instances for which a given solutionquality can be expected within a given solution time.|Enda Ridge,Daniel Kudenko","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57650|GECCO|2006|Innovization innovating design principles through optimization|This paper introduces a new design methodology (we call it \"innovization\") in the context of finding new and innovative design principles by means of optimization techniques. Although optimization algorithms are routinely used to find an optimal solution corresponding to an optimization problem, the task of innovization stretches the scope beyond an optimization task and attempts to unveil new, innovative, and important design principles relating to decision variables and objectives, so that a deeper understanding of the problem can be obtained. The variety of problems chosen in the paper and the resulting innovations obtained for each problem amply demonstrate the usefulness of the innovization task. The results should encourage a wide spread applicability of the proposed innovization procedure (which is not simply an optimization procedure) to other problem-solving tasks.|Kalyanmoy Deb,Aravind Srinivasan"],["57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","43729|IEICE Transations|2006|Increasing the Edge-Connectivity by Contracting a Vertex Subset in Graphs|Let G  (V,E) be an edge weighted graph with n vertices and m edges. For a given integer p with  p n, we call a set X  V of p vertices a p-maximizer if X has a property that the edge-connectivity of the graph obtained by contracting X into a single vertex is no less than that of the graph obtained by contracting any other subset of p vertices. In this paper, we first show that there always exists an ordering v,v,...,vn of vertices in V such that, for each i  ,,...,n - , set v,v,...,vi is an i-maximizer. We give an O(mn + nlog n) time algorithm for finding such an ordering and then show an application to the source location problem.|Hiroshi Nagamochi","43205|IEICE Transations|2006|Maximum-Cover Source-Location Problems|Given a graph G  (V,E), a set of vertices S  V covers   V if the edge connectivity between S and  is at least a given number k. Vertices in S are called sources. The source location problem is a problem of finding a minimum-size source set covering all vertices of a given graph. This paper presents a new variation of the problem, called maximum-cover source-location problem, which finds a source set S with a given size p, maximizing the sum of the weight of vertices covered by S. It presents an O(np + m + n log n)-time algorithm for k  , where n  V and m  E. Especially it runs linear time if G is connected. This algorithm uses a subroutine for finding a subtree with the maximum weight among p-leaf trees of a given vertex-weighted tree. For the problem we give a greedy-based linear-time algorithm, which is an extension of the linear-time algorithm for finding a longest path of a given tree presented by E. W. Dijkstra around . Moreover, we show some polynomial solvable cases, e.g., a given graph is a tree or (k - )-edge-connected, and NP-hard cases, e.g., a vertex-cost function is given or G is a digraph.|Kenya Sugihara,Hiro Ito","44292|IEICE Transations|2007|Score Sequence Pair Problems of   -Tournaments - - Determination of Realizability - - |Let G be any graph with property P (for example, general graph, directed graph, etc.) and S be nonnegative and non-decreasing integer sequence(s). The prescribed degree sequence problem is a problem to determine whether there is a graph G having S as the prescribed sequence(s) of degrees or outdegrees of the vertices. From 's, P has attracted wide attentions, and its many extensions have been considered. Let P be the property satisfying the following () and () G is a directed graph with two disjoint vertex sets A and B. There are r (r, respectively) directed edges between every pair of vertices in A(B), and r directed edges between every pair of vertex in A and vertex in B. Then G is called an (r, r, r) -tournament (\"tournament\", for short). The problem is called the score sequence pair problem of a \"tournament\" (realizable, for short). S is called a score sequence pair of a \"tournament\" if the answer of the problem is \"yes.\" In this paper, we propose the characterizations of a score sequence pair of a \"tournament\" and an algorithm for determining in linear time whether a pair of two integer sequences is realizable or not.|Masaya Takahashi,Takahiro Watanabe,Takeshi Yoshimura","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","43283|IEICE Transations|2006|An Approximation Algorithm for Minimum Certificate Dispersal Problems|We consider a network, where a special data called certificate is issued between two users, and all certificates issued by the users in the network can be represented by a directed graph. For any two users u and v, when u needs to send a message to v securely, v's public-key is needed. The user u can obtain v's public-key using the certificates stored in u and v. We need to disperse the certificates to the users such that when a user wants to send a message to the other user securely, there are enough certificates in them to get the reliable public-key. In this paper, when a certificate graph and a set of communication requests are given, we consider the problem to disperse the certificates among the nodes in the network, such that the communication requests are satisfied and the total number of certificates stored in the nodes is minimized. We formulate this problem as MINIMUM CERTIFICATE DISPERSAL (MCD for short). We show that MCD is NP-Complete, even if its input graph is restricted to a strongly connected graph. We also present a polynomial-time -approximation algorithm MinPivot for strongly connected graphs, when the communication requests satisfy some restrictions. We introduce some graph classes for which MinPivot can compute optimal dispersals, such as trees, rings, and some Cartesian products of graphs.|Hua Zheng,Shingo Omura,Koichi Wada","43530|IEICE Transations|2006|An Efficient Algorithm for Evacuation Problem in Dynamic Network Flows with Uniform Arc Capacity|In this paper, we consider the quickest flow problem in a network which consists of a directed graph with capacities and transit times on its arcs. We present an O(n log n) time algorithm for the quickest flow problem in a network of grid structure with uniform arc capacity which has a single sink where n is the number of vertices in the network.|Naoyuki Kamiyama,Naoki Katoh,Atsushi Takizawa","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","43421|IEICE Transations|2006|Bi-Connectivity Augmentation for Specified Vertices of a Graph with Upper Bounds on Vertex-Degree Increase|The -vertex-connectivity augmentation problem for a specified set of vertices of a graph with degree constraints, VCA-SV-DC, is defined as follows \"Given an undirected graph G  (V,E), a specified set of vertices S  V with S   and a function g  V  Z+  , find a smallest set E' of edges such that (V,E  E') has at least two internally-disjoint paths between any pair of vertices in S and such that vertex-degree increase of each v  V by the addition of E' to G is at most g(v), where Z+ is the set of nonnegative integers.\" This paper shows a linear time algorithm for VCA-SV-DC.|Toshiya Mashima,Takanori Fukuoka,Satoshi Taoka,Toshimasa Watanabe"],["43827|IEICE Transations|2007|Visible Watermarking for Halftone Images|This letter proposes a visible watermarking scheme for halftone images. It exploits HVS filtering to transform the image in binary domain into continuous-tone domain for watermark embedding. Then a codeword search operation converts the watermarked continuous-tone image into binary domain. The scheme is flexible for two weighting factors are involved to adjust the watermark embedding strength and the average intensity of the watermarked image. Moreover, it can be used in some applications where original continuous-tone images are not available and the halftoning method is unknown.|Jeng-Shyang Pan,Hao Luo,Zhe-Ming Lu","44275|IEICE Transations|2007|Error Concealment Technique of Satellite Imagery Transmission through Information Hiding|Imperfect transmission of satellite imagery results in the loss of image lines. This paper proposes a novel error concealment technique using LSB-based watermarking. We generate block description information and insert it into the LSB bit plane of the image. Missing lines after transmission are restored by extracting this block description information. Simulation results show outstanding performance of the proposed technique.|Hae-Yeoun Lee,Dong-Hyuck Im,Heung-Kyu Lee","43139|IEICE Transations|2006|QoS Estimation Method for JPEG  Coded Image at RTP Layer|In this paper, we propose a novel QoS (Quality of Service) estimation scheme for JPEG  coded image at RTP (realtime transfer protocol) layer without decoding the image. QoS of streaming video is estimated in view of several points, such as, transmission delay, or quality of received images. In this paper, we evaluate the QoS in terms of quality of received images. Generally, RTP is carried on top of UDP, and hence, quality of transmitted images could be degraded due to packet loss. To estimate the quality of received JPEG  coded image without decoding, we use RTP header extension in order to send additional information to the receiver. The effectiveness of the proposed method is confirmed by the computer simulations.|Kiyoshi Nishikawa,Shinichi Nagawara,Hitoshi Kiya","43375|IEICE Transations|2006|Connectivity-Based Image Watermarking|A novel robust watermarking scheme based on image connectivity is proposed. Having obtained the connected objects according to the selected connectivity pattern, the gravity centers are calculated in several larger objects as the reference points for watermark embedding. Based on these reference points and the center of the whole image, several sectors are formed, and the same version watermarks are embedded into these sectors. Thanks to the very stable gravity center of the connected objects, watermark detection is synchronized successfully. Simulation results show that our scheme can survive under both local and global geometrical distortions.|Jian Luo,Hongxia Wang","43844|IEICE Transations|2007|Attacking Phase Shift Keying Based Watermarking|The letter describes a phase perturbation attack to the Discrete Fourier Transform (DFT) and Phase Shift Keying (PSK) based watermarking scheme which is proposed in . In that paper the watermark information is embedded in the phase of the DFT coefficients. But this kind of PSK based watermarking scheme is very vulnerable to the phase perturbation attack, when some noise is added on the phase of the DFT coefficients, the watermark can't be correctly extracted anymore, while the quality degradation of the attacked watermarked image is visually acceptable.|Jeng-Shyang Pan,Chuang Lin","43981|IEICE Transations|2007|Quadruple Watermarking against Geometrical Attacks Based on Searching for Vertexes|A new quadruple watermarking scheme of digital images against geometrical attacks is proposed in this letter. We treat the center and the four vertexes of the original image as the reference points and embed the same quadruple watermarks by means of polar coordinates, which is geometrically invariant. The center of an image is assumed to not to be removed after rotating, scaling and local distortions according to the general practical image processing. In the watermark extraction process, the vertexes of the image are found by a searching method. Thus watermark synchronization is obtained. Experimental results show that the scheme is robust to the geometrical distortions including rotation, scaling, cropping and local distortions.|Hai-Yan Zhao,Hong-Xia Wang","44134|IEICE Transations|2007|A More Robust Subsampling-Based Image Watermarking|In this letter, we propose a novel subsampling based image watermark sequentially embedding scheme to reduce the risk of common permutation attack. The image is still perceptual after watermarking, and experimental results also show its effectiveness and robustness.|Chih-Cheng Lo,Pao-Tung Wang,Jeng-Shyang Pan,Bin-Yih Liao","44094|IEICE Transations|2007|Image Authentication Scheme for Resisting JPEG JPEG Compression and Scaling|This paper presents a secure and simple content-based digital signature method for verifying the image authentication under JPEG, JPEG compression and scaling based on a novel concept named lowest authenticable difference (LAD). The whole method, which is extended from the crypto-based digital signature scheme, mainly consists of statistical analysis and signature generationverification. The invariant features, which are generated from the relationship among image blocks in the spatial domain, are coded and signed by the sender's private key to create a digital signature for each image, regardless of the image size. The main contributions of the proposed scheme are () only the spatial domain is adopted during feature generation and verification, making domain transformation process unnecessary () more non-malicious manipulated images (JPEG, JPEG compressed and scaled images) than related studies can be authenticated by LAD, achieving a good trade-off of image authentication between fragile and robust under practical image processing () non-malicious manipulation is clearly defined to meet closely the requirements of storing images or sending them over the Internet. The related analysis and discussion are presented. The experimental results indicate that the proposed method is feasible and effective.|Chih-Hung Lin,Wen-Shyong Hsieh","43954|IEICE Transations|2007|A New Scheme to Realize the Optimum Watermark Detection for the Additive Embedding Scheme with the Spatial Domain|A typical watermarking scheme consists of an embedding scheme and a detection scheme. In detecting a watermark, there are two kinds of detection errors, a false positive error (FPE) and a false negative error (FNE). A detection scheme is said to be optimum if the FNE probability is minimized for a given FPE probability. In this paper, we present an optimum watermark detection scheme for an additive embedding scheme with a spatial domain. The key idea of the proposed scheme is to use the differences between two brightnesses for detecting a watermark. We prove that under the same FPE probability the FNE probability of the proposed optimum detection scheme is no more than that of the previous optimum detection scheme for the additive embedding scheme with the spatial domain. Then, it is confirmed that for an actual image, the FNE probability of the proposed optimum detection scheme is much lower than that of the previous optimum detection scheme. Moreover, it is confirmed experimentally that the proposed optimum detection scheme can control the FPE probability strictly so that the FPE probability is close to a given probability.|Takaaki Fujita,Maki Yoshida,Toru Fujiwara","44340|IEICE Transations|2007|Content Adaptive Visible Watermarking during Ordered Dithering|This letter presents an improved visible watermarking scheme for halftone images. It incorporates watermark embedding into ordered dither halftoning by threshold modulation. The input images include a continuous-tone host image (e.g. an -bit gray level image) and a binary watermark image, and the output is a halftone image with a visible watermark. Our method is content adaptive because it takes local intensity information of the host image into account. Experimental results demonstrate effectiveness of the proposed technique. It can be used in practical applications for halftone images, such as commercial advertisement, content annotation, copyright announcement, etc.|Hao Luo,Jeng-Shyang Pan,Zhe-Ming Lu"],["43903|IEICE Transations|2007|Sufficient Conditions for a Regular LDPC Code Better than an Irregular LDPC Code|Decoding performance of LDPC (Low-Density Parity-Check) codes is highly dependent on the degree distributions of the Tanner graphs which define the LDPC codes. We compare two LDPC code ensembles, one has a uniform degree distribution and the other a non-uniform one over a BEC (Binary Erasure Channel) and a BSC (Binary Symmetric Channel) thorough DE (Density Evolution). We then derive sufficient conditions on the erasure probability of a BEC and the error probability of a BSC, under which the LDPC code ensembles with uniform degree distributions outperform those with non-uniform degree distributions.|Shinya Miyamoto,Kenta Kasai,Kohichi Sakaniwa","43175|IEICE Transations|2006|Rate Compatible Low-Density Parity-Check Codes Based on Progressively Increased Column Weights|Effective and simply realizable rate compatible low-density parity-check (LDPC) codes are proposed. A parity check matrix is constructed with the progressively increased column weights (PICW) order and adopted to achieve a punctured LDPC coding scheme for a wide range of the code rates of the rate compatible systems. Using the proposed rate compatible punctured LDPC codes, low complex adaptive communication systems, such as wireless communication systems, can be achieved with the reliable transmissions.|Chen Zheng,Noriaki Miyazaki,Toshinori Suzuki","43342|IEICE Transations|2006|Power-Efficient LDPC Decoder Architecture Based on Accelerated Message-Passing Schedule|In this paper, we propose a power-efficient LDPC decoder architecture based on an accelerated message-passing schedule. The proposed decoder architecture is characterized as follows (i) Partitioning a pipelined operation not to read and write intermediate messages simultaneously enables the accelerated message-passing schedule to be implemented with single-port SRAMs. (ii) FIFO-based buffering reduces the number of SRAM banks and words of the LDPC decoder based on the accelerated message-passing schedule. The proposed LDPC decoder keeps a single message for each non-zero bit in a parity check matrix as well as a classical schedule while achieving the accelerated message-passing schedule. Implementation results in . m CMOS technology show that the proposed decoder architecture reduces an area of the LDPC decoder by % and a power dissipation by % compared to the conventional architecture based on the accelerated message-passing schedule.|Kazunori Shimizu,Tatsuyuki Ishikawa,Nozomu Togawa,Takeshi Ikenaga,Satoshi Goto","43993|IEICE Transations|2007|Performance Bound for Finite-Length LDPC Coded Modulation|Coded modulation systems based on low density parity check (LDPC) codes of finite lengths are considered. The union bounds on the bit error probabilities of the maximum likelihood (ML) decoding are presented for both additive white Gaussian noise (AWGN) and flat Rayleigh fading channels. The tightness of the derived bound is verified by simulating the ML decoding of a very short LDPC code. For medium-length codes, performance of the sum-product decoding can asymptotically approach the bounds.|Huy G. Vu,Ha H. Nguyen,David E. Dodds","43815|IEICE Transations|2007|UWB Radio Digital Communication with Chaotic and Impulse Wavelets|Radio communications via channels already occupied by traditional telecommunication systems can be achieved by using ultra-wideband (UWB) radio where extremely wideband wavelets are used in order to reduce the power spectral density (psd) of transmitted signal. Since the recovery of these UWB carriers is not feasible, noncoherent demodulation techniques have to be used. The letter evaluates and compares the noise performances of the feasible noncoherent UWB modulation schemes, namely, that of the noncoherent pulse polarity modulation and the transmitted reference system.|G√©za Kolumb√°n,Tam√°s Kr√©besz","43274|IEICE Transations|2006|A Modification Method for Constructing Low-Density Parity-Check Codes for Burst Erasures|We study a modification method for constructing low-density parity-check (LDPC) codes for solid burst erasures. Our proposed modification method is based on a column permutation technique for a parity-check matrix of the original LDPC codes. It can change the burst erasure correction capabilities without degradation in the performance over random erasure channels. We show by simulation results that the performance of codes permuted by our method are better than that of the original codes, especially with two or more solid burst erasures.|Gou Hosoya,Hideki Yagi,Toshiyasu Matsushima,Shigeichi Hirasawa","43860|IEICE Transations|2007|A Combined Coding and Modulation to Support Both Coherent and Non-coherent Ultra-Wideband Receivers|This letter proposes a simple combined coding and modulation based on super-orthogonal convolutional codes (SOCs) in order to support both coherent and non-coherent ultra-wideband (UWB) receivers. In the proposed scheme, the coherent receivers obtain a coding gain as large as the SOC while simultaneously supporting non-coherent receivers. In addition, their performance can be freely adapted by changing the encoder constraint length and the number of PPM slots according to its application. Thus, the proposal enables a more flexible system design for low data-rate UWB systems.|Tomoko Matsumoto,Ryuji Kohno","43367|IEICE Transations|2006|VLSI Design of a Fully-Parallel High-Throughput Decoder for Turbo Gallager Codes|The most powerful channel coding schemes, namely those based on turbo codes and low-density parity-check (LDPC) Gallager codes, have in common the principle of iterative decoding. However, the relative coding structures and decoding algorithms are substantially different. This paper presents a -bit, rate- soft decision decoder for a new class of codes known as Turbo Gallager Codes. These codes are turbo codes with properly chosen component convolutional codes such that they can be successfully decoded by means of the decoding algorithm used for LDPC codes, i.e., the belief propagation algorithm working on the code Tanner graph. These coding schemes are important in practical terms for two reasons (i) they can be encoded as classical turbo codes, giving a solution to the encoding problem of LDPC codes (ii) they can also be decoded in a fully parallel manner, partially overcoming the routing congestion bottleneck of parallel decoder VLSI implementations thanks to the locality of the interconnections. The implemented decoder can support up to  Gbits data rate and performs up to  decoding iterations ensuring both high throughput and good coding gain. In order to evaluate the performance and the gate complexity of the decoder VLSI architecture, it has been synthesized in a . m standard-cell CMOS technology.|Luca Fanucci,Pasquale Ciao,Giulio Colavolpe","43088|IEICE Transations|2006|Encoding LDPC Codes Using the Triangular Factorization|An algorithm for encoding low-density parity check (LDPC) codes is investigated. The algorithm computes parity check symbols by solving a set of sparse equations, and the triangular factorization is employed to solve the equations efficiently. It is shown analytically and experimentally that the proposed algorithm is more efficient than the Richardson's encoding algorithm if the code has a small gap.|Yuichi Kaji","43053|IEICE Transations|2006|Average Coset Weight Distribution of Multi-Edge Type LDPC Code Ensembles|Multi-Edge type Low-Density Parity-Check codes (MET-LDPC codes) introduced by Richardson and Urbanke are generalized LDPC codes which can be seen as LDPC codes obtained by concatenating several standard (ir)regular LDPC codes. We prove in this paper that MET-LDPC code ensembles possess a certain symmetry with respect to their Average Coset Weight Distributions (ACWD). Using this symmetry, we drive ACWD of MET-LDPC code ensembles from ACWD of their constituent ensembles.|Kenta Kasai,Yuji Shimoyama,Tomoharu Shibuya,Kohichi Sakaniwa"]]},"title":{"entropy":6.453903237488823,"topics":["algorithm for, genetic algorithm, genetic programming, for the, genetic for, particle swarm, for problem, the problem, the, algorithm the, evolutionary algorithm, for optimization, the and, algorithm and, using genetic, genetic and, its application, algorithm problem, evolutionary for, algorithm optimization","for system, for and, method for, and system, scheme for, system using, support vector, for channel, for ofdm, control for, adaptive for, wireless networks, control system, for with, for using, filter for, and its, video coding, using and, system with","the web, model for, for and, description logic, semantic web, framework for, for logic, reasoning about, least squares, for recognition, named entity, for reasoning, and reasoning, language and, the logic, the semantic, planning with, model, efficient for, semantic","neural networks, reinforcement learning, method for, for data, learning for, speech recognition, image using, data, speech synthesis, based and, for image, for speech, and data, speech enhancement, with and, using and, sensor networks, feature for, learning and, efficient for","for networks, the evolution, new for, for evolution, the networks, neural networks, evolution strategies, for strategies, algorithm networks, and evolution, differential evolution, the crossover, distribution algorithm, crossover for, for building, building block, bayesian networks, and strategies, using evolution, the population","algorithm for, particle swarm, for problem, for optimization, algorithm problem, evolutionary algorithm, algorithm optimization, algorithm and, using algorithm, algorithm with, particle optimization, swarm optimization, selection for, solving problem, multi-objective evolutionary, algorithm the, the optimization, for multi-objective, for constraint, optimization problem","for detection, support vector, analysis for, for using, detection system, detection using, using and, for vector, and verification, analysis and, key agreement, linear for, detection based, digital using, detection the, support machine, encryption scheme, acoustic echo, linear and, support for","method for, with and, between and, for with, for and, zero-correlation zone, for scan, zone sequence, parallel for, design and, construction using, for sequence, delay for, design for, zero-correlation sequence, for signal, signal with, against attacks, for set, for embedded","efficient for, framework for, modeling and, for agent, modeling for, probabilistic and, for mobile, for query, for and, indexing and, situation calculus, query processing, for virtual, for environments, for memory, intelligent system, mechanism design, for computing, for xml, for system","for graph, language and, bounds for, for interface, bayesian networks, for model-based, language for, business processes, natural language, user interface, partitioning for, cognitive architecture, inference for, for and, graph, for user, for bayesian, upper bounds, with specified, spatial reasoning","for data, neural networks, and data, for classification, data, sensor networks, data with, data mining, for networks, data using, question answering, classification using, neural for, using neural, for sensor, for text, answering queries, top-k queries, data hiding, neural with","efficient for, for speech, speech synthesis, speech enhancement, search for, speech and, local search, and search, speech based, speech using, hmm-based speech, estimation distribution, search space, fast for, search, hmm-based synthesis, local and, estimation for, synthesis using, for synthesis"],"ranking":[["57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","43356|IEICE Transations|2006|Solving the Graph Planarization Problem Using an Improved Genetic Algorithm|An improved genetic algorithm for solving the graph planarization problem is presented. The improved genetic algorithm which is designed to embed a graph on a plane, performs crossover and mutation conditionally instead of probability. The improved genetic algorithm is verified by a large number of simulation runs and compared with other algorithms. The experimental results show that the improved genetic algorithm performs remarkably well and outperforms its competitors.|Rong Long Wang,Kozo Okazaki","57654|GECCO|2006|The snake in the box problem mathematical conjecture and a genetic algorithm approach|With applications in coding theory and hypercube-based computing and networking, the \"snake in the box\" problem is of great practical importance. Moreover, it is both mathematically elegant and highly difficult. The problem is simply to find the longest \"snake\" in a hypercube. However, as the hypercube grows in dimensionality, the size of the search space increases exponentially. Moreover, as the maximum snake length is only known for the smallest dimensions (where the snakes themselves have already been identified), there is no known stopping criterion for the search in higher dimensions. In this paper we make a mathematical conjecture about the possible maximum length of a snake in a hypercube of dimension d. We use a genetic algorithm for finding snakes in a  hypercube to show some results.|Pedro A. Diaz-Gomez,Dean F. Hougen","58182|GECCO|2007|ExGA II an improved exonic genetic algorithm for the multiple knapsack problem|ExGA I, a previously presented genetic algorithm, successfully solved numerous instances of the multiple knapsack problem (MKS) by employing an adaptive repair function that made use of the algorithm's modular encoding. Here we present ExGA II, an extension of ExGA I that implements additional features which allow the algorithm to perform more reliably across a larger set of benchmark problems. In addition to some basic modifications of the algorithm's framework, more specific extensions include the use of a biased mutation operator and adaptive control sequences which are used to guide the repair procedure. The success rate of ExGA II is superior to its predecessor, and other algorithms in the literature, without an overall increase in the number of function evaluations required to reach the global optimum. In fact, the new algorithm exhibits a significant reduction in the number of function evaluations required for the largest problems investigated. We also address the computational cost of using a repair function and show that the algorithm remains highly competitive when this cost is accounted for.|Philipp Rohlfshagen,John A. Bullinaria","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["43862|IEICE Transations|2007|A New Adaptive Filter Algorithm for System Identification Using Independent Component Analysis|This paper proposes a new adaptive filter algorithm for system identification by using an independent component analysis (ICA) technique, which separates the signal from noisy observation under the assumption that the signal and noise are independent. We first introduce an augmented state-space expression of the observed signal, representing the problem in terms of ICA. By using a nonparametric Parzen window density estimator and the stochastic information gradient, we derive an adaptive algorithm to separate the noise from the signal. The proposed ICA-based algorithm does not suppress the noise in the least mean square sense but to maximize the independence between the signal part and the noise. The computational complexity of the proposed algorithm is compared with that of the standard NLMS algorithm. The stationary point of the proposed algorithm is analyzed by using an averaging method. We can directly use the new ICA-based algorithm in an acoustic echo canceller without double-talk detector. Some simulation results are carried out to show the superiority of our ICA method to the conventional NLMS algorithm.|Jun-Mei Yang,Hideaki Sakai","57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","66250|AAAI|2007|MasDISPO A Multiagent Decision Support System for Steel Production and Control|In the majority of cases, steel production constitutes the inception of the Supply Chains they are involved Just as in automotive clusters or aerospace. Steel manufactunng companies are affected strongest by bull whip effects or other unpredictable influences along the production chain to the customers. Therefore, flexible planning and realisation as well as fast reorganisation after interferences are indispensable requirements for a competitive position on the market. In thiS paper, MasDISPO, an agent-based decision support system for production and control inside the steel works of Saarstahl AG, a globally respected steel manufacturer, is presented. It is based on a distributed online planning and online scheduling algorithm to calculate solutions supporting production and control inside the melting shop. It monitors the execution of their chosen solutions and responds to unpredicted changes during production by dynamically adapting the schedules. This paper gives an overview of the system, the approach for solving the complex problem of steel production and control, the development process, the main experiences as well as lessons learned.|Sven Jacobi,Esteban Le√≥n-Soto,Cristi√°n Madrigal-Mora,Klaus Fischer","43687|IEICE Transations|2006|Environmental Control Aid System for People with Physical Disabilities|Assistive technology (AT) is becoming increasingly important for improving the mobility and language learning capabilities of persons with disabilities, thus enabling them to function independently and to improve their social opportunities. The Morse code has been shown to be a valuable tool in assistive technology, augmentative and alternative communication, and rehabilitation for people with neuromuscular diseases such as amyotrophic lateral sclerosis, multiple sclerosis, and muscular dystrophy. In this paper, we designed and implemented a wireless environmental control aid system using the Morse code as an adapted access communication tool, which includes three types of switch single-switch, double-switch, and six-switch types. People with disabilities can easily control all types of electronic appliance without restrictions owing to spatial arrangements using a signal transmission based on radio frequency (RF). Experimental results revealed that three participants with disabilities were able to gain access to electronic facilities after six weeks of practice with the new system.|Cheng-Hong Yang,Li-Yeh Chuang,Cheng-Huei Yang,Ching-Hsing Luo","43182|IEICE Transations|2006|Interface for Barge-in Free Spoken Dialogue System Using Nullspace Based Sound Field Control and Beamforming|In this paper, we describe a new interface for a barge-in free spoken dialogue system combining multichannel sound field control and beamforming, in which the response sound from the system can be canceled out at the microphone points. The conventional method inhibits a user from moving because the system forces the user to stay at a fixed position where the response sound is reproduced. However, since the proposed method does not set control points for the reproduction of the response sound to the user, the user is allowed to move. Furthermore, the relaxation of strict reproduction for the response sound enables us to design a stable system with fewer loudspeakers than those used in the conventional method. The proposed method shows a higher performance in speech recognition experiments.|Shigeki Miyabe,Hiroshi Saruwatari,Kiyohiro Shikano,Yosuke Tatekura","43643|IEICE Transations|2006|Efficient Media Synchronization Method for Video Telephony System|In this letter, we derive an efficient audiovideo synchronization method for video telephony. For synchronization, this method does not require any further RTCP packet processing except for the first one. The derived decision rule is far more compact than the conventional method. This decision rule is incorporated in an actual video telephony system adopting Texas Instruments (TI) OMAP  processor and Qualcomm MSM . The computational requirement was compared with the conventional method and through simulations the superiority of the proposed method is proved.|Chanwoo Kim,Kwang-deok Seo,Wonyong Sung","43790|IEICE Transations|2007|Improvement of the Stability and Cancellation Performance for the Active Noise Control System Using the Simultaneous Perturbation Method|In this paper, we propose the introduction of a frequency domain variable perturbation control and a leaky algorithm to the frequency domain time difference simultaneous perturbation (FDTDSP) method in order to improve the cancellation performance and the stability of the active noise control (ANC) system using the perturbation method. Since the ANC system using the perturbation method does not need the secondary path model, it has an advantage of being able to track the secondary path changes. However, the conventional perturbation method has the problem that the cancellation performance deteriorates over the entire frequency band when the frequency response of the secondary path has dips because the magnitude of the perturbation is controlled in the time domain. Moreover, the stability of this method also deteriorates in consequence of the dips. On the other hand, the proposed method can improve the cancellation performance by providing the appropriate magnitude of the perturbation over the entire frequency band and stabilizing the system operation. The effectiveness of the proposed method is demonstrated through simulation and experimental results.|Yukinobu Tokoro,Yoshinobu Kajikawa,Yasuo Nomura","43752|IEICE Transations|2007|Collision Recovery for OFDM System over Wireless Channel|We present an effective method of collision recovery for orthogonal frequency division multiplexing (OFDM)-based communications. For the OFDM system, the modulated message data can be demodulated using the partial time-domain OFDM signal. Therefore, the partial time-domain signal can be adopted to reconstruct the whole OFDM time-domain signal with estimated channel information. This property can be utilized to recover packets from the collisions. Since most collisions are cases in which a long packet collides with a short packet, the collided part is assumed to be short. The simulated results show that the method can recover the two collided packets with a certain probability and can be developed to solve the problem of hidden terminals. This method will dramatically benefit the protocol design of wireless networks, including ad hoc and sensor networks.|Yafei Hou,Masanori Hamamura","43395|IEICE Transations|2006|A Method of Simple Adaptive Control for Nonlinear Systems Using Neural Networks|This paper presents a method of simple adaptive control (SAC) using neural networks for a class of nonlinear systems with bounded-input bounded-output (BIBO) and bounded nonlinearity. The control input is given by the sum of the output of the simple adaptive controller and the output of the neural network. The neural network is used to compensate for the nonlinearity of the plant dynamics that is not taken into consideration in the usual SAC. The role of the neural network is to construct a linearized model by minimizing the output error caused by nonlinearities in the control systems. Furthermore, convergence and stability analysis of the proposed method is performed. Finally, the effectiveness of the proposed method is confirmed through computer simulation.|Muhammad Yasser,Agus Trisanto,Jianming Lu,Takashi Yahagi","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"],["16550|IJCAI|2007|A General Framework for Reasoning about Inconsistency|Numerous logics have been developed for reasoning about inconsistency which differ in (i) the logic to which they apply, and (ii) the criteria used to draw inferences. In this paper, we propose a general framework for reasoning about inconsistency in a wide variety of logics including ones for which inconsistency resolution methods have not yet been studied (e.g. various temporal and epistemic logics). We start with Tarski and Scott's axiomatization of logics, but drop their monotonicity requirements that we believe are too strong for AI. For such a logic L, we define the concept of an option. Options are sets of formulas in L that are closed and consistent according to the notion of consequence and consistency in L. We show that by defining an appropriate preference relation on options, we can capture several existing works such as Brewka's subtheories. We also provide algorithms to compute most preferred options.|V. S. Subrahmanian,Leila Amgoud","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","16542|IJCAI|2007|Using the Probabilistic Logic Programming Language P-log for Causal and Counterfactual Reasoning and Non-Naive Conditioning|P-log is a probabilistic logic programming language, which combines both logic programming style knowledge representation and probabilistic reasoning. In earlier papers various advantages of P-log have been discussed. In this paper we further elaborate on the KR prowess of P-log by showing that (i) it can be used for causal and counterfactual reasoning and (ii) it provides an elaboration tolerant way for non-naive conditioning.|Chitta Baral,Matt Hunsaker","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|C√©cile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","66048|AAAI|2007|A Planning Approach for Message-Oriented Semantic Web Service Composition|In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm.|Zhen Liu,Anand Ranganathan,Anton Riabov","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas W√∂lfl"],["44119|IEICE Transations|2007|State Duration Modeling for HMM-Based Speech Synthesis|This paper describes the explicit modeling of a state duration's probability density function in HMM-based speech synthesis. We redefine, in a statistically correct manner, the probability of staying in a state for a time interval used to obtain the state duration PDF and demonstrate improvements in the duration of synthesized speech.|Heiga Zen,Takashi Masuko,Keiichi Tokuda,Takayoshi Yoshimura,Takao Kobayashi,Tadashi Kitamura","43657|IEICE Transations|2006|A Style Adaptation Technique for Speech Synthesis Using HSMM and Suprasegmental Features|This paper proposes a technique for synthesizing speech with a desired speaking style andor emotional expression, based on model adaptation in an HMM-based speech synthesis framework. Speaking styles and emotional expressions are characterized by many segmental and suprasegmental features in both spectral and prosodic features. Therefore, it is essential to take account of these features in the model adaptation. The proposed technique called style adaptation, deals with this issue. Firstly, the maximum likelihood linear regression (MLLR) algorithm, based on a framework of hidden semi-Markov model (HSMM) is presented to provide a mathematically rigorous and robust adaptation of state duration and to adapt both the spectral and prosodic features. Then, a novel tying method for the regression matrices of the MLLR algorithm is also presented to allow the incorporation of both the segmental and suprasegmental speech features into the style adaptation. The proposed tying method uses regression class trees with contextual information. From the results of several subjective tests, we show that these techniques can perform style adaptation while maintaining naturalness of the synthetic speech.|Makoto Tachibana,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi","44341|IEICE Transations|2007|Critical Band Subspace-Based Speech Enhancement Using SNR and Auditory Masking Aware Technique|In this paper, a new subspace-based speech enhancement algorithm is presented. First, we construct a perceptual filterbank from psycho-acoustic model and incorporate it in the subspace-based enhancement approach. This filterbank is created through a five-level wavelet packet decomposition. The masking properties of the human auditory system are then derived based on the perceptual filterbank. Finally, the prior SNR and the masking threshold of each critical band are taken to decide the attenuation factor of the optimal linear estimator. Five different types of in-car noises in TAICAR database were used in our evaluation. The experimental results demonstrated that our approach outperformed conventional subspace and spectral subtraction methods.|Jia-Ching Wang,Hsiao Ping Lee,Jhing-Fa Wang,Chung-Hsien Yang","44257|IEICE Transations|2007|Average-Voice-Based Speech Synthesis Using HSMM-Based Speaker Adaptation and Adaptive Training|In speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as F and phone duration. For simultaneous adaptation of spectrum, F and phone duration within the HMM framework, we need to transform not only the state output distributions corresponding to spectrum and F but also the duration distributions corresponding to phone duration. However, it is not straightforward to adapt the state duration because the original HMM does not have explicit duration distributions. Therefore, we utilize the framework of the hidden semi-Markov model (HSMM), which is an HMM having explicit state duration distributions, and we apply an HSMM-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. Furthermore, we propose an HSMM-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. We incorporate these techniques into our HSMM-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.|Junichi Yamagishi,Takao Kobayashi","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","44330|IEICE Transations|2007|A Model-Based Learning Process for Modeling Coarticulation of Human Speech|Machine learning techniques have long been applied in many fields and have gained a lot of success. The purpose of learning processes is generally to obtain a set of parameters based on a given data set by minimizing a certain objective function which can explain the data set in a maximum likelihood or minimum estimation error sense. However, most of the learned parameters are highly data dependent and rarely reflect the true physical mechanism that is involved in the observation data. In order to obtain the inherent knowledge involved in the observed data, it is necessary to combine physical models with learning process rather than only fitting the observations with a black box model. To reveal underlying properties of human speech production, we proposed a learning process based on a physiological articulatory model and a coarticulation model, where both of the models are derived from human mechanisms. A two-layer learning framework was designed to learn the parameters concerned with physiological level using the physiological articulatory model and the parameters in the motor planning level using the coarticulation model. The learning process was carried out on an articulatory database of human speech production. The learned parameters were evaluated by numerical experiments and listening tests. The phonetic targets obtained in the planning stage provided an evidence for understanding the virtual targets of human speech production. As a result, the model based learning process reveals the inherent mechanism of the human speech via the learned parameters with certain physical meaning.|Jianguo Wei,Xugang Lu,Jianwu Dang","43405|IEICE Transations|2006|A Two-Stage Method for Single-Channel Speech Enhancement|A time domain (TD) speech enhancement technique to improve SNR in noise-contaminated speech is proposed. Additional supplementary scheme is applied to estimate the degree of noise of noisy speech. This is estimated from a function, which is previously prepared as the function of the parameter of the degree of noise. The function is obtained by least square (LS) method using the given degree of noise and the estimated parameter of the degree of noise. This parameter is obtained from the autocorrelation function (ACF) on frame-by-frame basis. This estimator almost accurately estimates the degree of noise and it is useful to reduce noise. The proposed method is based on two-stage processing. In the first stage, subtraction in time domain (STD), which is equivalent to ordinary spectral subtraction (SS), is carried out. In the result, the noise is reduced to a certain level. Further reduction of noise and by-product noise residual is carried out in the second stage, where blind source separation (BSS) technique is applied in time domain. Because the method is a single-channel speech enhancement, the other signal is generated by taking the noise characteristics into consideration in order to apply BSS. The generated signal plays a very important role in BSS. This paper presents an adaptive algorithm for separating sources in convolutive mixtures modeled by finite impulse response (FIR) filters. The coefficients of the FIR filter are estimated from the decorrelation of two mixtures. Here we are recovering only one signal of interest, in particular the voice of primary speaker free from interfering noises. In the experiment, the different levels of noise are added to the clean speech signal and the improvement of SNR at each stage is investigated. The noise types considered initially in this study consist of the synthesized white and color noise with SNR set from  to  dB. The proposed method is also tested with other real-world noises. The results show that the satisfactory SNR improvement is attained in the two-stage processing.|Mohammad E. Hamid,Takeshi Fukabayashi","43520|IEICE Transations|2006|Robust Speech Recognition by Using Compensated Acoustic Scores|This paper proposes a new compensation method of acoustic scores in the Viterbi search for robust speech recognition. This method introduces noise models to represent a wide variety of noises and realizes robust decoding together with conventional techniques of subtraction and adaptation. This method uses likelihoods of noise models in two ways. One is to calculate a confidence factor for each input frame by comparing likelihoods of speech models and noise models. Then the weight of the acoustic score for a noisy frame is reduced according to the value of the confidence factor for compensation. The other is to use the likelihood of noise model as an alternative that of a silence model when given noisy input. Since a lower confidence factor compresses acoustic scores, the decoder rather relies on language scores and keeps more hypotheses within a fixed search depth for a noisy frame. An experiment using commentary transcriptions of a broadcast sports program (MLB Major League Baseball) showed that the proposed method obtained a .% relative word error reduction. The method also reduced the relative error rate of key words by .%, and this is expected lead to an improvement metadata extraction accuracy.|Shoei Sato,Kazuo Onoe,Akio Kobayashi,Toru Imai","43509|IEICE Transations|2006|Robust Recognition of Fast Speech|This letter describes a robust speech recognition system for recognizing fast speech by stretching the length of the utterance in the cepstrum domain. The degree of stretching for an utterance is determined by its rate of speech (ROS), which is based on a maximum likelihood (ML) criterion. The proposed method was evaluated on -digits mobile phone numbers. The results of the simulation show that the overall error rate was reduced by .% when the proposed method was employed.|Ki-Seung Lee"],["57605|GECCO|2006|Hierarchically organised evolution strategies on the parabolic ridge|Organising evolution strategies hierarchically has been proposed as a means for adapting strategy parameters such as step lengths. Experimental research has shown that on ridge functions, hierarchically organised strategies can significantly outperform strategies that rely on mutative self-adaptation. This paper presents a first theoretical analysis of the behaviour of a hierarchically organised evolution strategy. Quantitative results are derived for the parabolic ridge that describe the dependence on the length of the isolation periods of the mutation strength and the progress rate. The issue of choosing an appropriate length of the isolation periods is discussed and comparisons with recent results for cumulative step length adaptation are drawn.|Dirk V. Arnold,Alexander MacLeod","58076|GECCO|2007|On the scalability of evolution strategies in the optimization of dynamic molecular alignment|We consider the numerical optimization of dynamic molecular alignment by shaped femtosecond pulses, and study the scalability of the electric field subject to optimization by Evolution Strategies.The trade-off between fine-tuning of the electric field versus the evolutionary optimization feasibility is investigated.|Ofer M. Shir,Thomas B√§ck,Marc J. J. Vrakking","65895|AAAI|2006|Real-Time Evolution of Neural Networks in the NERO Video Game|A major goal for AI is to allow users to interact with agents that learn in real time, making new kinds of interactive simulations, training applications, and digital entertainment possible. This paper describes such a learning technology, called real-time NeuroEvolution of Augmenting Topologies (rtNEAT), and describes how rtNEAT was used to build the NeuroEvolving Robotic Operatives (NERO) video game. This game represents a new genre of machine learning games where the player trains agents in real time to perform challenging tasks in a virtual environment. Providing laymen the capability to effectively train agents in real time with no prior knowledge of AI or machine learning has broad implications, both in promoting the field of AI and making its achievements accessible to the public at large.|Kenneth O. Stanley,Bobby D. Bryant,Igor Karpov,Risto Miikkulainen","57728|GECCO|2006|Mixed-integer optimization of coronary vessel image analysis using evolution strategies|In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.|Rui Li,Michael Emmerich,Jeroen Eggermont,Ernst G. P. Bovenkamp","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","57607|GECCO|2006|Reconsidering the progress rate theory for evolution strategies in finite dimensions|This paper investigates the limits of the predictions based on the classical progress rate theory for Evolution Strategies. We explain on the sphere function why positive progress rates give convergence in mean, negative progress rates divergence in mean and show that almost sure convergence can take place despite divergence in mean. Hence step-sizes associated to negative progress can actually lead to almost sure convergence. Based on these results we provide an alternative progress rate definition related to almost sure convergence. We present Monte Carlo simulations to investigate the discrepancy between both progress rates and therefore both types of convergence. This discrepancy vanishes when dimension increases. The observation is supported by an asymptotic estimation of the new progress rate definition.|Anne Auger,Nikolaus Hansen","58005|GECCO|2007|Preliminary investigations into the evolution of cooperative strategies in a minimally spatial model|In this paper we outline a simple model of spatially structured populations that is an extension of the replicator dynamics approach used in evolutionary game theory. Using this model, we are able to investigate issues such as how the degree of spatial localisation affects the evolution of cooperative and selfish genotypes in a resource sharing scenario.|Simon T. Powers,Richard A. Watson","58199|GECCO|2007|Stability in the self-organized evolution of networks|The modeling and analysis of large networks of autonomous agents is an important topic with applications in many different disciplines. One way of modeling the development of such networks is by means of an evolutionary process. The autonomous agents are randomly chosen to become active, may apply some kind of local mutation operators to the network and decide about accepting these changes via some fitness-based selection whereas the fitness models the agent's preferences. This general framework for the self-organized evolution of networks can be instantiated in many different ways. For interesting instances, one would like to know whether stable topologies eventually evolve and how long this process may take. Here, known results for one instantiation are improved. Moreover, a more natural and local instantiation is presented and analyzed with respect to the expected time needed to reach a stable state.|Thomas Jansen,Madeleine Theile","57979|GECCO|2007|Using evolution strategies for automatic extraction of parameters for stellar population synthesis of galaxy spectra from sdss|In this work we employ Evolution Strategies (ES) to automatically extract a set of physical parameters (ages, metallicities, reddening and contributions) from a sample of galaxy spectra taken from Sloan Digital Sky Survey (SDSS) for stellar populations studies. We pose this parameter extraction as an optimization problem and then solve it using ES. The idea is to reconstruct each galactic spectrum from the sample by means of a linear combination of three different theoretical models of stellar population synthesis. This combination produces a model spectrum that is compared with the original spectrum using a difference function. The goal is to find a model that minimizes this difference, using ES as the algorithm to explore the parameter space.|Juan Carlos Gomez,Olac Fuentes","57697|GECCO|2006|A computational efficient covariance matrix update and a -CMA for evolution strategies|First, the covariance matrix adaptation (CMA) with rank-one update is introduced into the (+)-evolution strategy. An improved implementation of the -th success rule is proposed for step size adaptation, which replaces cumulative path length control. Second, an incremental Cholesky update for the covariance matrix is developed replacing the computational demanding and numerically involved decomposition of the covariance matrix. The Cholesky update can replace the decomposition only for the update without evolution path and reduces the computational effort from O(n) to O(n). The resulting (+)-Cholesky-CMA-ES is an elegant algorithm and the perhaps simplest evolution strategy with covariance matrix and step size adaptation. Simulations compare the introduced algorithms to previously published CMA versions.|Christian Igel,Thorsten Suttorp,Nikolaus Hansen"],["58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Su√°rez,Manuel Valenzuela-Rend√≥n,Hugo Terashima-Mar√≠n,Eduardo Uresti-Charre","57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,J√ºrgen Branke,Hartmut Schmeck","43119|IEICE Transations|2006|Particle Swarm Optimization Algorithm for Energy-Efficient Cluster-Based Sensor Networks|In order to reduce the traffic load and improve the system's lifetime, a cluster-based routing protocol has attracted more attention. In cluster-based sensor networks, energy can be conserved by combining redundant data from nearby sensors into cluster head nodes before forwarding the data to the destination. The lifespan of the whole network can also be expanded by the clustering of sensor nodes and through data aggregation. In this paper, we propose a cluster-based routing protocol which uses the location information of sensors to assist in network clustering. Our protocol partitions the entire network into several clusters by a particle swarm optimization (PSO) clustering algorithm. In each cluster, a cluster head is selected to deal with data aggregation or compression of nearby sensor nodes. For this clustering technique, the correct selection of the number of clusters is challenging and important. To cope with this issue, an energy dissipation model is used in our protocol to automatically estimate the optimal number of clusters. Several variations of PSO-clustering algorithm are proposed to improve the performance of our protocol. Simulation results show that the performance of our protocol is better than other protocols.|Tzay-Farn Shih","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","65853|AAAI|2006|ODPOP An Algorithm for OpenDistributed Constraint Optimization|We propose ODPOP, a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domains (Faltings & Macho-Gonzalez ). The ODPOP algorithm explores the same search space as the dynamic programming algorithm DPOP (Petcu & Faltings b) or ADOPT (Modi et at. ). but does so in an incremental, best-first fashion suitable for open problems. ODPOP has several advantages over DPOP. First, it uses messages whose size only grows linearly with the treewidth of the problem. Second, by letting agents explore values in a best-first order, it avoids incurring always the worst case complexity as DPOP, and on average it saves a significant amount of computation and information exchange. To show the merits of our approach, we report on experiments with practically sized distributed meeting scheduling problems on a multiagent system.|Adrian Petcu,Boi Faltings","43209|IEICE Transations|2006|Objective Function Adjustment Algorithm for Combinatorial Optimization Problems|An improved algorithm of Guided Local Search called objective function adjustment algorithm is proposed for combinatorial optimization problems. The performance of Guided Local Search is improved by objective function adjustment algorithm using multipliers which can be adjusted during the search process. Moreover, the idea of Tabu Search is introduced into the objective function adjustment algorithm to further improve the performance. The simulation results based on some TSPLIB benchmark problems showed that the objective function adjustment algorithm could find better solutions than Local Search, Guided Local Search and Tabu Search.|Hiroki Tamura,Zongmei Zhang,Zheng Tang,Masahiro Ishii","57813|GECCO|2006|Dynamic fitness inheritance proportion for multi-objective particle swarm optimization|In this paper, we propose a dynamic mechanism to vary the probability by which fitness inheritance is applied throughout the run of a multi-objective particle swarm optimizer, in order to obtain a greater reduction in computational cost (than the obtained with a fixed probability), without dramatically affecting the quality of the results. The results obtained show that it is possible to reduce the computational cost by % without affecting the quality of the obtained Pareto front.|Margarita Reyes Sierra,Carlos A. Coello Coello"],["43420|IEICE Transations|2006|A Shape-Preserving Method for Watermarking D Vector Maps Based on Statistic Detection|A statistic based algorithm for watermarking D vector maps is proposed. Instead of D coordinates, a one-dimensional distance sequence extracted from the original map is used as the cover data to achieve the shape-preserving ability. The statistical feature of the cover data is utilized for data embedding. Experiment results indicate the scheme's better performance in invisibility, as well as its robustness to certain attacks.|Cheng Yong Shao,Hai Long Wang,Xia Mu Niu,Xiao Tong Wang","43068|IEICE Transations|2006|Detection of Overlapping Speech in Meetings Using Support Vector Machines and Support Vector Regression|In this paper, a method of detecting overlapping speech segments in meetings is proposed. It is known that the eigenvalue distribution of the spatial correlation matrix calculated from a multiple microphone input reflects information on the number and relative power of sound sources. However, in a reverberant sound field, the feature of the number of sources in the eigenvalue distribution is degraded by the room reverberation. In the Support Vector Machines approach, the eigenvalue distribution is classified into two classes (overlapping speech segments and single speech segments). In the Support Vector Regression approach, the relative power of sound sources is estimated by using the eigenvalue distribution, and overlapping speech segments are detected based on the estimated relative power. The salient feature of this approach is that the sensitivity of detecting overlapping speech segments can be controlled simply by changing the threshold value of the relative power. The proposed method was evaluated using recorded data of an actual meeting.|Kiyoshi Yamamoto,Futoshi Asano,Takeshi Yamada,Nobuhiko Kitawaki","44016|IEICE Transations|2007|Independent Component Analysis for Image Recovery Using SOM-Based Noise Detection|In this paper, a novel independent component analysis (ICA) approach is proposed, which is robust against the interference of impulse noise. To implement ICA in a noisy environment is a difficult problem, in which traditional ICA may lead to poor results. We propose a method that consists of noise detection and image signal recovery. The proposed approach includes two procedures. In the first procedure, we introduce a self-organizing map (SOM) network to determine if the observed image pixels are corrupted by noise. We will mark each pixel to distinguish normal and corrupted ones. In the second procedure, we use one of two traditional ICA algorithms (fixed-point algorithm and Gaussian moments-based fixed-point algorithm) to separate the images. The fixed-point algorithm is proposed for general ICA model in which there is no noise interference. The Gaussian moments-based fixed-point algorithm is robust to noise interference. Therefore, according to the mark of image pixel, we choose the fixed-point or the Gaussian moments-based fixed-point algorithm to update the separation matrix. The proposed approach has the capacity not only to recover the mixed images, but also to reduce noise from observed images. The simulation results and analysis show that the proposed approach is suitable for practical unsupervised separation problem.|Xiaowei Zhang,Nuo Zhang,Jianming Lu,Takashi Yahagi","16405|IJCAI|2007|Detection of Cognitive States from fMRI Data Using Machine Learning Techniques|Over the past decade functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful technique to locate activity of human brain while engaged in a particular task or cognitive state. We consider the inverse problem of detecting the cognitive state of a human subject based on the fMRI data. We have explored classification techniques such as Gaussian Naive Bayes, k-Nearest Neighbour and Support Vector Machines. In order to reduce the very high dimensional fMRI data, we have used three feature selection strategies. Discriminating features and activity based features were used to select features for the problem of identifying the instantaneous cognitive state given a single fMRI scan and correlation based features were used when fMRI data from a single time interval was given. A case study of visuo-motor sequence learning is presented. The set of cognitive states we are interested in detecting are whether the subject has learnt a sequence, and if the subject is paying attention only towards the position or towards both the color and position of the visual stimuli. We have successfully used correlation based features to detect position-color related cognitive states with % accuracy and the cognitive states related to learning with .% accuracy.|Vishwajeet Singh,Krishna P. Miyapuram,Raju S. Bapi","44223|IEICE Transations|2007|Geometrical Physical and TextSymbol Analysis Based Approach of Traffic Sign Detection System|Traffic sign detection is a valuable part of future driver support system. In this paper, we present a novel framework to accurately detect traffic signs from a single color image by analyzing geometrical, physical and textsymbol features of traffic signs. First, we utilize an elaborate edge detection algorithm to extract edge map and accurate edge pixel gradient information. Then, we extract -D geometric primitives (circles, ellipses, rectangles and triangles) efficiently from image edge map. Third, the candidate traffic sign regions are selected by analyzing the intrinsic color features, which are invariant to different illumination conditions, of each region circumvented by geometric primitives. Finally, a text and symbol detection algorithm is introduced to classify true traffic signs. Experimental results demonstrated the capabilities of our algorithm to detect traffic signs with respect to different size, shape, color and illumination conditions.|Yangxing Liu,Takeshi Ikenaga,Satoshi Goto","65738|AAAI|2006|Object Boundary Detection in Images using a Semantic Ontology|We present a novel method for detecting the boundaries between objects in images that uses a large, hierarchical, semantic ontology - WordNet. The semantic object hierarchy in WordNet grounds this ill-posed segmentation problem, so that true boundaries are defined as edges between instances of different classes, and all other edges are clutter. To avoid fully classifying each pixel, which is very difficult in generic images, we evaluate the semantic similarity of the two regions bounding each edge in an initial oversegmentation. Semantic similarity is computed using WordNet enhanced with appearance information, and is largely orthogonal to visual similarity. Hence two regions with very similar visual attributes, but from different categories, can have a large semantic distance and therefore evidence of a strong boundary between them, and vice versa. The ontology is trained with images from the UC Berkeley image segmentation benchmark, extended with manual labeling of the semantic content of each image segment. Results on boundary detection against the benchmark images show that semantic similarity computed through WordNet can significantly improve boundary detection compared to generic segmentation.|Anthony Hoogs,Roderic Collins","43694|IEICE Transations|2006|An Anomaly Intrusion Detection System Based on Vector Quantization|Machine learning and data mining algorithms are increasingly being used in the intrusion detection systems (IDS), but their performances are laggard to some extent especially applied in network based intrusion detection the larger load of network traffic monitoring requires more efficient algorithm in practice. In this paper, we propose and design an anomaly intrusion detection (AID) system based on the vector quantization (VQ) which is widely used for data compression and high-dimension multimedia data index. The design procedure optimizes the performance of intrusion detection by jointly accounting for accurate usage profile modeling by the VQ codebook and fast similarity measures between feature vectors to reduce the computational cost. The former is just the key of getting high detection rate and the later is the footstone of guaranteeing efficiency and real-time style of intrusion detection. Experiment comparisons to other related researches show that the performance of intrusion detection is improved greatly.|Jun Zheng,Mingzeng Hu","66091|AAAI|2007|Detection of Multiple Deformable Objects using PCA-SIFT|In this paper, we address the problem of identifying and localizing multiple instances of highly deformable objects in real-time video data. We present an approach which uses PCA-SIFT (Scale Invariant Feature Transform) in combination with a clustered voting scheme to achieve detection and localization of multiple objects while providing robustness against rapid shape deformation, partial occlusion, and perspective changes. We test our approach in two highly deformable robot domains and evaluate Its performance using ROC (Receiver Operating Characteristic) statistics.|Stefan Zickler,Alexei A. Efros","43148|IEICE Transations|2006|Detection of Bearing Faults Using Haar Wavelets|This paper presents a new bearing fault detection algorithm based on analyzing singular points of vibration signals using the Haar wavelet. The proposed Haar Fault Detection (HFD) algorithm is compared with a previously-developed algorithm associated with the Morlet wavelet. We also substitute the Haar wavelet with Daubechies wavelets with larger compact supports and evaluate the results. Simulations carried on real data demonstrate that the HFD algorithm achieves a comparable accuracy while having a lower computational cost. This makes the HFD algorithm an appropriate candidate for fast processing of bearing faults.|Mohammad Hossein Kahaei,Mehdi Torbatian,Javad Poshtan","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge"],["43211|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Construction Using a Cyclic Hadamard Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone (hereinafter binary ZCA sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCA sequence set by using a cyclic difference set and a collection of mutually orthogonal complementary sets.|Takafumi Hayashi","43383|IEICE Transations|2006|Sequence Set with Three Zero Correlation Zones and Its Application in MC-CDMA System|Sequence set with Three Zero Correlation Zones (T-ZCZ) can efficiently mitigate the Multiple Access Interference (MAI) and Inter Symbol Interference (ISI) caused by multi-path in CDMA system. In this paper, an algorithm for generating T-ZCZ sequence set is presented. Moreover, in order to study the restrictions among the parameters such as the length of the sequence, the number of the sequences and the length of the T-ZCZ etc., the corresponding theoretical bound is investigated and proved. Additionally, the performance of T-ZCZ sequence in MC-CDMA system is evaluated to confirm the capability of interference cancellation as well as system capacity improvement.|Chao Zhang,Xiaoming Tao,Shigeki Yamada,Mitsutoshi Hatori","43164|IEICE Transations|2006|Ternary Sequence Set Having Periodic and Aperiodic Zero-Correlation Zone|A new class of ternary sequence with a zero-correlation zone is introduced. The proposed sequence sets have a zero-correlation zone for both periodic and aperiodic correlation functions. The proposed sequences can be constructed from a pair of Hadamard matrices of size n  n and a Hadamard matrix of size n  n. The constructed sequence set consists of nn ternary sequences, and the length of each sequence is n(m+)(n + ) for a non-negative integer m. The zero-correlation zone of the proposed sequences is  nm-, where  is the phase shift. The sequence member size of the proposed sequence set is equal to nn+ times that of the theoretical upper bound of the member size of a sequence set with a zero-correlation zone.|Takafumi Hayashi","43974|IEICE Transations|2007|Zero-Correlation Zone Sequence Set Construction Using an Even-Perfect Sequence and an Odd-Perfect Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone sequence set. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary zcz sequence set by using an arbitrary pair of an even-perfect sequence and an odd-perfect sequence. The proposed zcz sequence set reaches the theoretical upper bound of the member size of the sequence set.|Takafumi Hayashi","43814|IEICE Transations|2007|Zero-Correlation Zone Sequence Set Constructed from a Perfect Sequence|The present paper introduces the construction of a class of sequence sets with zero-correlation zones called zero-correlation zone sequence sets. The proposed zero-correlation zone sequence set can be generated from an arbitrary perfect sequence, the length of which is longer than . The proposed sets of ternary sequences, which can be constructed from an arbitrary perfect sequence, can successfully provide CDMA communication without co-channel interference. In an ultrasonic synthetic aperture imaging system, the proposed sequence set can improve the signal-to-noise ratio of the acquired image.|Takafumi Hayashi","43874|IEICE Transations|2007|An Integrated Sequence Construction of Binary Zero-Correlation Zone Sequences|The present paper introduces an integrated construction of binary sequences having a zero-correlation zone. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The proposed method enables more flexible design of the binary zero-correlation zone sequence set with respect to its member size, length, and width of zero-correlation zone. Several previously reported sequence construction methods of binary zero-correlation zone sequence sets can be explained as special cases of the proposed method.|Takafumi Hayashi","43279|IEICE Transations|2006|Families of Sequence Pairs with Zero Correlation Zone|A family of sequences with zero correlation zone, which is shortly called a ZCZ set, can provide CDMA system without co-channel interference nor influence of multipath. This paper presents two types of ZCZ sets of non-binary sequence pairs, which achieve the upper bound of family size for length and zero correlation zone. One, which is produced by use of a perfect complementary pair and an orthogonal code, can change zero correlation zone, while the upper bound is kept. The other, which is generated by use of a newly defined orthogonal pair and an orthogonal code, can offer such CDMA system as a binary ZCZ set seems to be used.|Shinya Matsufuji","43187|IEICE Transations|2006|Hardware Design Verification Using Signal Transitions and Transactions|Hardware prototyping has been widely used for ASICSoC verification. This paper proposes a new hardware design verification method, Transition and Transaction Tracer (TTT), which probes and records the signals of interest for a long time, hours, days, or even weeks, without a break. It compresses the captured data in real time and stores it in a state transition format in memory. Since it records all the transitions, it is effective in finding and fixing errors, even ones that occur rarely or intermittently. It can also be programmed to generate a trigger for a logic analyzer when it detects certain transitions. This is useful for debugging situations where the engineer has trouble finding an appropriate trigger condition to pinpoint the source of errors. We have been using the method in hardware prototyping for ASICSoC development for two years and found it useful for system level tests, and in particular for long running tests.|Nobuyuki Ohba,Kohji Takano","43089|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Constructed from an M-Sequence|The present paper introduces an improved construction of a class of binary sequences having a zero-correlation zone (hereafter binary ZCZ sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCZ sequence set from an arbitrary M-sequence. The previously reported sequence construction of binary ZCZ sequence sets from an M-sequence can generate a single series of binary ZCZ sequence sets from an M-sequence. The present paper proposes an improved sequence construction that can generate more than one series of binary ZCZ sequence sets from an M-sequence.|Takafumi Hayashi","43035|IEICE Transations|2006|Fast Multiple Reference Frame Selection Method Using Correlation of Sequence in JVTH|H. video coding standard has a significant performance better than the other standards are the adoption of variable block sizes, multiple reference frames, and the consideration of rate distortion optimization within the codec. However, these features incur a considerable complexity in the encoder for motion estimation. As for the multiple reference frames motion estimation, the increased computation is in proportion to the number of searched reference frames. In this paper, a fast multiple frame reference frames selection method is proposed for H. video coding. The proposed algorithm can efficiently determine the best reference frame from the allowed five reference frames. As determine the number of reference frames to search the motion using the correlation of the different block between the block of current frame and that of previous frame, this scheme can efficiently reduce the computational cost while keeping the similar quality and bit-rate. Simulation results show that the speed of the proposed method is faster than that of the original scheme adapted in JVT reference software JM while keeping the similar video quality and bit-rate.|Jae-Sik Sohn,Duk-Gyoo Kim"],["80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","44033|IEICE Transations|2007|An Algebraic Framework for Modeling of Mobile Systems|We present MobileOBJ, a formal framework for specifying and verifying mobile systems. Based on hidden algebra, the components of a mobile system are specified as behavioral objects or Observational Transition Systems, a kind of transition system, enriched with special action and observation operators related to the distinct characteristics of mobile computing systems. The whole system comes up as the concurrent composition of these components. The implementation of the abstract model is achieved using CafeOBJ, an executable, industrial strength algebraic specification language. The visualization of the specification can be done using CafeOBJ graphical notation. In addition, invariant and behavioral properties of mobile systems can be proved through theorem proving techniques, such as structural induction and coinduction that are fully supported by the CafeOBJ system. The application of the proposed framework is presented through the modeling of a mobile computing environment and the services that need to be supported by the former.|Iakovos Ouranos,Petros S. Stefaneas,Panagiotis Frangos","80790|VLDB|2007|A General Framework for Modeling and Processing Optimization Queries|An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function andor a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that IO optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.|Michael Gibas,Ning Zheng,Hakan Ferhatosmanoglu","43550|IEICE Transations|2006|Design of the Mobile Agent Anonymity Framework in Ubiquitous Computing Environments|In ubiquitous computing environments supporting mobile agents, agent anonymity is a critical issue in protecting the privacy of users. Agent anonymity means that no other entity can identify the identity of the agent performing an action. For agent anonymity, the information, that the user creates or contributes to, and the agent itself should not be revealed, which can be accomplished by hiding the identity of the agent working on behalf of the user. In this paper, an anonymity framework is described for mobile agent systems, providing agent anonymity facilities using agent identity encryption and access control facilities, based on partially blind signature. It is possible to service anonymous mobile agents in ubiquitous computing environments while reducing the abuse resulting from anonymity, by deploying the proposed agent anonymity framework.|Jae-Gon Kim,Gu Su Kim,Young Ik Eom","44300|IEICE Transations|2007|S-VFS Searchable Virtual File System for an Intelligent Ubiquitous Storage|With advances in ubiquitous environments, user demand for easy data-lookup is growing rapidly. Not only users but intelligent ubiquitous applications also require data-lookup services for a ubiquitous computing framework. This paper proposes a backward-compatible, searchable virtual file system (S-VFS) for easy data-lookup. We add search functionality to the VFS, the de facto standard abstraction layer over the file system. Users can find a file by its attributes without remembering the full path. S-VFS maintains the attributes and the indexing structures in a normal file per partition. It processes queries and returns the results in a form of a virtual directory. S-VFS is the modified VFS, but uses legacy file systems without any modification. Since S-VFS supports full backward compatibility, users can even browse hierarchically with the legacy path name. We implement S-VFS in Linux kernel ..-. Experiments with randomly generated queries demonstrate outstanding lookup performance with a small overhead for indexing.|YongJoo Song,Yongjin Choi,HyunBin Lee,Daeyeon Park","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","80690|VLDB|2006|An Algebraic Query Model for Effective and Efficient Retrieval of XML Fragments|Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.|Sujeet Pradhan","80606|VLDB|2006|Query Co-Processing on Commodity Processors|The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), Cell processors and graphics processing units (GPUs). This tutorial will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips.|Anastassia Ailamaki,Naga K. Govindaraju,Stavros Harizopoulos,Dinesh Manocha","80859|VLDB|2007|Query language support for incomplete information in the MayBMS system|MayBMS , , ,  is a data management system for incomplete information developed at Saarland University. Its main features are a simple and compact representation system for incomplete information and a language called I-SQL with explicit operations for handling uncertainty. MayBMS is currently an extension of PostgreSQL and manages both complete and incomplete data and evaluates I-SQL queries.|Lyublena Antova,Christoph Koch,Dan Olteanu"],["16602|IJCAI|2007|Natural Language Query Recommendation in Conversation Systems|In this paper, we address a critical problem in conversation systems limited input interpretation capabilities. When an interpretation error occurs, users often get stuck and cannot recover due to a lack of guidance from the system. To solve this problem, we present a hybrid natural language query recommendation framework that combines natural language generation with query retrieval. When receiving a problematic user query, our system dynamically recommends valid queries that are most relevant to the current user request so that the user can revise his request accordingly. Compared with existing methods, our approach offers two main contributions first, improving query recommendation quality by combining query generation with query retrieval second, adapting generated recommendations dynamically so that they are syntactically and lexically consistent with the original user input. Our evaluation results demonstrate the effectiveness of this approach.|Shimei Pan,James Shaw","80711|VLDB|2006|NUITS A Novel User Interface for Efficient Keyword Search over Databases|The integration of database and information retrieval techniques provides users with a wide range of high quality services. We present a prototype system, called NUITS, for efficiently processing keyword queries on top of a relational database. Our NUITS allows users to issue simple keyword queries as well as advanced keyword queries with conditions. The efficiency of keyword query processing and the user-friendly result display will also be addressed in this paper.|Shan Wang,Zhaohui Peng,Jun Zhang,Lu Qin,Sheng Wang,Jeffrey Xu Yu,Bolin Ding","16582|IJCAI|2007|On Natural Language Processing and Plan Recognition|The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR.|Christopher W. Geib,Mark Steedman","43050|IEICE Transations|2006|On Minimum -Edge-Connectivity Augmentation for Specified Vertices of a Graph with Upper Bounds on Vertex-Degree|The k-edge-connectivity augmentation problem for a specified set of vertices of a graph with degree constraints, kECA-SV-DC, is defined as follows \"Given an undirected multigraph G  (V, E), a specified set of vertices S  V and a function g VZ+, find a smallest set E' of edges such that (V, E  E') has at least k edge-disjoint paths between any pair of vertices in S and such that, for any  V, E' includes at most g(v) edges incident to v, where Z+ is the set of nonnegative integers.\" This paper first shows polynomial time solvability of kECA-SV-DC and then gives a linear time algorithm for ECA-SV-DC.|Toshiya Mashima,Satoshi Taoka,Toshimasa Watanabe","44097|IEICE Transations|2007|Enhanced Framework for a Personalized User Interface Based on a Unified Context-Aware Application Model for Virtual Environments|In this letter, we propose a enhanced framework for a Personalized User Interface (PUI). This framework allows users to access and customize virtual objects in virtual environments in the sense of sharing user centric context with virtual objects. The proposed framework is enhanced by integrating a unified context-aware application for virtual environments (vr-UCAM .) into virtual objects in the PUI framework. It allows a virtual object to receive context from both real and virtual environments, to decide responses based on context and if-then rules, and to communicate with other objects individually. To demonstrate the effectiveness of the proposed framework, we applied it to a virtual heritage system. Experimental results show that we enhance the accessibility and the customizability of virtual objects through the PUI. The proposed framework is expected to play an important role in VR applications such as education, entertainment, and storytelling.|Youngho Lee,Sejin Oh,Youngjung Suh,Seiie Jang,Woontack Woo","66177|AAAI|2007|Using Spatial Language in Multi-Modal Knowledge Capture|The ability to understand and communicate spatial relationships is central to many human-level reasoning tasks. People often describe spatial relationships using prepositions (i.e., in, on, under). Being able to use and interpret spatial prepositions could help create interactive systems for many tasks, including knowledge capture. Here I describe my thesis work modeling the learning and use of spatial prepositions and applying this model to the task of knowledge capture.|Kate Lockwood","65952|AAAI|2007|Enabling Domain-Awareness for a Generic Natural Language Interface|In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.|Yunyao Li,Ishan Chaudhuri,Huahai Yang,Satinder Singh,H. V. Jagadish","43421|IEICE Transations|2006|Bi-Connectivity Augmentation for Specified Vertices of a Graph with Upper Bounds on Vertex-Degree Increase|The -vertex-connectivity augmentation problem for a specified set of vertices of a graph with degree constraints, VCA-SV-DC, is defined as follows \"Given an undirected graph G  (V,E), a specified set of vertices S  V with S   and a function g  V  Z+  , find a smallest set E' of edges such that (V,E  E') has at least two internally-disjoint paths between any pair of vertices in S and such that vertex-degree increase of each v  V by the addition of E' to G is at most g(v), where Z+ is the set of nonnegative integers.\" This paper shows a linear time algorithm for VCA-SV-DC.|Toshiya Mashima,Takanori Fukuoka,Satoshi Taoka,Toshimasa Watanabe","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II","43702|IEICE Transations|2006|A Framework for Virtual Reality with Tangible Augmented Reality-Based User Interface|In this paper, we propose a framework for virtual reality, I-NEXT, which enables users to interact with virtual objects by tangible objects in immersive networked virtual environment. The primary goal of this framework is to support rapid development of immersive and interactive virtual reality systems as well as various types of user interfaces. The proposed framework consists of user interface for interactions, immersive virtual environment, and networking interface. In this framework, we adopt several design patterns to guarantee that either developers or users (artists) can easily implement their VR applications without strong knowledge of VR techniques such as programming, libraries etc. One of the key features of this framework is the presence of the device module which supports a natural user interaction in a virtual environment. For example, the proposed framework provides users with tangible objects so that the users are able to manipulate virtual objects by touching real objects. The proposed framework also supports large scale stereoscopic display through clustering technique. To realize the effectiveness of the proposed framework, we have been developing an application for digital heritage reconstruction. Having been through development of the system, we believe that virtual reality technology is one of the promising technologies which enable users to experience realities in a digital space. Detailed explanations of each component and system architecture are presented.|Dongpyo Hong,Woontack Woo"],["80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","58034|GECCO|2007|Clustering gene expression data via mining ensembles of classification rules evolved using moses|A novel approach, model-based clustering, is described foridentifying complex interactions between genes or gene-categories based on static gene expression data. The approach deals with categorical data, which consists of a set of gene expressionprofiles belonging to one category, and a set belonging to anothercategory. An evolutionary algorithm (Meta-Optimizing Semantic Evolutionary Search, or MOSES) is used to learn an ensemble of classification models distinguishing the two categories, based on inputs that are features corresponding to gene expression values. Each feature is associated with a model-based vector, which encodes quantitative information regarding the utilization of the feature across the ensembles of models. Two different ways of constructing these vectors are explored. These model-based vectors are then clustered using a variant of hierarchical clustering called Omniclust. The result is a set of model-based clusters, in which features are gathered together if they are often considered together by classification models -- which may be because they're co-expressed, or may be for subtler reasons involving multi-gene interactions. The method is illustrated by applying it to two datasets regarding human gene expression, one drawn from brain cells and pertinent to the neurogenetics of aging, and the other drawn from blood cells and relating to differentiating between types of lymphoma. We find that, compared to traditional expression-based clustering, the new method often yields clusters that have higher mathematical quality (in the sense of homogeneity and separation) and also yield novel and meaningful insights into the underlying biological processes.|Moshe Looks,Ben Goertzel,L√∫cio de Souza Coelho,Mauricio Mudado,Cassio Pennachin","80639|VLDB|2006|Answering Top-k Queries Using Views|The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Dimitris Tsirogiannis","43674|IEICE Transations|2006|Exploiting Intelligence in Fighting Action Games Using Neural Networks|This paper proposes novel methods to provide intelligence for characters in fighting action games by using neural networks. First, how a character learns basic game rules and matches against randomly acting opponents is considered. Since each action takes more than one time unit in general fighting action games, the results of a character's action are exposed not immediately but several time units later. We evaluate the fitness of a decision by using the relative score change caused by the decision. Whenever the scores of fighting characters are changed, the decision causing the score change is identified, and then the neural network is trained by using the score difference and the previous input and output values which induced the decision. Second, how to cope more properly with opponents that act with predefined action patterns is addressed. The opponents' past actions are utilized to find out the optimal counter-actions for the patterns. Lastly, a method in order to learn moving actions is proposed. To evaluate the performance of the proposed algorithm, we implement a simple fighting action game. Then the proposed intelligent character (IC) fights with the opponent characters (OCs) which act randomly or with predefined action patterns. The results show that the IC understands the game rules and finds out the optimal counter-actions for the opponents' action patterns by itself.|Byeong Heon Cho,Sung Hoon Jung,Yeong Rak Seong,Ha Ryoung Oh","80854|VLDB|2007|Sum-Max Monotonic Ranked Joins for Evaluating Top-K Twig Queries on Weighted Data Graphs|In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirabilitypenalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.|Yan Qi 0002,K. Sel√ßuk Candan,Maria Luisa Sapino","57760|GECCO|2006|Coevolution of neural networks using a layered pareto archive|The Layered Pareto Coevolution Archive (LAPCA) was recently proposed as an effective Coevolutionary Memory (CM) which, under certain assumptions, approximates monotonic progress in coevolution. In this paper, a technique is developed that interfaces the LAPCA algorithm with NeuroEvolution of Augmenting Topologies (NEAT), a method to evolve neural networks with demonstrated efficiency in game playing domains. In addition, the behavior of LAPCA is analyzed for the first time in a complex game-playing domain evolving neural network controllers for the game Pong. The technique is shown to keep the total number of evaluations in the order of those required by NEAT, making it applicable to complex domains. Pong players evolved with a LAPCA and with the Hall of Fame (HOF) perform equally well, but the LAPCA is shown to require significantly less space than the HOF. Therefore, combining NEAT and LAPCA is found to be an effective approach to coevolution.|German A. Monroy,Kenneth O. Stanley,Risto Miikkulainen","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings","43395|IEICE Transations|2006|A Method of Simple Adaptive Control for Nonlinear Systems Using Neural Networks|This paper presents a method of simple adaptive control (SAC) using neural networks for a class of nonlinear systems with bounded-input bounded-output (BIBO) and bounded nonlinearity. The control input is given by the sum of the output of the simple adaptive controller and the output of the neural network. The neural network is used to compensate for the nonlinearity of the plant dynamics that is not taken into consideration in the usual SAC. The role of the neural network is to construct a linearized model by minimizing the output error caused by nonlinearities in the control systems. Furthermore, convergence and stability analysis of the proposed method is performed. Finally, the effectiveness of the proposed method is confirmed through computer simulation.|Muhammad Yasser,Agus Trisanto,Jianming Lu,Takashi Yahagi","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis","80705|VLDB|2006|Online Outlier Detection in Sensor Data Using Non-Parametric Models|Sensor networks have recently found many popular applications in a number of different settings. Sensors at different locations can generate streaming data, which can be analyzed in real-time to identify events of interest. In this paper, we propose a framework that computes in a distributed fashion an approximation of multi-dimensional data distributions in order to enable complex applications in resource-constrained sensor networks.We motivate our technique in the context of the problem of outlier detection. We demonstrate how our framework can be extended in order to identify either distance- or density-based outliers in a single pass over the data, and with limited memory requirements. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques. We also demonstrate the applicability of our technique to other related problems in sensor networks.|Sharmila Subramaniam,Themis Palpanas,Dimitris Papadopoulos,Vana Kalogeraki,Dimitrios Gunopulos"],["44119|IEICE Transations|2007|State Duration Modeling for HMM-Based Speech Synthesis|This paper describes the explicit modeling of a state duration's probability density function in HMM-based speech synthesis. We redefine, in a statistically correct manner, the probability of staying in a state for a time interval used to obtain the state duration PDF and demonstrate improvements in the duration of synthesized speech.|Heiga Zen,Takashi Masuko,Keiichi Tokuda,Takayoshi Yoshimura,Takao Kobayashi,Tadashi Kitamura","44125|IEICE Transations|2007|Two-Band Excitation for HMM-Based Speech Synthesis|This letter describes a two-band excitation model for HMM-based speech synthesis. The HMM-based speech synthesis system generates speech from the HMM training data of the spectral and excitation parameters. Synthesized speech has a typical quality of \"vocoded sound\" mostly because of the simple excitation model with the voicedunvoiced selection. In this letter, two-band excitation based on the harmonic plus noise speech model is proposed for generating the mixed excitation source. With this model, we can generate the mixed excitation more accurately and reduce the memory for the trained excitation data as well.|Sang-Jin Kim,Minsoo Hahn","44211|IEICE Transations|2007|Details of the Nitech HMM-Based Speech Synthesis System for the Blizzard Challenge |In January , an open evaluation of corpus-based text-to-speech synthesis systems using common speech datasets, named Blizzard Challenge , was conducted. Nitech group participated in this challenge, entering an HMM-based speech synthesis system called Nitech-HTS . This paper describes the technical details, building processes, and performance of our system. We first give an overview of the basic HMM-based speech synthesis system, and then describe new features integrated into Nitech-HTS  such as STRAIGHT-based vocoding, HSMM-based acoustic modeling, and a speech parameter generation algorithm considering GV. Constructed Nitech-HTS  voices can generate speech waveforms at . RT (real-time ratio) on a . GHz Pentium  machine, and footprints of these voices are less than  Mbytes. Subjective listening tests showed that the naturalness and intelligibility of the Nitech-HTS  voices were much better than expected.|Heiga Zen,Tomoki Toda,Masaru Nakamura,Keiichi Tokuda","43724|IEICE Transations|2006|Implementation and Evaluation of an HMM-Based Korean Speech Synthesis System|Development of a hidden Markov model (HMM)-based Korean speech synthesis system and its evaluation is described. Statistical HMM models for Korean speech units are trained with the hand-labeled speech database including the contextual information about phoneme, morpheme, word phrase, utterance, and break strength. The developed system produced speech with a fairly good prosody. The synthesized speech is evaluated and compared with that of our corpus-based unit concatenating Korean text-to-speech system. The two systems were trained with the same manually labeled speech database.|Sang-Jin Kim,Jong-Jin Kim,Minsoo Hahn","44257|IEICE Transations|2007|Average-Voice-Based Speech Synthesis Using HSMM-Based Speaker Adaptation and Adaptive Training|In speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as F and phone duration. For simultaneous adaptation of spectrum, F and phone duration within the HMM framework, we need to transform not only the state output distributions corresponding to spectrum and F but also the duration distributions corresponding to phone duration. However, it is not straightforward to adapt the state duration because the original HMM does not have explicit duration distributions. Therefore, we utilize the framework of the hidden semi-Markov model (HSMM), which is an HMM having explicit state duration distributions, and we apply an HSMM-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. Furthermore, we propose an HSMM-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. We incorporate these techniques into our HSMM-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.|Junichi Yamagishi,Takao Kobayashi","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","44338|IEICE Transations|2007|A Speech Parameter Generation Algorithm Considering Global Variance for HMM-Based Speech Synthesis|This paper describes a novel parameter generation algorithm for an HMM-based speech synthesis technique. The conventional algorithm generates a parameter trajectory of static features that maximizes the likelihood of a given HMM for the parameter sequence consisting of the static and dynamic features under an explicit constraint between those two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed speech parameters usually causes muffled sounds. In order to alleviate the over-smoothing effect, we propose a generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance (GV) of the generated trajectory. The latter likelihood works as a penalty for the over-smoothing, i.e., a reduction of the GV of the generated trajectory. The result of a perceptual evaluation demonstrates that the proposed algorithm causes considerably large improvements in the naturalness of synthetic speech.|Tomoki Toda,Keiichi Tokuda","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","44161|IEICE Transations|2007|A Hidden Semi-Markov Model-Based Speech Synthesis System|A statistical speech synthesis system based on the hidden Markov model (HMM) was recently proposed. In this system, spectrum, excitation, and duration of speech are modeled simultaneously by context-dependent HMMs, and speech parameter vector sequences are generated from the HMMs themselves. This system defines a speech synthesis problem in a generative model framework and solves it based on the maximum likelihood (ML) criterion. However, there is an inconsistency although state duration probability density functions (PDFs) are explicitly used in the synthesis part of the system, they have not been incorporated into its training part. This inconsistency can make the synthesized speech sound less natural. In this paper, we propose a statistical speech synthesis system based on a hidden semi-Markov model (HSMM), which can be viewed as an HMM with explicit state duration PDFs. The use of HSMMs can solve the above inconsistency because we can incorporate the state duration PDFs explicitly into both the synthesis and the training parts of the system. Subjective listening test results show that use of HSMMs improves the reported naturalness of synthesized speech.|Heiga Zen,Keiichi Tokuda,Takashi Masuko,Takao Kobayashi,Tadashi Kitamura","44200|IEICE Transations|2007|A Style Control Technique for HMM-Based Expressive Speech Synthesis|This paper describes a technique for controlling the degree of expressivity of a desired emotional expression andor speaking style of synthesized speech in an HMM-based speech synthesis framework. With this technique, multiple emotional expressions and speaking styles of speech are modeled in a single model by using a multiple-regression hidden semi-Markov model (MRHSMM). A set of control parameters, called the style vector, is defined, and each speech synthesis unit is modeled by using the MRHSMM, in which mean parameters of the state output and duration distributions are expressed by multiple-regression of the style vector. In the synthesis stage, the mean parameters of the synthesis units are modified by transforming an arbitrarily given style vector that corresponds to a point in a low-dimensional space, called style space, each of whose coordinates represents a certain specific speaking style or emotion of speech. The results of subjective evaluation tests show that style and its intensity can be controlled by changing the style vector.|Takashi Nose,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi"]]}}