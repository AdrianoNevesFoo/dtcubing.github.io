{"abstract":{"entropy":6.568608063178399,"topics":["markov decision, markov processes, decision processes, partially observable, game player, consider problem, spanning tree, neural network, artificial intelligence, agents environments, supply chains, challenging problem, markov mdp, observable markov, problem agents, teaching students, processes mdp, observable processes, partially markov, partially processes","genetic algorithm, evolutionary algorithm, genetic programming, algorithm, algorithm optimization, algorithm problem, evolutionary computation, present algorithm, estimation distribution, algorithm search, multi-objective evolutionary, evolution strategies, multi-objective optimization, describe algorithm, optimization problem, search heuristics, differential evolution, evolutionary problem, hierarchical bayesian, use algorithm","knowledge base, data web, semantic web, actions uncertainty, description logic, artificial intelligence, web, natural language, preferences voting, aggregation voting, real world, present semantic, mining web, plays role, web pages, cognitive architecture, case-based reasoning, web services, information, important applications","machine learning, particle swarm, learning, recent years, solving problem, learning system, novel approach, optimization problem, learning classifier, present approach, classifier system, support vector, transfer knowledge, data mining, reinforcement learning, genetic cartesian, xcs fault, programming cartesian, last decade, constraint satisfaction","immune system, intelligent system, scheduling problem, teaching students, problem actions, planning system, actions effects, planning actions, planning plans, diagnosis problem, problem given, problem students, planning, planning problem, given, number, students, plans, theory, investigates","artificial intelligence, agents environments, problem agents, artificial system, autonomous agents, agents, state system, distributed autonomous, autonomous robotics, system agents, autonomous system, problem state, distributed agents, agents team, autonomous robots, autonomous, robots, artificial, recognition, group","algorithm random, fitness function, evolution strategy, algorithm used, evolutionary population, selection algorithm, crossover permutation, evolutionary evolving, fitness landscape, genetic crossover, algorithm fitness, widely algorithm, algorithm introduced, values parameters, evolutionary selection, algorithm parameters, genetic random, introduced crossover, crossover problem, algorithm population","evolutionary algorithm, genetic algorithm, describe algorithm, estimation distribution, algorithm, use algorithm, crossover operators, hierarchical bayesian, algorithm applied, distribution algorithm, paper operators, building blocks, algorithm problem, estimation algorithm, called algorithm, estimation edas, present algorithm, algorithm hierarchical, distribution edas, efficient algorithm","actions uncertainty, real world, agents goal, information, agents information, source uncertainty, information multiple, information data, world, research, world actions, information source, goal, goal research, process, understand, measures, human, database, integration","data web, knowledge base, semantic web, present semantic, mining web, web pages, web, web services, web information, web become, use web, knowledge task, social network, paper services, paper web, semantic knowledge, automatically semantic, algorithm web, system knowledge, challenge semantic","recent years, genetic cartesian, programming cartesian, last decade, extension cartesian, embedded cartesian, recent techniques, recent problem, detection problem, genetic programming, learning programming, system detection, embedded genetic, problem programming, system problem, recent, extension programming, immune system, genetic learning, extension genetic","constraint satisfaction, constraint csps, applications problem, satisfaction csps, constraint problem, microarray data, problem, problem network, address problem, bayesian network, network, problem data, problem mapping, present problem, problem finding, problem csps, optimization problem, problem practical, satisfaction problem, problem objective"],"ranking":[["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","66171|AAAI|2007|Authorial Idioms for Target Distributions in TTD-MDPs|In designing Markov Decision Processes (MDP), one must define the world, its dynamics, a set of actions, and a reward function. MDPs are often applied in situations where there is a clear choice of reward functions and in these cases significant care must be taken to construct a reward function that induces the desired behavior. In this paper, we consider an analogous design problem crafting a target distribution in Targeted Trajectory Distribution MDPs (TTD-MDPs). TTD-MDPs produce probabilistic policies that minimize divergence from a target distribution of trajectories from an underlying MDP. They are an extension of MDPs that provide variety of experience during repeated execution. Here, we present a brief overview of TTD-MDPs with approaches for constructing target distributions. Then we present a novel authorial idiom for creating target distributions using prototype trajectories. We evaluate these approaches on a drama manager for an interactive game.|David L. Roberts,Sooraj Bhat,Kenneth St. Clair,Charles Lee Isbell Jr.","66123|AAAI|2007|Continuous State POMDPs for Object Manipulation Tasks|My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. ) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.|Emma Brunskill","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65864|AAAI|2006|Targeting Specific Distributions of Trajectories in MDPs|We define TTD-MDPs, a novel class of Markov decision processes where the traditional goal of an agent is changed from finding an optimal trajectory through a state space to realizing a specified distribution of trajectories through the space. After motivating this formulation, we show how to convert a traditional MDP into a TTD-MDP. We derive an algorithm for finding non-deterministic policies by constructing a trajectory tree that allows us to compute locally-consistent policies. We specify the necessary conditions for solving the problem exactly and present a heuristic algorithm for constructing policies when an exact answer is impossible or impractical. We present empirical results for our algorithm in two domains a synthetic grid world and stories in an interactive drama or game.|David L. Roberts,Mark J. Nelson,Charles Lee Isbell Jr.,Michael Mateas,Michael L. Littman","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"],["58180|GECCO|2007|Alternative techniques to solve hard multi-objective optimization problems|In this paper, we propose the combination of different optimization techniques in order to solve \"hard\" two- and three-objective optimization problems at a relatively low computational cost. First, we use the -constraint method in order to obtain a few points over (or very near of) the true Pareto front, and then we use an approach based on rough sets to spread these solutions, so that the entire Pareto front can be covered. The constrained single-objective optimizer required by the -constraint method, is the cultured differential evolution, which is an efficient approach for approximating the global optimum of a problem with a low number of fitness function evaluations. The proposed approach is validated using several difficult multi-objective test problems, and our results are compared with respect to a multi-objective evolutionary algorithm representative of the state-of-the-art in the area the NSGA-II.|Ricardo Landa Becerra,Carlos A. Coello Coello,Alfredo García Hernández-Díaz,Rafael Caballero,Julián Molina Luque","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","57860|GECCO|2006|The LEM implementation of learnable evolution model and its testing on complex function optimization problems|Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to ). In every experiment, LEM outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.|Janusz Wojtusiak,Ryszard S. Michalski","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque","58130|GECCO|2007|An EC-memory based method for the multi-objective TSP|In this paper we present a new method for the multi-objective TSP. This method is a modified version of an earlier multi-objective evolutionary algorithm, which uses an explicit collective memory (EC-memory) method, named EVL. We adapted and improved the algorithm and the EVL for the multi-objective TSP and developed a new evolutionary algorithm.|István Borgulya"],["65710|AAAI|2006|Inconsistencies Negations and Changes in Ontologies|Ontology management and maintenance are considered cornerstone issues in current Semantic Web applications in which semantic integration and ontological reasoning play a fundamental role. The ability to deal with inconsistency and to accommodate change is of utmost importance in real-world applications of ontological reasoning and management, wherein the need for expressing negated assertions also arises naturally. For this purpose, precise, formal definitions of the the different types of inconsistency and negation in ontologies are required. Unfortunately, ontology languages based on Description Logics (DLs) do not provide enough expressive power to represent axiom negations. Furthermore, there is no single, well-accepted notion of inconsistency and negation in the Semantic Web community, due to the lack of a common and solid foundational framework. In this paper, we propose a general framework accounting for inconsistency, negation and change in ontologies. Different levels of negation and inconsistency in DL-based ontologies are distinguished. We demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontologies.|Giorgos Flouris,Zhisheng Huang,Jeff Z. Pan,Dimitris Plexousakis,Holger Wache","66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy Lécué,Alexandre Delteil","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","66013|AAAI|2007|Towards Large Scale Argumentation Support on the Semantic Web|This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW) a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Users can also attack or support parts of existing arguments, use existing parts of an argument in the creation of new arguments, or create new argumentation schemes. As such, this initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.|Iyad Rahwan,Fouad Zablith,Chris Reed","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65668|AAAI|2006|Using Semantics to Identify Web Objects|Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.|Nathanael Chambers,James F. Allen,Lucian Galescu,Hyuckchul Jung,William Taysom","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","58011|GECCO|2007|Introducing fault tolerance to XCS|In this paper, we introduce fault tolerance to XCS and propose a new XCS framework called XCS with Fault Tolerance (XCSFT). As an important branch of learning classifier systems, XCS has been proven capable of evolving maximally accurate, maximally general problem solutions. However, in practice, it oftentimes generates a lot of rules, which lower the readability of the evolved classification model, and thus, people may not be able to get the desired knowledge or useful information out of the model. Inspired by the fault tolerance mechanism proposed in field of data mining, we devise a new XCS framework by integrating the concept and mechanism of fault tolerance into XCS in order to reduce the number of classification rules and therefore to improve the readability of the generated prediction model. A series of $N$-multiplexer experiments, including -bit, -bit, -bit, and -bit multiplexers, are conducted to examine whether XCSFT can accomplish its goal of design. According to the experimental results, XCSFT can offer the same level of prediction accuracy on the test problems as XCS can, while the prediction model evolved by XCSFT consists of significantly fewer classification rules.|Hong-Wei Chen,Ying-Ping Chen","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","58139|GECCO|2007|Ensemble learning for free with evolutionary algorithms|Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.|Christian Gagné,Michèle Sebag,Marc Schoenauer,Marco Tomassini","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","65812|AAAI|2006|A Simple and Effective Method for Incorporating Advice into Kernel Methods|We propose a simple mechanism for incorporating advice (prior knowledge), in the form of simple rules, into support-vector methods for both classification and regression. Our approach is based on introducing inequality constraints associated with datapoints that match the advice. These constrained datapoints can be standard examples in the training set, but can also be unlabeled data in a semi-supervised, advice-taking approach. Our new approach is simpler to implement and more efficiently solved than the knowledge-based support vector classification methods of Fung, Mangasarian and Shavlik ( ) and the knowledge-based support vector regression method of Mangasarian, Shavlik, and Wild (), while performing approximately as well as these more complex approaches. Experiments using our new approach on a synthetic task and a reinforcement-learning problem within the RoboCup soccer simulator show that our advice-taking method can significantly outperform a method without advice and perform similarly to prior advice-taking, support-vector machines.|Richard Maclin,Jude W. Shavlik,Trevor Walker,Lisa Torrey","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["66087|AAAI|2007|Asymptotically Optimal Encodings of Conformant Planning in QBF|The world is unpredictable, and acting intelligently requires anticipating possible consequences of actions that are taken. Assuming that the actions and the world are deterministic, planning can be represented in the classical propositional logic. Introducing nondeterminism (but not probabilities) or several initial states increases the complexity of the planning problem and requires the use of quantified Boolean formulae (QBF). The currently leading logic-based approaches to conditional planning use explicitly or implicitly a QBF with the prefix . We present formalizations of the planning problem as QBF which have an asymptotically optimal linear size and the optimal number of quantifier alternations in the prefix  and . This is in accordance with the fact that the planning problem (under the restriction to polynomial size plans) is on the second level of the polynomial hierarchy, not on the third.|Jussi Rintanen","66250|AAAI|2007|MasDISPO A Multiagent Decision Support System for Steel Production and Control|In the majority of cases, steel production constitutes the inception of the Supply Chains they are involved Just as in automotive clusters or aerospace. Steel manufactunng companies are affected strongest by bull whip effects or other unpredictable influences along the production chain to the customers. Therefore, flexible planning and realisation as well as fast reorganisation after interferences are indispensable requirements for a competitive position on the market. In thiS paper, MasDISPO, an agent-based decision support system for production and control inside the steel works of Saarstahl AG, a globally respected steel manufacturer, is presented. It is based on a distributed online planning and online scheduling algorithm to calculate solutions supporting production and control inside the melting shop. It monitors the execution of their chosen solutions and responds to unpredicted changes during production by dynamically adapting the schedules. This paper gives an overview of the system, the approach for solving the complex problem of steel production and control, the development process, the main experiences as well as lessons learned.|Sven Jacobi,Esteban León-Soto,Cristián Madrigal-Mora,Klaus Fischer","66120|AAAI|2007|Filtering Decomposition and Search Space Reduction for Optimal Sequential Planning|We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.|Stéphane Grandcolas,C. Pain-Barre","65871|AAAI|2006|Deeper Natural Language Processing for Evaluating Student Answers in Intelligent Tutoring Systems|This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.|Vasile Rus,Arthur C. Graesser","66230|AAAI|2007|Concurrent Action Execution with Shared Fluents|Concurrent action execution is important for plan-length minimization. However, action specifications are often limited to avoid conflicts arising from preconditioneffect interactions. PDDL -- the planning domain definition language -- for example, implements the \"no moving targets\" rule, which means that no two actions can simultaneously make use of a value if one of the two is updating the value. This rule poses problems for resource allocation planning in which resource values are accessed in preconditions and effects. A simple example is construction actions that consume certain amounts of a resource. For speeding up plan execution, we would like to be able to dispatch several construction actions simultaneously. Because action preconditions depend on resource values and action effects change them, the \"no moving targets\" rule does not allow concurrent execution. However, if sufficient resources are available, executing actions simultaneously poses no problems. This paper addresses the problem of deciding whether a set of actions produced by a planning system can be executed concurrently in the presence of fluent variables that occur in both action preconditions and effects. We first motivate the concurrent action execution problem by introducing a fair action scheduling algorithm for real-time strategy (RTS) games. Then we prove that the general decision problem, when restricting effects and preconditions to polynomial time computations, is co-NP complete. Thereafter, we focus on problem restrictions based on commutative operators which allow us to specify sufficient conditions for concurrent executability that can be checked quickly if the number of shared fluents is small. Finally, we apply these findings to action execution with shared resources in RTS games.|Michael Buro,Alexander Kovarsky","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","66016|AAAI|2007|Action-Space Partitioning for Planning|For autonomous artificial decision-makers to solve realistic tasks, they need to deal with searching through large state and action spaces under time pressure. We study the problem of planning in such domains and show how structured representations of the environment's dynamics can help partition the action space into a set of equivalence classes at run time. The partitioned action space is then used to produce a reduced set of actions. This technique speeds up search and can yield significant gains in planning efficiency.|Natalia Hernandez-Gardiol,Leslie Pack Kaelbling","66042|AAAI|2007|Planning as Satisfiability with Preferences|Planning as Satisfiability is one of the most well-known and effective technique for classical planning SATPLAN has been the winning system in the deterministic track for optimal planners in the th International Planning Competition (IPC) and a co-winner in the th IPC. In this paper we extend the Planning as Satisfiability approach in order to handle preferences and SATPLAN in order to solve problems with simple preferences. The resulting system, SATPLAN(P) is competitive with SGPLAN, the winning system in the category \"simple preferences\" at the last IPC. Further, we show that SATPLAN(P) performances are (almost) always comparable to those of SATPLAN when solving the same problems without preferences in other words, introducing simple preferences in SATPLAN does not affect its performances. This latter result is due both to the particular mechanism we use in order to incorporate preferences in SAT-PLAN and to the relative low number of soft goals (each corresponding to a simple preference) usually present in planning problems. Indeed, if we consider the issue of determining minimal plans (corresponding to problems with thousands of preferences) the performances of SATPLAN(P) are comparable to those of SATPLAN in many cases, but can be significantly worse when the number of preferences is very high compared to the total number of variables in the problem. Our analysis is conducted considering both qualitative and quantitative preferences, different reductions from quantitative to qualitative ones, and most of the propositional planning domains from the IPCs and that SATPLAN can handle.|Enrico Giunchiglia,Marco Maratea","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir","65795|AAAI|2006|Functional Value Iteration for Decision-Theoretic Planning with General Utility Functions|We study how to find plans that maximize the expected total utility for a given MDP, a planning objective that is important for decision making in high-stakes domains. The optimal actions can now depend on the total reward that has been accumulated so far in addition to the current state. We extend our previous work on functional value iteration from one-switch utility functions to all utility functions that can be approximated with piecewise linear utility functions (with and without exponential tails) by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function. Functional value iteration does not maintain a value for every state but a value function that maps the total reward that has been accumulated so far into a value. We describe how functional value iteration represents these value functions in finite form, how it performs dynamic programming by manipulating these representations and what kinds of approximation guarantees it is able to make. We also apply it to a probabilistic blocksworld problem, a standard test domain for decision-theoretic planners.|Yaxin Liu,Sven Koenig"],["65628|AAAI|2006|Keeping in Touch Maintaining Biconnected Structure by Homogeneous Robots|For many distributed autonomous robotic systems, it is important to maintain communication connectivity among the robots. That is, each robot must be able to communicate with each other robot, perhaps through a series of other robots. Ideally, this property should be robust to the removal of any single robot from the system. In (Ahmadi & Stone a) we define a property of a team's communication graph that ensures this property, called biconnectivity. In that paper, a distributed algorithm to check if a team of robots is biconnected and its correctness proof are also presented. In this paper we provide distributed algorithms to add and remove robots tofrom a multi-robot team while maintaining the biconnected property. These two algorithms are implemented and tested in the PlayerStage simulator.|Mazda Ahmadi,Peter Stone","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65629|AAAI|2006|Biconnected Structure for Multi-Robot Systems|Many applications of distributed autonomous robotic systems can benefit from, or even may require, the team of robots staying within communication connectivity. For example, consider the problem of multirobot surveillance (Ahmadi & Stone ), in which a team of robots must collaboratively patrol a given area. If any two robots can directly communicate at all times, the robots can coordinate for efficient behavior. This condition holds trivially in environments that are smaller than the robots' communication range. However in larger environments, the robots must actively maintain physical locations such that any two robots can communicate -- possibly through a series of other robots. Otherwise, the robots may lose track of each others' activities and become miscoordinated. Furthermore, since robots are relatively unreliable andor may need to change tasks (for example if a robot is suddenly called by a human user to perform some other task), in a stable multirobot surveillance system, if one of the robots leaves or crashes, the rest should still be able to communicate. Some examples of other tasks that could benefit from any pair of robots being able to communicate with each other, are multi-robot exploration, search and rescue, and cleaning robots. We say that robot R is connected to robot R if there is a series of robots, each within communication range of the previous, which can pass a message from R to R. It is not possible to maintain connectivity in the face of arbitrary numbers of robot departures if there are any two robots that are not within communication of one another and all other robots simultaneously depart, the system becomes disconnected. Thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust its positioning in response to a departure more quickly than a second departure will occur. In order for the team to stay connected, even in the face of any single departure, it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. We call this property biconnectivity the removal of any one robot from the system does not disconnect the remaining robots from each other.|Mazda Ahmadi,Peter Stone","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","58080|GECCO|2007|A phenotypic analysis of GP-evolved team behaviours|This paper presents an approach to analyse the behaviours of teams of autonomous agents who work together to achieve a common goal. The agents in a team are evolved together using a genetic programming (GP)  approach where each team of agents is represented as a single GP tree or chromosome. A number of such teams are evolved and their behaviours analysed in an attempt to identify combinations of individual agent behaviours that constitute good (or bad) team behaviour. For each team we simulate a number of games and periodically capture the agents' behavioural information from the gaming environment during each simulation. This information is stored in a series of status records that can be later analysed. We compare and contrast the behaviours of agents in the evolved teams to see if there is a correlation between a team's performance(fitness score) and the combined behaviours of the team's agents. This approach could also be applied to other GP-evolved teams indifferent domains.|Darren Doherty,Colm O'Riordan","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","66216|AAAI|2007|Anytime Optimal Coalition Structure Generation|A key problem when forming effective coalitions of autonomous agents is determining the best groupings, or the optimal coalition structure, to select to achieve some goal. To this end, we present a novel, anytime algorithm for this task that is significantly faster than current solutions. Specifically, we empirically show that we are able to find solutions that are optimal in .% of the time taken by the state of the art dynamic programming algorithm (for  agents), using much less memory (O(n) instead of O(n) for n agents). Moreover, our algorithm is the first to be able to find solutions for more than  agents in reasonable time (less than  minutes for  agents, as opposed to around  months for the best previous solution).|Talal Rahwan,Sarvapali D. Ramchurn,Viet Dung Dang,Andrea Giovannucci,Nicholas R. Jennings","65695|AAAI|2006|Traffic Intersections of the Future|Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.|Kurt M. Dresner,Peter Stone","65651|AAAI|2006|Performance Evaluation Methods for the Trading Agent Competition|This paper proposes a novel method to characterize the performance of autonomous agents in the Trading Agent Competition for Supply Chain Management (TAC-SCM). We create benchmarking tools that manipulate market environments to control the conditions and provide guidelines to test trading agents. Using these tools, we show how developers can inspect their agents and unveil behaviors that might otherwise have gone undiscovered.|Brett Borghetti,Eric Sodomka","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir"],["57807|GECCO|2006|Convergence to global optima for genetic programming systems with dynamically scaled operators|This work shows asymptotic convergence to global optima for a family of dynamically scaled genetic programming systems where the underlying population consists of a fixed number of creatures (individuals) each of arbitrary size. The genetic programming systems use common mutation and crossover operators as well as fitness-proportional selection. In addition, the mutation and crossover rates are annealed to zero in predefined fashion over the course of the algorithm, and power-law scaling is used for the (possibly population-dependent) initial fitness function with (unbounded) logarithmic growth in the exponent.We assume that a set of globally optimal creatures for the optimization problem instance exists. In addition, it is assumed that the ratio of the best fitness of globally optimal creatures vs the fitness of other creatures is greater or equal a constant  in any population they jointly reside in. We discuss how both conditions can usually be satisfied in application settings. Under the above conditions, a selected, traceable sequence of probability distributions over the possible states of the properly scaled genetic programming system converge in time towards the convex set of probability distributions over uniform populations that contain only globally optimal creatures.|Lothar M. Schmitt,Stefan Droste","58026|GECCO|2007|A multimodal particle swarm optimizer based on fitness Euclidean-distance ratio|One of the most critical issues that remains to be fully addressed in existing multimodal evolutionary algorithms is the difficulty in pre-specifying parameters used for estimating how far apart optima are. These parameters are typically represented as some sorts of niching parameters in existing EAs. Without prior knowledge of a problem, it is almost impossible to determine appropriate values for such niching parameters. This paper proposes a PSO for multimodal optimization that removes the need of these niching parameters. Our results show that the proposed algorithm, Fitness Euclidean-distance Ratio based PSO (FER-PSO) is able to reliably locate multiple global optima on the search landscape over some widely used multimodal optimization test functions, given that the population size is sufficiently large.|Xiaodong Li","57692|GECCO|2006|Comparing genetic robustness in generational vs steady state evolutionary algorithms|Previous research has shown that evolutionary systems not only try to develop solutions that satisfy a fitness requirement, but indirectly attempt to develop genetically robust solutions as well -solutions where average loss of fitness due to crossover and other genetic variation operators is minimized. It has been shown that in a simple \"two peaks\" problem, where the fitness landscape consists of a broad, low peak, and a narrow, high peak, individuals initially converge on the lower (less fit), but broader peak, and that increasing an individual's genetic robustness through growth is a necessary prerequisite for convergence on the higher, narrower peak . If growth is restricted, the population remains converged on the less fit solution. We tested whether this result holds true only for generational algorithms, or whether it applies to steady state algorithms as well. We conclude that although growth occurs with both algorithms, the steady state algorithm is able to converge on the higher peak without this growth. This result shows that the role of genetic robustness in the evolutionary process is significantly different in generational versus steady state algorithms.|Josh Jones,Terry Soule","57936|GECCO|2007|Another investigation on tournament selection modelling and visualisation|Tournament selection has been widely used and studied in evolutionary algorithms. To supplement the study of tournament selection, this paper provides several models describing the probabilities that a program of a particular rank is sampled and is selected in the standard tournament selection in a simple situation and a complex situation. This paper discovers that, with the same tournament size, trends of sampling probability of a program and selection probability distributions of a population are the same regardless ofthe population size. This paper also models and investigates an alternative tournament selection method which eliminates one of the drawbacks in the standard tournament selection. Finally, this paper proposes a new fitness evaluation saving algorithm via the use of not-sampled individuals, which is a special property of tournament selection.|Huayang Xie,Mengjie Zhang,Peter Andreae","57626|GECCO|2006|A genetic model based on simulated crossover of quaternary genes for quadratic fitness|We present a genetic model based on simulated recombination of fixed sequences of quaternary genes (assuming four distinct forms, or alleles). States and dynamics of the infinite population genetic system, represented by the model, are derived for quadratic fitness functions. The bivariate marginal distribution genetic algorithm, devised to simulate the system, is experimentally compared with the univariate marginal distribution genetic algorithm with bit-based simulated crossover (in the case of in finite populations) for the MAX .CUT problem.|Marco Carpentieri,Vito Fedullo","57710|GECCO|2006|Geometric crossover for multiway graph partitioning|Geometric crossover is a representation-independent generalization of the traditional crossover defined using the distance of the solution space. Using a distance tailored to the problem at hand, the formal definition of geometric crossover allows to design new problem-specific crossovers that embed problem-knowledge in the search. The standard encoding for multiway graph partitioning is highly redundant each solution has a number of representations, one for each way of labeling the represented partition. Traditional crossover does not perform well on redundant encodings. We propose a new geometric crossover for graph partitioning based on a labeling-independent distance that filters the redundancy of the encoding. A correlation analysis of the fitness landscape based on this distance shows that it is well suited to graph partitioning. Our new genetic algorithm outperforms existing ones.|Yong-Hyuk Kim,Yourim Yoon,Alberto Moraglio,Byung Ro Moon","57939|GECCO|2007|Peer-to-peer evolutionary algorithms with adaptive autonomous selection|In this paper we describe and evaluate a fully distributed PP evolutionary algorithm (EA) with adaptive autonomous selection. Autonomous selection means that decisions regarding survival and reproduction are taken by the individuals themselves independently, without any central control.This allows for a fully distributed EA, where not only reproduction (crossover and mutation) but also selection is performed at local level. An unwanted consequence of adding and removing individuals in a non-synchronized manner is that the population size gets out of control too. This problem is resolved by addingan adaptation mechanism allowing individuals to regulate their own selection pressure. The key tothis is a gossiping algorithm that enables individuals to maintain estimates on the size andthe fitness of the population. The algorithm is experimentally evaluated on a test problem to show the viability of the idea and to gain insight into the run-time dynamics of such an algorithm. The results convincingly demonstrate the feasibility of a fully decentralized EA in which the population size can be kept stable.|W. R. M. U. K. Wickramasinghe,Maarten van Steen,A. E. Eiben","57793|GECCO|2006|A survey of mutation techniques in genetic programming|The importance of mutation varies across evolutionary computation domains including genetic programming, evolution strategies, and genetic algorithms. In the genetic programming community, researchers' view of mutation's effectiveness spans the range from an ineffective or marginal operator, to a neutral operator, to a highly effective operator that evolves solutions more effectively than genetic programming with crossover alone. Mutation implementation and associated parameters are often under reported in genetic programming research and typically lack context that justifies the technique and parameter selection. In part, reporting variance stems from the adaptation of mutation developed by the genetic algorithm community, and the creation of new mutation techniques in genetic programming. This survey describes the controversial operator in genetic programming applications, mutation selection operators, mutation techniques and offers an organization of mutation characteristics. We suggest methodologies to improve reporting of mutation parameters and related individual selection methods.|Alan Piszcz,Terence Soule","57953|GECCO|2007|Hill climbing on discrete HIFF exploring the role of DNA transposition in long-term artificial evolution|We show how a random mutation hill climber that does multi-level selection utilizes transposition to escape local optima on the discrete Hierarchical-If-And-Only-If (HIFF) problem. Although transposition is often deleterious to an individual, we outline two population models where recently transposed individuals can survive. In these models, transposed individuals survive selection through cooperation with other individuals. In the multi-population model, individuals were allowed a maturation stage to realize their potential fitness. In the genetic algorithm model, transposition helped maintain genetic diversity even within small populations. However, the results for transposition on the discrete Hierarchical-Exclusive-Or (HXOR) problem were less positive. Unlike HIFF, HXOR does not benefit from random drift. This led us to hypothesize that two conditions necessary for transposition to enhance evolvability are (i) the presence of local optima and (ii) susceptibility to random drift. This hypothesis is supported by further experiments. The findings of this paper suggest that epistasis and large mutations can sustain artificial evolution in the long-term by providing a way for individuals and populations to escape evolutionary dead ends. Paradoxically, epistasis creates local optima and holds a key to its resolution, while deleterious mutations such as transposition enhance evolvability. However, not all large mutations are equal.|Susan Khor","57862|GECCO|2006|Strong recombination weak selection and mutation|We show that there are unimodal fitness functions and genetic algorithm (GA) parameter settings where the GA, when initialized with a random population, will not move close to the fitness peak in a practically useful time period. When the GA is initialized with a population close to the fitness peak, the GA will be able to stay close to the fitness peak. Roughly speaking, the parameter settings involve strong recombination, weak selection, and require mutation. This \"bistability\" phenomenon has been previously investigated with needle-in-the-haystack fitness functions, but this fitness, when used with a GA with random initialization, requires a population size exponential in the string length for the GA to have nontrivial behavior. We introduce sloping-plateau fitness functions which show the bistability phenomenon and should scale to arbitrary string lengths. We introduce and use an unitation infinite population model to investigate the bistability phenomenon. For the fitnesses and GAs considered in the paper, we show that the use of crossover moves the GA to its fixed point faster in comparison to the same GA without crossover.|Alden H. Wright,J. Neal Richter"],["57951|GECCO|2007|An estimation of distribution algorithm with guided mutation for a complex flow shop scheduling problem|An Estimation of Distribution Algorithm (EDA) is proposed toapproach the Hybrid Flow Shop with Sequence Dependent Setup Times and Uniform Machines in parallel (HFS-SDST-UM) problem. The latter motivated by the needs of a real world company. The proposed EDA implements a fairly new mechanism to improve the search of more traditional EDAs. This is the Guided Mutation (GM). EDA-GM generates new solutions by using the information from a probability model, as all EDAs, and the local information from a good known solution. The approach is tested on several instances of HFS-SDST-UM and compared with adaptations of meta-heuristics designed for very similarproblems. Encouraging results are reported.|Abdellah Salhi,José Antonio Vázquez Rodríguez,Qingfu Zhang","57787|GECCO|2006|Sporadic model building for efficiency enhancement of hierarchical BOA|This paper describes and analyzes sporadic model building, which can be used to enhance the efficiency of the hierarchical Bayesian optimization algorithm (hBOA) and other advanced estimation of distribution algorithms (EDAs) that use complex multivariate probabilistic models. With sporadic model building, the structure of the probabilistic model is updated once every few iterations (generations), whereas in the remaining iterations only model parameters (conditional and marginal probabilities) are updated. Since the time complexity of updating model parameters is much lower than the time complexity of learning the model structure, sporadic model building decreases the overall time complexity of model building. The paper shows that for boundedly difficult nearly decomposable and hierarchical optimization problems, sporadic model building leads to a significant model-building speedup that decreases the asymptotic time complexity of model building in hBOA by a factor of (n .) to (n .), where n is the problem size. On the other hand, sporadic model building also increases the number of evaluations until convergence nonetheless, the evaluation slowdown is insignificant compared to the gains in the asymptotic complexity of model building.|Martin Pelikan,Kumara Sastry,David E. Goldberg","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","57887|GECCO|2007|An application of EDA and GA to dynamic pricing|E-commerce has transformed the way firms develop their pricing strategies, producing shift away from fixed pricing to dynamic pricing. In this paper, we use two different Estimation of distribution algorithms (EDAs), a Genetic Algorithm (GA) and a Simulated Annealing (SA) algorithm for solving two different dynamic pricing models. Promising results were obtained for an EDA confirming its suitability for resource management in the proposed model. Our analysis gives interesting insights into the application of population based optimization techniques for dynamic pricing.|Siddhartha Shakya,Fernando Oliveira,Gilbert Owusu","58170|GECCO|2007|Parameter cross-validation and early-stopping in univariate marginal distribution algorithm|In this paper, a cross-validation and early-stopping algorithm is devised for parameter updating in the Univariate Marginal Distribution Algorithm (UMDA) to reduce overftting. Our hypothesis is that the well-known problem of diversity loss in UMDA is a consequence of overfitting during the parameter estimation step at each generation. It is tested by experiments on two different optimization problems.|Hao Wu,Jonathan L. Shapiro","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang","57674|GECCO|2006|The correlation-triggered adaptive variance scaling IDEA|It has previously been shown analytically and experimentally that continuous Estimation of Distribution Algorithms (EDAs) based on the normal pdf can easily suffer from premature convergence. This paper takes a principled first step towards solving this problem. First, prerequisites for the successful use of search distributions in EDAs are presented. Then, an adaptive variance scaling theme is introduced that aims at reducing the risk of premature convergence. Integrating the scheme into the iterated density--estimation evolutionary algorithm (IDEA) yields the correlation-triggered adaptive variance scaling IDEA (CT-AVS-IDEA). The CT-AVS-IDEA is compared to the original IDEA and the Evolution Strategy with Covariance Matrix Adaptation (CMA-ES) on a wide range of unimodal test-problems by means of a scalability analysis. It is found that the average number of fitness evaluations grows subquadratically with the dimensionality, competitively with the CMA-ES. In addition, CT-AVS-IDEA is indeed found to enlarge the class of problems that continuous EDAs can solve reliably.|Jörn Grahl,Peter A. N. Bosman,Franz Rothlauf","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith","57969|GECCO|2007|Dependency trees permutations and quadratic assignment problem|This paper describes and analyzes an estimation of distribution algorithm based on dependency tree models (dtEDA), which can explicitly encode probabilistic models for permutations. dtEDA is tested on deceptive ordering problems and a number of instances of the quadratic assignment problem. The performance of dtEDA is compared to that of the standard genetic algorithm with the partially matched crossover (PMX) and the linear order crossover (LOX). In the quadratic assignment problem, the robust tabu search is also included in the comparison.|Martin Pelikan,Shigeyoshi Tsutsui,Rajiv Kalapala"],["57915|GECCO|2007|A spatial model of the red queen effect|Van Valen first discovered the \"Red Queen Effect\" (RQE), where two species can dramatically co-evolve their phenotypes over time but never gain a relative advantage . In the ideal version of the RQE, regardless of the actual values that the species evolve to obtain, they have not moved in relation to each other. Though previous models of the RQE exist, we developed an agent-based model (ABM) which has a base ontology more similar to real world coevolutionary systems than equation-based models (EBMs). For instance, this model contains spatial information and an individuallevel reproduction mechanism. Yet this model recreates traditional EBM results. For instance Dieckmann et al show that there are three possible outcomes of competitive coevolution predator dominance, prey dominance and evolutionary cycling (RQE) . By reconceptualizing this EBM using an ABM, we make it easier for students and researchers to understand, manipulate, and modify this model . The model is written in the NetLogo agent-based modeling environment . The model is initialized with  predator and  prey agents. Predator agents have a resistance level r and prey agents have a poison p. The agents are initially randomly distributed on a toroidal real-valued  by  grid. The initial resistance and poison values for the predators and prey are drawn from normal distributions with means r and p and a standard deviation of . During a model timestep, each agent moves one unit at a random heading. If at the end of its move a predator is within  unit of a prey, then it will challenge the prey. The predator will compare its resistance value to the prey's poison value and which ever agent has the larger value will win the challenge and the other agent will be killed. At the end of an agent's turn if the total number of agents is less than the maximum carrying capacity the agent will reproduce with a % probability. The new agent's initial poison  resistance will be drawn from a normal distribution with the parent's poison  resistance as the mean value and a standard deviation of . Our goal was to investigate whether this ABMwould replicate the results of the EBM of . Our parameter of interest was p we held r constant at  and varied p from  to  at increments of . For each value we ran the model  times for  timesteps. Figure  illustrates the final average values of both the resistance and the poison for the various initial values. If there are no predators or prey then a value of  is plotted for the respective final trait. In most cases, one species drives the other to extinction, and there is little change in the initial trait values. However when the value of p is similar to r then neither species is completely destroyed, but if there is any significant difference between r and p then one species will die off. The highest final trait values are found when p  r, in this case we see the results of the RQE since the final trait values are much higher than the initial values. These final values are more than . orders of magnitude larger than the initial values. This model reproduces classical models of the RQE, but has two different mobile species interacting on a spatial grid over time which is a closer representation of reality than traditional models. This closer representation makes ABMs excellent teaching and experimental tools because their basic assumptions can easily be manipulated and explored. Acknowledgments We thank the National Science Foundation and NICO for supporting this research.|Jules Ottino-Loffler,William Rand,Uri Wilensky","65692|AAAI|2006|Unifying Logical and Statistical AI|Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system.|Pedro Domingos,Stanley Kok,Hoifung Poon,Matthew Richardson,Parag Singla","57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","65789|AAAI|2006|Efficient Active Fusion for Decision-Making via VOI Approximation|Active fusion is a process that purposively selects the most informative information from multiple sources as well as combines these information for achieving a reliable result efficiently. This paper presents a general mathematical framework based on Influence Diagrams (IDs) for active fusion and timely decision making. Within this framework, an approximation algorithm is proposed to efficiently compute nonmyopic value-of-information (VOI) for multiple sensory actions. Meanwhile a sensor selection algorithm is proposed to choose optimal sensory action sets efficiently. Both the experiments with synthetic data and real data from a real-world application demonstrate that the proposed framework together with the algorithms are well suited to applications where the decision must be made efficiently and timely from dynamically available information of diverse and disparate sources.|Wenhui Liao,Qiang Ji","65995|AAAI|2007|R-CAST Integrating Team Intelligence for Human-Centered Teamwork|Developing human-centered agent architectures requires the integral consideration of architectural flexibility, teamwork adaptability, and context reasoning capability. With the integration of various forms of team intelligence including shared teamwork process and progress, dynamic context management and infomation dependency reasoning, and recognition-primed collaborative decision mechanism, R-CAST offers a flexible solution to developing cognitive aids for the support of human-centered teamwork in information and knowledge intensive domains. In this paper, we present the key features of R-CAST. As evidence of its applications in complex real-world problems, we give two experimental evaluations of R-CAST as teammates and decision aids of human Command and Control teams.|Xiaocong Fan,John Yen","58184|GECCO|2007|MDL-based fitness for feature construction|Primitive data representation of real-world data facilitates attribute interactions, which make information opaque to most learners. Feature Construction (FC) aims to abstract and encapsulate interactions into new features and outline them to the learner. When a GA is applied to perform FC, the goal is to generate features that facilitate more accurate learning. Then the GA's fitness function should estimate the quality of the constructed features. We propose a new fitness function based on Minimum Description Length (MDL). This fitness is incorporated in MFEGA to improve its accuracy. The new system is compared with other systems based on Entropy or error-rate fitness.|Leila Shila Shafti,Eduardo Pérez","66165|AAAI|2007|Towards an Integrated Robot with Multiple Cognitive Functions|We present integration mechanisms for combining heterogeneous components in a situated information processing system, illustrated by a cognitive robot able to collaborate with a human and display some understanding of its surroundings. These mechanisms include an architectural schema that encourages parallel and incremental information processing, and a method for binding information from distinct representations that when faced with rapid change in the world can maintain a coherent, though distributed, view of it. Provisional results are demonstrated in a robot combining vision, manipulation, language, planning and reasoning capabilities interacting with a human and manipulable objects.|Nick Hawes,Aaron Sloman,Jeremy Wyatt,Michael Zillich,Henrik Jacobsson,Geert-Jan M. Kruijff,Michael Brenner,Gregor Berginc,Danijel Skocaj","66117|AAAI|2007|Handling Non-Sentential Utterances in a Continuous Understanding Framework|The goal of my research is to understand speech input in a continuous manner by treating the input stream as fragmental utterances. This allows us to use various approaches to predict what comes downstream. Possible interpretations are trimmed by such predictions which in turn also allow us to complete information not readily available in the fragmental utterance. Semantic frames can encode all possible arguments for domain actions. As utterances are processed continuously, appropriate frames can be activated so that fragment interpretations can fill, correct or extend frames under consideration. In turn, feedback can be provided to the parser as the frames are manipulated possibly based on the completeness of the semantic frame construction.|Carlos Gómez Gallo","58069|GECCO|2007|SwarmArchitect a swarm framework for collaborative construction|Computer game development has become increasingly popular in the field of autonomous systems. One of the main topics studies the building of various architectures in computer games. A realistic human-like architecture is expected in a thematic computer game, since it strongly motivates the game players in an intuitive way. However, the task of building a human-like architecture is non-trivial since the construction is a real time process without human supervision. In this paper, we present a collective building algorithm inspired by social insects for intelligent construction based on multiple agents. A swarm of virtual agents indirectly design edifications, which resemble basic features in human-like architecture by using a stigmergic mechanism along with branching rules. The main idea of the algorithm is to map sensory information to appropriate building actions.|Yifeng Zeng,Jorge Cordero Hernandez,Dennis Plougman Buus"],["66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy Lécué,Alexandre Delteil","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65730|AAAI|2006|Large Scale Knowledge Base Systems An Empirical Evaluation Perspective|In this paper, we discuss how our work on evaluating Semantic Web knowledge base systems (KBSs) contributes to address some broader AI problems. First, we show how our apprcach provides a benchmarking solution to the Semantic Web, a new application area of AI. Second, we discuss how the approach is also beneficial in a more traditional AI context. We focus on issues such as scalability, performance tradeoffs, and the comparison of different classes of systems.|Yuanbo Guo,Abir Qasem,Jeff Heflin","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["58132|GECCO|2007|Self-modifying cartesian genetic programming|In nature, systems with enormous numbers of components (i.e. cells) are evolved from a relatively small genotype. It has not yet been demonstrated that artificial evolution is sufficient to make such a system evolvable. Consequently researchers have been investigating forms of computational development that may allow more evolvable systems. The approaches taken have largely used re-writing, multi- cellularity, or genetic regulation. In many cases it has been difficult to produce general purpose computation from such systems.In this paper we introduce computational development using a form of Cartesian Genetic Programming that includes self-modification operations. One advantage of this approach is that ab initio the system can be used to solve computational problems. We present results on a number of problems and demonstrate the characteristics and advantages that self-modification brings.|Simon Harding,Julian Francis Miller,Wolfgang Banzhaf","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57960|GECCO|2007|A new crossover technique for Cartesian genetic programming|Genetic Programming was first introduced by Koza using tree representation together with a crossover technique in which random sub-branches of the parents' trees are swapped to create the offspring. Later Miller and Thomson introduced Cartesian Genetic Programming, which uses directed graphs as a representation to replace the tree structures originally introduced by Koza. Cartesian Genetic Programming has been shown to perform better than the traditional Genetic Programming but it does not use crossover to create offspring, it is implemented using mutation only. In this paper a new crossover method in Genetic Programming is introduced. The new technique is based on an adaptation of the Cartesian Genetic Programming representation and is tested on two simple regression problems. It is shown that by implementing the new crossover technique, convergence is faster than that of using mutation only in the Cartesian Genetic Programming method.|Janet Clegg,James Alfred Walker,Julian Francis Miller","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57850|GECCO|2006|A multi-chromosome approach to standard and embedded cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is an extension of Cartesian Genetic Programming (CGP) that can automatically acquire, evolve and re-use partial solutions in the form of modules. In this paper, we introduce for the first time a new multi-chromosome approach to CGP and ECGP that allows difficult problems with multiple outputs to be broken down into many smaller, simpler problems with single outputs, whilst still encoding the entire solution in a single genotype. We also propose a multi-chromosome evolutionary strategy which selects the best chromosomes from the entire population to form the new fittest individual, which may not have been present in the population. The multi-chromosome approach to CGP and ECGP is tested on a number of multiple output digital circuits. Computational Effort figures are calculated for each problem and compared against those for CGP and ECGP. The results indicate that the use of multiple chromosomes in both CGP and ECGP provide a significant performance increase on all problems tested.|James Alfred Walker,Julian Francis Miller,Rachel Cavill","57963|GECCO|2007|Graph structured program evolution|In recent years a lot of Automatic Programming techniques have developed. A typical example of Automatic Programming is Genetic Programming (GP), and various extensions and representations for GP have been proposed so far. However, it seems that more improvements are necessary to obtain complex programs automatically. In this paper we proposed a new method called Graph Structured Program Evolution (GRAPE). The representation of GRAPE is graph structure, therefore it can represent complex programs (e.g. branches and loops) using its graph structure. Each program is constructed as an arbitrary directed graph of nodes and data set. The GRAPE program handles multiple data types using the data set for each type, and the genotype of GRAPE is the form of a linear string of integers. We apply GRAPE to four test problems, factorial, Fibonacci sequence, exponentiation and reversing a list, and demonstrate that the optimum solution in each problem is obtained by the GRAPE system.|Shinichi Shirakawa,Shintaro Ogino,Tomoharu Nagao","58087|GECCO|2007|Unwitting distributed genetic programming via asynchronous JavaScript and XML|The success of a genetic programming system in solving a problem is often a function of the available computational resources. For many problems, the larger the population size and the longer the genetic programming run the more likely the system is to find a solution. In order to increase the probability of success on difficult problems, designers and users of genetic programming systems often desire access to distributed computation, either locally or across the internet, to evaluate fitness cases more quickly. Most systems for internet-scale distributed computation require a user's explicit participation and the installation of client side software. We present a proof-of-concept system for distributed computation of genetic programming via asynchronous javascript and XML (AJAX) techniques which requires no explicit user interaction and no installation of client side software. Clients automatically and possibly even unknowingly participate in a distributed genetic programming system simply by visiting a webpage, thereby allowing for the solution of genetic programming problems without running a single local fitness evaluation. The system can be easily introduced into existing webpages to exploit unused client-side computation for the solution of genetic programming and other problems.|Jon Klein,Lee Spector","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","57990|GECCO|2007|Solving real-valued optimisation problems using cartesian genetic programming|Classical Evolutionary Programming (CEP) and Fast Evolutionary Programming (FEP) have been applied to real-valued function optimisation. Both of these techniques directly evolve the real-values that are the arguments of the real-valued function. In this paper we have applied a form of genetic programming called Cartesian Genetic Programming (CGP) to a number of real-valued optimisation benchmark problems. The approach we have taken is to evolve a computer program that controls a writing-head, which moves along and interacts with a finite set of symbols that are interpreted as real numbers, instead of manipulating the real numbers directly. In other studies, CGP has already been shown to benefit from a high degree of neutrality. We hope to exploit this for real-valued function optimisation problems to avoid being trapped on local optima. We have also used an extended form of CGP called Embedded CGP (ECGP) which allows the acquisition, evolution and re-use of modules. The effectiveness of CGP and ECGP are compared and contrasted with CEP and FEP on the benchmark problems. Results show that the new techniques are very effective.|James Alfred Walker,Julian Francis Miller","57808|GECCO|2006|Predicting currency exchange rates by genetic programming with trigonometric functions and high-order statistics|This paper describes an extension of the traditional application of Genetic Programming in the domain of the prediction of daily currency exchange rates. In combination with trigonometric operators, we introduce a new set of high-order statistical functions in a unique representation and analyze each system performance using daily returns of the British Pound and Japanese Yen. We will demonstrate that the introduction of high-order statistical functions in combination with trigonometric functions will outperform other traditional models such as Genetic Programming with the basic function set and ARMA models. Performance will be measured on hit percentage, average percentage change, and profit.|Roy Schwaerzel,Tom Bylander"],["57678|GECCO|2006|Exploring network topology evolution through evolutionary computations|We present an evolutionary methodology that explores the evolution of network topology when a uniform growth of the network traffic is considered. The network redesign problem is formulated as an optimization problem, subject to a set of design and performance constraints, while minimizing the redesign cost by maintaining as many as possible of the network devices that constitute the original topology. The experimental results for a -level network redesign problem (consisting of  client nodes) demonstrate the value of the search technique within the genetic algorithms in finding good solutions with respect to redesign cost and time.|Sami J. Habib,Alice C. Parker","57629|GECCO|2006|GRASP - evolution for constraint satisfaction problems|There are several evolutionary approaches for solving random binary Constraint Satisfaction Problems (CSPs). In most of these strategies we find a complex use of information regarding the problem at hand. Here we present a hybrid Evolutionary Algorithm that outperforms previous approaches in terms of effectiveness and compares well in terms ofefficiency. Our algorithm is conceptual and simple, featuring a GRASP-like (GRASP stands for Greedy Randomized Adaptive Search Procedure) mechanism for genotype-to-phenotype mapping, and without considering any specific knowledge of the problem. Therefore, we provide a simple algorithm that harnesses generality while boosting performance.|Manuel Cebrián,Iván Dotú","65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","65631|AAAI|2006|The Impact of Balancing on Problem Hardness in a Highly Structured Domain|Random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider random problem distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCPQWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness.|Carlos Ansótegui,Ramón Béjar,Cèsar Fernández,Carla P. Gomes,Carles Mateu","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","57720|GECCO|2006|Multiobjective genetic algorithms for materialized view selection in OLAP data warehouses|On-Line Analytical Processing (OLAP) tools are frequently used in business, science and health to extract useful knowledgefrom massive databases. An important and hard optimization problem in OLAP data warehouses is the view selection problem, consisting of selecting a set of aggregate views of the data for speeding up future query processing. A common variant of the view selection problem addressed in the literature minimizes the sum of maintenance cost and query time on the view set. Converting what is inherently an optimization problem with multiple conflicting objectives into one with a single objective ignores the need and value of a variety of solutions offering various levels of trade-off between the objectives. We apply two non-elitist multiobjective evolutionary algorithms (MOEAs) to view selection under a size constraint. Our emphasis is to determine the suitability of the combination of MOEAs with constraint handling to the view selection problem, compared to a widely used greedy algorithm. We observe that the evolutionary process mimics that of the greedy in terms of the convergence process in the population. The MOEAs are competitive with the greedy on a variety of problem instances, often finding solutions dominating it in a reasonable amount of time.|Michael Lawrence","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65733|AAAI|2006|A BDD-Based Polytime Algorithm for Cost-Bounded Interactive Configuration|Interactive configurators are decision support systems assisting users in selecting values for parameters that respect given constraints. The underlying knowledge can be conveniently formulated as a Constraint Satisfaction Problem where the constraints are propositional formulas. The problem of interactive configuration was originally inspired by the product configuration problem with the emergence of the masscustomization paradigm in product manufacturing, but has also been applied to other tasks requiring user interaction, such as specifying services or setting up complex equipment. The user-friendly requirements of complete, backtrack-free and real-time interaction makes the problem computationally challenging. Therefore, it is beneficial to compile the configuration constraints into a tractable representation such as Binary Decision Diagrams (BOD) (Bryant ) to support efficient user interaction. The compilation deals with the NP-hardness such that the online interaction is in polynomial time in the size of the BOD. In this paper we address the problem of extending configurators so that a user can interactively limit configuration choices based on a maximum cost (such as price or weight of a product) of any valid configuration, in a complete, backtrack-free and real-time manner. The current BOD compilation approach is not adequate for this purpose, since adding the total cost information to the constraints description can dramatically increase the size of the compiled BOD. We show how to extend this compilation approach to solve the problem while keeping the polynomial time guarantees.|Tarik Hadzic,Henrik Reif Andersen","65979|AAAI|2007|Nonmyopic Informative Path Planning in Spatio-Temporal Models|In many sensing applications we must continuously gather information to provide a good estimate of the state of the environment at every point in time. A robot may tour an environment, gathering information every hour. In a wireless sensor network, these tours correspond to packets being transmitted. In these settings, we are often faced with resource restrictions, like energy constraints. The users issue queries with certain expectations on the answer quality. Thus, we must optimize the tours to ensure the satisfaction of the user constraints, while at the same time minimize the cost of the query plan. For a single timestep, this optimization problem is NP-hard, but recent approximation algorithms with theoretical guarantees provide good solutions. In this paper, we present a new efficient algorithm, exploiting dynamic programming and submodularity of the information collected, that efficiently plans data collection tours for an entire (finite) horizon. Our algorithm can use any single step procedure as a black box, and, based on its properties, provides strong theoretical guarantees for the solution. We also provide an extensive empirical analysis demonstrating the benefits of nonmyopic planning in two real world sensing applications.|Alexandra Meliou,Andreas Krause,Carlos Guestrin,Joseph M. Hellerstein","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith"]]},"title":{"entropy":5.8907711864408725,"topics":["genetic algorithm, genetic programming, algorithm for, genetic for, evolutionary algorithm, using genetic, for, for problem, neural networks, for networks, using algorithm, evolutionary for, genetic and, genetic with, algorithm with, algorithm problem, genetic problem, using evolutionary, evolutionary computation, new for","learning and, reinforcement learning, semantic web, for systems, for learning, learning, learning with, classifier systems, systems, web service, support vector, artificial immune, case-based reasoning, decision processes, learning classifier, for reasoning, learning systems, reasoning about, service composition, web planning","for and, and its, and, for scheduling, description logic, local search, search space, flexible shop, for logic, search for, method for, and search, for information, constraint satisfaction, shop scheduling, the logic, its application, based approach, genetic flexible, the and","particle swarm, the, the problem, for the, the algorithm, the web, the and, optimization algorithm, for optimization, the effects, particle optimization, swarm optimization, multi-objective optimization, multiobjective optimization, for evolution, evolution strategies, dynamic environments, optimization, differential evolution, estimation distribution","for classification, for using, using evolutionary, for data, evolutionary design, using algorithm, for design, ensemble for, selection using, selection data, using and, using, for gene, algorithm design, gene using, for computing, classification using, design and, feature for, for random","algorithm for, evolutionary algorithm, evolutionary for, genetic algorithm, algorithm problem, for problem, algorithm with, evolutionary with, algorithm and, evolutionary computation, hybrid for, and tree, using algorithm, for selection, algorithm selection, hybrid algorithm, the algorithm, and evolutionary, spanning tree, for tree","for systems, learning with, learning classifier, classifier systems, systems, systems and, support vector, artificial immune, learning systems, with, systems using, for with, support for, with and, for classifier, immune systems, artificial systems, machine learning, vector machine, for machine","learning and, for learning, reinforcement learning, for reasoning, and reasoning, language and, case-based reasoning, learning, decision processes, reasoning about, markov processes, learning model, markov decision, action model, and processing, temporal learning, spatial systems, the reasoning, for action, integrating and","for logic, and logic, for constraint, the logic, description logic, framework for, and constraint, constraint satisfaction, belief and, with constraint, for description, the description, and set, logic, default logic, modal logic, constraint, with and, for belief, and framework","for scheduling, and flexible, planning and, flexible shop, and hybrid, for planning, genetic and, for and, genetic flexible, and problem, for agent, shop scheduling, and shop, hybrid for, for flexible, flexible scheduling, and scheduling, shop problem, genetic shop, for shop","adaptive for, for parameter, test for, memetic algorithm, variance scaling, adaptive algorithm, population for, tool for, adaptive variance, parameter and, adaptive scaling, adaptive, for the, robustness, continuous, eda, assignment, virtual, tuning, size","for optimization, optimization algorithm, the evolution, the optimization, for evolution, evolution strategies, multi-objective optimization, multiobjective optimization, evolutionary optimization, for multi-objective, optimization problem, the algorithm, differential evolution, algorithm for, optimization, multi-objective evolutionary, and evolution, estimation algorithm, multi-objective problem, multi-objective algorithm"],"ranking":[["57669|GECCO|2006|Segmentation of medical images using a genetic algorithm|Segmentation of medical images is challenging due to poor image contrast and artifacts that result in missing or diffuse organtissue boundaries. Consequently, this task involves incorporating as much prior information as possible (e.g., texture, shape, and spatial location of organs) into a single framework. In this paper, we present a genetic algorithm for automating the segmentation of the prostate on two-dimensional slices of pelvic computed tomography (CT) images. In this approach the segmenting curve is represented using a level set function, which is evolved using a genetic algorithm (GA). Shape and textural priors derived from manually segmented images are used to constrain the evolution of the segmenting curve over successive generations.We review some of the existing medical image segmentation techniques. We also compare the results of our algorithm with those of a simple texture extraction algorithm (Laws' texture measures) as well as with another GA-based segmentation tool called GENIE. Our preliminary tests on a small population of segmenting contours show promise by converging on the prostate region. We expect that further improvements can be achieved by incorporating spatial relationships between anatomical landmarks, and extending the methodology to three dimensions.|Payel Ghosh,Melanie Mitchell","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","57654|GECCO|2006|The snake in the box problem mathematical conjecture and a genetic algorithm approach|With applications in coding theory and hypercube-based computing and networking, the \"snake in the box\" problem is of great practical importance. Moreover, it is both mathematically elegant and highly difficult. The problem is simply to find the longest \"snake\" in a hypercube. However, as the hypercube grows in dimensionality, the size of the search space increases exponentially. Moreover, as the maximum snake length is only known for the smallest dimensions (where the snakes themselves have already been identified), there is no known stopping criterion for the search in higher dimensions. In this paper we make a mathematical conjecture about the possible maximum length of a snake in a hypercube of dimension d. We use a genetic algorithm for finding snakes in a  hypercube to show some results.|Pedro A. Diaz-Gomez,Dean F. Hougen","58182|GECCO|2007|ExGA II an improved exonic genetic algorithm for the multiple knapsack problem|ExGA I, a previously presented genetic algorithm, successfully solved numerous instances of the multiple knapsack problem (MKS) by employing an adaptive repair function that made use of the algorithm's modular encoding. Here we present ExGA II, an extension of ExGA I that implements additional features which allow the algorithm to perform more reliably across a larger set of benchmark problems. In addition to some basic modifications of the algorithm's framework, more specific extensions include the use of a biased mutation operator and adaptive control sequences which are used to guide the repair procedure. The success rate of ExGA II is superior to its predecessor, and other algorithms in the literature, without an overall increase in the number of function evaluations required to reach the global optimum. In fact, the new algorithm exhibits a significant reduction in the number of function evaluations required for the largest problems investigated. We also address the computational cost of using a repair function and show that the algorithm remains highly competitive when this cost is accounted for.|Philipp Rohlfshagen,John A. Bullinaria","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","57747|GECCO|2006|Multi-step environment learning classifier systems applied to hyper-heuristics|Heuristic Algorithms (HA) are very widely used to tackle practical problems in operations research. They are simple, easy to understand and inspire confidence. Many of these HAs are good for some problem instances while very poor for other cases. While Meta-Heuristics try to find which is the best heuristic andor parameters to apply for a given problem instance Hyper-Heuristics (HH) try to combine several heuristics in the same solution searching process, switching among them whenever the circumstances vary. Besides, instead to solve a single problem instance it tries to find a general algorithm to apply to whole families of problems. HH use evolutionary methods to search for such a problem-solving algorithm and, once produced, to apply it to any new problem instance desired. Learning Classifier Systems (LCS), and in particular XCS, represents an elegant and simple way to try to fabricate such a composite algorithm. This represents a different kind of problem to those already studied by the LCS community. Previous work, using single step environments, already showed the usefulness of the approach. This paper goes further and studies the novel use of multi-step environments for HH and an alternate way to consider states to see if chains of actions can be learnt. A non-trivial, NP-hard family of problems, the Bin Packing one, is used as benchmark for the procedure. Results of the approach are very encouraging, showing outperformance over all HAs used individually and over previously reported work by the authors, including non-LCS (a GA based approach used for the same BP set of problems) and LCS (using single step environments).|Javier G. Marín-Blázquez,Sonia Schulenburg","57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","65761|AAAI|2006|Learning Systems of Concepts with an Infinite Relational Model|Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.|Charles Kemp,Joshua B. Tenenbaum,Thomas L. Griffiths,Takeshi Yamada,Naonori Ueda","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","66248|AAAI|2007|Automatic Algorithm Configuration Based on Local Search|The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variablevalue ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at httpwww.cs.ubc.calabsbetaProjectsParamILS.|Frank Hutter,Holger H. Hoos,Thomas Stützle","66094|AAAI|2007|Synthesis of Constraint-Based Local Search Algorithms from High-Level Models|The gap in automation between MIPSAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location.|Pascal Van Hentenryck,Laurent D. Michel","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","57869|GECCO|2006|Genetic local search for multicast routing|We describe a population-based search algorithm for cost minimization of multicast routing. The algorithm utilizes the partially mixed crossover operation (PMX) and a landscape analysis in a pre-processing step. The aim of the landscape analysis is to estimate the depth  of the deepest local minima in the landscape generated by the routing tasks and the objective function. The analysis employs simulated annealing with a logarithmic cooling schedule (LSA). The local search performs alternating sequences of descending and ascending steps for each individual of the population, where the length of a sequence with uniform direction is controlled by the estimated value of . We present results from computational experiments on a synthetic routing tasks, and we provide experimental evidence that our genetic local search procedure, that combines LSA and PMX, performs better than algorithms using either LSA or PMX only.|Mohammed S. Zahrani,Martin J. Loomes,James A. Malcolm,Andreas Alexander Albrecht","57723|GECCO|2006|Combining simplex with niche-based evolutionary computation for job-shop scheduling|We propose a hybrid algorithm (called ALPINE) between Genetic Algorithm and Dantzig's Simplex method to approximate optimal solutions for the Flexible Job-Shop Problem. Locally, Simplex is extended for the JSP linear program to reduce the number of infeasible solutions while solution quality is improved with an operation order search. Globally, a niche-based evolutionary strategy is employed to gain parallelization while solution diversity is maintained in two ways composite dispatching rule-based population initialization and memory-based machine assignment. Performance results on benchmark problems show that ALPINE outperforms existing hybrid techniques with a new global optima found for the x Flexible Job Shop Problem.|Syhlin Kuah,Joc Cing Tay","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","58198|GECCO|2007|Using evolutionary computation and local search to solve multi-objective flexible job shop problems|Finding realistic schedules for Flexible Job Shop Problems has attracted many researchers recently due to its NP-hardness. In this paper, we present an efficient approach for solving the multi-objective flexible job shop by combining Evolutionary Algorithm and Guided Local Search. Instead of applying random local search to find neighborhood solutions, we introduce a guided local search procedure to accelerate the process of convergence to Pareto-optimal solutions. The main improvement of this combination is to help diversify the population towards the Pareto-front. Empirical studies show that ) the gaps between the obtained results and known lower bounds are small, and ) the multi-objective solutions of our algorithms dominate previous designs for solving the same benchmarks while incurring less computational time.|Nhu Binh Ho,Joc Cing Tay","57682|GECCO|2006|Local search for multiobjective function optimization pareto descent method|Genetic Algorithm (GA) is known as a potent multiobjective optimization method, and the effectiveness of hybridizing it with local search (LS) has recently been reported in the literature. However, there is a relatively small number of studies on LS methods for multiobjective function optimization. Although each of the existing LS methods has some strong points, they have respective drawbacks such as high computational cost and inefficiency in improving objective functions. Hence, a more effective and efficient LS method is being sought, which can be used to enhance the performance of the hybridization.Defining Pareto descent directions as descent directions to which no other descent directions are superior in improving all objective functions, this paper proposes a new LS method, Pareto Descent Method (PDM), which finds Pareto descent directions and moves solutions in such directions thereby improving all objective functions simultaneously. In the case part or all of them are infeasible, it finds feasible Pareto descent directions or descent directions as appropriate. PDM finds these directions by solving linear programming problems, which is computationally inexpensive. Experiments have shown PDM's superiority over existing methods.|Ken Harada,Jun Sakuma,Shigenobu Kobayashi","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan"],["57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","58076|GECCO|2007|On the scalability of evolution strategies in the optimization of dynamic molecular alignment|We consider the numerical optimization of dynamic molecular alignment by shaped femtosecond pulses, and study the scalability of the electric field subject to optimization by Evolution Strategies.The trade-off between fine-tuning of the electric field versus the evolutionary optimization feasibility is investigated.|Ofer M. Shir,Thomas Bäck,Marc J. J. Vrakking","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58208|GECCO|2007|Two-level of nondominated solutions approach to multiobjective particle swarm optimization|In multiobjective particle swarm optimization (MOPSO) methods, selecting the local best and the global best for each particle of the population has a great impact on the convergence and diversity of solutions, especially when optimizing problems with high number of objectives. This paper presents a two-level of nondominated solutions approach to MOPSO. The ability of the proposed approach to detect the true Pareto optimal solutions and capture the shape of the Pareto front is evaluated through experiments on well-known non-trivial test problems. The diversity of the nondominated solutions obtained is demonstrated through different measures. The proposed approach has been assessed through a comparative study with the reported results in the literature.|M. A. Abido","57813|GECCO|2006|Dynamic fitness inheritance proportion for multi-objective particle swarm optimization|In this paper, we propose a dynamic mechanism to vary the probability by which fitness inheritance is applied throughout the run of a multi-objective particle swarm optimizer, in order to obtain a greater reduction in computational cost (than the obtained with a fixed probability), without dramatically affecting the quality of the results. The results obtained show that it is possible to reduce the computational cost by % without affecting the quality of the obtained Pareto front.|Margarita Reyes Sierra,Carlos A. Coello Coello"],["58060|GECCO|2007|Volatility forecasting using time series data mining and evolutionary computation techniques|Traditional parametric methods have limited success in estimating and forecasting the volatility of financial securities. Recent advance in evolutionary computation has provided additional tools to conduct data mining effectively. The current work applies the genetic programming in a Time Series Data Mining framework to characterize the S&P high frequency data in order to forecast the one step ahead integrated volatility. Results of the experiment have shown to be superior to those derived by the traditional methods.|Irwin Ma,Tony Wong,Thiagas Sankar","57997|GECCO|2007|Feature selection and classification in noisy epistatic problems using a hybrid evolutionary approach|A hybrid evolutionary approach is proposed for the combined problem of feature selection (using a genetic algorithm with IntersectionUnion recombination and a fitness function based on a counter-propagation artificial neural network) and subsequent classifier construction (using strongly-typed genetic programming), for use in nonlinear association studies with relatively large potential feature sets and noisy class data. The method was tested using synthetic data with various degrees of injected noise, based on a proposed mental health database.allResults show the algorithm has good potential for feature selection, classification and function characterization.|Drew DeHaas,Jesse Craig,Colin Rickert,Margaret J. Eppstein,Paul Haake,Kirsten Stor","58034|GECCO|2007|Clustering gene expression data via mining ensembles of classification rules evolved using moses|A novel approach, model-based clustering, is described foridentifying complex interactions between genes or gene-categories based on static gene expression data. The approach deals with categorical data, which consists of a set of gene expressionprofiles belonging to one category, and a set belonging to anothercategory. An evolutionary algorithm (Meta-Optimizing Semantic Evolutionary Search, or MOSES) is used to learn an ensemble of classification models distinguishing the two categories, based on inputs that are features corresponding to gene expression values. Each feature is associated with a model-based vector, which encodes quantitative information regarding the utilization of the feature across the ensembles of models. Two different ways of constructing these vectors are explored. These model-based vectors are then clustered using a variant of hierarchical clustering called Omniclust. The result is a set of model-based clusters, in which features are gathered together if they are often considered together by classification models -- which may be because they're co-expressed, or may be for subtler reasons involving multi-gene interactions. The method is illustrated by applying it to two datasets regarding human gene expression, one drawn from brain cells and pertinent to the neurogenetics of aging, and the other drawn from blood cells and relating to differentiating between types of lymphoma. We find that, compared to traditional expression-based clustering, the new method often yields clusters that have higher mathematical quality (in the sense of homogeneity and separation) and also yield novel and meaningful insights into the underlying biological processes.|Moshe Looks,Ben Goertzel,Lúcio de Souza Coelho,Mauricio Mudado,Cassio Pennachin","57922|GECCO|2007|Reducing the number of transistors in digital circuits using gate-level evolutionary design|This paper shows that the evolutionary design of digital circuits which is conducted at the gate level is able to produce human-competitive circuits at the transistor level. In addition to standard gates, we utilize unconventional gates (such as the NANDNOR gate and NORNAND gate) that consist of a few transistors but exhibit non-trivial -input logic functions. Novel implementations of adders and majority circuits evolved using these gates contain fewer transistors than the smallest existing implementations of these circuits. Moreover, it was shown that the use of these gates significantly improves the success rate of the search process.|Zbysek Gajda,Lukás Sekanina","57625|GECCO|2006|Ensemble selection for evolutionary learning using information theory and prices theorem|This paper presents an information theoretic perspective on design and analysis of evolutionary algorithms. Indicators of solution quality are developed and applied not only to individuals but also to ensembles, thereby ensuring information diversity. Price's Theorem is extended to show how joint indicators can drive reproductive sampling rate of potential parental pairings. Heritability of mutual information is identified as a key issue.|Stuart W. Card,Chilukuri K. Mohan","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","58073|GECCO|2007|Using genetic algorithms for naval subsystem damage assessment and design improvements|Some auxiliary systems of next generation naval ships will utilize distributed automatic control. Such distributed control systems will use interconnected sensors, actuators, controllers and networking components to diagnose and reconfigure the auxiliary systems. Testing these systems will be difficult with traditional methods of fault analysis due to the interconnected and automatic nature of these subsystems. We have designed a suite of genetic algorithms to find interesting and hidden damage scenarios in a testbed of a naval subsystem. Given this knowledge, we use a genetic algorithm to improve upon the design of this subsystem.|Christopher McCubbin,David Scheidt,Oliver Bandte,Steven Marshall,Iavor Trifonov","57930|GECCO|2007|Optimal design of ad hoc injection networks by using genetic algorithms|This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.|Grégoire Danoy,Enrique Alba,Pascal Bouvry,Matthias R. Brust","57934|GECCO|2007|Evolutionary selection of minimum number of features for classification of gene expression data using genetic algorithms|Selecting the most relevant factors from genetic profiles that can optimally characterize cellular states is of crucial importance in identifying complex disease genes and biomarkers for disease diagnosis and assessing drug efficiency. In this paper, we present an approach using a genetic algorithm for a feature subset selection problem that can be used in selecting the near optimum set of genes for classification of cancer data. In substantial improvement over existing methods, we classified cancer data with high accuracy with less features.|Alper Küçükural,Reyyan Yeniterzi,Süveyda Yeniterzi,Osman Ugur Sezerman","58075|GECCO|2007|A platform for the selection of genes in DNA microarraydata using evolutionary algorithms|This paper presents a flexible framework to the task of featureselection in classification of DNA microarray data. Theuser can select a number of filter methods in the preprocessingstage and choose from a wide set of classifiers (models and algorithms from WEKA  are available) and accuracy estimation methods. This approach implements wrapper methods, where Evolutionary Algorithms, with variable sized set based representations are used to reduce the number of attributes. Two case studies were used to validate the approach, with three distinct classifiers (-nearest neighbour, decision trees, SVMs), a filter method based on discriminant fuzzy patterns and k-fold cross-validation to estimate the generalization error.|Miguel Rocha,Rui Mendes,Paulo Maia,Daniel Glez-Peña,Florentino Fdez-Riverola"],["58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|?ngel M. Pérez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ortíz-García,Antonio Portilla-Figueras","57706|GECCO|2006|On the local performance of simulated annealing and the  evolutionary algorithm|Simulated annealing and the (+) EA, a simple evolutionary algorithm, are both general randomized search heuristics that optimize any objective function with probability converging to . But they use very different techniques to achieve this global convergence. The (+) EA applies global mutations than can reach any point in the search space in one step together with an elitist selection mechanism. Simulated annealing restricts its search to a neighborhood but employs a randomized selection scheme where the probability for accepting a move to a new point in the search space depends on the difference in function values as well as on the current time step. Otherwise, the two algorithms are equal. It is known that the different philosophies of search implemented in the two heuristics can lead to exponential performance gaps between the two algorithms with respect to the expected optimization time. Even for very restricted classes of objective functions where the differences in function values between neighboring points are strictly limited the performance differences can be huge. Here, a more local point of view is taken. Considering obstacles in the fitness landscapes it is proven that the local performance of the two algorithms is remarkably similar in spite of their different search behaviors.|Thomas Jansen,Ingo Wegener","58092|GECCO|2007|Guided hyperplane evolutionary algorithm|A new evolutionary technique for multicriteria optimization called Guiding Hyper-plane Evolutionary Algorithm (GHEA) is proposed. The originality of the approach consists in the fact that the fitness assignment is realized by using a guiding hyperplane and a new non Pareto optimality concept. Numerical experiments illustrate the performance of GHEA compared with the popular NSGA-II and SPEA.|Corina Rotar,D. Dumitrescu,Rodica Ioana Lung","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","65866|AAAI|2006|A Fast Decision Tree Learning Algorithm|There is growing interest in scaling up the widely-used decision-tree learning algorithms to very large data sets. Although numerous diverse techniques have been proposed, a fast tree-growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential. In this paper, we present a novel, fast decision-tree learning algorithm that is based on a conditional independence assumption. The new algorithm has a time complexity of O(m  n), where m is the size of the training data and n is the number of attributes. This is a significant asymptotic improvement over the time complexity O(m  n) of the standard decision-tree learning algorithm C., with an additional space increase of only O(n). Experiments show that our algorithm performs competitively with C. in accuracy on a large number of UCI benchmark data sets, and performs even better and significantly faster than C. on a large number of text classification data sets. The time complexity of our algorithm is as low as naive Bayes'. Indeed, it is as fast as naive Bayes but outperforms naive Bayes in accuracy according to our experiments. Our algorithm is a core tree-growing algorithm that can be combined with other scaling-up techniques to achieve further speedup.|Jiang Su,Harry Zhang","57984|GECCO|2007|Discrimination of metabolic flux profiles using a hybrid evolutionary algorithm|Studying metabolic fluxes is a crucial aspect of understanding biological phenotypes. However, it is often not possible to measure these fluxes directly. As an alternative, fluxome profiling provides indirect information about fluxes in a high-throughput setting. In this paper, we consider a scenario where fluxome profiling is used to investigate characteristic differences between a number of bacterial mutant strains. The goal is to identify groups of mutants that show maximally different fluxome profiles. We propose an evolutionary algorithm for this optimization problem and demonstrate that it outperforms alternative methods based on principle component analysis and independent component analysis on both real and synthetic data sets.|Stefan Bleuler,Eckart Zitzler","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","65859|AAAI|2006|A Sequential Covering Evolutionary Algorithm for Expressive Music Performance|In this paper, we describe an evolutionary approach to one of the most challenging problems in computer music modeling the knowledge applied by a musician when performing a score of a piece in order to produce an expressive performance of the piece. We extract a set of acoustic features from Jazz recordings thereby providing a symbolic representation of the musician's expressive performance. By applying a sequential covering evolutionary algorithm to the symbolic representation, we obtain an expressive performance computational model capable of endowing a computer generated music performance with the timing and energy expressiveness that characterizes human generated music.|Rafael Ramirez,Amaury Hazan,Jordi Marine,Esteban Maestre"],["65680|AAAI|2006|Building Explainable Artificial Intelligence Systems|As artificial intelligence (AI) systems and behavior models in military simulations become increasingly complex, it has been difficult for users to understand the activities of computer-controlled entities. Prototype explanation systems have been added to simulators, but designers have not heeded the lessons learned from work in explaining expert system behavior. These new explanation systems are not modular and not portable they are tied to a particular AI system. In this paper, we present a modular and generic architecture for explaining the behavior of simulated entities. We describe its application to the Virtual Humans, a simulation designed to teach soft skills such as negotiation and cultural awareness.|Mark G. Core,H. Chad Lane,Michael van Lent,Dave Gomboc,Steve Solomon,Milton Rosenberg","57747|GECCO|2006|Multi-step environment learning classifier systems applied to hyper-heuristics|Heuristic Algorithms (HA) are very widely used to tackle practical problems in operations research. They are simple, easy to understand and inspire confidence. Many of these HAs are good for some problem instances while very poor for other cases. While Meta-Heuristics try to find which is the best heuristic andor parameters to apply for a given problem instance Hyper-Heuristics (HH) try to combine several heuristics in the same solution searching process, switching among them whenever the circumstances vary. Besides, instead to solve a single problem instance it tries to find a general algorithm to apply to whole families of problems. HH use evolutionary methods to search for such a problem-solving algorithm and, once produced, to apply it to any new problem instance desired. Learning Classifier Systems (LCS), and in particular XCS, represents an elegant and simple way to try to fabricate such a composite algorithm. This represents a different kind of problem to those already studied by the LCS community. Previous work, using single step environments, already showed the usefulness of the approach. This paper goes further and studies the novel use of multi-step environments for HH and an alternate way to consider states to see if chains of actions can be learnt. A non-trivial, NP-hard family of problems, the Bin Packing one, is used as benchmark for the procedure. Results of the approach are very encouraging, showing outperformance over all HAs used individually and over previously reported work by the authors, including non-LCS (a GA based approach used for the same BP set of problems) and LCS (using single step environments).|Javier G. Marín-Blázquez,Sonia Schulenburg","57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","57804|GECCO|2006|A dynamic approach to artificial immune systems utilizing neural networks|The purpose of this work is to propose an immune-inspired setup to use a self-organizing map as a computational model for the interaction of antigens and antibodies. The proposed approach may be used as a part in other immune algorithms, or can possibly be used to detect anomalies in time series data.|Stefan Schadwinkel,Werner Dilger","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|Luís C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","65840|AAAI|2006|Approximate Compilation for Embedded Model-based Reasoning|The use of embedded technology has become widespread. Many complex engineered systems comprise embedded features to perform self-diagnosis or self-reconfiguration. These features require fast response times in order to be useful in domains where embedded systems are typically deployed. Researchers often advocate the use of compilation-based approaches to store the set of environments (resp. solutions) to a diagnosis (resp. reconfiguration) problem, in some compact representation. However, the size of a compiled representation may be exponential in the treewidth of the problem. In this paper we propose a novel method for compiling the most preferred environments in order to reduce the large space requirements of our compiled representation. We show that approximate compilation is an effective means of generating the highest-valued environments, while obtaining a representation whose size can be tailored to any embedded application. The method also provides a graceful way to tradeoff space requirements with the completeness of our coverage of the environment space.|Barry O'Sullivan,Gregory M. Provan","66101|AAAI|2007|Efficient Reinforcement Learning with Relocatable Action Models|Realistic domains for learning possess regularities that make it possible to generalize experience across related states. This paper explores an environment-modeling framework that represents transitions as state-independent outcomes that are common to all states that share the same type. We analyze a set of novel learning problems that arise in this framework, providing lower and upper bounds. We single out one particular variant of practical interest and provide an efficient algorithm and experimental results in both simulated and robotic environments.|Bethany R. Leffler,Michael L. Littman,Timothy Edmunds","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","66073|AAAI|2007|Representing and Reasoning about Commitments in Business Processes|A variety of business relationships in open settings can be understood in terms of the creation and manipulation of commitments among the participants. These include BC and BB contracts and processes, as realized via Web services and other such technologies. Business protocols, an interaction-oriented approach for modeling business processes, are formulated in terms of the commitments. Commitments can support other forms of semantic service composition as well. This paper shows how to represent and reason about commitments in a general manner. Unlike previous formalizations, the proposed formalization accommodates complex and nested commitment conditions, and concurrent commitment operations. In this manner, a rich variety of open business scenarios are enabled.|Nirmit Desai,Amit K. Chopra,Munindar P. Singh","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II"],["66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","65822|AAAI|2006|Finding Maximally Satisfiable Terminologies for the Description Logic ALC|For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.|Thomas Andreas Meyer,Kevin Lee,Richard Booth,Jeff Z. Pan","65720|AAAI|2006|On the Update of Description Logic Ontologies at the Instance Level|We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.|Giuseppe De Giacomo,Maurizio Lenzerini,Antonella Poggi,Riccardo Rosati","65774|AAAI|2006|Towards an Axiom System for Default Logic|Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning.|Gerhard Lakemeyer,Hector J. Levesque","66034|AAAI|2007|Generality and Equivalence Relations in Default Logic|Generality or refinement relations between different theories have important applications to generalization in inductive logic programming, refinement of ontologies, and coordination in multi-agent systems. We study generality relations in disjunctive default logic by comparing the amounts of information brought by default theories. Intuitively, a default theory is considered more general than another default theory if the former brings more information than the latter. Using techniques in domain theory, we introduce different types of generality relations over default theories. We show that generality relations based on the Smyth and Hoare orderings reflect orderings on skeptical and credulous consequences, respectively, and that two default theories are equivalent if and only if they are equally general under these orderings. These results naturally extend both generality relations over first-order theories and those for answer set programming.|Katsumi Inoue,Chiaki Sakama","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski","65891|AAAI|2006|Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms|In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs. The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms. We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.|Tran Cao Son,Enrico Pontelli,Phan Huy Tu","58063|GECCO|2007|Swarming with logic|A robot swarm is a distributed entity that can sense andperform many actions simultaneously at different spatial locations. But how, with all this sensory information and capacity for action, can the individuals unite for a common purpose, coordinating their actions in space and time Inthis preliminary simulation study, we suggest that a swarmcan be designed as an engineering control system characterised by a problem specific inputoutput (IO) relationship that can be implemented through the simple (socialinsect inspired) indirect transfer of information between individuals. For such a system, the inputs represent sensory information acquired by one or more agents and the outputs are used to trigger actions that agents perform.|Robert L. Stewart,Michael Kirley"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","57951|GECCO|2007|An estimation of distribution algorithm with guided mutation for a complex flow shop scheduling problem|An Estimation of Distribution Algorithm (EDA) is proposed toapproach the Hybrid Flow Shop with Sequence Dependent Setup Times and Uniform Machines in parallel (HFS-SDST-UM) problem. The latter motivated by the needs of a real world company. The proposed EDA implements a fairly new mechanism to improve the search of more traditional EDAs. This is the Guided Mutation (GM). EDA-GM generates new solutions by using the information from a probability model, as all EDAs, and the local information from a good known solution. The approach is tested on several instances of HFS-SDST-UM and compared with adaptations of meta-heuristics designed for very similarproblems. Encouraging results are reported.|Abdellah Salhi,José Antonio Vázquez Rodríguez,Qingfu Zhang","57647|GECCO|2006|Comparison of genetic representation schemes for scheduling soft real-time parallel applications|This paper presents a hybrid technique that combines List Scheduling (LS) with Genetic Algorithms (GA) for constructing non-preemptive schedules for soft real-time parallel applications represented as directed acyclic graphs (DAGs). The execution time requirements of the applications' tasks are assumed to be stochastic and are represented as probability distribution functions. The performance in terms of schedule lengths for three different genetic representation schemes are evaluated and compared for a number of different DAGs.The approaches presented here produce shorter schedules than HLFET, a popular LS approach for all of the sample problems. Of the three genetic representation schemes investigated, PosCT, the technique that allows the GA to learn which tasks to delay in order to allow other tasks to complete produced the shortest schedules for a majority of the sample DAGs.|Yoginder S. Dandass,Amit C. Bugde","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","57723|GECCO|2006|Combining simplex with niche-based evolutionary computation for job-shop scheduling|We propose a hybrid algorithm (called ALPINE) between Genetic Algorithm and Dantzig's Simplex method to approximate optimal solutions for the Flexible Job-Shop Problem. Locally, Simplex is extended for the JSP linear program to reduce the number of infeasible solutions while solution quality is improved with an operation order search. Globally, a niche-based evolutionary strategy is employed to gain parallelization while solution diversity is maintained in two ways composite dispatching rule-based population initialization and memory-based machine assignment. Performance results on benchmark problems show that ALPINE outperforms existing hybrid techniques with a new global optima found for the x Flexible Job Shop Problem.|Syhlin Kuah,Joc Cing Tay","58198|GECCO|2007|Using evolutionary computation and local search to solve multi-objective flexible job shop problems|Finding realistic schedules for Flexible Job Shop Problems has attracted many researchers recently due to its NP-hardness. In this paper, we present an efficient approach for solving the multi-objective flexible job shop by combining Evolutionary Algorithm and Guided Local Search. Instead of applying random local search to find neighborhood solutions, we introduce a guided local search procedure to accelerate the process of convergence to Pareto-optimal solutions. The main improvement of this combination is to help diversify the population towards the Pareto-front. Empirical studies show that ) the gaps between the obtained results and known lower bounds are small, and ) the multi-objective solutions of our algorithms dominate previous designs for solving the same benchmarks while incurring less computational time.|Nhu Binh Ho,Joc Cing Tay","58055|GECCO|2007|Simultaneous optimization of production planning and inspection planning for flexible manufacturing systems|Quality assurance in flexible manufacturing systems (FMSs) has become a matter of great importance in recent years. The possibility for offering high-quality products at lower costs has become an essential for a manufacturer to keep in a competitive edge. In this paper, an approach to the multi-objective optimization of production planning and inspection planning in flexible manufacturing systems is presented. A multi-objective memetic algorithm MOMA is proposed to solve the problems having six objectives minimizing total machining time, machine workload unbalance, greatest machine workload, total tool cost, total inspection time and number of inspections. A schemata-guided local search strategy is proposed for enhancing performances of MOMA. High efficiency of MOMA arises from that multiple objectives can be optimized simultaneously without using heuristics and a set of good non-dominated solutions can be obtained providing additional degrees of freedom for the exploitation of resources of FMSs. Experimental results demonstrate effectiveness of the proposed approach using MOMA for production planning and inspection planning of FMSs.|Jian-Hung Chen","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao"],["58227|GECCO|2007|Self-adaptive simulated binary crossover for real-parameter optimization|Simulated binary crossover (SBX) is a real-parameter recombinationoperator which is commonly used in the evolutionary algorithm (EA) literature. The operatorinvolves a parameter which dictates the spread of offspring solutionsvis-a-vis that of the parent solutions. In all applications of SBX sofar, researchers have kept a fixed value throughout a simulation run. In this paper, we suggest a self-adaptive procedure of updating theparameter so as to allow a smooth navigation over the functionlandscape with iteration. Some basic principles of classicaloptimization literature are utilized for this purpose. The resultingEAs are found to produce remarkable and much better results comparedto the original operator having a fixed value of the parameter. Studieson both single and multiple objective optimization problems are madewith success.|Kalyanmoy Deb,Karthik Sindhya,Tatsuya Okabe","58222|GECCO|2007|Cross entropy and adaptive variance scaling in continuous EDA|This paper deals with the adaptive variance scaling issue incontinuous Estimation of Distribution Algorithms. A phenomenon is discovered that current adaptive variance scaling method in EDA suffers from imprecise structure learning. A new type of adaptation method is proposed to overcome this defect. The method tries to measure the difference between the obtained population and the prediction of the probabilistic model, then calculate the scaling factor by minimizing the cross entropy between these two distributions. This approach calculates the scaling factor immediately rather than adapts it incrementally. Experiments show that this approach extended the class of problems that can be solved, and improve the search efficiency in some cases. Moreover, the proposed approach features in that each decomposed subspace can be assigned an individual scaling factor, which helps to solve problems with special dimension property.|Yunpeng Cai,Xiaomin Sun,Hua Xu,Peifa Jia","57946|GECCO|2007|SDR a better trigger for adaptive variance scaling in normal EDAs|Recently, advances have been made in continuous, normal-distribution-based Estimation-of-DistributionAlgorithms (EDAs) by scaling the variance upfrom the maximum-likelihood estimate. When doneproperly, such scaling has been shown to preventpremature convergence on slope-like regions ofthe search space. In this paper we specificallyfocus on one way of scaling that was previouslyintroduced as Adaptive Variance Scaling (AVS). It wasfound that when using AVS, the average number offitness evaluations grows subquadratically withthe dimensionality on a wide range of unimodaltest-problems, competitively with the CMA-ES.Still, room for improvement exists because thevariance doesn't always have to be scaled. Apreviously introduced trigger based on correlationthat determines when to apply scaling was shownto fail on higher dimensional problems. Here weprovide a new solution called the Standard-DeviationRatio (SDR) trigger that is integrated with theIterated Density-Estimation Evolutionary Algorithm(IDEA). Intuitively put, scaling istriggered with SDR only if improvements are foundto be far away from the mean. SDR works even inhigh dimensions as a result of factorizing thedecision rule behind the trigger according to theestimated Bayesian factorization. We evaluateSDR-AVS-IDEA on the same set ofbenchmark problems and compare it with AVS-IDEAand CMA-ES. We find that the addition of SDR givesAVS-IDEA an important extra edgefor it to be used in future research and inapplications both in single-objective optimizationas well as in multi-objective and dynamicoptimization. In addition, we provide practical rulesof thumb for parameter settings for usingSDR-AVS-IDEA that result in anasymptotic scale-up behavior that is sublinearfor the population size (O(l.)) andsubquadratic (O(l.)) for thenumber of evaluations.|Peter A. N. Bosman,Jörn Grahl,Franz Rothlauf","57925|GECCO|2007|Credit assignment in adaptive memetic algorithms|Adaptive Memetic Algorithms couple an evolutionary algorithm with a number of local search heuristics for improving the evolving solutions. They are part of a broad family of meta-heuristics which maintain a set of local search operators applying them at different stages of the search. This creates a need to make decisions about which operator to use when. Several different schemes have been proposed, but most of them assume there is a fixed set of predefined operators. This makes them unsuitable for use within the broader context of adaptive learning systems where the set of available operators can change over time. Here we investigate a range of different schemes, and propose a novel method for estimating an operator's current utility, which is shown to avoid some of the problems of noise inherent in simpler schemes. Results on arange of combinatorial optimisation problems show that algorithms embodying this mechanism locate the global optimum more reliably, without a significant computational overhead compared to the simpler schemes.|J. E. Smith","57854|GECCO|2006|Credit assignment in adaptive evolutionary algorithms|In this paper, a new method for assigning credit to search operators is presented. Starting with the principle of optimizing search bias, search operators are selected based on an ability to create solutions that are historically linked to future generations. Using a novel framework for defining performance measurements, distributing credit for performance, and the statistical interpretation of this credit, a new adaptive method is developed and shown to outperform a variety of adaptive and non-adaptive competitors.|James M. Whitacre,Q. Tuan Pham,Ruhul A. Sarker","58244|GECCO|2007|A self-adaptive multiagent evolutionary algorithm for electrical machine design|This paper presents a self-adaptive algorithm that hybridises evolutionary and multiagent concepts. Each evolutionary individual is implemented as a simple agent capable of re-production and predation. The transitions between these two states depend on the agent's local environment. Thus, no explicit global process is defined to select neither the mates nor the preys. The convergence of the algorithm emerges from the behaviour of the agents. This brings interesting properties, such as population size self-regulation. Two sets of experimental results are provided a comparison with Saw-Tooth Algorithm and micro-GA using four classical functions and an optimisation of the efficiency and the weight of an electrical motor. Some possible evolutions and prospects are finally proposed.|Jean-Laurent Hippolyte,Christelle Bloch,Pascal Chatonnay,Christophe Espanet,Didier Chamagne","57762|GECCO|2006|Adaptive diversity in PSO|Spatial Extension PSO (SEPSO) and Attractive-Repulsive PSO (ARPSO) are methods for artificial injection of diversity into particle swarm optimizers that are intended to encourage converged swarms to engage in exploration. While simple to implement, effective when tuned correctly, and benefiting from intuitive appeal, SEPSO behavior can be improved by adapting its radius and bounce parameters in response to collisions. In fact, adaptation can allow SEPSO to compete with and outperform ARPSO. The adaptation strategies presented here are simple to implement, easy to tune, and retain SEPSO's intuitive appeal.|Christopher K. Monson,Kevin D. Seppi","66141|AAAI|2007|An Architecture for Adaptive Algorithmic Hybrids|We describe a cognitive architecture for creating more robust intelligent systems by executing hybrids of algorithms based on different computational formalisms. The architecture is motivated by the belief that () most existing computational methods often exhibit some of the characteristics desired of intelligent systems at the cost of other desired characteristics and () a system exhibiting robust intelligence can be designed by implementing hybrids of these computational methods. The main obstacle to this approach is that the various relevant computational methods are based on data structures and algorithms that are very difficult to integrate into one system. We describe a new method of executing hybrids of algorithms using the focus of attention of multiple modules. This approach has been embodied in the Polyscheme cognitive architecture. Systems based on Polyscheme can integrate reactive robotic controllers, logical and probabilistic inference algorithms, frame-based formalisms and sensor-processing algorithms into one system. Existing applications involve human-robot interaction, heterogeneous information retrieval and natural language understanding. Systems built using Polyscheme demonstrate that algorithmic hybrids implemented using a focus of attention can () exhibit more characteristics of intelligence than individual computational methods alone and () deal with problems that have formerly been beyond the reach of synthetic computational intelligence.|Nicholas L. Cassimatis,Magdalena D. Bugajska,Scott Dugas,Arthi Murugesan,Paul Bello","58218|GECCO|2007|Adaptive variance scaling in continuous multi-objective estimation-of-distribution algorithms|Recent research into single-objective continuous Estimation-of-Distribution Algorithms (EDAs)has shown that when maximum-likelihood estimationsare used for parametric distributions such as thenormal distribution, the EDA can easily suffer frompremature convergence. In this paper we argue thatthe same holds for multi-objective optimization.Our aim in this paper is to transfer a solutioncalled Adaptive Variance Scaling (AVS) from thesingle-objective case to the multi-objectivecase. To this end, we zoom in on an existing EDAfor continuous multi-objective optimization, theMIDEA, which employs mixturedistributions. We propose a means to combine AVSwith the normal mixture distribution, as opposedto the single normal distribution for which AVS wasintroduced. In addition, we improve the AVS schemeusing the Standard-Deviation Ratio(SDR) trigger. Intuitively put, variance scalingis triggered by the SDR trigger only ifimprovements are found to be far awayfrom the mean. For the multi-objective case,this addition is important to keep the variancefrom being scaled to excessively large values.From experiments performed on five well-knownbenchmark problems, the addition of SDR andAVS is found to enlarge the class of problems thatcontinuous multi-objective EDAs can solve reliably.|Peter A. N. Bosman,Dirk Thierens","57674|GECCO|2006|The correlation-triggered adaptive variance scaling IDEA|It has previously been shown analytically and experimentally that continuous Estimation of Distribution Algorithms (EDAs) based on the normal pdf can easily suffer from premature convergence. This paper takes a principled first step towards solving this problem. First, prerequisites for the successful use of search distributions in EDAs are presented. Then, an adaptive variance scaling theme is introduced that aims at reducing the risk of premature convergence. Integrating the scheme into the iterated density--estimation evolutionary algorithm (IDEA) yields the correlation-triggered adaptive variance scaling IDEA (CT-AVS-IDEA). The CT-AVS-IDEA is compared to the original IDEA and the Evolution Strategy with Covariance Matrix Adaptation (CMA-ES) on a wide range of unimodal test-problems by means of a scalability analysis. It is found that the average number of fitness evaluations grows subquadratically with the dimensionality, competitively with the CMA-ES. In addition, CT-AVS-IDEA is indeed found to enlarge the class of problems that continuous EDAs can solve reliably.|Jörn Grahl,Peter A. N. Bosman,Franz Rothlauf"],["57684|GECCO|2006|Dynamic multi-objective optimization with evolutionary algorithms a forward-looking approach|This work describes a forward-looking approach for the solution of dynamic (time-changing) problems using evolutionary algorithms. The main idea of the proposed method is to combine a forecasting technique with an evolutionary algorithm. The location, in variable space, of the optimal solution (or of the Pareto optimal set in multi-objective problems) is estimated using a forecasting method. Then, using this forecast, an anticipatory group of individuals is placed on and near the estimated location of the next optimum. This prediction set is used to seed the population when a change in the objective landscape arrives, aiming at a faster convergence to the new global optimum. The forecasting model is created using the sequence of prior optimum locations, from which an estimate for the next location is extrapolated. Conceptually this approach encompasses advantages of memory methods by making use of information available from previous time steps. Combined with a convergencediversity balance mechanism it creates a robust algorithm for dynamic optimization. This strategy can be applied to single objective and multi-objective problems, however in this work it is tested on multi-objective problems. Initial results indicate that the approach improves algorithm performance, especially in problems where the frequency of objective change is high.|Iason Hatzakis,David Wallace","57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57649|GECCO|2006|Reference point based multi-objective optimization using evolutionary algorithms|Evolutionary multi-objective optimization (EMO) methodologies have been amply applied to find a representative set of Pareto-optimal solutions in the past decade and beyond. Although there are advantages of knowing the range of each objective for Pareto-optimality and the shape of the Pareto-optimal frontier itself in a problem for an adequate decision-making, the task of choosing a single preferred Pareto-optimal solution is also an important task which has received a lukewarm attention so far. In this paper, we combine one such preference based strategy with an EMO methodology and demonstrate how, instead of one solution, a preferred set solutions near the reference points can be found parallely. We propose a modified EMO procedure based on the elitist non-dominated sorting GAor NSGA-II. On two-objective to -objective optimization problems, the modified NSGA-II approach shows its efficacy in finding an adequate set of Pareto-optimal points. Such procedures will provide the decision-maker with a set of solutions near herhis preference so that a better and a more reliable decision can be made.|Kalyanmoy Deb,J. Sundar","57746|GECCO|2006|A comparative study of differential evolution variants for global optimization|In this paper, we present an empirical comparison of some Differential Evolution variants to solve global optimization problems. The aim is to identify which one of them is more suitable to solve an optimization problem, depending on the problem's features and also to identify the variant with the best performance, regardless of the features of the problem to be solved. Eight variants were implemented and tested on  benchmark problems taken from the specialized literature. These variants vary in the type of recombination operator used and also in the way in which the mutation is computed. A set of statistical tests were performed in order to obtain more confidence on the validity of the results and to reinforce our discussion. The main aim is that this study can help both researchers and practitioners interested in using differential evolution as a global optimizer, since we expect that our conclusions can provide some insights regarding the advantages or limitations of each of the variants studied.|Efrén Mezura-Montes,Jesús Velázquez-Reyes,Carlos A. Coello Coello","57919|GECCO|2007|Parallel skeleton for multi-objective optimization|Many real-world problems are based on the optimization of more than one objective function. This work presents a tool for the resolution of multi-objective optimization problems based on the cooperation of a set of algorithms. The invested time in the resolution is decreased by means of a parallel implementation of an evolutionary team algorithm. This model keeps the advantages of heterogeneous island models but also allows to assign more computational resources to the algorithms with better expectations. The elitist scheme applied aims to improve the results obtained with single executions of independent evolutionary algorithms. The user solves the problem without the need of knowing the internal operation details of the used evolutionary algorithms. The computational results obtained on a cluster of PCs for some tests available in the literature are presented.|Coromoto León,Gara Miranda,Carlos Segura","57739|GECCO|2006|MOGE GP classification problem decomposition using multi-objective optimization|A novel approach to classification is proposed in which a Pareto-based ranking of individuals is used to encourage multiple individuals to participate in the solution. To do so, the classification problem is re-expressed as a cluster consistency problem, thus allowing utilization of techniques from multi-objective optimization. Such a formulation enables classification problems to be automatically decomposed and solved by several specialist classifiers rather than by a single 'super' individual. In this paper, we demonstrate the proposed approach to two benchmark binary problems and recommend a natural extension to multi-class problems. Results indicate the general appropriateness of the approach.|Andrew R. McIntyre,Malcolm I. Heywood","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"]]}}