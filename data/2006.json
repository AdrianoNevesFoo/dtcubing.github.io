{"abstract":{"entropy":6.876672880791834,"topics":["speech recognition, low-density parity-check, zero-correlation zone, presents novel, speech noise, parity-check code, paper based, algorithm adaptive, acoustic speech, parity-check matrix, speech enhancement, sequence zone, novel approach, based model, based, paper presented, estimation algorithm, testing test, control access, low-density code","genetic algorithm, genetic programming, evolutionary algorithm, particle swarm, algorithm problem, solving problem, markov decision, algorithm search, algorithm, evolutionary computation, markov processes, optimization problem, decision processes, evolution strategies, partially observable, presents algorithm, programming cartesian, presents learning, evolutionary multi-objective, classifier xcs","data, software quality, artificial intelligence, recent years, mapping schema, cancer detection, web, semantic web, widely used, knowledge base, web sites, queries relational, early cancer, play role, popular web, part cancer, search engine, software development, web server, xml data","consider problem, wireless network, graph vertices, spanning tree, neural network, spanning vertex, given vertices, network, orthogonal division, sensor network, graph edges, grid computing, graph vertex, game player, frequency division, problem graph, aggregation voting, discrete continuous, preferences voting, minimum spanning","speech recognition, zero-correlation zone, speech noise, acoustic speech, speech enhancement, testing test, noise environments, ultra wideband, sequence zero-correlation, acoustic recognition, robust speech, speech system, class zero-correlation, class zone, robust recognition, noise reduction, noise system, automatic recognition, automatic speech, presents class","presents novel, paper presented, novel approach, estimation algorithm, paper novel, proposes novel, estimation based, algorithm proposed, predictive classification, based proposed, estimation distribution, novel, novel based, model predictive, presents approach, technique proposed, paper robust, paper introduce, introduce based, efficient algorithm","algorithm problem, solving problem, algorithm solving, algorithm used, hardware verification, problem, multi-objective problem, presents design, optimization problem, widely used, genetic solving, combinatorial optimization, solving multi-objective, genetic problem, problem constraint, genetic optimization, presents problem, combinatorial problem, design, evolutionary solving","evolutionary algorithm, algorithm search, solve problem, evolutionary computation, evolutionary multi-objective, evolution strategies, well known, search, evolutionary search, algorithm dynamic, operator crossover, paper operator, search heuristic, algorithm optimization, evolutionary solution, optimization technique, evolutionary problem, search methods, presents evolution, search test","recent years, query processing, large data, query database, data, system large, data system, large complex, large, recent, time data, technique data, problem data, processing, data become, recent data, query, become, common, ubiquitous","software quality, machine data, software development, information data, information system, wide range, information retrieval, information integration, task data, information, system software, important information, system support, software, applications, system development, problem information, important software, machine system, describe software","spanning tree, spanning vertex, game player, discrete continuous, system surface, minimum spanning, continuous dynamic, system behavior, object, require, behavior, human, actions, neural, observed, utility, ranking, conditions, following, model","aggregation voting, preferences voting, general voting, agents system, system developed, structure network, system multiple, private information, agents voting, multiple voting, problem agents, investigates system, multiple, agents, preferences agents, problem system, multiple agents, study, general, structure"],"ranking":[["43521|IEICE Transations|2006|Gamma Modeling of Speech Power and Its On-Line Estimation for Statistical Speech Enhancement|This study shows the effectiveness of using gamma distribution in the speech power domain as a more general prior distribution for the model-based speech enhancement approaches. This model is a super-set of the conventional Gaussian model of the complex spectrum and provides more accurate prior modeling when the optimal parameters are estimated. We develop a method to adapt the modeled distribution parameters from each actual noisy speech in a frame-by-frame manner. Next, we derive and investigate the minimum mean square error (MMSE) and maximum a posterior probability (MAP) estimations in different domains of speech spectral magnitude, generalized power and its logarithm, using the proposed gamma modeling. Finally, a comparative evaluation of the MAP and MMSE filters is conducted. As the MMSE estimations tend to more complicated using more general prior distributions, the MAP estimations are given in closed-form extractions and therefore are suitable in the implementation. The adaptive estimation of the modeled distribution parameters provides more accurate prior modeling and this is the principal merit of the proposed method and the reason for the better performance. From the experiments, the MAP estimation is recommended due to its high efficiency and low complexity. Among the MAP based systems, the estimation in log-magnitude domain is shown to be the best for the speech recognition as the estimation in power domain is superior for the noise reduction.|Tran Huy Dat,Kazuya Takeda,Fumitada Itakura","43561|IEICE Transations|2006|Single-Channel Multiple Regression for In-Car Speech Enhancement|We address issues for improving hands-free speech enhancement and speech recognition performance in different car environments using a single distant microphone. This paper describes a new single-channel in-car speech enhancement method that estimates the log spectra of speech at a close-talking microphone based on the nonlinear regression of the log spectra of noisy signal captured by a distant microphone and the estimated noise. The proposed method provides significant overall quality improvements in our subjective evaluation on the regression-enhanced speech, and performed best in most objective measures. Based on our isolated word recognition experiments conducted under  real car environments, the proposed adaptive nonlinear regression approach shows an advantage in average relative word error rate (WER) reductions of .% and .%, respectively, compared to original noisy speech and ETSI advanced front-end (ETSI ES  ).|Weifeng Li,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","43175|IEICE Transations|2006|Rate Compatible Low-Density Parity-Check Codes Based on Progressively Increased Column Weights|Effective and simply realizable rate compatible low-density parity-check (LDPC) codes are proposed. A parity check matrix is constructed with the progressively increased column weights (PICW) order and adopted to achieve a punctured LDPC coding scheme for a wide range of the code rates of the rate compatible systems. Using the proposed rate compatible punctured LDPC codes, low complex adaptive communication systems, such as wireless communication systems, can be achieved with the reliable transmissions.|Chen Zheng,Noriaki Miyazaki,Toshinori Suzuki","43155|IEICE Transations|2006|Transformation of a Parity-Check Matrix for a Message-Passing Algorithm over the BEC|We propose transformation of a parity-check matrix of any low-density parity-check code. A code with transformed parity-check matrix is an equivalent of a code with the original parity-check matrix. For the binary erasure channel, performance of a message-passing algorithm with a transformed parity-check matrix is better than that with the original matrix.|Naoto Kobayashi,Toshiyasu Matsushima,Shigeichi Hirasawa","43558|IEICE Transations|2006|Acoustic Model Adaptation Using First-Order Linear Prediction for Reverberant Speech|This paper describes a hands-free speech recognition technique based on acoustic model adaptation to reverberant speech. In hands-free speech recognition, the recognition accuracy is degraded by reverberation, since each segment of speech is affected by the reflection energy of the preceding segment. To compensate for the reflection signal we introduce a frame-by-frame adaptation method adding the reflection signal to the means of the acoustic model. The reflection signal is approximated by a first-order linear prediction from the observation signal at the preceding frame, and the linear prediction coefficient is estimated with a maximum likelihood method by using the EM algorithm, which maximizes the likelihood of the adaptation data. Its effectiveness is confirmed by word recognition experiments on reverberant speech.|Tetsuya Takiguchi,Masafumi Nishimura,Yasuo Ariki","43274|IEICE Transations|2006|A Modification Method for Constructing Low-Density Parity-Check Codes for Burst Erasures|We study a modification method for constructing low-density parity-check (LDPC) codes for solid burst erasures. Our proposed modification method is based on a column permutation technique for a parity-check matrix of the original LDPC codes. It can change the burst erasure correction capabilities without degradation in the performance over random erasure channels. We show by simulation results that the performance of codes permuted by our method are better than that of the original codes, especially with two or more solid burst erasures.|Gou Hosoya,Hideki Yagi,Toshiyasu Matsushima,Shigeichi Hirasawa","43467|IEICE Transations|2006|Noise Reduction in Time Domain Using Referential Reconstruction|We present a novel approach for single-channel noise reduction of speech signals contaminated by additive noise. In this approach, the system requires speech samples to be uttered in advance by the same speaker as that of the input signal. Speech samples used in this method must have enough phonetic variety to reconstruct the input signal. In the proposed method, which we refer to as referential reconstruction, we have used a small database created from examples of speech, which will be called reference signals. Referential reconstruction uses an example-based approach, in which the objective is to find the candidate speech frame which is the most similar to the clean input frame without noise, although the input frame is contaminated with noise. When candidate frames are found, they become final outputs without any special processing. In order to find the candidate frames, a correlation coefficient is used as a similarity measure. Through automatic speech recognition experiments, the proposed method was shown to be effective, particularly for low-SNR speech signals corrupted with white noise or noise in high-frequency bands. Since the direct implementation of this method requires infeasible computational cost for searching through reference signals, a coarse-to-fine strategy is introduced in this paper.|Takehiro Ihara,Takayuki Nagai,Kazuhiko Ozeki,Akira Kurematsu","43367|IEICE Transations|2006|VLSI Design of a Fully-Parallel High-Throughput Decoder for Turbo Gallager Codes|The most powerful channel coding schemes, namely those based on turbo codes and low-density parity-check (LDPC) Gallager codes, have in common the principle of iterative decoding. However, the relative coding structures and decoding algorithms are substantially different. This paper presents a -bit, rate- soft decision decoder for a new class of codes known as Turbo Gallager Codes. These codes are turbo codes with properly chosen component convolutional codes such that they can be successfully decoded by means of the decoding algorithm used for LDPC codes, i.e., the belief propagation algorithm working on the code Tanner graph. These coding schemes are important in practical terms for two reasons (i) they can be encoded as classical turbo codes, giving a solution to the encoding problem of LDPC codes (ii) they can also be decoded in a fully parallel manner, partially overcoming the routing congestion bottleneck of parallel decoder VLSI implementations thanks to the locality of the interconnections. The implemented decoder can support up to  Gbits data rate and performs up to  decoding iterations ensuring both high throughput and good coding gain. In order to evaluate the performance and the gate complexity of the decoder VLSI architecture, it has been synthesized in a . m standard-cell CMOS technology.|Luca Fanucci,Pasquale Ciao,Giulio Colavolpe","43088|IEICE Transations|2006|Encoding LDPC Codes Using the Triangular Factorization|An algorithm for encoding low-density parity check (LDPC) codes is investigated. The algorithm computes parity check symbols by solving a set of sparse equations, and the triangular factorization is employed to solve the equations efficiently. It is shown analytically and experimentally that the proposed algorithm is more efficient than the Richardson's encoding algorithm if the code has a small gap.|Yuichi Kaji","43524|IEICE Transations|2006|A Non-stationary Noise Suppression Method Based on Particle Filtering and Polyak Averaging|This paper addresses a speech recognition problem in non-stationary noise environments the estimation of noise sequences. To solve this problem, we present a particle filter-based sequential noise estimation method for front-end processing of speech recognition in noise. In the proposed method, a noise sequence is estimated in three stages a sequential importance sampling step, a residual resampling step, and finally a Markov chain Monte Carlo step with Metropolis-Hastings sampling. The estimated noise sequence is used in the MMSE-based clean speech estimation. We also introduce Polyak averaging and feedback into a state transition process for particle filtering. In the evaluation results, we observed that the proposed method improves speech recognition accuracy in the results of non-stationary noise environments a noise compensation method with stationary noise assumptions.|Masakiyo Fujimoto,Satoshi Nakamura"],["57603|GECCO|2006|Instance similarity and the effectiveness of case injection in a genetic algorithm for binary quadratic programming|When an evolutionary algorithm addresses a sequence of instances of the same problem, it can seed its population with solutions that it found for previous instances. This technique is called case injection. How similar must the instances be for case injection to help an EA's search We consider this question by applying a genetic algorithm, without and with case injection, to sequences of instances of binary quadratic programming. When the instances are similar, case injection helps when the instances differ sufficiently, case injection is no help at all.|Jason Amunrud,Bryant A. Julstrom","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57860|GECCO|2006|The LEM implementation of learnable evolution model and its testing on complex function optimization problems|Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to ). In every experiment, LEM outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.|Janusz Wojtusiak,Ryszard S. Michalski","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","57792|GECCO|2006|Dynamics of evolutionary robustness|Recently there has been considerable interest in determining whether, and how much, evolutionary pressure for genetic robustness influences evolutionary processes. In this paper, we attempt to show that this evolutionary pressure does have a significant effect in typical genetic programming problems. Specifically we demonstrate that in a standard genetic programming implementation to solve a symbolic regression problem, pressure for genetic robustness forces the population away from high fitness, but less robust, solutions in favor of solutions with lower fitness, but higher genetic robustness.|Alan Piszcz,Terence Soule","57793|GECCO|2006|A survey of mutation techniques in genetic programming|The importance of mutation varies across evolutionary computation domains including genetic programming, evolution strategies, and genetic algorithms. In the genetic programming community, researchers' view of mutation's effectiveness spans the range from an ineffective or marginal operator, to a neutral operator, to a highly effective operator that evolves solutions more effectively than genetic programming with crossover alone. Mutation implementation and associated parameters are often under reported in genetic programming research and typically lack context that justifies the technique and parameter selection. In part, reporting variance stems from the adaptation of mutation developed by the genetic algorithm community, and the creation of new mutation techniques in genetic programming. This survey describes the controversial operator in genetic programming applications, mutation selection operators, mutation techniques and offers an organization of mutation characteristics. We suggest methodologies to improve reporting of mutation parameters and related individual selection methods.|Alan Piszcz,Terence Soule","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan"],["43591|IEICE Transations|2006|An Efficient Schema-Based Technique for Querying XML Data|As data integration over the Web has become an increasing demand, there is a growing desire to use XML as a standard format for data exchange. For sharing their grammars efficiently, most of the XML documents in use are associated with a document structure description, such as DTD or XML schema. However, the document structure information is not utilized efficiently in previously proposed techniques of XML query processing. In this paper, we present a novel technique that reduces the disk IO complexity of XML query processing. We design a schema-based numbering scheme called SPAR that incorporates both structure information and tag names extracted from DTD or XML schema. Based on SPAR, we develop a mechanism called VirtualJoin that significantly reduces disk IO workload for processing XML queries. As shown by experiments, VirtualJoin outperforms many prior techniques.|Dao Dinh Kha,Masatoshi Yoshikawa","43500|IEICE Transations|2006|Distributing Requests by around -Bounded Load-Balancing in Web Server Cluster with High Scalability|Popular Web sites form their Web servers into Web server clusters. The Web server cluster operates with a load-balancing algorithm to distribute Web requests evenly among Web servers. The load-balancing algorithms founded on conventional periodic load-information update mechanism are not scalable due to the synchronized update of load-information. We propose a load-balancing algorithm that the load-information update is not synchronized by exploiting variant execution times of executing scripts in dynamic Web pages. The load-information of each server is updated 'individually' by a new load-information update mechanism, and the proposed algorithm supports high scalability based on this individual update. Simulation results have proven the improvement in system performance through another aspect of high scalability. Furthermore, the proposed algorithm guarantees some level of QoS for Web clients by fairly distributing requests. A fundamental merit of the proposed algorithm is its simplicity, which supports higher throughput of the Web switch.|MinHwan Ok,Myong-Soon Park","43112|IEICE Transations|2006|Analysis System of Endoscopic Image of Early Gastric Cancer|Gastric cancer is a great part of the cancer occurrence and the mortality from cancer in Korea, and the early detection of gastric cancer is very important in the treatment and convalescence. This paper, for the early detection of gastric cancer, proposes the analysis system of an endoscopic image of the stomach, which detects abnormal regions by using the change of color in the image and by providing the surface tissue information to the detector. While advanced inflammation or cancer may be easily detected, early inflammation or cancer is difficult to detect and requires more attention to be detected. This paper, at first, converts an endoscopic image to an image of the IHb (Index of Hemoglobin) model and removes noises incurred by illumination and, automatically detects the regions suspected as cancer and provides the related information to the detector, or provides the surface tissue information for the regions appointed by the detector. This paper does not intend to provide the final diagnosis of abnormal regions detected as gastric cancer, but it intends to provide a supplementary mean to reduce the load and mistaken diagnosis of the detector, by automatically detecting the abnormal regions not easily detected by the human eye and this provides additional information for diagnosis. The experiments using practical endoscopic images for performance evaluation showed that the proposed system is effective in the analysis of endoscopic images of the stomach.|Kwang-Baek Kim,Sungshin Kim,Gwang-Ha Kim","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","43660|IEICE Transations|2006|A New Question Answering System for Chinese Restricted Domain|In this paper, we propose the construction of a web-based Question Answering (QA) system for restricted domain, which combines three resource information databases for the retrieval mechanism, including a Question&Answer database, a special domain documents database and the web resource retrieved by Google search engine. We describe a new retrieval technique of integrating a probabilistic technique based on OkapiBM and a semantic analysis which based on the ontology of HowNet knowledge base and a special domain HowNet created for the restricted domain. Furthermore, we provide a method of question expansion by computing word semantic similarity. The system is first developed for a middle-size domain of sightseeing information. The experiments proved the efficiency of our method for restricted domain and it is feasible to transfer to other domains expediently using the proposed method.|Haiqing Hu,Peilin Jiang,Fuji Ren,Shingo Kuroiwa","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Calì,Michael Kifer","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","43729|IEICE Transations|2006|Increasing the Edge-Connectivity by Contracting a Vertex Subset in Graphs|Let G  (V,E) be an edge weighted graph with n vertices and m edges. For a given integer p with  p n, we call a set X  V of p vertices a p-maximizer if X has a property that the edge-connectivity of the graph obtained by contracting X into a single vertex is no less than that of the graph obtained by contracting any other subset of p vertices. In this paper, we first show that there always exists an ordering v,v,...,vn of vertices in V such that, for each i  ,,...,n - , set v,v,...,vi is an i-maximizer. We give an O(mn + nlog n) time algorithm for finding such an ordering and then show an application to the source location problem.|Hiroshi Nagamochi","43205|IEICE Transations|2006|Maximum-Cover Source-Location Problems|Given a graph G  (V,E), a set of vertices S  V covers   V if the edge connectivity between S and  is at least a given number k. Vertices in S are called sources. The source location problem is a problem of finding a minimum-size source set covering all vertices of a given graph. This paper presents a new variation of the problem, called maximum-cover source-location problem, which finds a source set S with a given size p, maximizing the sum of the weight of vertices covered by S. It presents an O(np + m + n log n)-time algorithm for k  , where n  V and m  E. Especially it runs linear time if G is connected. This algorithm uses a subroutine for finding a subtree with the maximum weight among p-leaf trees of a given vertex-weighted tree. For the problem we give a greedy-based linear-time algorithm, which is an extension of the linear-time algorithm for finding a longest path of a given tree presented by E. W. Dijkstra around . Moreover, we show some polynomial solvable cases, e.g., a given graph is a tree or (k - )-edge-connected, and NP-hard cases, e.g., a vertex-cost function is given or G is a digraph.|Kenya Sugihara,Hiro Ito","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","43613|IEICE Transations|2006|A Multiple-Layer Self-Organizing Wireless Network|A self-organizing wireless network has to deal with reliability and congestion problems when the network size increases. In order to alleviate such problems, we designed and analyzed protocols and algorithms for a reliable and efficient multiple-layer self-organizing wireless network architecture. Each layer uses a high-power root node to supervise the self-organizing functions, to capture and maintain the physical topology, and to serve as the root of the hierarchical routing topology of the layer. We consider the problem of adding a new root with its own rooted spanning tree to the network. Based on minimum-depth and minimum-load metrics, we present efficient algorithms that achieve optimum selection of root(s). We then exploit layer scheduling algorithms that adapt to network load fluctuations in order to optimize the performance. For optimality we consider a load balancing objective and a minimum delay objective respectively. The former attempts to optimize the overall network performance while the latter strives to optimize the per-message performance. Four algorithms are presented and simulations were used to evaluate and compare their performance. We show that the presented algorithms have superior performance in terms of data throughput andor message delay, compared to a heuristic approach that does not account for network load fluctuations.|Hyunjeong Lee,Chung-Chieh Lee","43435|IEICE Transations|2006|A Polynomial Time Algorithm for Obtaining a Minimum Vertex Ranking Spanning Tree in Outerplanar Graphs|The minimum vertex ranking spanning tree problem is to find a spanning tree of G whose vertex ranking is minimum. This problem is NP-hard and no polynomial time algorithm for solving it is known for non-trivial classes of graphs other than the class of interval graphs. This paper proposes a polynomial time algorithm for solving the minimum vertex ranking spanning tree problem on outerplanar graphs.|Shin-ichi Nakayama,Shigeru Masuyama","43530|IEICE Transations|2006|An Efficient Algorithm for Evacuation Problem in Dynamic Network Flows with Uniform Arc Capacity|In this paper, we consider the quickest flow problem in a network which consists of a directed graph with capacities and transit times on its arcs. We present an O(n log n) time algorithm for the quickest flow problem in a network of grid structure with uniform arc capacity which has a single sink where n is the number of vertices in the network.|Naoyuki Kamiyama,Naoki Katoh,Atsushi Takizawa","43283|IEICE Transations|2006|An Approximation Algorithm for Minimum Certificate Dispersal Problems|We consider a network, where a special data called certificate is issued between two users, and all certificates issued by the users in the network can be represented by a directed graph. For any two users u and v, when u needs to send a message to v securely, v's public-key is needed. The user u can obtain v's public-key using the certificates stored in u and v. We need to disperse the certificates to the users such that when a user wants to send a message to the other user securely, there are enough certificates in them to get the reliable public-key. In this paper, when a certificate graph and a set of communication requests are given, we consider the problem to disperse the certificates among the nodes in the network, such that the communication requests are satisfied and the total number of certificates stored in the nodes is minimized. We formulate this problem as MINIMUM CERTIFICATE DISPERSAL (MCD for short). We show that MCD is NP-Complete, even if its input graph is restricted to a strongly connected graph. We also present a polynomial-time -approximation algorithm MinPivot for strongly connected graphs, when the communication requests satisfy some restrictions. We introduce some graph classes for which MinPivot can compute optimal dispersals, such as trees, rings, and some Cartesian products of graphs.|Hua Zheng,Shingo Omura,Koichi Wada","43421|IEICE Transations|2006|Bi-Connectivity Augmentation for Specified Vertices of a Graph with Upper Bounds on Vertex-Degree Increase|The -vertex-connectivity augmentation problem for a specified set of vertices of a graph with degree constraints, VCA-SV-DC, is defined as follows \"Given an undirected graph G  (V,E), a specified set of vertices S  V with S   and a function g  V  Z+  , find a smallest set E' of edges such that (V,E  E') has at least two internally-disjoint paths between any pair of vertices in S and such that vertex-degree increase of each v  V by the addition of E' to G is at most g(v), where Z+ is the set of nonnegative integers.\" This paper shows a linear time algorithm for VCA-SV-DC.|Toshiya Mashima,Takanori Fukuoka,Satoshi Taoka,Toshimasa Watanabe"],["43561|IEICE Transations|2006|Single-Channel Multiple Regression for In-Car Speech Enhancement|We address issues for improving hands-free speech enhancement and speech recognition performance in different car environments using a single distant microphone. This paper describes a new single-channel in-car speech enhancement method that estimates the log spectra of speech at a close-talking microphone based on the nonlinear regression of the log spectra of noisy signal captured by a distant microphone and the estimated noise. The proposed method provides significant overall quality improvements in our subjective evaluation on the regression-enhanced speech, and performed best in most objective measures. Based on our isolated word recognition experiments conducted under  real car environments, the proposed adaptive nonlinear regression approach shows an advantage in average relative word error rate (WER) reductions of .% and .%, respectively, compared to original noisy speech and ETSI advanced front-end (ETSI ES  ).|Weifeng Li,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","43118|IEICE Transations|2006|A Noise Reduction System for Wideband and Sinusoidal Noise Based on Adaptive Line Enhancer and Inverse Filter|A noise reduction technique to reduce wideband and sinusoidal noise in a noisy speech is proposed. In an actual environment, background noise includes not only wideband noise but also sinusoidal noise, such as ventilation fan and engine noise. In this paper, we propose a new noise reduction system which uses two types of adaptive line enhancers (ALE) and a noise estimation filter (NEF). First, the two ALEs are used to estimate speech components. The first ALE is used to reduce sinusoidal noise superposed on speech and wideband noise, while the second ALE is used to reduce wideband noise superposed on speech. However, since the quality of the speech enhanced by two ALEs is not good enough due to the difficulty in estimating unvoiced sound using the two ALEs, the NEF is used to improve on noise reduction capability. The NEF accurately estimates the background noise from the signal occupied by noise components, which is obtained by subtracting the speech enhanced by two ALEs from noisy speech. The enhanced speech is obtained by subtracting the estimated noise from noisy speech. Furthermore, the noise reduction system with feedback path is proposed to improve further the quality of enhanced speech.|Naoto Sasaoka,Keisuke Sumi,Yoshio Itoh,Kensaku Fujii,Arata Kawamura","43612|IEICE Transations|2006|A Hybrid HMMBN Acoustic Model Utilizing Pentaphone-Context Dependency|The most widely used acoustic unit in current automatic speech recognition systems is the triphone, which includes the immediate preceding and following phonetic contexts. Although triphones have proved to be an efficient choice, it is believed that they are insufficient in capturing all of the coarticulation effects. A wider phonetic context seems to be more appropriate, but often suffers from the data sparsity problem and memory constraints. Therefore, an efficient modeling of wider contexts needs to be addressed to achieve a realistic application for an automatic speech recognition system. This paper presents a new method of modeling pentaphone-context units using the hybrid HMMBN acoustic modeling framework. Rather than modeling pentaphones explicitly, in this approach the probabilistic dependencies between the triphone context unit and the second precedingfollowing contexts are incorporated into the triphone state output distributions by means of the BN. The advantages of this approach are that we are able to extend the modeled phonetic context within the triphone framework, and we can use a standard decoding system by assuming the next precedingfollowing context variables hidden during the recognition. To handle the increased parameter number, tying using knowledge-based phoneme classes and a data-driven clustering method is applied. The evaluation experiments indicate that the proposed model outperforms the standard HMM based triphone model, achieving a --% relative word error rate (WER) reduction.|Sakriani Sakti,Konstantin Markov,Satoshi Nakamura","43571|IEICE Transations|2006|ATR Parallel Decoding Based Speech Recognition System Robust to Noise and Speaking Styles|In this paper, we describe a parallel decoding-based ASR system developed of ATR that is robust to noise type, SNR and speaking style. It is difficult to recognize speech affected by various factors, especially when an ASR system contains only a single acoustic model. One solution is to employ multiple acoustic models, one model for each different condition. Even though the robustness of each acoustic model is limited, the whole ASR system can handle various conditions appropriately. In our system, there are two recognition sub-systems which use different features such as MFCC and Differential MFCC (DMFCC). Each sub-system has several acoustic models depending on SNR, speaker gender and speaking style, and during recognition each acoustic model is adapted by fast noise adaptation. From each sub-system, one hypothesis is selected based on posterior probability. The final recognition result is obtained by combining the best hypotheses from the two sub-systems. On the AURORA-J task used widely for the evaluation of noise robustness, our system achieved higher recognition performance than a system which contains only a single model. Also, our system was tested using normal and hyper-articulated speech contaminated by several background noises, and exhibited high robustness to noise and speaking styles.|Shigeki Matsuda,Takatoshi Jitsuhiro,Konstantin Markov,Satoshi Nakamura","43467|IEICE Transations|2006|Noise Reduction in Time Domain Using Referential Reconstruction|We present a novel approach for single-channel noise reduction of speech signals contaminated by additive noise. In this approach, the system requires speech samples to be uttered in advance by the same speaker as that of the input signal. Speech samples used in this method must have enough phonetic variety to reconstruct the input signal. In the proposed method, which we refer to as referential reconstruction, we have used a small database created from examples of speech, which will be called reference signals. Referential reconstruction uses an example-based approach, in which the objective is to find the candidate speech frame which is the most similar to the clean input frame without noise, although the input frame is contaminated with noise. When candidate frames are found, they become final outputs without any special processing. In order to find the candidate frames, a correlation coefficient is used as a similarity measure. Through automatic speech recognition experiments, the proposed method was shown to be effective, particularly for low-SNR speech signals corrupted with white noise or noise in high-frequency bands. Since the direct implementation of this method requires infeasible computational cost for searching through reference signals, a coarse-to-fine strategy is introduced in this paper.|Takehiro Ihara,Takayuki Nagai,Kazuhiko Ozeki,Akira Kurematsu","43524|IEICE Transations|2006|A Non-stationary Noise Suppression Method Based on Particle Filtering and Polyak Averaging|This paper addresses a speech recognition problem in non-stationary noise environments the estimation of noise sequences. To solve this problem, we present a particle filter-based sequential noise estimation method for front-end processing of speech recognition in noise. In the proposed method, a noise sequence is estimated in three stages a sequential importance sampling step, a residual resampling step, and finally a Markov chain Monte Carlo step with Metropolis-Hastings sampling. The estimated noise sequence is used in the MMSE-based clean speech estimation. We also introduce Polyak averaging and feedback into a state transition process for particle filtering. In the evaluation results, we observed that the proposed method improves speech recognition accuracy in the results of non-stationary noise environments a noise compensation method with stationary noise assumptions.|Masakiyo Fujimoto,Satoshi Nakamura","43651|IEICE Transations|2006|PS-ZCPA Based Feature Extraction with Auditory Masking Modulation Enhancement and Noise Reduction for Robust ASR|A pitch-synchronous (PS) auditory feature extraction method based on ZCPA (Zero-Crossings Peak-Amplitudes) was proposed previously and showed more robustness over a conventional ZCPA and MFCC based features. In this paper, firstly, a non-linear adaptive threshold adjustment procedure is introduced into the PS-ZCPA method to get optimal results in noisy conditions with different signal-to-noise ratio (SNR). Next, auditory masking, a well-known auditory perception, and modulation enhancement that simulates a strong relationship between modulation spectrums and intelligibility of speech are embedded into the PS-ZCPA method. Finally, a Wiener filter based noise reduction procedure is integrated into the method to make it more noise-robust, and the performance is evaluated against ETSI ES (WI), which is a standard front-end for distributed speech recognition. All the experiments were carried out on Aurora-J database. The experimental results demonstrated improved performance of the PS-ZCPA method by embedding auditory masking into it, and a slightly improved performance by using modulation enhancement. The PS-ZCPA method with Wiener filter based noise reduction also showed better performance than ETSI ES (WI).|Muhammad Ghulam,Takashi Fukuda,Kouichi Katsurada,Junsei Horikawa,Tsuneo Nitta","43453|IEICE Transations|2006|What HMMs Can Do|Since their inception almost fifty years ago, hidden Markov models (HMMs) have have become the predominant methodology for automatic speech recognition (ASR) systems---today, most state-of-the-art speech systems are HMM-based. There have been a number of ways to explain HMMs and to list their capabilities, each of these ways having both advantages and disadvantages. In an effort to better understand what HMMs can do, this tutorial article analyzes HMMs by exploring a definition of HMMs in terms of random variables and conditional independence assumptions. We prefer this definition as it allows us to reason more throughly about the capabilities of HMMs. In particular, it is possible to deduce that there are, in theory at least, no limitations to the class of probability distributions representable by HMMs. This paper concludes that, in search of a model to supersede the HMM (say for ASR), rather than trying to correct for HMM limitations in the general case, new models should be found based on their potential for better parsimony, computational requirements, and noise insensitivity.|Jeff A. Bilmes","43520|IEICE Transations|2006|Robust Speech Recognition by Using Compensated Acoustic Scores|This paper proposes a new compensation method of acoustic scores in the Viterbi search for robust speech recognition. This method introduces noise models to represent a wide variety of noises and realizes robust decoding together with conventional techniques of subtraction and adaptation. This method uses likelihoods of noise models in two ways. One is to calculate a confidence factor for each input frame by comparing likelihoods of speech models and noise models. Then the weight of the acoustic score for a noisy frame is reduced according to the value of the confidence factor for compensation. The other is to use the likelihood of noise model as an alternative that of a silence model when given noisy input. Since a lower confidence factor compresses acoustic scores, the decoder rather relies on language scores and keeps more hypotheses within a fixed search depth for a noisy frame. An experiment using commentary transcriptions of a broadcast sports program (MLB Major League Baseball) showed that the proposed method obtained a .% relative word error reduction. The method also reduced the relative error rate of key words by .%, and this is expected lead to an improvement metadata extraction accuracy.|Shoei Sato,Kazuo Onoe,Akio Kobayashi,Toru Imai","43606|IEICE Transations|2006|Pitch-Synchronous Peak-Amplitude PS-PA-Based Feature Extraction Method for Noise-Robust ASR|A novel pitch-synchronous auditory-based feature extraction method for robust automatic speech recognition (ASR) is proposed. A pitch-synchronous zero-crossing peak-amplitude (PS-ZCPA)-based feature extraction method was proposed previously and it showed improved performances except when modulation enhancement was integrated with Wiener filter (WF)-based noise reduction and auditory masking. However, since zero-crossing is not an auditory event, we propose a new pitch-synchronous peak-amplitude (PS-PA)-based method to render the feature extractor of ASR more auditory-like. We also examine the effects of WF-based noise reduction, modulation enhancement, and auditory masking in the proposed PS-PA method using the Aurora-J database. The experimental results show superiority of the proposed method over the PS-ZCPA and other conventional methods. Furthermore, the problem due to the reconstruction of zero-crossings from a modulated envelope is eliminated. The experimental results also show the superiority of PS over PA in terms of the robustness of ASR, though PS and PA lead to significant improvement when applied together.|Muhammad Ghulam,Kouichi Katsurada,Junsei Horikawa,Tsuneo Nitta"],["43657|IEICE Transations|2006|A Style Adaptation Technique for Speech Synthesis Using HSMM and Suprasegmental Features|This paper proposes a technique for synthesizing speech with a desired speaking style andor emotional expression, based on model adaptation in an HMM-based speech synthesis framework. Speaking styles and emotional expressions are characterized by many segmental and suprasegmental features in both spectral and prosodic features. Therefore, it is essential to take account of these features in the model adaptation. The proposed technique called style adaptation, deals with this issue. Firstly, the maximum likelihood linear regression (MLLR) algorithm, based on a framework of hidden semi-Markov model (HSMM) is presented to provide a mathematically rigorous and robust adaptation of state duration and to adapt both the spectral and prosodic features. Then, a novel tying method for the regression matrices of the MLLR algorithm is also presented to allow the incorporation of both the segmental and suprasegmental speech features into the style adaptation. The proposed tying method uses regression class trees with contextual information. From the results of several subjective tests, we show that these techniques can perform style adaptation while maintaining naturalness of the synthetic speech.|Makoto Tachibana,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi","80716|VLDB|2006|Anatomy Simple and Effective Privacy Preservation|This paper presents a novel technique, anatomy, for publishing sensitive data. Anatomy releases all the quasi-identifier and sensitive values directly in two separate tables. Combined with a grouping mechanism, this approach protects privacy, and captures a large amount of correlation in the microdata. We develop a linear-time algorithm for computing anatomized tables that obey the l-diversity privacy requirement, and minimize the error of reconstructing the microdata. Extensive experiments confirm that our technique allows significantly more effective data analysis than the conventional publication method based on generalization. Specifically, anatomy permits aggregate reasoning with average error below %, which is lower than the error obtained from a generalized table by orders of magnitude.|Xiaokui Xiao,Yufei Tao","43233|IEICE Transations|2006|Entropy Based Associative Memory|In this paper, an entropy based associative memory model will be proposed and applied to memory retrievals with an orthogonal learning model to compare with the conventional model based on the quadratic Lyapunov functional to be minimized. In the present approach, the updating dynamics will be constructed on the basis of the entropy minimization strategy which may be reduced asymptotically to the above-mentioned autocorrelation dynamics as a special case. From numerical results, it will be found that the presently proposed novel approach realizes twice of the memory capacity in comparison with the autocorrelation based dynamics such as associatron.|Masahiro Nakagawa","43070|IEICE Transations|2006|Projection Based Adaptive Window Size Selection for Efficient Motion Estimation in HAVC|This paper introduces a block based motion estimation algorithm based on projection with adaptive window size selection. The blocks cannot match well if their corresponding D projection does not match well, with this as foundation D block matching problem is translated to a simpler D matching, which eliminates majority of potential pixel participation. This projection method is combined with adaptive window size selection in which, appropriate search window for each block is determined on the basis of motion vectors and prediction errors obtained for the previous block, which makes this novel method several times faster than exhaustive search with negligible performance degradation. Encoding QCIF size video by the proposed method results in reduction of computational complexity of motion estimation by roughly % and over all encoding by %, while maintaining imagevideo quality.|Anand Paul,Jhing-Fa Wang,Jia-Ching Wang,An-Chao Tsai,Jang-Ting Chen","43701|IEICE Transations|2006|Robust Active Shape Model Using AdaBoosted Histogram Classifiers and Shape Parameter Optimization|Active Shape Model (ASM) has been shown to be a powerful tool to aid the interpretation of images, especially in face alignment. ASM local appearance model parameter estimation is based on the assumption that residuals between model fit and data have a Gaussian distribution. Moreover, to generate an allowable face shape, ASM truncates coefficients of shape principal components into the bounds determined by eigenvalues. In this paper, an algorithm of modeling local appearances, called AdaBoosted ASM, and a shape parameter optimization method are proposed. In the algorithm of modeling the local appearances, we describe our novel modeling method by using AdaBoosted histogram classifiers, in which the assumption of the Gaussian distribution is not necessary. In the shape parameter optimization, we describe that there is an inadequacy on controlling shape parameters in ASM, and our novel method on how to solve it. Experimental results demonstrate that the AdaBoosted histogram classifiers improve robustness of landmark displacement greatly, and the shape parameter optimization solves the inadequacy problem of ASM on shape constraint effectively.|Yuanzhong Li,Wataru Ito","43288|IEICE Transations|2006|Hierarchical-Analysis-Based Fast Chip-Scale Power Estimation Method for Large and Complex LSIs|This paper presents a novel power estimation method for large and complex LSIs. The proposed method is based on simulation and is used for analyzing the ways in chip-scale gate-level circuits including processors and memory are affected by gated-clock power reduction and the voltage drop due to electrical resistance. The chip-scale power estimation based on simulation patterns generally takes enormous time. In order to reduce the time to obtain accurate estimation results based on simulation patterns, we introduce three approaches \"partitioning of target LSIs and simulation pattern,\" \"memory modeling,\" and \"processor modeling.\" After placing and routing, the target LSIs are partitioned into hierarchical blocks, memory, and processors. The power consumption of each hierarchical block is calculated by using the partitioned patterns generated from chip-scale simulation patterns. The power consumption of the processor and memory blocks is estimated by a method considering the static power consumption and the rate of LSI activity ratio. Experimental results for a commercial . m-technology media processing chip show that the proposed method is  times faster than the conventional method without partitioning and that both the results are almost the same.|Yuichi Nakamura,Takeshi Yoshimura","43664|IEICE Transations|2006|A Block Smoothing-Based Method for Flicker Removal in Image Sequences|An automatic and efficient algorithm for removal of intensity flicker is proposed. The novel repair process is founded on the block-based estimation and restoration algorithm with regard to luminance variation. It is easily realized and controlled to remove most intensity flicker and preserve the wanted effects, like fade in and fade out.|Lei Zhou,Qiang Ni,Yuanhua Zhou","43511|IEICE Transations|2006|ECA Rule-Based Workflow Modeling and Implementation for Service Composition|Changes in recent business and scientific environment have created a necessity for more efficient and effective workflow infrastructure. With increasing emphasis on Service-oriented architecture, service composition becomes a hot topic in workflow research. This paper proposes a novel approach of using ECA rules to realize the workflow modeling and implementation for service composition. First of all, the concept and formalization of ECA rule-based workflow is presented. Special activities and data structures are customized for the purpose of service composition. Second, an automatic event composition and decomposition algorithm is developed to ensure the correctness and validness of service composition at design time. Finally, the proposed ECA rule-based approach for service composition is illustrated through the implementation of a workflow prototype system.|Lin Chen,Minglu Li,Jian Cao","80661|VLDB|2006|Mining Frequent Closed Cubes in D Datasets|In this paper, we introduce the concept of frequent closed cube (FCC), which generalizes the notion of D frequent closed pattern to D context. We propose two novel algorithms to mine FCCs from D datasets. The first scheme is a Representative Slice Mining (RSM) framework that can be used to extend existing D FCP mining algorithms for FCC mining. The second technique, called CubeMiner, is a novel algorithm that operates on the D space directly. We have implemented both schemes, and evaluated their performance on both real and synthetic datasets. The experimental results show that the RSM-based scheme is efficient when one of the dimensions is small, while CubeMiner is superior otherwise.|Liping Ji,Kian-Lee Tan,Anthony K. H. Tung","43690|IEICE Transations|2006|Speech Recognition Based on Students t-Distribution Derived from Total Bayesian Framework|We introduce a robust classification method based on the Bayesian predictive distribution (Bayesian Predictive Classification, referred to as BPC) for speech recognition. We and others have recently proposed a total Bayesian framework named Variational Bayesian Estimation and Clustering for speech recognition (VBEC). VBEC includes the practical computation of approximate posterior distributions that are essential for BPC, based on variational Bayes (VB). BPC using VB posterior distributions (VB-BPC) provides an analytical solution for the predictive distribution as the Student's t-distribution, which can mitigate the over-training effects by marginalizing the model parameters of an output distribution. We address the sparse data problem in speech recognition, and show experimentally that VB-BPC is robust against data sparseness.|Shinji Watanabe,Atsushi Nakamura"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","57698|GECCO|2006|Multi-objective genetic algorithms for pipe arrangement design|This paper presents an automatic design method for piping arrangement. A pipe arrangement design problem is proposed for a space in which many pipes and objects co-exist. This problem includes large-scale numerical optimization and combinatorial optimization problems, as well as two criteria. For these reasons, it is difficult to optimize the problem using usual optimization techniques such as Random Search. Therefore, multi-objective genetic algorithms suitable for this problem are developed. The proposed method for optimizing a pipe arrangement efficiently is demonstrated through several experiments.|Satoshi Ikehira,Hajime Kimura","57845|GECCO|2006|Heterogeneous cooperative coevolution strategies of integration between GP and GA|Cooperative coevolution has proven to be a promising technique for solving complex combinatorial optimization problems. In this paper, we present four different strategies which involve cooperative coevolution of a genetic program and of a population of constants evolved by a genetic algorithm. The genetic program evolves expressions that solve a problem, while the genetic algorithm provides \"good\" values for the numeric terminal symbols used by those expressions. Experiments have been performed on three symbolic regression problems and on a \"real-world\" biomedical application. Results are encouraging and confirm that our coevolutionary algorithms can be used effectively in different domains.|Leonardo Vanneschi,Giancarlo Mauri,Andrea Valsecchi,Stefano Cagnoni","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57650|GECCO|2006|Innovization innovating design principles through optimization|This paper introduces a new design methodology (we call it \"innovization\") in the context of finding new and innovative design principles by means of optimization techniques. Although optimization algorithms are routinely used to find an optimal solution corresponding to an optimization problem, the task of innovization stretches the scope beyond an optimization task and attempts to unveil new, innovative, and important design principles relating to decision variables and objectives, so that a deeper understanding of the problem can be obtained. The variety of problems chosen in the paper and the resulting innovations obtained for each problem amply demonstrate the usefulness of the innovization task. The results should encourage a wide spread applicability of the proposed innovization procedure (which is not simply an optimization procedure) to other problem-solving tasks.|Kalyanmoy Deb,Aravind Srinivasan","43265|IEICE Transations|2006|Multiobjective Evolutionary Approach to the Design of Optimal Controllers for Interval Plants via Parallel Computation|Design of optimal controllers satisfying performance criteria of minimum tracking error and disturbance level for an interval system using a multi-objective evolutionary approach is proposed in this paper. Based on a worst-case design philosophy, the design problem is formulated as a minimax optimization problem, subsequently solved by a proposed two-phase multi-objective genetic algorithm (MOGA). By using two sets of interactive genetic algorithms where the first one determines the maximum (worst-case) cost function values for a given set of controller parameters while the other one minimizes the maximum cost function values passed from the first genetic algorithm, the proposed approach evolutionarily derives the optimal controllers for the interval system. To suitably assess chromosomes for their fitness in a population, root locations of the  generalized Kharitonov polynomials will be used to establish a constraints handling mechanism, based on which a fitness function can be constructed for effective evaluation of the chromosomes. Because of the time-consuming process that genetic algorithms generally exhibit, particularly the problem nature of minimax optimization, a parallel computation scheme for the evolutionary approach in the MATLAB-based working environment is also proposed to accelerate the design process.|Chen-Chien James Hsu,Chih-Yung Yu,Shih-Chi Chang","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. Gürgen"],["57678|GECCO|2006|Exploring network topology evolution through evolutionary computations|We present an evolutionary methodology that explores the evolution of network topology when a uniform growth of the network traffic is considered. The network redesign problem is formulated as an optimization problem, subject to a set of design and performance constraints, while minimizing the redesign cost by maintaining as many as possible of the network devices that constitute the original topology. The experimental results for a -level network redesign problem (consisting of  client nodes) demonstrate the value of the search technique within the genetic algorithms in finding good solutions with respect to redesign cost and time.|Sami J. Habib,Alice C. Parker","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57724|GECCO|2006|Biobjective evolutionary and heuristic algorithms for intersection of geometric graphs|Wire routing in a VLSI chip often requires minimization of ire-length as well as the number of intersections among multiple nets. Such an optimization problem is computationally hard for which no efficient algorithm or good heuristic is known to exist. Additionally, in a biobjective setting, the major challenge to solve a problem is to obtain representative diverse solutions across the (near-) Pareto-front.In this work, we consider the problem of constructing spanning trees of two geometric graphs corresponding to two nets, each with multiple terminals, with a goal to minimize the total edge cost and the number of intersections among the edges of the two trees. We first design simple heuristics to obtain the extreme points in the solution space, which however, could not produce diverse solutions. Search algorithms based on evolutionary multiobjective optimization (EMO) are then proposed to obtain diverse solutions in the feasible solution space. Each element of this solution set is a tuple of two spanning trees corresponding to the given geometric graphs. Empirical evidence shows that the proposed evolutionary algorithms cover a larger range and are much superior to the heuristics.|Rajeev Kumar,Pramod Kumar Singh,Bhargab B. Bhattacharya","57860|GECCO|2006|The LEM implementation of learnable evolution model and its testing on complex function optimization problems|Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to ). In every experiment, LEM outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.|Janusz Wojtusiak,Ryszard S. Michalski","57812|GECCO|2006|The complete-basis-functions parameterization in ES and its application to laser pulse shaping|This paper presents a new parameterization method for the Evolution Strategies (ES) field, and its application to a challenging real-life high-dimensional Physics optimization problem, namely Femtosecond Laser Pulse Shaping. The so-called Complete-Basis-Functions Parameterization method (CBFP), to be introduced here for the first time, is developed for tackling efficiently the given laser optimization task, but nevertheless is a general method that can be used for learning any n-variables functions. The emphasis is on dimensionality reduction of the search space and the speeding-up of the convergence process respectively. This is achieved by learning the target function by using complete-basis functions as building blocks in an evolutionary search. The method is shown to boost the learning process of the given laser problem, and to yield highly satisfying results.|Ofer M. Shir,Christian Siedschlag,Thomas Bäck,Marc J. J. Vrakking","57671|GECCO|2006|Maximum cardinality matchings on trees by randomized local search|To understand the working principles of randomized search heuristics like evolutionary algorithms they are analyzed on optimization problems whose structure is well-studied. The idea is to investigate when it is possible to simulate clever optimization techniques for combinatorial optimization problems by random search. The maximum matching problem is well suited for this approach since long augmenting paths do not allow immediate improvements by local changes. It is known that randomized search heuristics like simulated annealing, the Metropolis algorithm, the (+) EA and randomized local search efficiently approximate maximum matchings for any graph however, there are graphs where they fail to find maximum matchings in polynomial time. In this paper, we examine randomized local search (RLS) for graphs whose structure is simple. We show that RLS finds maximum matchings on trees in expected polynomial time.|Oliver Giel,Ingo Wegener","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","57793|GECCO|2006|A survey of mutation techniques in genetic programming|The importance of mutation varies across evolutionary computation domains including genetic programming, evolution strategies, and genetic algorithms. In the genetic programming community, researchers' view of mutation's effectiveness spans the range from an ineffective or marginal operator, to a neutral operator, to a highly effective operator that evolves solutions more effectively than genetic programming with crossover alone. Mutation implementation and associated parameters are often under reported in genetic programming research and typically lack context that justifies the technique and parameter selection. In part, reporting variance stems from the adaptation of mutation developed by the genetic algorithm community, and the creation of new mutation techniques in genetic programming. This survey describes the controversial operator in genetic programming applications, mutation selection operators, mutation techniques and offers an organization of mutation characteristics. We suggest methodologies to improve reporting of mutation parameters and related individual selection methods.|Alan Piszcz,Terence Soule","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"],["80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","80607|VLDB|2006|On Biased Reservoir Sampling in the Presence of Stream Evolution|The method of reservoir based sampling is often used to pick an unbiased sample from a data stream. A large portion of the unbiased sample may become less relevant over time because of evolution. An analytical or mining task (eg. query estimation) which is specific to only the sample points from a recent time-horizon may provide a very inaccurate result. This is because the size of the relevant sample reduces with the horizon itself. On the other hand, this is precisely the most important case for data stream algorithms, since recent history is frequently analyzed. In such cases, we show that an effective solution is to bias the sample with the use of temporal bias functions. The maintenance of such a sample is non-trivial, since it needs to be dynamically maintained, without knowing the total number of points in advance. We prove some interesting theoretical properties of a large class of memory-less bias functions, which allow for an efficient implementation of the sampling algorithm. We also show that the inclusion of bias in the sampling process introduces a maximum requirement on the reservoir size. This is a nice property since it shows that it may often be possible to maintain the maximum relevant sample with limited storage requirements. We not only illustrate the advantages of the method for the problem of query estimation, but also show that the approach has applicability to broader data mining problems such as evolution analysis and classification.|Charu C. Aggarwal","80604|VLDB|2006|Scalable Continuous Query Processing by Tracking Hotspots|This paper considers the problem of scalably processing a large number of continuous queries. We propose a flexible framework with novel data structures and algorithms for group-processing and indexing continuous queries by exploiting potential overlaps in query predicates. Our approach partitions the collection of continuous queries into groups based on the clustering patterns of the query ranges, and then applies specialized processing strategies to those heavily-clustered groups (or hotspots). To maintain the partition dynamically, we present efficient algorithms that maintain a nearly optimal partition in nearly amortized logarithmic time. We show how to use the hotspots to scalably process large numbers of continuous select-join and band-join queries, which are much more challenging than simple range selection queries. Experiments demonstrate that this approach can improve the processing throughput by orders of magnitude. As another application of hotspots, we show how to use them to build a high-quality histogram for intervals in linear time.|Pankaj K. Agarwal,Junyi Xie,Jun Yang 0001,Hai Yu","80683|VLDB|2006|Continuous Nearest Neighbor Monitoring in Road Networks|Recent research has focused on continuous monitoring of nearest neighbors (NN) in highly dynamic scenarios, where the queries and the data objects move frequently and arbitrarily. All existing methods, however, assume the Euclidean distance metric. In this paper we study k-NN monitoring in road networks, where the distance between a query and a data object is determined by the length of the shortest path connecting them. We propose two methods that can handle arbitrary object and query moving patterns, as well as fluctuations of edge weights. The first one maintains the query results by processing only updates that may invalidate the current NN sets. The second method follows the shared execution paradigm to reduce the processing time. In particular, it groups together the queries that fall in the path between two consecutive intersections in the network, and produces their results by monitoring the NN sets of these intersections. We experimentally verify the applicability of the proposed techniques to continuous monitoring of large data and query sets.|Kyriakos Mouratidis,Man Lung Yiu,Dimitris Papadias,Nikos Mamoulis","80673|VLDB|2006|Data Mining with the SAP Netweaver BI Accelerator|The new SAP NetWeaver Business Intelligence accelerator is an engine that supports online analytical processing. It performs aggregation in memory and in query runtime over large volumes of structured data. This paper first briefly describes the accelerator and its main architectural features, and cites test results that indicate its power. Then it describes in detail how the accelerator may be used for data mining. The accelerator can perform data mining in the same large repositories of data and using the same compact index structures that it uses for analytical processing. A first such implementation of data mining is described and the results of a performance evaluation are presented. Association rule mining in a distributed architecture was implemented with a variant of the BUC iceberg cubing algorithm. Test results suggest that useful online mining should be possible with wait times of less than  seconds on business data that has not been preprocessed.|Thomas Legler,Wolfgang Lehner,Andrew Ross","43549|IEICE Transations|2006|High-Volume Continuous XPath Querying in XML Message Brokers|The core technical issue in XML message brokers, which play a key role in exchanging information in ubiquitous environments, is processing a large set of continuous XPath queries over incoming XML streams. In this paper, a new system as an epochal solution for this issue is proposed. The system is designed to minimize the runtime workload of continuous query processing by transforming XPath expressions and XML streams into newly proposed data structures and matching them efficiently. Also, system performances are estimated both in terms of space and time, and verified through a variety of experimental studies, showing that the proposed system is practically linear-scalable and stable in terms of processing a set of XPath queries in a continuous and timely fashion.|Hyunho Lee,Wonsuk Lee","80642|VLDB|2006|Randomized Algorithms for Matrices and Massive Data Sets|The tutorial will cover randomized sampling algorithms that extract structure from very large data sets modeled as matrices or tensors. Both provable algorithmic results and recent work on applying these methods to large biological and internet data sets will be discussed.|Petros Drineas,Michael W. Mahoney","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","80686|VLDB|2006|Delay Aware Querying with Seaweed|Large highly distributed data sets are poorly supported by current query technologies. Applications such as endsystem-based network management are characterized by data stored on large numbers of endsystems, with frequent local updates and relatively infrequent global one-shot queries. The challenges are scale ( to  endsystems) and endsystem unavailability. In such large systems, a significant fraction of endsystems and their data will be unavailable at any given time. Existing methods to provide high data availability despite endsystem unavailability involve centralizing, redistributing or replicating the data. At large scale these methods are not scalable. We advocate a design that trades query delay for completeness, incrementally returning results as endsystems become available. We also introduce the idea of completeness prediction, which provides the user with explicit feedback about this delaycompleteness trade-off. Completeness prediction is based on replication of compact data summaries and availability models. This metadata is orders of magnitude smaller than the data. Seaweed is a scalable query infrastructure supporting incremental results, online in-network aggregation and completeness prediction. It is built on a distributed hash table (DHT) but unlike previous DHT based approaches it does not redistribute data across the network. It exploits the DHT infrastructure for failure-resilient metadata replication, query dissemination, and result aggregation. We analytically compare Seaweed's scalability against other approaches and also evaluate the Seaweed prototype running on a large-scale network simulator driven by real-world traces.|Dushyanth Narayanan,Austin Donnelly,Richard Mortier,Antony I. T. Rowstron","80606|VLDB|2006|Query Co-Processing on Commodity Processors|The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), Cell processors and graphics processing units (GPUs). This tutorial will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips.|Anastassia Ailamaki,Naga K. Govindaraju,Stavros Harizopoulos,Dinesh Manocha"],["65865|AAAI|2006|Machine Translation for Manufacturing A Case Study at Ford Motor Company|Machine Translation was one of the first applications of Artificial Intelligence technology that was deployed to solve real-world problems. Since the early s, researchers have been building and utilizing computer systems that can translate from one language to another without extensive human intervention. In the late s, Ford Vehicle Operations began working with Systran Software Inc to adapt and customize their Machine Translation (MT) technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch and Portuguese. The use of Machine Translation (MT) was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. Our MT system has already translated over  million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation (Iwanska & Shapiro ) and natural language processing (Gazdar & Mellish ), can improve the accuracy of Machine Translation in a dynamic environment such as auto manufacturing.|Nestor Rychtyckyj","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","57810|GECCO|2006|Search-based determination of refactorings for improving the class structure of object-oriented systems|A software system's structure degrades over time, a phenomenon that is known as software decay or design drift. Since the quality of the structure has major impact on the maintainability of a system, the structure has to be reconditioned from time to time. Even if recent advances in the fields of automated detection of bad smells and refactorings have made life easier for software engineers, this is still a very complex and resource consuming task.Search-based approaches have turned out to be helpful in aiding a software engineer to improve the subsystem structure of a software system. In this paper we show that such techniques are also applicable when reconditioning the class structure of a system. We describe a novel search-based approach that assists a software engineer who has to perform this task by suggesting a list of refactorings. Our approach uses an evolutionary algorithm and simulated refactorings that do not change the system's externally visible behavior. The approach is evaluated using the open-source case study JHotDraw.|Olaf Seng,Johannes Stammel,David Burkhart","65672|AAAI|2006|Integrated AI in Space The Autonomous Sciencecraft on Earth Observing One|The Earth Observing One spacecraft has been under the control of AI software for several years - experimentally since  and since November  as the primary operations system. This software includes model-based planning and scheduling, procedural execution, and event detection software learned by support vector machine (SVM) techniques. This software has enabled a x increase in the mission science return per data downlinked and a $Myear reduction in operations costs. In this paper we discuss the AI software used, the impact of the software, and lessons learned with implications for future AI research.|Steve Chien","80725|VLDB|2006|A Semantic Information Integration Tool Suite|We describe a prototype software tool suite for semantic information integration it has the following features. First, it can import local metadata as well as a domain ontology. Imported metadata is stored persistently in an ontological format. Second, it provides a semantic query facility that allows users to retrieve information across multiple data sources using the domain ontology directly. Third, it has a GUI for users to define mappings between the local metadata and the domain ontology. Fourth, it incorporates a novel mechanism to improve system reliability by dynamically adapting query execution upon detecting various types of environmental changes. In addition, this tool suite is compatible with WC Semantic Web specifications such as RDF and OWL. It also uses the query engine of Commercial EII products for low level query processing.|Jun Yuan,Ali Bahrami,Changzhou Wang,Marie O. Murray,Anne Hunt","43188|IEICE Transations|2006|A Method for English-Korean Target Word Selection Using Multiple Knowledge Sources|Target word selection is one of the most important and difficult tasks in English-Korean Machine Translation. It effects on the overall translation accuracy of machine translation systems. In this paper, we present a new approach to Korean target word selection for an English noun with translation ambiguities using multiple knowledge such as verb frame patterns, sense vectors based on collocations, statistical Korean local context information and co-occurring POS information. Verb frame patterns constructed with dictionary and corpus play an important role in resolving the sparseness problem of collocation data. Sense vectors are a set of collocation data when an English word having target selection ambiguities is to be translated to specific Korean target word. Statistical Korean Local Context Information is an N-gram information generated using Korean corpus. The co-occurring POS information is a statistically significant POS clue which appears with ambiguous word. To evaluate our approach, we applied the method to Tellus-EK* system, English-Korean automatic translation system currently developed at ETRI ,. The experiment showed promising results for diverse sentences from web documents.|Ki-Young Lee,Sang-Kyu Park,Han-Woo Kim","65699|AAAI|2006|LOCATE Intelligent Systems Demonstration Adapting Help to the Cognitive Styles of Users|LOCATE is workspace layout design software that also serves as a testbed for developing and refining principles of adaptive aiding. This demonstration illustrates LOCATE's ability to determine user cognitive styles and provide help matched to those styles. Users are assessed along a Wholist-Analytic dimension and a Verbal-Imagery-Kinesthetic \"trimension\" and that information is stored in a User Model maintained by LOCATE. Help options provided to users for selecting alternative forms of help permit the system to track those selections and allow for system adaptation to the user's preferred style of help.|Jack L. Edwards,Greg Scott","43438|IEICE Transations|2006|Sizing Data-Intensive Systems from ER Model|There is still much problem in sizing software despite the existence of well-known software sizing methods such as Function Point method. Many developers still continue to use ad-hoc methods or so called \"expert\" approaches. This is mainly due to the fact that the existing methods require much information that is difficult to identify or estimate in the early stage of a software project. The accuracy of ad-hoc and \"expert\" methods also has much problem. The entity-relationship (ER) model is widely used in conceptual modeling (requirements analysis) for data-intensive systems. The characteristic of a data-intensive system, and therefore the source code of its software, is actually well characterized by the ER diagram that models its data. This paper proposes a method for building software size model from extended ER diagram through the use of regression models. We have collected some real data from the industry to do a preliminary validation of the proposed method. The result of the validation is very encouraging.|Hee Beng Kuan Tan,Yuan Zhao","57622|GECCO|2006|A novel approach to optimize clone refactoring activity|Software evolution and software quality are ever changing phenomena. As software evolves, evolution impacts software quality. On the other hand, software quality needs may drive software evolution strategies.This paper presents an approach to schedule quality improvement under constraints and priority. The general problem of scheduling quality improvement has been instantiated into the concrete problem of planning duplicated code removal in a geographical information system developed in C throughout the last  years. Priority and constraints arise from development team and from the adopted development process. The developer team long term goal is to get rid of duplicated code, improve software structure, decrease coupling, and improve cohesion.We present our problem formulation, the adopted approach, including a model of clone removal effort and preliminary results obtained on a real world application.|Salah Bouktif,Giuliano Antoniol,Ettore Merlo,Markus Neteler","43535|IEICE Transations|2006|Design and Implementation of a Software Inspection Support System for UML Diagrams|Software inspection is a widely acknowledged effective quality improvement method in software development by detecting defects involved in software artifacts and removing them. In research on software inspection, constructing computer supported inspection systems is a major topic in the field. A lot of systems have been reported. However few inspection support systems for model diagrams, especially UML diagrams, have been emerged. We identified four key requirements an inspection support system for UML diagrams should have. They are as follows ) direct annotations are given to model diagrams, ) version management is provided so that evolution of artifacts can be managed, ) the whole inspection process should be supported, ) horizontal and vertical readings are supported. This paper describes design and implementation of our inspection support system for UML diagrams to realize the four requirements.|Yoshihide Ohgame,Atsuo Hazeyama"],["65721|AAAI|2006|A Competitive Texas Holdem Poker Player via Automated Abstraction and Real-Time Equilibrium Computation|We present a game theory-based heads-up Texas Hold'em poker player, GS. To overcome the computational obstacles stemming from Texas Hold'em's gigantic game tree, the player employs our automated abstraction techniques to reduce the complexity of the strategy computations. Texas Hold'em consists of four betting rounds. Our player solves a large linear program (offline) to compute strategies for the abstracted first and second rounds. After the second betting round, our player updates the probability of each possible hand based on the observed betting actions in the first two rounds as well as the revealed cards. Using these updated probabilities, our player computes in real-time an equilibrium approximation for the last two abstracted rounds. We demonstrate that our player, which incorporates very little poker-specific knowledge, is competitive with leading poker-playing programs which incorporate extensive domain knowledge, as well as with advanced human players.|Andrew Gilpin,Tuomas Sandholm","43115|IEICE Transations|2006|A New Evolutionary Approach for the Optimal Communication Spanning Tree Problem|This paper deals with the Optimum Communication Spanning Tree Problem (OCST) which is well known as an NP-hard problem. For solving the problem, we uses an evolutionary approach. This paper presents a new effective tree encoding and proposes a tree construction routine (TCR) to generate a tree from the encoding. The basic principle is to break a cycle. We also propose a new crossover operator that focuses on the inheritance of parental information and the use of network information. Consequently, we confirm that the proposed algorithm is superior to other algorithms applied to the OCST problem or other tree problems. Moreover, our method can find a better solution than the solution which was previously known as the best solution. In addition, we analyzed the locality and diversity property of encoding and observed that the proposed method has high locality and at the same time it preserves population diversity for many generations. Finally, we conclude that these properties are the main reasons why the proposed method outperforms the other encodings.|Sang-Moon Soak","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","43699|IEICE Transations|2006|Prediction of Human Driving Behavior Using Dynamic Bayesian Networks|This paper presents a method of predicting future human driving behavior under the condition that its resultant behavior and past observations are given. The proposed method makes use of a dynamic Bayesian network and the junction tree algorithm for probabilistic inference. The method is applied to behavior prediction for a vehicle assumed to stop at an intersection. Such a predictive system would facilitate warning and assistance to prevent dangerous activities, such as red-light violations, by allowing detection of a deviation from normal behavior.|Toru Kumagai,Motoyuki Akamatsu","43380|IEICE Transations|2006|Analysis of Discretely Controlled Continuous Systems by means of Embedded Maps|Discretely controlled continuous systems are characterised by their interacting continuous and discrete dynamics, where the continuous subsystem usually represents the system to be controlled and the discrete part describes the controller that switches the continuous system among different operation modes. This paper analyses systems where a perpetual switching of the operation mode has to occur in order to maintain the system's state in a prescribed operating region. It is shown how the analysis of the overall hybrid system can be simplified by using an embedded map that determines the behaviour at the switching instants.|Jörg Krupar,Jan Lunze,Axel Schild,Wolfgang M. Schwarz","65849|AAAI|2006|Overconfidence or Paranoia Search in Imperfect-Information Games|We derive a recursive formula for expected utility values in imperfect- information game trees, and an imperfect-information game tree search algorithm based on it. The formula and algorithm are general enough to incorporate a wide variety of opponent models. We analyze two opponent models. The \"paranoid\" model is an information-set analog of the minimax rule used in perfect-information games. The \"overconfident\" model assumes the opponent moves randomly. Our experimental tests in the game of kriegspiel chess (an imperfect-information variant of chess) produced surprising results () against each other, and against one of the kriegspiel algorithms presented at IJCAI-, the overconfident model usually outperformed the paranoid model () the performance of both models depended greatly on how well the model corresponded to the opponent's behavior. These results suggest that the usual assumption of perfect-information game tree search--that the opponent will choose the best possible move--isn't as useful in imperfect-information games.|Austin Parker,Dana S. Nau,V. S. Subrahmanian","43538|IEICE Transations|2006|Calibration Free Virtual Display System Using Video Projector onto Real Object Surface|In this paper, we propose a novel virtual display system for a real object surface by using a video projector, so that the viewer can feel as if digital images are printed on the real surface with arbitrary shape. This system consists of an uncalibrated camera and video projector connected to a same PC and creates a virtual object by rendering D contents preserved beforehand onto a white object in a real world via a projector. For geometry registration between the rendered image and the object surface correctly, we regard the object surface as a set of a number of small rectangular regions and perform geometry registration by calculating homographies between the projector image plane and the each divided regions. By using such a homography-based method, we can avoid calibration of a camera and a projector that is necessary in a conventional method. In this system, we perform following two processes. First of all, we acquire the status of the object surface from images which capture the scene that color-coded checker patterns are projected on it and generate image rendered on it without distortion by calculating homographies. After once the projection image is generated, the rendered image can be updated if the object surface moves, or refined when it is stationary by observing the object surface. By this second process, the system always offers more accurate display. In implementation, we demonstrate our system in various conditions. This system enables it to project them as if it is printed on a real paper surface of a book. By using this system, we expect the realization of a virtual museum or other industrial application.|Shinichiro Hirooka,Hideo Saito","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","43435|IEICE Transations|2006|A Polynomial Time Algorithm for Obtaining a Minimum Vertex Ranking Spanning Tree in Outerplanar Graphs|The minimum vertex ranking spanning tree problem is to find a spanning tree of G whose vertex ranking is minimum. This problem is NP-hard and no polynomial time algorithm for solving it is known for non-trivial classes of graphs other than the class of interval graphs. This paper proposes a polynomial time algorithm for solving the minimum vertex ranking spanning tree problem on outerplanar graphs.|Shin-ichi Nakayama,Shigeru Masuyama","57732|GECCO|2006|Facilitating neural dynamics for delay compensation and prediction in evolutionary neural networks|Delay in the nervous system is a serious issue for an organism that needs to act in real time. For example, during the time a signal travels from a peripheral sensor to the central nervous system, a moving object in the environment can cover a significant distance which can lead to critical errors in the effect of the corresponding motor output. This paper proposes that facilitating synapses which show a dynamic sensitivity to the changing input may play an important role in compensating for neural delays, through extrapolation. The idea was tested in a modified D pole-balancing problem which included sensory delays. Within this domain, we tested the behavior of recurrent neural networks with facilitatory neural dynamics trained via neuroevolution. Analysis of the performance and the evolved network parameters showed that, under various forms of delay, networks utilizing extrapolatory dynamics are at a significant competitive advantage compared to networks without such dynamics. In sum, facilitatory (or extrapolatory) dynamics can be used to compensate for delay at a single-neuron level, thus allowing a developing nervous system to stay in touch with the present environmental state.|Heejin Lim,Yoonsuck Choe"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65627|AAAI|2006|QUICR-Learning for Multi-Agent Coordination|Coordinating multiple agents that need to perform a sequence of actions to maximize a system level reward requires solving two distinct credit assignment problems. First, credit must be assigned for an action taken at time step t that results in a reward at time step t  t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first credit assignment problem is typically addressed with temporal difference methods such as Q-learning. The second credit assignment problem is typically addressed by creating custom reward functions. To address both credit assignment problems simultaneously, we propose the \"Q Updates with Immediate Counterfactual Rewards-learning\" (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods.|Adrian K. Agogino,Kagan Tumer","57711|GECCO|2006|Dominance hierarchies and social diversity in multi-agent systems|In this study, we investigate self-organizing social hierarchies in multi-agent systems. Agents occupy the nodes of a small-world network and interact exclusively with other agents in their local neighbourhood. Here, the interactions represent competition for a limited resource. Monte-Carlo simulations show that the changes in a network's structure can alter the steady-state attributes for fixed rewardpenalty mechanisms. The results suggest that the expected phase transition from a homogeneous to a hierarchical society depends on (a) the relative strengths of the feedback mechanisms employed, (b) the underlying communication topology, and (c) whether previously dominated agents are replaced in the population by agents with higher social status. A key contribution of this paper is the coherent picture painted of the relationship between social differentiation and spatial structure in a multi-agent system.|Michael Kirley","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","57597|GECCO|2006|Distributed evaluation functions for fault tolerant multi-rover systems|The ability to evolve fault tolerant control strategies for large collections of agents is critical to the successful application of evolutionary strategies to domains where failures are common. Furthermore, while evolutionary algorithms have been highly successful in discovering single-agent control strategies, extending such algorithms to multi-agent domains has proven to be difficult. In this paper we present a method for shaping evaluation functions for agents that provide control strategies that are both tolerant to different types of failures and lead to coordinated behavior in a multi-agent setting. This method neither relies on a centralized strategy (susceptible to single points of failures) nor a distributed strategy where each agent uses a system wide evaluation function (severe credit assignment problem). In a multi-rover problem, we show that agents using our agent-specific evaluation perform up to % better than agents using the system evaluation. In addition we show that agents are still able to maintain a high level of performance when up to % of the agents fail due to actuator, communication or controller faults.|Adrian K. Agogino,Kagan Tumer","65890|AAAI|2006|Multiagent Coalition Formation for Computer-Supported Cooperative Learning|In this paper, we describe a computer-supported cooperative learning system in education and the results of its deployment. The system, called I-MINDS, consists of a set of teacher agents, group agents, and student agents. While the agents possess individual intelligent capabilities, the novel invention of I-MINDS lies in multiagent intelligence and coalition formation. I-MINDS supports student participation and collaboration and helps the instructor manage large, distance classrooms. Specifically, it uses a Vickrey auction-based and learning-enabled algorithm called VALCAM to form student groups in a structured cooperative learning setting. We have deployed I-MINDS in an introductory computer science course (CS) and conducted experiments in the Spring and Fall semesters of  to study how I-MINOS-supported collaboration fares against traditional, face-to-face collaboration. Results showed that students using I-MINDS performed (and outperformed in some aspects) as well as students in traditional settings.|Leen-Kiat Soh,Nobel Khandaker,Hong Jiang","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm"]]},"title":{"entropy":6.2004003200453175,"topics":["and its, the web, for and, and, its applications, with and, the and, semantic web, using and, design and, and applications, linear transconductor, information and, and quality, control mutation, for data, quality mutation, the semantic, case-based reasoning, efficient xml","algorithm for, genetic algorithm, for the, the problem, algorithm the, genetic for, genetic programming, for problem, the, and algorithm, evolutionary algorithm, using genetic, for and, algorithm problem, genetic and, using algorithm, the performance, evolutionary for, the and, for optimal","for system, method for, system using, scheme for, least squares, system based, and system, speech recognition, image using, adaptive filter, adaptive for, control for, for communication, for image, system with, video coding, noise and, based, for filter, for ofdm","neural networks, for networks, for learning, reinforcement learning, support vector, model for, particle swarm, support machines, vector machines, learning, efficient for, learning system, learning with, order crossover, learning classifier, using model, and networks, learning and, classifier system, efficient networks","with and, interactive and, linear transconductor, for with, interface for, recognition and, combining with, source with, and reality, using and, semantic for, for interactive, extracting from, indexing and, linear and, for virtual, for reality, using semantic, semantic and, for source","design and, and relations, design for, design with, sequence zone, design the, design, evaluation and, sequence set, analog for, design system, language and, design using, for sequence, and cellular, sequence and, knowledge and, sequence with, the and, using the","for problem, for scheduling, hybrid for, for and, for flexible, hybrid algorithm, and flexible, hybrid and, genetic scheduling, petri nets, hybrid genetic, and bottleneck, genetic flexible, for hierarchical, effective for, local search, and scheduling, flexible scheduling, bottleneck flexible, genetic problem","for optimal, for graph, algorithm for, algorithm tree, for tree, algorithm graph, fast algorithm, for with, bounds for, for and, and tree, spanning tree, for computing, for, grid computing, with bounds, vertex graph, fast for, for grid, time algorithm","method for, for image, image using, estimation for, for based, method and, method system, estimation and, for communication, communication with, method based, based and, image and, method using, for rate, image based, based, for ofdm, for jpeg, image color","scheme for, for mobile, for wireless, protocol for, for channel, protocol based, wireless networks, key agreement, human-robot interaction, protocol and, system environments, energy efficient, scheme using, scheme based, protocol networks, for environments, for system, for multicast, for over, system channel","for networks, neural networks, efficient for, and networks, using neural, for neural, algorithm networks, using networks, efficient networks, for constraint, constraint satisfaction, efficient algorithm, and neural, and verification, networks with, domain using, adaptive for, for generating, for software, selection for","model for, using model, linear for, model, with model, classifier system, using classifier, parameter for, model based, distributed for, linear and, for prediction, parameter and, hardware for, for space, using prediction, using algorithm, for belief, using decomposition, for using"],"ranking":[["65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","43178|IEICE Transations|2006|A Bipolar Linear Transconductor Using Translinear Cells and Its Application|A novel linear transconductor using translinear cells is proposed. It consists of a voltage follower, a resistor, and a current follower. SPICE simulations using an  GHz bipolar transistor-array parameter show that the linear transconductor with a transconductance of  mS exhibits a linearity error of less than .% over an input voltage range of  V for a supply voltage of . V. The temperature coefficient of the transconductance is less than  ppmC. The --dB frequency of the transconductance is more than . GHz. Applying the linear transconductor as a building block, the design of a bandpass filter with center frequency of  MHz and Q-factor of  is presented.|Won-Sup Chung,Seong-Hoon Kim,Sang-Hee Son,Hee-Jun Kim","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","65674|AAAI|2006|An Edge Deletion Semantics for Belief Propagation and its Practical Impact on Approximation Quality|We show in this paper that the influential algorithm of iterative belief propagation can be understood in terms of exact inference on a polytree, which results from deleting enough edges from the original network. We show that deleting edges implies adding new parameters into a network, and that the iterations of belief propagation are searching for values of these new parameters which satisfy intuitive conditions that we characterize. The new semantics lead to the following question Can one improve the quality of approximations computed by belief propagation by recovering some of the deleted edges, while keeping the network easy enough for exact inference We show in this paper that the answer is yes, leading to another question How do we choose which edges to recover To answer, we propose a specific method based on mutual information which is motivated by the edge deletion semantics. Empirically, we provide experimental results showing that the quality of approximations can be improved without incurring much additional computational cost. We also show that recovering certain edges with low mutual information may not be worthwhile as they increase the computational complexity, without necessarily improving the quality of approximations.|Arthur Choi,Adnan Darwiche","65780|AAAI|2006|A Look at Parsing and Its Applications|This paper provides a brief introduction to recent work in statistical parsing and its applications. We highlight successes to date, remaining challenges, and promising future work.|Matthew Lease,Eugene Charniak,Mark Johnson,David McClosky","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","57755|GECCO|2006|Extraction of landscape information based on a quality control approach and its applications to mutation in GAExtraction of landscape information based on a quality control approach and its applications to mutation in GA|We introduce an attraction hypothesis and repulsion hypothesis on combinations of genes and we characterize \"genelocus pair\" as a \"Unique Inheritance\" if the pair satisfies one of the hypotheses. We propose a method based on a statistical approach to extract a set of gene-locus pairs characterized as \"Unique Inheritance\", and also two new genetic operations, attraction mutation and repulsion mutation.|Mitsukuni Matayoshi,Morikazu Nakamura,Hayao Miyagi","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["57669|GECCO|2006|Segmentation of medical images using a genetic algorithm|Segmentation of medical images is challenging due to poor image contrast and artifacts that result in missing or diffuse organtissue boundaries. Consequently, this task involves incorporating as much prior information as possible (e.g., texture, shape, and spatial location of organs) into a single framework. In this paper, we present a genetic algorithm for automating the segmentation of the prostate on two-dimensional slices of pelvic computed tomography (CT) images. In this approach the segmenting curve is represented using a level set function, which is evolved using a genetic algorithm (GA). Shape and textural priors derived from manually segmented images are used to constrain the evolution of the segmenting curve over successive generations.We review some of the existing medical image segmentation techniques. We also compare the results of our algorithm with those of a simple texture extraction algorithm (Laws' texture measures) as well as with another GA-based segmentation tool called GENIE. Our preliminary tests on a small population of segmenting contours show promise by converging on the prostate region. We expect that further improvements can be achieved by incorporating spatial relationships between anatomical landmarks, and extending the methodology to three dimensions.|Payel Ghosh,Melanie Mitchell","57632|GECCO|2006|Using a genetic algorithm to evolve cellular automata for DD computational development|Form generation or morphogenesis is one of the main stages of both artificial and natural development. This paper provides results from experiments in which a genetic algorithm (GA) was used to evolve cellular automata (CA) to produce predefined D and D shapes. The GA worked by evolving the CA rule table and the number of iterations that the model was to run. After the final chromosomes were obtained for all shapes, the CA model was allowed to run starting with a single cell in the middle of the lattice until the allowed number of iterations was reached and a shape was formed. In all cases, mean fitness values of evolved chromosomes were above .|Arturo Chavoya,Yves Duthen","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","43029|IEICE Transations|2006|Antenna Selection Using Genetic Algorithm for MIMO Systems|Recent work has shown that the usage of multiple antennas at the transmitter and receiver in a flat fading environment results in a linear increase in channel capacity. But increasing the number of antennas induces the higher hardware costs and computational burden. To overcome those problems, it is effective to select antennas appropriately among all available ones. In this paper, a new antenna selection method is proposed. The transmit antennas are selected so as to maximize the channel capacity using the genetic algorithm (GA) which is the one of the general random search algorithm. The results show that the proposed GA achieves almost the same performance as the optimal selection method with less computational amount.|Qianjing Guo,Suk Chan Kim,Dong Chan Park","43356|IEICE Transations|2006|Solving the Graph Planarization Problem Using an Improved Genetic Algorithm|An improved genetic algorithm for solving the graph planarization problem is presented. The improved genetic algorithm which is designed to embed a graph on a plane, performs crossover and mutation conditionally instead of probability. The improved genetic algorithm is verified by a large number of simulation runs and compared with other algorithms. The experimental results show that the improved genetic algorithm performs remarkably well and outperforms its competitors.|Rong Long Wang,Kozo Okazaki","57654|GECCO|2006|The snake in the box problem mathematical conjecture and a genetic algorithm approach|With applications in coding theory and hypercube-based computing and networking, the \"snake in the box\" problem is of great practical importance. Moreover, it is both mathematically elegant and highly difficult. The problem is simply to find the longest \"snake\" in a hypercube. However, as the hypercube grows in dimensionality, the size of the search space increases exponentially. Moreover, as the maximum snake length is only known for the smallest dimensions (where the snakes themselves have already been identified), there is no known stopping criterion for the search in higher dimensions. In this paper we make a mathematical conjecture about the possible maximum length of a snake in a hypercube of dimension d. We use a genetic algorithm for finding snakes in a  hypercube to show some results.|Pedro A. Diaz-Gomez,Dean F. Hougen","57783|GECCO|2006|Characterizing large text corpora using a maximum variation sampling genetic algorithm|There exists an enormous amount of information available via the Internet. Much of this data is in the form of text-based documents. These documents cover a variety of topics that are vitally important to the scientific, business, and defensesecurity communities. Currently, there are a many techniques for processing and analyzing such data. However, the ability to quickly characterize a large set of documents still proves challenging. Previous work has successfully demonstrated the use of a genetic algorithm for providing a representative subset for text documents via adaptive sampling. In this work, we further expand and explore this approach on much larger data sets using a parallel Genetic Algorithm (GA) with adaptive parameter control. Experimental results are presented and discussed.|Robert M. Patton,Thomas E. Potok","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius"],["43234|IEICE Transations|2006|Speech Noise Reduction System Based on Frequency Domain ALE Using Windowed Modified DFT Pair|The speech noise reduction system based on the frequency domain adaptive line enhancer using a windowed modified DFT (MDFT) pair is presented. The adaptive line enhancer (ALE) is effective for extracting sinusoidal signals blurred by a broadband noise. In addition, it utilizes only one microphone. Therefore, it is suitable for the realization of speech noise reduction in portable electronic devices. In the ALE, an input signal is generated by delaying a desired signal using the decorrelation parameter, which makes the noise in the input signal decorrelated with that in the desired one. In the present paper, we propose to set decorrelation parameters in the frequency domain and adjust them to optimal values according to the relationship between speech and noise. Such frequency domain decorrelation parameters enable the reduction of the computational complexity of the proposed system. Also, we introduce the window function into MDFT for suppressing spectral leakage. The performance of the proposed noise reduction system is examined through computer simulations.|Isao Nakanishi,Yuudai Nagata,Takenori Asakura,Yoshio Itoh,Yutaka Fukui","43186|IEICE Transations|2006|A High-Accuracy Passive D Measurement System Using Phase-Based Image Matching|This paper presents a high-accuracy D (three-dimen-sional) measurement system using multi-camera passive stereo vision to reconstruct D surfaces of free form objects. The proposed system is based on an efficient stereo correspondence technique, which consists of (i) coarse-to-fine correspondence search, and (ii) outlier detection and correction, both employing phase-based image matching. The proposed sub-pixel correspondence search technique contributes to dense reconstruction of arbitrary-shaped D surfaces with high accuracy. The outlier detection and correction technique contributes to high reliability of reconstructed D points. Through a set of experiments, we show that the proposed system measures D surfaces of objects with sub-mm accuracy. Also, we demonstrate high-quality dense D reconstruction of a human face as a typical example of free form objects. The result suggests a potential possibility of our approach to be used in many computer vision applications.|Mohammad Abdul Muquit,Takuma Shibahara,Takafumi Aoki","43118|IEICE Transations|2006|A Noise Reduction System for Wideband and Sinusoidal Noise Based on Adaptive Line Enhancer and Inverse Filter|A noise reduction technique to reduce wideband and sinusoidal noise in a noisy speech is proposed. In an actual environment, background noise includes not only wideband noise but also sinusoidal noise, such as ventilation fan and engine noise. In this paper, we propose a new noise reduction system which uses two types of adaptive line enhancers (ALE) and a noise estimation filter (NEF). First, the two ALEs are used to estimate speech components. The first ALE is used to reduce sinusoidal noise superposed on speech and wideband noise, while the second ALE is used to reduce wideband noise superposed on speech. However, since the quality of the speech enhanced by two ALEs is not good enough due to the difficulty in estimating unvoiced sound using the two ALEs, the NEF is used to improve on noise reduction capability. The NEF accurately estimates the background noise from the signal occupied by noise components, which is obtained by subtracting the speech enhanced by two ALEs from noisy speech. The enhanced speech is obtained by subtracting the estimated noise from noisy speech. Furthermore, the noise reduction system with feedback path is proposed to improve further the quality of enhanced speech.|Naoto Sasaoka,Keisuke Sumi,Yoshio Itoh,Kensaku Fujii,Arata Kawamura","43571|IEICE Transations|2006|ATR Parallel Decoding Based Speech Recognition System Robust to Noise and Speaking Styles|In this paper, we describe a parallel decoding-based ASR system developed of ATR that is robust to noise type, SNR and speaking style. It is difficult to recognize speech affected by various factors, especially when an ASR system contains only a single acoustic model. One solution is to employ multiple acoustic models, one model for each different condition. Even though the robustness of each acoustic model is limited, the whole ASR system can handle various conditions appropriately. In our system, there are two recognition sub-systems which use different features such as MFCC and Differential MFCC (DMFCC). Each sub-system has several acoustic models depending on SNR, speaker gender and speaking style, and during recognition each acoustic model is adapted by fast noise adaptation. From each sub-system, one hypothesis is selected based on posterior probability. The final recognition result is obtained by combining the best hypotheses from the two sub-systems. On the AURORA-J task used widely for the evaluation of noise robustness, our system achieved higher recognition performance than a system which contains only a single model. Also, our system was tested using normal and hyper-articulated speech contaminated by several background noises, and exhibited high robustness to noise and speaking styles.|Shigeki Matsuda,Takatoshi Jitsuhiro,Konstantin Markov,Satoshi Nakamura","43036|IEICE Transations|2006|Adaptive OFDM System with Optimal RCPC Code-Rate for Each Subcarrier|Adaptive transmission methods improve the performance of wireless communication system by adjusting parameters like modulation, code-rate, and power depending on the channel state adaptively. In this letter, we consider the adaptive code-rate OFDM system in which code-rate of each subcarrier is adapted optimally. RCPC code is used to obtain different code-rate for each subcarrier. Performance analysis shows that -- dB SNR gain or up to --% data rate increase is achieved at bit error rate -.|Dong Chan Park,Suk Chan Kim,Seokho Yoon","43112|IEICE Transations|2006|Analysis System of Endoscopic Image of Early Gastric Cancer|Gastric cancer is a great part of the cancer occurrence and the mortality from cancer in Korea, and the early detection of gastric cancer is very important in the treatment and convalescence. This paper, for the early detection of gastric cancer, proposes the analysis system of an endoscopic image of the stomach, which detects abnormal regions by using the change of color in the image and by providing the surface tissue information to the detector. While advanced inflammation or cancer may be easily detected, early inflammation or cancer is difficult to detect and requires more attention to be detected. This paper, at first, converts an endoscopic image to an image of the IHb (Index of Hemoglobin) model and removes noises incurred by illumination and, automatically detects the regions suspected as cancer and provides the related information to the detector, or provides the surface tissue information for the regions appointed by the detector. This paper does not intend to provide the final diagnosis of abnormal regions detected as gastric cancer, but it intends to provide a supplementary mean to reduce the load and mistaken diagnosis of the detector, by automatically detecting the abnormal regions not easily detected by the human eye and this provides additional information for diagnosis. The experiments using practical endoscopic images for performance evaluation showed that the proposed system is effective in the analysis of endoscopic images of the stomach.|Kwang-Baek Kim,Sungshin Kim,Gwang-Ha Kim","43404|IEICE Transations|2006|An Active Noise Control System Based on Simultaneous Equations Method without Auxiliary Filters|A simultaneous equations method is one of active noise control algorithms without estimating an error path. This algorithm requires identification of a transfer function from a reference microphone to an error microphone containing the effect of a noise control filter. It is achieved by system identification of an auxiliary filter. However, the introduction of the auxiliary filter requires more number of samples to obtain the noise control filter and brings a requirement of some undesirable assumption in the multiple channel case. In this paper, a new simultaneous equations method without the identification of the auxiliary filter is proposed. By storing a small number of input signals and error signals, we avoid this identification. Therefore, we can reduce the number of samples to obtain the noise control filters and can avoid the undesirable assumption. From simulation examples, it is verified that the merits of the ordinary method is also retained in the proposed method.|Mitsuji Muneyasu,Osamu Hisayasu,Kensaku Fujii,Takao Hinamoto","43182|IEICE Transations|2006|Interface for Barge-in Free Spoken Dialogue System Using Nullspace Based Sound Field Control and Beamforming|In this paper, we describe a new interface for a barge-in free spoken dialogue system combining multichannel sound field control and beamforming, in which the response sound from the system can be canceled out at the microphone points. The conventional method inhibits a user from moving because the system forces the user to stay at a fixed position where the response sound is reproduced. However, since the proposed method does not set control points for the reproduction of the response sound to the user, the user is allowed to move. Furthermore, the relaxation of strict reproduction for the response sound enables us to design a stable system with fewer loudspeakers than those used in the conventional method. The proposed method shows a higher performance in speech recognition experiments.|Shigeki Miyabe,Hiroshi Saruwatari,Kiyohiro Shikano,Yosuke Tatekura","43368|IEICE Transations|2006|Communication Scheme for a Highly Collision-Resistive RFID System|A highly collision-resistive RFID system multiplexes communications between thousands of tags and a single reader in combination with time-domain multiplexing code division multiple access (TD-CDMA), CRC error detection, and re-transmission for error recovery. The collision probability due to a random selection of CDMA codes and TDMA channels bounds the number of IDs successfully transmitted to a reader during a limited time frame. However, theoretical analysis showed that the re-transmission greatly reduced the collision probability and that an ID error rate of .  - could be achieved when , ID tags responded within a time frame of  msec in ideal communication channels. The proposed collision-resistive communication scheme for a thousand multiplexed channels was modeled on a discrete-time digital expression and an FPGA-based emulator was built to evaluate a practical ID error rate under the presence of background noise in communication channels. To achieve simple anti-noise communication in a multiple-response RFID system, as well as unurged re-transmission of ID data, adjusting of correlator thresholds provides a significant improvement to the error rate. Thus, the proposed scheme does not require a reader to request ID transmission to erroneously responding tags. A reader also can lower noise influence by using correlator thresholds, since the scheme multiplexes IDs by CDMA-based communication. The effectiveness of the re-transmission was confirmed experimentally even in noisy channels, and the ID error rate derived from the emulation was .  -. The emulation was useful for deriving an optimum set of RFID system parameters to be used in the design of mixed analog and digital integrated circuits for RFID communication.|Yohei Fukumizu,Shuji Ohno,Makoto Nagata,Kazuo Taki","43643|IEICE Transations|2006|Efficient Media Synchronization Method for Video Telephony System|In this letter, we derive an efficient audiovideo synchronization method for video telephony. For synchronization, this method does not require any further RTCP packet processing except for the first one. The derived decision rule is far more compact than the conventional method. This decision rule is incorporated in an actual video telephony system adopting Texas Instruments (TI) OMAP  processor and Qualcomm MSM . The computational requirement was compared with the conventional method and through simulations the superiority of the proposed method is proved.|Chanwoo Kim,Kwang-deok Seo,Wonyong Sung"],["57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","57594|GECCO|2006|Smart crossover operator with multiple parents for a Pittsburgh learning classifier system|This paper proposes a new smart crossover operator for a Pittsburgh Learning Classifier System. This operator, unlike other recent LCS approaches of smart recombination, does not learn the structure of the domain, but it merges the rules of N parents (N  ) to generate a new offspring. This merge process uses an heuristic that selects the minimum subset of candidate rules that obtains maximum training accuracy. Moreover the operator also includes a rule pruning scheme to avoid the inclusion of over-specific rules, and to guarantee as much as possible the robust behaviour of the LCS. This operator takes advantage from the fact that each individual in a Pittsburgh LCS is a complete solution, and the system has a global view of the solution space that the proposed rule selection algorithm exploits. We have empirically evaluated this operator using a recent LCS called GAssist. First with the standard LCS benchmark, the  bits multiplexer, and later using  standard real datasets. The results of the experiments over these datasets indicate that the new operator manages to increase the accuracy of the system over the classical crossover in  of the  datasets, and never having a significantly worse performance than the classic operator.|Jaume Bacardit,Natalio Krasnogor","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","57803|GECCO|2006|Reward allotment in an event-driven hybrid learning classifier system for online soccer games|This paper describes our study into the concept of using rewards in a classifier system applied to the acquisition of decision-making algorithms for agents in a soccer game. Our aim is to respond to the changing environment of video gaming that has resulted from the growth of the Internet, and to provide bug-free programs in a short time. We have already proposed a bucket brigade algorithm (a reinforcement learning method for classifiers) and a procedure for choosing what to learn depending on the frequency of events with the aim of facilitating real-time learning while a game is in progress. We have also proposed a hybrid system configuration that combines existing algorithm strategies with a classifier system, and we have reported on the effectiveness of this hybrid system. In this paper, we report on the results of performing reinforcement learning with different reward values assigned to reflect differences in the roles performed by forward, midfielder and defense players, and we describe the results obtained when learning is performed with different combinations of success rewards for various type of play such as dribbling and passing. In  matches played against an existing soccer game incorporating an algorithm devised by humans, a better win ratio and better convergence were observed compared with the case where learning was performed with no roles assigned to all of the in-game agents.|Yuji Sato,Yosuke Akatsuka,Takenori Nishizono","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci","43085|IEICE Transations|2006|An Efficient Method for Simplifying Decision Functions of Support Vector Machines|A novel method to simplify decision functions of support vector machines (SVMs) is proposed in this paper. In our method, a decision function is determined first in a usual way by using all training samples. Next those support vectors which contribute less to the decision function are excluded from the training samples. Finally a new decision function is obtained by using the remaining samples. Experimental results show that the proposed method can effectively simplify decision functions of SVMs without reducing the generalization capability.|Jun Guo,Norikazu Takahashi,Tetsuo Nishi"],["65738|AAAI|2006|Object Boundary Detection in Images using a Semantic Ontology|We present a novel method for detecting the boundaries between objects in images that uses a large, hierarchical, semantic ontology - WordNet. The semantic object hierarchy in WordNet grounds this ill-posed segmentation problem, so that true boundaries are defined as edges between instances of different classes, and all other edges are clutter. To avoid fully classifying each pixel, which is very difficult in generic images, we evaluate the semantic similarity of the two regions bounding each edge in an initial oversegmentation. Semantic similarity is computed using WordNet enhanced with appearance information, and is largely orthogonal to visual similarity. Hence two regions with very similar visual attributes, but from different categories, can have a large semantic distance and therefore evidence of a strong boundary between them, and vice versa. The ontology is trained with images from the UC Berkeley image segmentation benchmark, extended with manual labeling of the semantic content of each image segment. Results on boundary detection against the benchmark images show that semantic similarity computed through WordNet can significantly improve boundary detection compared to generic segmentation.|Anthony Hoogs,Roderic Collins","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","43609|IEICE Transations|2006|A Novel Test-Bed for Immersive and Interactive Broadcasting Production Using Augmented Reality and Haptics|In this paper, we demonstrate an immersive and interactive broadcasting production system with a new haptically enhanced multimedia broadcasting chain. The system adapts Augmented Reality (AR) techniques, which merges captured videos and virtual D media seamlessly through multimedia streaming technology, and haptic interaction technology in near real-time. In this system, viewers at the haptic multimedia client can interact with AR broadcasting production transmitted via communication network. We demonstrate two test applications, which show that the addition of AR- and haptic-interaction to the conventional audio-visual contents can improve immersiveness and interactivity of viewers with rich contents service.|Seungjun Kim,Jongeun Cha,Jong-Phil Kim,Jeha Ryu,Seongeun Eom,Nitaigour-Premchand Mahalik,Byung-Ha Ahn","43178|IEICE Transations|2006|A Bipolar Linear Transconductor Using Translinear Cells and Its Application|A novel linear transconductor using translinear cells is proposed. It consists of a voltage follower, a resistor, and a current follower. SPICE simulations using an  GHz bipolar transistor-array parameter show that the linear transconductor with a transconductance of  mS exhibits a linearity error of less than .% over an input voltage range of  V for a supply voltage of . V. The temperature coefficient of the transconductance is less than  ppmC. The --dB frequency of the transconductance is more than . GHz. Applying the linear transconductor as a building block, the design of a bandpass filter with center frequency of  MHz and Q-factor of  is presented.|Won-Sup Chung,Seong-Hoon Kim,Sang-Hee Son,Hee-Jun Kim","43556|IEICE Transations|2006|Accuracy of Two-Dipole Source Localization Using a Method Combining BP Neural Network with NLS Method from -Channel EEGs|The electroencephalogram (EEG) has become a widely used tool for investigating brain function. Brain signal source localization is a process of inverse calculation from sensor information (electric potentials for EEG) to the identification of multiple brain sources to obtain the locations and orientation parameters. In this paper, we describe a combination of the backpropagation neural network (BPNN) with the nonlinear least-square (NLS) method to localize two dipoles with reasonable accuracy and speed from EEG data computerized by two dipoles randomly positioned in the brain. The trained BPNN, obtains the initial values for the two dipoles through fast calculation and also avoids the influence of noise. Then the NLS method (Powell algorithm) is used to accurately estimate the two dipole parameters. In this study, we also obtain the minimum distance between the assumed dipole pair, . cm, in order to localize two sources from a smaller limited distance between the dipole pair. The present simulation results demonstrate that the combined method can allow us to localize two dipoles with high speed and accuracy, that is, in  seconds and with the position error of around .%, and to reduce the influence of noise.|Zhuoming Li,Xiaoxiao Bai,Qinyu Zhang,Masatake Akutagawa,Fumio Shichijo,Yohsuke Kinouchi","43335|IEICE Transations|2006|Four-Quadrant-Input Linear Transconductor Employing Source and Sink Currents Pair for Analog Multiplier|A four-quadrant-input linear transconductor generating a product or a product sum current is proposed. The proposed circuit eliminates the influence of channel length modulation and expands a dynamic input voltage range. As an application of the proposed circuit, the four-quadrant analog multiplier is designed. The four-quadrant analog multiplier consists of the proposed circuit, an input circuit and a class AB current buffer. HSPICE simulation results with . m n-well single CMOS process parameter are shown in order to evaluate the proposed circuit.|Masakazu Mizokami,Kawori Takakubo,Hajime Takakubo","43325|IEICE Transations|2006|A New Linear Transconductor Combining a Source Coupled Pair with a Transconductor Using Bias-Offset Technique|Linearity of a transconductor with a theoretical linear characteristic is deteriorated by mobility degradation, in practice. In this paper, a technique to improve the linearity by combining a source-coupled pair with the transconductor is proposed. The proposed transconductor is the circuit that the deteriorated linearity of the conventional part is compensated by the transconductance characteristic of the source-coupled pair. In order to confirm the validity of the proposed technique, SPICE simulation is carried out. The transconductance change ratio of the proposed technique is about % and is  or less of the conventional circuit.|Isamu Yamaguchi,Fujihiko Matsumoto,Makoto Izuma,Yasuaki Noguchi","43702|IEICE Transations|2006|A Framework for Virtual Reality with Tangible Augmented Reality-Based User Interface|In this paper, we propose a framework for virtual reality, I-NEXT, which enables users to interact with virtual objects by tangible objects in immersive networked virtual environment. The primary goal of this framework is to support rapid development of immersive and interactive virtual reality systems as well as various types of user interfaces. The proposed framework consists of user interface for interactions, immersive virtual environment, and networking interface. In this framework, we adopt several design patterns to guarantee that either developers or users (artists) can easily implement their VR applications without strong knowledge of VR techniques such as programming, libraries etc. One of the key features of this framework is the presence of the device module which supports a natural user interaction in a virtual environment. For example, the proposed framework provides users with tangible objects so that the users are able to manipulate virtual objects by touching real objects. The proposed framework also supports large scale stereoscopic display through clustering technique. To realize the effectiveness of the proposed framework, we have been developing an application for digital heritage reconstruction. Having been through development of the system, we believe that virtual reality technology is one of the promising technologies which enable users to experience realities in a digital space. Detailed explanations of each component and system architecture are presented.|Dongpyo Hong,Woontack Woo","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang","65899|AAAI|2006|WikiRelate Computing Semantic Relatedness Using Wikipedia|Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by integrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the performance of an NLP application processing naturally occurring texts.|Michael Strube,Simone Paolo Ponzetto"],["43211|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Construction Using a Cyclic Hadamard Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone (hereinafter binary ZCA sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCA sequence set by using a cyclic difference set and a collection of mutually orthogonal complementary sets.|Takafumi Hayashi","43120|IEICE Transations|2006|A Plan-Generation-Evaluation Framework for Design Space Exploration of Digital Systems Design|Modern digital systems design requires us to explore a large and complex design space to find a best configuration which satisfies design requirements. Such exploration requires a sound representation of design space from which design candidates are efficiently generated, each of which then is evaluated. This paper proposes a plan-generation-evaluation framework which supports a complete process of such design space exploration. The plan phase constitutes a design space of all possible design alternatives by means of a formally defined representation scheme of attributed AND-OR graph. The generation phase generates a set of candidates by algorithmic pruning of the design space in an attributed AND-OR graph with respect to design requirements as well as architectural constraints. Finally, the evaluation phase measures performance of design candidates in a pruned graph to select a best one. A complete process of cache design is exemplified to show the effectiveness of the proposed framework.|Jun Kyoung Kim,Tag Gon Kim","43164|IEICE Transations|2006|Ternary Sequence Set Having Periodic and Aperiodic Zero-Correlation Zone|A new class of ternary sequence with a zero-correlation zone is introduced. The proposed sequence sets have a zero-correlation zone for both periodic and aperiodic correlation functions. The proposed sequences can be constructed from a pair of Hadamard matrices of size n  n and a Hadamard matrix of size n  n. The constructed sequence set consists of nn ternary sequences, and the length of each sequence is n(m+)(n + ) for a non-negative integer m. The zero-correlation zone of the proposed sequences is  nm-, where  is the phase shift. The sequence member size of the proposed sequence set is equal to nn+ times that of the theoretical upper bound of the member size of a sequence set with a zero-correlation zone.|Takafumi Hayashi","43352|IEICE Transations|2006|Automated Design of Analog Circuits Starting with Idealized Elements|This paper presents an automated design of analog circuits starting with idealized elements. Our system first synthesizes circuits using idealized elements by a genetic algorithm (GA). GA evolves circuit topologies and transconductances of idealized elements to achieve the given specifications. The use of idealized elements effectively reduces search space and make the synthesis efficient. Second, idealized elements in a generated circuit are replaced by MOSFETs. Through the two processes, a circuit satisfying the given specifications can be obtained. The capability of this method was demonstrated through experiments of synthesis of a trans-impedance amplifier and a cubing circuit and benchmark tests. The results of the benchmark tests show the proposed CAD is more than  times faster than the CAD which does not use idealized elements.|Naoyuki Unno,Nobuo Fujii","43187|IEICE Transations|2006|Hardware Design Verification Using Signal Transitions and Transactions|Hardware prototyping has been widely used for ASICSoC verification. This paper proposes a new hardware design verification method, Transition and Transaction Tracer (TTT), which probes and records the signals of interest for a long time, hours, days, or even weeks, without a break. It compresses the captured data in real time and stores it in a state transition format in memory. Since it records all the transitions, it is effective in finding and fixing errors, even ones that occur rarely or intermittently. It can also be programmed to generate a trigger for a logic analyzer when it detects certain transitions. This is useful for debugging situations where the engineer has trouble finding an appropriate trigger condition to pinpoint the source of errors. We have been using the method in hardware prototyping for ASICSoC development for two years and found it useful for system level tests, and in particular for long running tests.|Nobuyuki Ohba,Kohji Takano","43738|IEICE Transations|2006|Redundant Design for Wallace Multiplier|To increase the yield of data processing circuits such as adders and logic operation circuits, the bit-slice reconfiguration design has been proposed as an efficient redundant technology for defect-tolerance. Wallace multipliers are a well-known class of high-speed parallel multipliers. Unfortunately, the bit-slice reconfiguration design is not applicable to Wallace multipliers because Wallace multipliers do not have regular bit-slice structure. Therefore, redundant designs for Wallace multipliers have been regarded impossible. This paper proposes a redundant design for Wallace multipliers. In order to design redundant Wallace multipliers, first, n heterogeneous slices are considered in a non-redundant n  n Wallace multiplier. The proposed redundant Wallace multipliers contain reconfigurable slices which can play the role of both i-th and (i+)-th slices. Since the i-th slice has a similar structure to the (i+)-th slice, the reconfigurable slice is not much larger than the i-th slice. This paper also shows a repair procedure for the proposed design. This paper evaluates the proposed design from the viewpoint of the yield, area, effective yield and delay time. For example, the yield of a    Wallace multiplier increases from . to . by applying the proposed design while the area increases by a factor of ..|Kazuteru Namba,Hideo Ito","43414|IEICE Transations|2006|Quantum-Dot Cellular Automata Design Guideline|Quantum-dot Cellular Automata (QCA) is attracting a lot of attentions due to its extremely small feature sizes and ultra low power consumption. Up to now several designs using QCA technology have been proposed. However, we found not all of the designs function properly. Further, no general design guidelines have been proposed so far. A straightforward extension of a simple functional design pattern may fail. This makes designing a large scale circuits using QCA technology an extremely time-consuming process. In this paper we show several critical vulnerabilities in the structures of primitive QCA gates and QCA interconnects, and propose a disciplinary guideline to prevent any additional plausible but malfunctioning QCA designs.|Kyosun Kim,Kaijie Wu,Ramesh Karri","57653|GECCO|2006|Evolutionary design of pseudorandom sequence generators based on cellular automata and its applicability in current cryptosystems|In this work, a genetic algorithm is used to find cellular automata rules that make cellular automata behave like good pseudorandom sequence generators. Pseudorandom sequence generators based on one-dimensional cellular automata with non-homogeneous rules and arbitrary neighbors are proposed. The fitness function combines entropy measures and standard statistical tests for random sequences. The generators found are statistically compared to some well-known pseudorandom sequences generators.|David Delgado,David Vidal,German Hernandez","43089|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Constructed from an M-Sequence|The present paper introduces an improved construction of a class of binary sequences having a zero-correlation zone (hereafter binary ZCZ sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCZ sequence set from an arbitrary M-sequence. The previously reported sequence construction of binary ZCZ sequence sets from an M-sequence can generate a single series of binary ZCZ sequence sets from an M-sequence. The present paper proposes an improved sequence construction that can generate more than one series of binary ZCZ sequence sets from an M-sequence.|Takafumi Hayashi","43133|IEICE Transations|2006|The AMS Extension to System Level Design Language - SpecC|Recently, system level design languages (SLDLs), which can describe both hardware and software aspects of the design, are receiving attentions. Analog mixed-signal (AMS) extensions to SLDLs enable current discrete-oriented SLDLs to describe and simulate not only digital systems but also digital-analog mixed-signal systems. In this paper, we present our work on the AMS extension to one of the system level design language---SpecC. The extended language supports designer to describe all the analog, digital and software aspects in a universal language.|Yu Liu,Satoshi Komatsu,Masahiro Fujita"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57647|GECCO|2006|Comparison of genetic representation schemes for scheduling soft real-time parallel applications|This paper presents a hybrid technique that combines List Scheduling (LS) with Genetic Algorithms (GA) for constructing non-preemptive schedules for soft real-time parallel applications represented as directed acyclic graphs (DAGs). The execution time requirements of the applications' tasks are assumed to be stochastic and are represented as probability distribution functions. The performance in terms of schedule lengths for three different genetic representation schemes are evaluated and compared for a number of different DAGs.The approaches presented here produce shorter schedules than HLFET, a popular LS approach for all of the sample problems. Of the three genetic representation schemes investigated, PosCT, the technique that allows the GA to learn which tasks to delay in order to allow other tasks to complete produced the shortest schedules for a majority of the sample DAGs.|Yoginder S. Dandass,Amit C. Bugde","43649|IEICE Transations|2006|Estimating Motion Parameters Using a Flexible Weight Function|In this paper, we propose a method to estimate affine motion parameters from consecutive images with the assumption that the motion in progress can be characterized by an affine model. The motion may be caused either by a moving camera or moving object. The proposed method first extracts motion vectors from a sequence of images and then processes them by adaptive robust estimation to obtain affine parameters. Typically, a robust estimation filters out outliers (velocity vectors that do not fit into the model) by fitting velocity vectors to a predefined model. To filter out potential outliers, our adaptive robust estimation defines a flexible weight function based on a sigmoid function. During the estimation process, we tune the sigmoid function gradually to its hard-limit as the errors between the input data and the estimation model are decreased, so that we can effectively separate non-outliers from outliers with the help of the finally tuned hard-limit form of the weight function. The experimental results show that the suggested approach is very effective in estimating affine parameters.|Seok-Woo Jang,Gye-Young Kim,Hyung-Il Choi","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","43679|IEICE Transations|2006|A Flexible Connection Model for Software Components|A component connection enables a component to use the functionality of other components directly, without generating adapters or other mechanisms at run-time. In conventional component connection models, the connection between components, particularly third-party components, is very costly for code reuse because the component source code must be modified if the types of requester-side and provider-side are different. This paper proposes a new component model, built upon an existing component architecture, which abandons a component service type and connects components based on a method type collection of the provider and requester components. Our model enables flexible connections owing to relaxed component matching, in which the system that implements our model automatically converts values of parameters, return values, and exceptions between required methods and provided ones within a well-defined range. As a result of experimental evaluations, it is found that our model is superior to conventional models in terms of the component-use cost and the capability of changing connections.|Hironori Washizaki,Daiki Hoshi,Yoshiaki Fukazawa","57764|GECCO|2006|A hybrid genetic search for multiple sequence alignment|This paper proposes a hybrid genetic algorithm for multiple sequence alignment. The algorithm evolves guide sequences and aligns input sequences based on the guide sequences. It also embeds a local search heuristic to search the problem space effectively. In the experiments for various data sets, the proposed algorithm showed the performance comparable to existing algorithms.|Seung-Hyun Moon,Sung-Soon Choi,Byung Ro Moon","80603|VLDB|2006|A Middleware for Fast and Flexible Sensor Network Deployment|A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from httpglobalsn.sourceforge.net.|Karl Aberer,Manfred Hauswirth,Ali Salehi","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius"],["65866|AAAI|2006|A Fast Decision Tree Learning Algorithm|There is growing interest in scaling up the widely-used decision-tree learning algorithms to very large data sets. Although numerous diverse techniques have been proposed, a fast tree-growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential. In this paper, we present a novel, fast decision-tree learning algorithm that is based on a conditional independence assumption. The new algorithm has a time complexity of O(m  n), where m is the size of the training data and n is the number of attributes. This is a significant asymptotic improvement over the time complexity O(m  n) of the standard decision-tree learning algorithm C., with an additional space increase of only O(n). Experiments show that our algorithm performs competitively with C. in accuracy on a large number of UCI benchmark data sets, and performs even better and significantly faster than C. on a large number of text classification data sets. The time complexity of our algorithm is as low as naive Bayes'. Indeed, it is as fast as naive Bayes but outperforms naive Bayes in accuracy according to our experiments. Our algorithm is a core tree-growing algorithm that can be combined with other scaling-up techniques to achieve further speedup.|Jiang Su,Harry Zhang","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","80664|VLDB|2006|A Linear Time Algorithm for Optimal Tree Sibling Partitioning and Approximation Algorithms in Natix|Document insertion into a native XML Data Store (XDS) requires to partition the document tree into a number of storage units with limited capacity, such as records on disk pages. As intra partition navigation is much faster than navigation between partitions, minimizing the number of partitions has a beneficial effect on query performance.We present a linear time algorithm to optimally partition an ordered, labeled, weighted tree such that each partition does not exceed a fixed weight limit. Whereas traditionally tree partitioning algorithms only allow child nodes to share a partition with their parent node (i.e. a partition corresponds to a subtree), our algorithm also considers partitions containing several subtrees as long as their roots are adjacent siblings. We call this sibling partitioning.Based on our study of the optimal algorithm, we further introduce two novel, near-optimal heuristics. They are easier to implement, do not need to hold the whole document instance in memory, and require much less runtime than the optimal algorithm.Finally, we provide an experimental study comparing our novel and existing algorithms. One important finding is that compared to partitioning that exclusively considers parent-child partitions, including sibling partitioning as well can decrease the total number of partitions by more than %, and improve query performance by more than a factor of two.|Carl-Christian Kanne,Guido Moerkotte","57609|GECCO|2006|An agent-based algorithm for generalized graph colorings|This paper presents an algorithm for solving a number of generalized graph coloring problems. Specifically, it gives an agent-based algorithm for the Bandwidth Coloring problem. Using a standard method for preprocessing the input, the same algorithm can also be used to solve the Multicoloring and Bandwidth Multicoloring problems. In the algorithm a number of agents, called ants, each of which colors a portion of the graph, collaborate to obtain a coloring of the entire graph. This coloring is then further improved by a local optimization algorithm. Experimental results on a set of benchmark graphs for these generalized coloring problems show that this algorithm performs very well compared to other heuristic approaches.|Thang Nguyen Bui,ThanhVu H. Nguyen","43592|IEICE Transations|2006|Node-Disjoint Paths Algorithm in a Transposition Graph|In this paper, we give an algorithm for the node-to-set disjoint paths problem in a transposition graph. The algorithm is of polynomial order of n for an n-transposition graph. It is based on recursion and divided into two cases according to the distribution of destination nodes. The maximum length of each path and the time complexity of the algorithm are estimated theoretically to be O(n) and n - , respectively, and the average performance is evaluated based on computer experiments.|Yasuto Suzuki,Keiichi Kaneko,Mario Nakamori","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","43435|IEICE Transations|2006|A Polynomial Time Algorithm for Obtaining a Minimum Vertex Ranking Spanning Tree in Outerplanar Graphs|The minimum vertex ranking spanning tree problem is to find a spanning tree of G whose vertex ranking is minimum. This problem is NP-hard and no polynomial time algorithm for solving it is known for non-trivial classes of graphs other than the class of interval graphs. This paper proposes a polynomial time algorithm for solving the minimum vertex ranking spanning tree problem on outerplanar graphs.|Shin-ichi Nakayama,Shigeru Masuyama","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","65748|AAAI|2006|A Polynomial-Time Algorithm for Action Graph Games|Action-Graph Games (AGGs) (Bhat & Leyton-Brown ) are a fully expressive game representation which can compactly express strict and context-specific independence and anonymity structure in players' utility functions. We present an efficient algorithm for computing expected payoffs under mixed strategy profiles. This algorithm runs in time polynomial in the size of the AGG representation (which is itself polynomial in the number of players when the in-degree of the action graph is bounded). We also present an extension to the AGG representation which allows us to compactly represent a wider variety of structured utility functions.|Albert Xin Jiang,Kevin Leyton-Brown","57675|GECCO|2006|A tree-based genetic algorithm for building rectilinear Steiner arborescences|A rectilinear Steiner arborescence (RSA) is a tree, whose nodes include a prescribed set of points, termed the vertices, in the first quadrant of the Cartesian plane, and whose tree edges from parent to child nodes must head either straight to the right or straight above. A minimal RSA (a MRSA) is one for which the total path length of the edges in the tree is minimal. RSAs have application in VLSI design. Curiously, although a RSA is a tree, to our knowledge, previous genetic attacks on the MRSA problem have not used tree-based approaches to representation, nor to the operations of crossover and mutation. We show why some care is needed in the choice of such genetic operators. Then we present tree-based operators for crossover and mutation, which are successful in creating true RSAs from source RSAs without the need of repair steps. We compare our results to two earlier researches, and find that our approach gives good results, but not results that are consistently better than those earlier approaches.|William A. Greene"],["43186|IEICE Transations|2006|A High-Accuracy Passive D Measurement System Using Phase-Based Image Matching|This paper presents a high-accuracy D (three-dimen-sional) measurement system using multi-camera passive stereo vision to reconstruct D surfaces of free form objects. The proposed system is based on an efficient stereo correspondence technique, which consists of (i) coarse-to-fine correspondence search, and (ii) outlier detection and correction, both employing phase-based image matching. The proposed sub-pixel correspondence search technique contributes to dense reconstruction of arbitrary-shaped D surfaces with high accuracy. The outlier detection and correction technique contributes to high reliability of reconstructed D points. Through a set of experiments, we show that the proposed system measures D surfaces of objects with sub-mm accuracy. Also, we demonstrate high-quality dense D reconstruction of a human face as a typical example of free form objects. The result suggests a potential possibility of our approach to be used in many computer vision applications.|Mohammad Abdul Muquit,Takuma Shibahara,Takafumi Aoki","43452|IEICE Transations|2006|A Simple Method for Detecting Tumor in T-Weighted MRI Brain Images An Image-Based Analysis|The objective of this paper is to present a decision support system which uses a computer-based procedure to detect tumor blocks or lesions in digitized medical images. The authors developed a simple method with a low computation effort to detect tumors on T-weighted Magnetic Resonance Imaging (MRI) brain images, focusing on the connection between the spatial pixel value and tumor properties from four different perspectives ) cases having minuscule differences between two images using a fixed block-based method, ) tumor shape and size using the edge and binary images, ) tumor properties based on texture values using spatial pixel intensity distribution controlled by a global discriminate value, and ) the occurrence of content-specific tumor pixel for threshold images. Measurements of the following medical datasets were performed ) different time interval images, and ) different brain disease images on single and multiple slice images. Experimental results have revealed that our proposed technique incurred an overall error smaller than those in other proposed methods. In particular, the proposed method allowed decrements of false alarm and missed alarm errors, which demonstrate the effectiveness of our proposed technique. In this paper, we also present a prototype system, known as PCB, to evaluate the performance of the proposed methods by actual experiments, comparing the detection accuracy and system performance.|Phooi-Yee Lau,Shinji Ozawa","43139|IEICE Transations|2006|QoS Estimation Method for JPEG  Coded Image at RTP Layer|In this paper, we propose a novel QoS (Quality of Service) estimation scheme for JPEG  coded image at RTP (realtime transfer protocol) layer without decoding the image. QoS of streaming video is estimated in view of several points, such as, transmission delay, or quality of received images. In this paper, we evaluate the QoS in terms of quality of received images. Generally, RTP is carried on top of UDP, and hence, quality of transmitted images could be degraded due to packet loss. To estimate the quality of received JPEG  coded image without decoding, we use RTP header extension in order to send additional information to the receiver. The effectiveness of the proposed method is confirmed by the computer simulations.|Kiyoshi Nishikawa,Shinichi Nagawara,Hitoshi Kiya","43048|IEICE Transations|2006|Evaluation of Image Corrected by Retinex Method Based on S-CIELAB and Gazing Information|The purpose of this research is to propose an effective color metric which can predict the perceptual image quality for Retinex method. In this paper, we first give a brief introduction of three kinds of typical single Retinex methods to improve the color reproduction. And then, we state the process for obtaining the observer rating value from the subjective evaluation experiment performed under the sRGB illumination condition. Next, we introduce the S-CIELAB metric and propose a new metric on the basis of S-CIELAB metric that considers the gazing information. The average S-CIELAB color differences with and without the consideration of gazing information were calculated as the objective image quality measures. The correlations between the observer rating values and the objective image quality measures were calculated. The result shows that all of the average S-CIELAB color differences based on the gazing information are better correlated to the observer rating value than the average S-CIELAB color difference over the whole area. The average S-CIELAB color difference weighted by the gazing frequency over the gazing area shows the strong correlation with the observer rating value.|Jie Bai,Toshiya Nakaguchi,Norimichi Tsumura,Yoichi Miyake","43375|IEICE Transations|2006|Connectivity-Based Image Watermarking|A novel robust watermarking scheme based on image connectivity is proposed. Having obtained the connected objects according to the selected connectivity pattern, the gravity centers are calculated in several larger objects as the reference points for watermark embedding. Based on these reference points and the center of the whole image, several sectors are formed, and the same version watermarks are embedded into these sectors. Thanks to the very stable gravity center of the connected objects, watermark detection is synchronized successfully. Simulation results show that our scheme can survive under both local and global geometrical distortions.|Jian Luo,Hongxia Wang","43417|IEICE Transations|2006|Nonlinear Blind Source Separation Method for X-Ray Image Separation|In this study, we propose a robust approach for blind source separation (BSS) by using radial basis function networks (RBFNs) and higher-order statistics (HOS). The RBFN is employed to estimate the inverse of a hypothetical complicated mixing procedure. It transforms the observed signals into high-dimensional space, in which one can simply separate the transformed signals by using a cost function. Recently, Tan et al. proposed a nonlinear BSS method, in which higher-order moments between source signals and observations are matched in the cost function. However, it has a strict restriction that it requires the higher-order statistics of sources to be known. We propose a cost function that consists of higher-order cumulants and the second-order moment of signals to remove the constraint. The proposed approach has the capacity of not only recovering the complicated mixed signals, but also reducing noise from observed signals. Simulation results demonstrate the validity of the proposed approach. Moreover, a result of application to X-ray image separation also shows its practical applicability.|Nuo Zhang,Jianming Lu,Takashi Yahagi","65903|AAAI|2006|Explanation-Based Learning for Image Understanding|Existing prior domain knowledge represents a valuable source of information for image interpretation problems such as classifying handwritten characters. Such domain knowledge must be translated into a form understandable by the learner. Translation can be realized with Explanation-Based Learning (EBL) which provides a kind of dynamic inductive bias, combining domain knowledge and training examples. The dynamic bias formed by the interaction of domain knowledge with training examples can yield solution knowledge of potential higher quality than can be anticipated by the static bias designer without seeing training examples. We detail how EBL can be used to dynamically integrate domain knowledge, training examples, and the learning mechanism, and describe the two EBL approaches in (Sun & DeJong a) and (Sun & DeJong b).|Qiang Sun,Li-Lun Wang,Gerald DeJong","43427|IEICE Transations|2006|Image Processing Based on Percolation Model|This paper proposes a novel image processing method based on a percolation model. The percolation model is used to represent the natural phenomenon of the permeation of liquid. The percolation takes into account the connectivity among the neighborhoods. In the proposed method, a cluster formation by the percolation process is performed first. Then, feature extraction from the cluster is carried out. Therefore, this method is a type of scalable window processing for realizing a robust and flexible feature extraction. The effectiveness of proposed method was verified by experiments on crack detection, noise reduction, and edge detection.|Tomoyuki Yamaguchi,Shuji Hashimoto","43664|IEICE Transations|2006|A Block Smoothing-Based Method for Flicker Removal in Image Sequences|An automatic and efficient algorithm for removal of intensity flicker is proposed. The novel repair process is founded on the block-based estimation and restoration algorithm with regard to luminance variation. It is easily realized and controlled to remove most intensity flicker and preserve the wanted effects, like fade in and fade out.|Lei Zhou,Qiang Ni,Yuanhua Zhou","43472|IEICE Transations|2006|Image Authentication Based on Modular Embedding|We consider a watermark application to assist in the integrity maintenance and verification of the associated images. There is a great benefit in using WM in the context of authentication since it does not require any additional storage space for supplementary metadata, in contrast with cryptographic signatures, for instance. However there is a fundamental problem in the case of exact authentication How to embed a signature into a cover message in such a way that it would be possible to restore the watermarked cover image into its original state without any error There are different approaches to solve this problem. We use the watermarking method consisting of modulo addition of a mark and investigate it in detail. Our contribution lies in investigating different modified techniques of both watermark embedding and detection in order to provide the best reliability of watermark authentication. The simulation results for different types of embedders and detectors in combination with the pictures of watermarked images are given.|Moon Ho Lee,Valery I. Korzhik,Guillermo Morales-Luna,Sergei Lusse,Evgeny Kurbatov"],["43623|IEICE Transations|2006|Improvement of Paging Extensions in Mobile Internet Protocol Based on Post Registration|Paging extensions for Mobile Internet Protocol (P-MIP) decreases only the number of registration, but it does not much improve the method of registration, which still gives rise to a lot of lost packets and long handoff latency, and may also waste the data buffering and time during registration. In the active state, P-MIP behaves in the same way as Mobile Internet Protocol (MIP), thus, in this state, the packet loss rate of P-MIP is the same as that of MIP. However, the packet loss rate of P-MIP is lower than that of MIP, when changing from idle state to active state, because P-MIP buffers packets at the registered FA. We propose an improvement method for the registration delay, while the mobile node is entering the active state to decrease the mobile node waiting time for data packets. The proposed method can reduce the requirement of data buffering and also improve the method of registration to decrease lost packets and handoff latency when the mobile node moves across the cell in the same paging area during active state.|Kortong Chiratana,Watit Benjapolakul","43145|IEICE Transations|2006|An Energy Efficient Leader Election Protocol for Radio Network with a Single Transceiver|In this work we present an energy efficient leader election protocol for anonymous radio network populated with n mobile stations. Previously, Nakano and Olariu have presented a leader election protocol that terminates, with probability exceeding  - f (f  ), in log logn + o(log log n)+ O(log f) time slots . As the above protocol works under the assumption that every station has the ability to transmit and monitor the channel at the same time, it requires every station to be equipped with two transceivers. This assumption, however, is unrealistic for most mobile stations due to constraints in cost, size, and energy dissipation. Our main contribution is to show that it is possible to elect a leader in an anonymous radio network where each station is equipped with a single transceiver. Quite surprisingly, although every station has only one transceiver, our leader election protocol still runs, with probability exceeding  - f(f  ), in log log n + o(log logn)+ O(log f) time slots. Moreover, our leader election protocol needs only expected O(n) total awake time slots, while Nakano and Olariu's protocol needs expected O(nlog logn) total awake time slots. Since every leader election protocol needs at least (n) awake time slots, our leader election protocol is optimal in terms of the expected awake time slots.|Jacir Luiz Bordim,Yasuaki Ito,Koji Nakano","43369|IEICE Transations|2006|Fingerprinting Protocol Based on Distributed Providers Using Oblivious Transfer|For the construction of a large fingerprinting system, conventional protocols need many computations to provide each fingerprinted contents to each user. In order to reduce the computational cost, we introduce a new concept of distributed providers in the fingerprinting protocol. Before a sale, our practical fingerprinting protocol using a concept of secure oblivious transfer is performed between a contents supplier and each provider. Then each provider obtains fingerprinted contents such that each bit of fingerprinting information is embedded in each segment of the contents. When a user orders some contents to the supplier, each segment of the contents is distributed from each provider specified by the supplier. The selection of providers who distribute the segments of contents is executed based on the user's identity so that the sequence of embedded bits in the collected segments may indicate the user's identity.|Urara Shinmyo,Minoru Kuribayashi,Masakatu Morii,Hatsukazu Tanaka","43131|IEICE Transations|2006|Investigation of Ultra-Wideband Propagation Channel based on a Cluster Scheme|In this paper, an Ultra-Wideband (UWB) double-directional channel sounding measurement and spatio-temporal analysis of UWB propagation based on the clusterization approach were reported. After separating the propagation paths and diffuse components both on the transmitter (Tx) antenna and receiver (Rx) antenna positions, the propagation paths both on Tx and Rx positions were observed for clusters separately, while coupling the clusters between Tx and Rx position based on similar time of arrivals, and ray tracing by utilizing high temporal and spatial resolution, respectively. The relation between direction of departure and direction of arrival will then be investigated. For cluster properties, parameters of model characteristics are discussed and compared to other earlier works.|Hiroaki Tsuchiya,Katsuyuki Haneda,Jun-ichi Takada","43330|IEICE Transations|2006|LLR-Based Decode-and-Forward Protocol for Relay Networks and Closed-Form BER Expressions|Decode-and-forward cooperative communications protocol (DFP) allows single-antenna users in wireless medium to obtain the powerful benefits of multi-antenna systems without physical antenna arrays. So far, only signal-to-noise ratio (SNR) or square amplitude of path gain has been used to evaluate the reliability of received signals for relays to decide whether to forward the decoded data so as to prevent unsuccessful detection at the relays. In this paper, we propose using log-likelihood ratio (LLR) as an alternative to SNR in the conventional DFP. Closed-form BER expressions for different versions of DFP are also derived and verified by Monte-Carlo simulations. A variety of numerical results reveal the significant superiority of LLR-based DFP to SNR-based DFP regardless of threshold level and relay position under flat Rayleigh fading channel plus additive white Gaussian noise (AWGN).|Ho Van Khuong,Hyung Yun Kong","43602|IEICE Transations|2006|A Multicast Based Anonymous Information Sharing Protocol for Peer-to-Peer Systems|A fundamental problem in a pure Peer-to-Peer (PP) file sharing system is how to protect the anonymity of peer nodes when providing efficient data access services. Most of existing work mainly focus on how to provide the initiator anonymity, but neglect the anonymity of the responder. In this paper, we propose a multicast-based protocol, called Mapper, for efficient file sharing with mutual anonymity. By seamlessly combining the technologies of multi-proxy and IP multicast together, the proposed protocol guarantees mutual anonymity during the entire session of file retrieval. Furthermore, Mapper replicates requested files inside the multicast group, so file distribution can be adjusted adaptively and the cost for multicast can be further reduced. Results of both simulations and theoretical analyses demonstrate that Mapper possesses the merits of scalability, reliability, and high adaptability.|Baoliu Ye,Minyi Guo,Jingyang Zhou,Daoxu Chen","43136|IEICE Transations|2006|An Energy Efficient Ranking Protocol for Radio Networks|A radio network (RN for short) is a distributed system with no central arbiter, consisting of n radio transceivers, henceforth referred to as stations. We assume that the stations run on batteries and expends power while broadcastingreceiving a data packet. Thus, the most important measure to evaluate protocols on the radio network is the number of awake time slots, in which a station is broadcastingreceiving a data packet. We also assume that the stations are identical and have no unique ID number, and no station knows the number n of the stations. For given n keys one for each station, the ranking problem asks each station to determine the number of keys in the RN smaller than its own key. The main contribution of this paper is to present an optimal randomized ranking protocol on the k-channel RN. Our protocol solves the ranking problem, with high probability, in O(nk + log n) time slots with every station being awake for at most O(log n) time slots. We also prove that any randomized ranking protocol is required to run in expected (nk + log n) time slots with at least one station being awake for expected (log n) time slots. Therefore, our ranking protocol is optimal.|Koji Nakano","43069|IEICE Transations|2006|A Key Management Scheme for Secure Mobile IP Registration Based on AAA Protocol|We introduce a new hierarchical key management scheme which can be applied for secure Mobile IP registration protocol. Contrary to the previous schemes, AAA protocol used for registration key distribution is separated from the base registration protocol, so that the registration key distribution can be simplified and the delay caused by the AAA protocol can be avoided. Also proposed is the non-repudiation service based on a hash chain, which is useful for secure auditing.|Hyun-Sun Kang,Chang-Seop Park","43652|IEICE Transations|2006|Branch Aggregation Multicast BAM An Energy Efficient and Highly Compatible Multicast Protocol for Wireless Sensor Networks|In this paper, we propose a multicast protocol, called BAM (Branch Aggregation Multicast), for wireless sensor networks. The main contribution of BAM is a reduction in the radio communication load, which is a key determinant of sensor energy consumption. BAM does not use any control packets such as joinleave messages and does not manage multicast groups. BAM is highly compatible with existing wireless sensor protocols, such as routing protocols, MAC protocols, and other kinds of energy efficient protocols. BAM implementation is quite simple and BAM works on various networks even if some sensors are not BAM-capable. BAM is composed of two aggregation techniques. One is single hop aggregation (S-BAM) and the other is multiple paths aggregation (M-BAM). S-BAM aggregates radio transmission within a single hop and enables single transmission to multiple intended receivers. M-BAM aggregates multiple paths into fewer ones and limits the range of radio transmission. S-BAM is designed to reduce redundant communication at every branch while M-BAM is designed to reduce the number of branches. SM-BAM, the combination of S-BAM and M-BAM, can reduce the radio communication load thus enabling energy efficient multicast communication. We evaluate BAM in three ways, qualitative evaluation by theoretical analysis, quantitative evaluation through computer simulations, and experiments using Cross-Bow's MICA. Our results show that BAM is a very energy efficient multicast protocol that well supports wireless sensor networks.|Akihito Okura,Takeshi Ihara,Akira Miura","43666|IEICE Transations|2006|A Localization Scheme for Sensor Networks Based on Wireless Communication with Anchor Groups|In this paper, we propose a new localization scheme for wireless sensor networks consisting of a huge number of sensor nodes equipped with simple wireless communication devices such as wireless LAN and Bluetooth. The proposed scheme is based on the Point-In-Triangle (PIT) test proposed by He et al. The scheme is actually implemented by using Bluetooth devices of Class  standard, and the performance of the scheme is evaluated in an actual environment. The result of experiments indicates that the proposed scheme could realize a localization with an error of less than  m.|Hiroyuki Ochi,Shigeaki Tagashira,Satoshi Fujita"],["43057|IEICE Transations|2006|Dynamical Behavior of Neural Networks with Anti-Symmetrical Cyclic Connections|We show that a unit-group, which represents a group of contiguous units with the same sign of output, is a dominant component for the dynamical behavior of a neural network with anti-symmetrical cyclic connections for the nearest neighbor connections and global connections. In transient state, it is shown that the unit-group has the dynamics such that the amount n of units which belong to the unit-group increases with time, and that the increasing rate of n decreases with increasing n. The dynamics cause the large difference of the number of limit-cycles between discrete and continuous time models. Additionally, the period of the limit-cycle depends on the size of the unit-groups. This dependency is obtained from computer simulations and two approximation methods. These approximations provide the lower and the upper bounds of the periods which depend on the gain of an activation function. Using these approximations, we also obtain detailed relations between a period and the other network parameters analytically.|Shinya Suenaga,Yoshihiro Hayakawa,Koji Nakajima","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65895|AAAI|2006|Real-Time Evolution of Neural Networks in the NERO Video Game|A major goal for AI is to allow users to interact with agents that learn in real time, making new kinds of interactive simulations, training applications, and digital entertainment possible. This paper describes such a learning technology, called real-time NeuroEvolution of Augmenting Topologies (rtNEAT), and describes how rtNEAT was used to build the NeuroEvolving Robotic Operatives (NERO) video game. This game represents a new genre of machine learning games where the player trains agents in real time to perform challenging tasks in a virtual environment. Providing laymen the capability to effectively train agents in real time with no prior knowledge of AI or machine learning has broad implications, both in promoting the field of AI and making its achievements accessible to the public at large.|Kenneth O. Stanley,Bobby D. Bryant,Igor Karpov,Risto Miikkulainen","65779|AAAI|2006|Object-Sorting-by-Color in a Variety of Lighting Conditions Using Neural Networks and Lego Mindstorms Robot|Recognizing object color in a variety of lighting conditions is a challenging area of pattern-recognition. Neural networks have been found to be a good solution for that problem, and they are also quick and accurate, and can be used in real-time. We use a LEGO Mindstorms  robot to sort objects based on color in a variety of lighting conditions. We will start from simpler objects (LEGO pieces) and move onto more complex objects (apples, oranges, etc). This project is in progress and we hope to achieve classification accuracies of at least %.|Natasa Lazetic,Jianna Zhang","43674|IEICE Transations|2006|Exploiting Intelligence in Fighting Action Games Using Neural Networks|This paper proposes novel methods to provide intelligence for characters in fighting action games by using neural networks. First, how a character learns basic game rules and matches against randomly acting opponents is considered. Since each action takes more than one time unit in general fighting action games, the results of a character's action are exposed not immediately but several time units later. We evaluate the fitness of a decision by using the relative score change caused by the decision. Whenever the scores of fighting characters are changed, the decision causing the score change is identified, and then the neural network is trained by using the score difference and the previous input and output values which induced the decision. Second, how to cope more properly with opponents that act with predefined action patterns is addressed. The opponents' past actions are utilized to find out the optimal counter-actions for the patterns. Lastly, a method in order to learn moving actions is proposed. To evaluate the performance of the proposed algorithm, we implement a simple fighting action game. Then the proposed intelligent character (IC) fights with the opponent characters (OCs) which act randomly or with predefined action patterns. The results show that the IC understands the game rules and finds out the optimal counter-actions for the opponents' action patterns by itself.|Byeong Heon Cho,Sung Hoon Jung,Yeong Rak Seong,Ha Ryoung Oh","57805|GECCO|2006|Modular thinking evolving modular neural networks for visual guidance of agents|This paper investigates whether replacing non-modular artificial neural network brains of visual agents with modular brains improves their ability to solve difficult tasks, specifically, survive in a changing environment. A set of experiments was conducted and confirmed that agents with modular brains are in fact better. Further analysis of the evolved modules characterised their function and determined their mechanism of operation. The results indicate that the greater survival ability is obtained due to functional specialisation of the evolved modules good agents do well because their modules are more specialised at tasks such as reproduction and consumption. Overall, the more specialised the modules, the fitter the agents.|Ehud Schlessinger,Peter J. Bentley,R. Beau Lotto","57804|GECCO|2006|A dynamic approach to artificial immune systems utilizing neural networks|The purpose of this work is to propose an immune-inspired setup to use a self-organizing map as a computational model for the interaction of antigens and antibodies. The proposed approach may be used as a part in other immune algorithms, or can possibly be used to detect anomalies in time series data.|Stefan Schadwinkel,Werner Dilger","57760|GECCO|2006|Coevolution of neural networks using a layered pareto archive|The Layered Pareto Coevolution Archive (LAPCA) was recently proposed as an effective Coevolutionary Memory (CM) which, under certain assumptions, approximates monotonic progress in coevolution. In this paper, a technique is developed that interfaces the LAPCA algorithm with NeuroEvolution of Augmenting Topologies (NEAT), a method to evolve neural networks with demonstrated efficiency in game playing domains. In addition, the behavior of LAPCA is analyzed for the first time in a complex game-playing domain evolving neural network controllers for the game Pong. The technique is shown to keep the total number of evaluations in the order of those required by NEAT, making it applicable to complex domains. Pong players evolved with a LAPCA and with the Hall of Fame (HOF) perform equally well, but the LAPCA is shown to require significantly less space than the HOF. Therefore, combining NEAT and LAPCA is found to be an effective approach to coevolution.|German A. Monroy,Kenneth O. Stanley,Risto Miikkulainen","43395|IEICE Transations|2006|A Method of Simple Adaptive Control for Nonlinear Systems Using Neural Networks|This paper presents a method of simple adaptive control (SAC) using neural networks for a class of nonlinear systems with bounded-input bounded-output (BIBO) and bounded nonlinearity. The control input is given by the sum of the output of the simple adaptive controller and the output of the neural network. The neural network is used to compensate for the nonlinearity of the plant dynamics that is not taken into consideration in the usual SAC. The role of the neural network is to construct a linearized model by minimizing the output error caused by nonlinearities in the control systems. Furthermore, convergence and stability analysis of the proposed method is performed. Finally, the effectiveness of the proposed method is confirmed through computer simulation.|Muhammad Yasser,Agus Trisanto,Jianming Lu,Takashi Yahagi","57732|GECCO|2006|Facilitating neural dynamics for delay compensation and prediction in evolutionary neural networks|Delay in the nervous system is a serious issue for an organism that needs to act in real time. For example, during the time a signal travels from a peripheral sensor to the central nervous system, a moving object in the environment can cover a significant distance which can lead to critical errors in the effect of the corresponding motor output. This paper proposes that facilitating synapses which show a dynamic sensitivity to the changing input may play an important role in compensating for neural delays, through extrapolation. The idea was tested in a modified D pole-balancing problem which included sensory delays. Within this domain, we tested the behavior of recurrent neural networks with facilitatory neural dynamics trained via neuroevolution. Analysis of the performance and the evolved network parameters showed that, under various forms of delay, networks utilizing extrapolatory dynamics are at a significant competitive advantage compared to networks without such dynamics. In sum, facilitatory (or extrapolatory) dynamics can be used to compensate for delay at a single-neuron level, thus allowing a developing nervous system to stay in touch with the present environmental state.|Heejin Lim,Yoonsuck Choe"],["43727|IEICE Transations|2006|Genetic Algorithm Based Optimization of Partly-Hidden Markov Model Structure Using Discriminative Criterion|A discriminative modeling is applied to optimize the structure of a Partly-Hidden Markov Model (PHMM). PHMM was proposed in our previous work to deal with the complicated temporal changes of acoustic features. It can represent observation dependent behaviors in both observations and state transitions. In the formulation of the previous PHMM, we used a common structure for all models. However, it is expected that the optimal structure which gives the best performance differs from category to category. In this paper, we designed a new structure optimization method in which the dependence of the states and the observations of PHMM are optimally defined according to each model using the weighted likelihood-ratio maximization (WLRM) criterion. The WLRM criterion gives high discriminability between the correct category and the incorrect categories. Therefore it gives model structures with good discriminative performance. We define the model structure combination which satisfy the WLRM criterion for any possible structure combinations as the optimal structures. A genetic algorithm is also applied to the adequate approximation of a full search. With results of continuous lecture talk speech recognition, the effectiveness of the proposed structure optimization is shown it reduced the word errors compared to HMM and PHMM with a common structure for all models.|Tetsuji Ogawa,Tetsunori Kobayashi","43676|IEICE Transations|2006|Improving Disk IO Load Prediction Using Statistical Parameter History in Online for Grid Computing|Resource performance prediction is known to be useful in resource scheduling in the Grid. The disk IO workload is another important factor that influences the performance of the CPU and the network which are commonly used in resource scheduling. In the case of disk IO workload time-series, the adaptation of a prediction algorithm to new time-series should be rapid. Further, the prediction should ensure that the prediction error is minimum in the heterogeneous environment. The storage workload (i.e., the disk IO load) is a dynamic variable. A prediction parameter based on the characteristics of the current workload must be prepared for prediction purposes. In this paper, we propose and implement the OPHB (On-Line Parameter History Bank). This is a method that stabilizes the incoming disk IO workload time-series fairly quickly with the help of accurately determined ESM (Exponential Smoothing Method) parameters. The parameters are drawn from a history database. In the case of forecasting with ESM, a smoothing parameter must be specified in advance. If the parameter is statically estimated from observed data found in previous executions, the forecasts would be inaccurate because they do not capture the actual IO behavior. The smoothing parameter has to be adjusted in accordance with the shape of the new disk IO workload. The ESM algorithms utilise one of the accumulated parameter histories chronicled by OPHB's Deposit operation. When a new time-series is started, an appropriate parameter value is looked up in the Bank by OPHB's Lookup operation. This is used for the time-series. This process is fully adaptive. We evaluate the proposed method with SES (Single Exponential Smoothing) and ARRSES (Auto-Responsive SES) methods.|DongWoo Lee,Rudrapatna S. Ramakrishna","57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","43717|IEICE Transations|2006|DRBAC Model Using a WSNM for Services in -Home|RBAC (Role Based Access Control) was added the concept of the role which user can have access to resources based on the role of the user, and it increased efficiency and expandability. But, evolution of computing power and internet technology has caused the up rise of the dynamic environments, in accordance with it, it will be expected to require a dynamic access control model considering various elements. In this paper, we propose DRBAC (Dynamic RBAC) model in intelligent Home (i-Home). This is an access control model suitable for user-oriented service in i-Home. In order to consider dynamic environment in the existing RBAC models, the proposed model executes assignments user-role and permission-role based on context. In addition, the proposed model provides scalable access control policies which are suitable for the characteristics of intelligent environment as considering the user location information as a temporary constraints condition. Furthermore, we design and implement WSNM (Wireless Sensor Network Module) for its services. Finally, the proposed model provides flexible and efficient authentication method which applied Domain-Group concept as well as user  device authentication.|Jong Hyuk Park,Sangjin Lee,In-Hwa Hong","43350|IEICE Transations|2006|On a Blind Speech Dereverberation Algorithm Using Multi-Channel Linear Prediction|It is well known that speech captured in a room by distant microphones suffers from distortions caused by reverberation. These distortions may seriously damage both speech characteristics and intelligibility, and consequently be harmful to many speech applications. To solve this problem, we proposed a dereverberation algorithm based on multi-channel linear prediction. The method is as follows. First we calculate prediction filters that cancel out the room reverberation but also degrade speech characteristics by causing excessive whitening of the speech. Then, we evaluate the prediction-filter degradation to compensate for the excessive whitening. As the reverberation lengthens, the compensation performance becomes worse due to computational accuracy problems. In this paper, we propose a new computation that may improve compensation accuracy when dealing with long reverberation.|Marc Delcroix,Takafumi Hikichi,Masato Miyoshi","57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","43299|IEICE Transations|2006|Model Predictive Control for Linear Parameter Varying Systems Using a New Parameter Dependent Terminal Weighting Matrix|In this paper, we propose a new robust model predictive control (MPC) technique for linear parameter varying (LPV) systems expressed as linear systems with feedback parameters. It is based on the minimization of the upper bound of finite horizon cost function using a new parameter dependent terminal weighting matrix. The proposed parameter dependent terminal weighting matrix for norm-bounded uncertain models provides a less conservative condition for terminal inequality. The optimization problem that satisfies the terminal inequality is solved by semi-definite programming involving linear matrix inequalities (LMIs). A numerical example is included to illustrate the effectiveness of the proposed method.|Sangmoon Lee,Sangchul Won","65716|AAAI|2006|Table Extraction Using Spatial Reasoning on the CSS Visual Box Model|Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS visual box model, which shows a high level of robustness even without any form of learning (F-measure  %). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism.|Wolfgang Gatterbauer,Paul Bohunsky","43558|IEICE Transations|2006|Acoustic Model Adaptation Using First-Order Linear Prediction for Reverberant Speech|This paper describes a hands-free speech recognition technique based on acoustic model adaptation to reverberant speech. In hands-free speech recognition, the recognition accuracy is degraded by reverberation, since each segment of speech is affected by the reflection energy of the preceding segment. To compensate for the reflection signal we introduce a frame-by-frame adaptation method adding the reflection signal to the means of the acoustic model. The reflection signal is approximated by a first-order linear prediction from the observation signal at the preceding frame, and the linear prediction coefficient is estimated with a maximum likelihood method by using the EM algorithm, which maximizes the likelihood of the adaptation data. Its effectiveness is confirmed by word recognition experiments on reverberant speech.|Tetsuya Takiguchi,Masafumi Nishimura,Yasuo Ariki","43701|IEICE Transations|2006|Robust Active Shape Model Using AdaBoosted Histogram Classifiers and Shape Parameter Optimization|Active Shape Model (ASM) has been shown to be a powerful tool to aid the interpretation of images, especially in face alignment. ASM local appearance model parameter estimation is based on the assumption that residuals between model fit and data have a Gaussian distribution. Moreover, to generate an allowable face shape, ASM truncates coefficients of shape principal components into the bounds determined by eigenvalues. In this paper, an algorithm of modeling local appearances, called AdaBoosted ASM, and a shape parameter optimization method are proposed. In the algorithm of modeling the local appearances, we describe our novel modeling method by using AdaBoosted histogram classifiers, in which the assumption of the Gaussian distribution is not necessary. In the shape parameter optimization, we describe that there is an inadequacy on controlling shape parameters in ASM, and our novel method on how to solve it. Experimental results demonstrate that the AdaBoosted histogram classifiers improve robustness of landmark displacement greatly, and the shape parameter optimization solves the inadequacy problem of ASM on shape constraint effectively.|Yuanzhong Li,Wataru Ito"]]}}