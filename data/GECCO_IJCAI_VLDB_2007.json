{"abstract":{"entropy":6.724424632624526,"topics":["data, real world, word sense, mining web, natural language, spatial reasoning, recent years, data management, data integration, large data, web page, recent interest, social network, data mining, recent research, data web, data important, sql query, analysis critical, search web","markov decision, decision processes, description logic, genetic programming, play role, markov processes, hierarchical bayesian, distribution algorithm, estimation distribution, gradient xcs, genetic crossover, mutation crossover, logic programming, algorithm crossover, differential evolution, genetic gas, genetic algorithm, modal logic, practical reasoning, estimation algorithm","machine learning, novel approach, learning, reinforcement learning, systems, learning classifier, present approach, learning data, support vector, information extraction, artificial immune, learning task, statistical relational, dimensionality reduction, present systems, present novel, immune systems, association rules, multi-agent systems, mobile robot","evolutionary algorithm, optimization problem, particle swarm, genetic algorithm, problem, optimization algorithm, present algorithm, algorithm, algorithm problem, solving problem, search heuristics, genetic programming, use algorithm, search algorithm, constraint satisfaction, search space, artificial intelligence, particle optimization, evolutionary computation, swarm optimization","real world, autonomous agents, management information, agents, allocation agents, management systems, information, agents resources, multiple agents, distributed agents, support, multiple, distributed, increasing, representation, concurrent, fundamental, interesting, decentralized, providing","data, data management, large data, data integration, problem data, management integration, integration problem, stream data, large sets, schema data, data sets, queries data, skyline data, data similar, processing data, data items, requirements data, given data, planning, given","distribution algorithm, estimation algorithm, estimation distribution, autonomous agents, algorithm model, practical reasoning, problem model, reasoning important, model, design model, reasoning, important, design, behavior, development, mechanism, present, probabilistic, limited, formalism","description logic, logic programming, modal logic, logic, logic knowledge, knowledge, consider, problem, belief, common, designed, represent, concept, need, tracking, sequence, complex, area, target, handle","statistical relational, methods learning, mobile robot, learning neural, describe systems, learning learn, neural network, systems aims, improve performance, learning performance, network learning, systems structure, describe use, performance, structure, describe, methods, use, human, software","information extraction, systems, artificial immune, dimensionality reduction, immune systems, multi-agent systems, association rules, artificial systems, systems trust, systems information, systems problem, key, recognition, patterns, dna, advances, first, determine, detection, large","genetic problem, genetic programming, challenging problem, study problem, solution problem, programming problem, selection algorithm, paper based, paper problem, study, paper, solution, framework, paper genetic, paper framework, selection, robust, market, extended, consists","optimization problem, solving problem, problem, constraint satisfaction, constraint problem, solve problem, present trading, scheduling problem, satisfiability problem, satisfaction problem, framework constraint, commonly used, problem network, approach problem, constraint optimization, applications problem, constraint network, present problem, effective problem, problem find"],"ranking":[["16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80762|VLDB|2007|On Dominating Your Neighborhood Profitably|Recent research on skyline queries has attracted much interest in the database and data mining community. Given a database, an object belongs to the skyline if it cannot be dominated with respect to the given attributes by any other database object. Current methods have only considered so-called minmax attributes like price and quality which a user wants to minimize or maximize. However, objects can also have spatial attributes like x, y coordinates which can be used to represent relevant constraints on the query results. In this paper, we introduce novel skyline query types taking into account not only minmax attributes but also spatial attributes and the relationships between these different attribute types. Such queries support a micro-economic approach to decision making, considering not only the quality but also the cost of solutions. We investigate two alternative approaches for efficient query processing, a symmetrical one based on off-the-shelf index structures, and an asymmetrical one based on index structures with special purpose extensions. Our experimental evaluation using a real dataset and various synthetic datasets demonstrates that the new query types are indeed meaningful and the proposed algorithms are efficient and scalable.|Cuiping Li,Anthony K. H. Tung,Wen Jin,Martin Ester","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","16411|IJCAI|2007|Directed Graph Embedding|In this paper, we propose the Directed Graph Embedding (DGE) method that embeds vertices on a directed graph into a vector space by considering the link structure of graphs. The basic idea is to preserve the locality property of vertices on a directed graph in the embedded space. We use the transition probability together with the stationary distribution of Markov random walks to measure such locality property. It turns out that by exploring the directed links of the graph using random walks, we can get an optimal embedding on the vector space that preserves the local affinity which is inherent in the directed graph. Experiments on both synthetic data and real-world Web page data are considered. The application of our method to Web page classification problems gets a significant improvement comparing with state-of-art methods.|Mo Chen,Qiong Yang,Xiaoou Tang","16483|IJCAI|2007|Learning Semantic Descriptions of Web Information Sources|The Internet is full of information sources providing various types of data from weather forecasts to travel deals. These sources can be accessed via web-forms, Web Services or RSS feeds. In order to make automated use of these sources, one needs to first model them semantically. Writing semantic descriptions for web sources is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions for web sources, in which we actively invoke sources and compare the data they produce with that of known sources of information. We perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. The paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.|Mark James Carman,Craig A. Knoblock","80829|VLDB|2007|EntityRank Searching Entities Directly and Holistically|As the Web has evolved into a data-rich repository, with the standard \"page view,\" current search engines are becoming increasingly inadequate for a wide range of query tasks. While we often search for various data \"entities\" (e.g., phone number, paper PDF, date), today's engines only take us indirectly to pages. While entities appear in many pages, current engines only find each page individually. Toward searching directly and holistically for finding information of finer granularity, we study the problem of entity search, a significant departure from traditional document retrieval. We focus on the core challenge of ranking entities, by distilling its underlying conceptual model Impression Model and developing a probabilistic ranking framework, EntityRank, that is able to seamlessly integrate both local and global information in ranking. We evaluate our online prototype over a TB Web corpus, and show that EntityRank performs effectively.|Tao Cheng,Xifeng Yan,Kevin Chen-Chuan Chang"],["58030|GECCO|2007|Addressing sampling errors and diversity loss in UMDA|Estimation of distribution algorithms replace the typical crossover and mutation operators by constructing a probabilistic model and generating offspring according to this model. In previous studies, it has been shown that this generally leads to diversity loss due to sampling errors. In this paper, for the case of the simple Univariate Marginal Distribution Algorithm (UMDA), we propose and test several methods for counteracting diversity loss. The diversity loss can come in two phases sampling from the probability model (offspring generation) and selection. We show that it is possible to completely remove the sampling error during offspring generation. Furthermore, we examine several plausible model construction variants which counteract diversity loss during selection and demonstrate that these update rules work better than the standard update on a variety of simple test problems.|Jürgen Branke,Clemens Lode,Jonathan L. Shapiro","57935|GECCO|2007|On the constructiveness of context-aware crossover|Crossover in Genetic Programming is mostly a destructive operator, generally producing children worse than the parents and occasionally producing those who are better. A recently introduced operator, Context-Aware Crossover, which implicitly discovers the best possible crossover site for a subtree has been shown to consistently attain higher fitnesses while processing fewer individuals.It has been observed that context-aware crossover is similar to Brood Crossover in that multiple children are produced during each crossover event. This paper performs a thorough analysis of these crossover operators and compares the performance of the two and demonstrates that, although they do work similarly, context-aware crossover performs a far better sampling of the search space and thus performs much better.We also demonstrate that context-aware crossover benefits from a speed up of almost an order of magnitude when using a simple and very small cache, which is over two orders of magnitute smaller than caches typically used.|Hammad Majeed,Conor Ryan","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","16363|IJCAI|2007|Adaptive Genetic Algorithm with Mutation and Crossover Matrices|A matrix formulation for an adaptive genetic algorithm is developed using mutation matrix and crossover matrix. Selection, mutation, and crossover are all parameter-free in the sense that the problem at a particular stage of evolution will choose the parameters automatically. This time dependent selection process was first developed in MOGA (mutation only genetic algorithm) Szeto and Zhang,  and now is extended to include crossover. The remaining parameters needed are population size and chromosome length. The adaptive behavior is based on locus statistics and fitness ranking of chromosomes. In crossover, two methods are introduced Long Hamming Distance Crossover (LHDC) and Short Hamming Distance Crossover (SHDC). LHDC emphasizes exploration of solution space. SHDC emphasizes exploitation of local search process. The one-dimensional random coupling Ising Spin Glass problem, which is similar to a knapsack problem, is used as a benchmark test for the comparison of various realizations of the adaptive genetic algorithms. Our results show that LHDC is better than SHDC, but both are superior to MOGA, which has been shown to be better than many traditional methods.|Nga Lam Law,Kwok Yip Szeto","57960|GECCO|2007|A new crossover technique for Cartesian genetic programming|Genetic Programming was first introduced by Koza using tree representation together with a crossover technique in which random sub-branches of the parents' trees are swapped to create the offspring. Later Miller and Thomson introduced Cartesian Genetic Programming, which uses directed graphs as a representation to replace the tree structures originally introduced by Koza. Cartesian Genetic Programming has been shown to perform better than the traditional Genetic Programming but it does not use crossover to create offspring, it is implemented using mutation only. In this paper a new crossover method in Genetic Programming is introduced. The new technique is based on an adaptation of the Cartesian Genetic Programming representation and is tested on two simple regression problems. It is shown that by implementing the new crossover technique, convergence is faster than that of using mutation only in the Cartesian Genetic Programming method.|Janet Clegg,James Alfred Walker,Julian Francis Miller","57885|GECCO|2007|Empirical analysis of ideal recombination on random decomposable problems|This paper analyzes the behavior of a selectorecombinative genetic algorithm (GA) with an ideal crossover on a class of random additively decomposable problems (rADPs). Specifically, additively decomposable problems of order k whose subsolution fitnesses are sampled from the standard uniform distribution U, are analyzed. The scalability of the selectorecombinative GA is investigated for , rADP instances. The validity of facetwise models in bounding the population size, run duration, and the number of function evaluations required to successfully solve the problems is also verified. Finally, rADP instances that are easiest and most difficult are also investigated.|Kumara Sastry,Martin Pelikan,David E. Goldberg","58067|GECCO|2007|Quality time tradeoff operator for designing efficient multi level genetic algorithms|We present a novel cost benefit operator that assists multi levelgenetic algorithm searches. Through the use of the cost benefitoperator, it is possible to dynamically constrain the search of thebase level genetic algorithms, to suit the users requirements. We note that the current literature has abundant studies on metaevolutionary GAs, however these approaches have not identifiedan efficient approach to the termination of base GA searchs or ameans to balance practical consideration such as quality ofsolution and the expense of computation. Our Quality timetradeoff operator (QTT) is user defined, and acts as a base leveltermination operator and also provides a fitness value for themeta-level GA. In this manner, the amount of computation timespent on less encouraging configurations can be specified by theuser. Our approach was applied to a computationally intensive test problem which evaluates a large set of configuration settings forthe base GAs to find suitable configuration settings (populationsize, crossover operator and rate, mutation operator and rate,repair or penalty and the use of adaptive mutation rates) forselected TSP problems.|George G. Mitchell,Barry McMullin,James Decraene,Ciaran Kelly","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang","16429|IJCAI|2007|First Order Decision Diagrams for Relational MDPs|Markov decision processes capture sequential decision making under uncertainty, where an agent must choose actions so as to optimize long term reward. The paper studies efficient reasoning mechanisms for Relational Markov Decision Processes (RMDP) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them. Two contributions are presented. First, the paper develops First Order Decision Diagrams (FODD), a new compact representation for functions over relational structures, together with a set of operators to combine FODDs, and novel reduction techniques to keep the representation small. Second, the paper shows how FODDs can be used to develop solutions for RMDPs, where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size (number of objects) or instantiation. In particular, a variant of the value iteration algorithm is developed by using special operations over FODDs, and the algorithm is shown to converge to the optimal policy.|Chenggang Wang,Saket Joshi,Roni Khardon","57969|GECCO|2007|Dependency trees permutations and quadratic assignment problem|This paper describes and analyzes an estimation of distribution algorithm based on dependency tree models (dtEDA), which can explicitly encode probabilistic models for permutations. dtEDA is tested on deceptive ordering problems and a number of instances of the quadratic assignment problem. The performance of dtEDA is compared to that of the standard genetic algorithm with the partially matched crossover (PMX) and the linear order crossover (LOX). In the quadratic assignment problem, the robust tabu search is also included in the comparison.|Martin Pelikan,Shigeyoshi Tsutsui,Rajiv Kalapala"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","57880|GECCO|2007|Do not match inherit fitness surrogates for genetics-based machine learning techniques|A byproduct benefit of using probabilistic model-building genetic algorithms is the creation of cheap and accurate surrogate models. Learning classifier systems -- and genetics-based machine learning in general -- can greatly benefit from such surrogates which may replace the costly matching procedure of a rule against large data sets. In this paper we investigate the accuracy of such surrogate fitness functions when coupled with the probabilistic models evolved by the $chi$-ary extended compact classifier system ($chi$eCCS). To achieve such a goal, we show the need that the probabilistic models should be able to represent all the accurate basis functions required for creating an accurate surrogate. We also introduce a procedure to transform populations of rules based into dependency structure matrices (DSMs) which allows building accurate models of overlapping building blocks -- a necessary condition to accurately estimate the fitness of the evolved rules.|Xavier Llorà,Kumara Sastry,Tian-Li Yu,David E. Goldberg","16538|IJCAI|2007|Change of Representation for Statistical Relational Learning|Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.|Jesse Davis,Irene M. Ong,Jan Struyf,Elizabeth S. Burnside,David Page,Vítor Santos Costa","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","16435|IJCAI|2007|Predicting and Preventing Coordination Problems in Cooperative Q-learning Systems|We present a conceptual framework for creating Q-learning-based algorithms that converge to optimal equilibria in cooperative multiagent settings. This framework includes a set of conditions that are sufficient to guarantee optimal system performance. We demonstrate the efficacy of the framework by using it to analyze several well-known multi-agent learning algorithms and conclude by employing it as a design tool to construct a simple, novel multi-agent learning algorithm.|Nancy Fulda,Dan Ventura","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","16377|IJCAI|2007|Improving Embeddings by Flexible Exploitation of Side Information|Dimensionality reduction is a much-studied task in machine learning in which high-dimensional data is mapped, possibly via a non-linear transformation, onto a low-dimensional manifold. The resulting embeddings, however, may fail to capture features of interest. One solution is to learn a distance metric which prefers embeddings that capture the salient features. We propose a novel approach to learning a metric from side information to guide the embedding process. Our approach admits the use of two kinds of side information. The first kind is class-equivalence information, where some limited number of pairwise \"samedifferent class\" statements are known. The second form of side information is a limited set of distances between pairs of points in the target metric space. We demonstrate the effectiveness of the method by producing embeddings that capture features of interest.|Ali Ghodsi,Dana F. Wilkinson,Finnegan Southey","16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli","57905|GECCO|2007|Automated alphabet reduction method with evolutionary algorithms for protein structure prediction|This paper focuses on automated procedures to reduce the dimensionality ofprotein structure prediction datasets by simplifying the way in which the primary sequence of a protein is represented. The potential benefits ofthis procedure are faster and easier learning process as well as the generationof more compact and human-readable classifiers.The dimensionality reduction procedure we propose consists on the reductionof the -letter amino acid (AA) alphabet, which is normally used to specify a protein sequence, into a lower cardinality alphabet. This reduction comes about by a clustering of AA types accordingly to their physical and chemical similarity. Our automated reduction procedure is guided by a fitness function based on the Mutual Information between the AA-based input attributes of the dataset and the protein structure featurethat being predicted. To search for the optimal reduction, the Extended Compact Genetic Algorithm (ECGA) was used, and afterwards the results of this process were fed into (and validated by) BioHEL, a genetics-based machine learningtechnique. BioHEL used the reduced alphabet to induce rules forprotein structure prediction features. BioHEL results are compared to two standard machine learning systems. Our results show that it is possible to reduce the size of the alphabet used for prediction fromtwenty to just three letters resulting in more compact, i.e. interpretable,rules. Also, a protein-wise accuracy performance measure suggests that the loss of accuracy acrued by this substantial alphabet reduction is not statistically significant when compared to the full alphabet.|Jaume Bacardit,Michael Stout,Jonathan D. Hirst,Kumara Sastry,Xavier Llorà,Natalio Krasnogor","16674|IJCAI|2007|Effective Control Knowledge Transfer through Learning Skill and Representation Hierarchies|Learning capabilities of computer systems still lag far behind biological systems. One of the reasons can be seen in the inefficient re-use of control knowledge acquired over the lifetime of the artificial learning system. To address this deficiency, this paper presents a learning architecture which transfers control knowledge in the form of behavioral skills and corresponding representation concepts from one task to subsequent learning tasks. The presented system uses this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy. Experimental results show that the presented method can significantly outperform learning on a flat state space representation and the MAXQ method for hierarchical reinforcement learning.|Mehran Asadi,Manfred Huber"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Suárez,Manuel Valenzuela-Rendón,Hugo Terashima-Marín,Eduardo Uresti-Charre","58190|GECCO|2007|Multi-objective hybrid PSO using -fuzzy dominance|This paper describes a PSO-Nelder Mead Simplex hybrid multi-objective optimization algorithm based on a numerical metric called  -fuzzy dominance. Within each iteration of this approach, in addition to the position and velocity update of each particle using PSO, the k-means algorithm is applied to divide the population into smaller sized clusters. The Nelder-Mead simplex algorithm is used separately within each cluster for added local search. The proposed algorithm is shown to perform better than MOPSO on several test problems as well as for the optimization of a genetic model for flowering time control in Arabidopsis. Adding the local search achieves faster convergence, an important feature in computationally intensive optimization of gene networks.|Praveen Koduru,Sanjoy Das,Stephen Welch","58068|GECCO|2007|Solving the artificial ant on the Santa Fe trail problem in   fitness evaluations|In this paper, we provide an algorithm that systematically considers all small trees in the search space of genetic programming. These small trees are used to generate useful subroutines for genetic programming. This algorithm is tested on the Artificial Ant on the Santa Fe Trail problem, a venerable problem for genetic programming systems. When four levels of iteration are used, the algorithm presented here generates better results than any known published result by a factor of .|Steffen Christensen,Franz Oppacher","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","58240|GECCO|2007|Symbiotic tabu search|Recombination in the Genetic Algorithm (GA) is supposed to extract the component characteristics from two parents and reassemble them in different combinations hopefully producing an offspring that has the good characteristics of both parents. Symbiotic Combination is formerly introduced as an alternative for sexual recombination operator to overcome the need of explicit design of recombination operators in GA all. This paper presents an optimization algorithm based on using this operator in Tabu Search. The algorithm is benchmarked on two problem sets and is compared with standard genetic algorithm and symbiotic evolutionary adaptation model, showing success rates higher than both cited algorithms.|Ramin Halavati,Saeed Bagheri Shouraki,Bahareh Jafari Jashmi,Mojdeh Jalali Heravi","58019|GECCO|2007|A particle swarm algorithm for symbols detection in wideband spatial multiplexing systems|This paper explores the application of the particle swarm algorithm for a NP-hard problem in the area of wireless communications. The specific problem is of detecting symbols in a Multi-Input Multi-Output (MIMO) communications system. This approach is particularly attractive as PSO is well suited for physically realizable, real-time applications, where low complexity and fast convergence is of absolute importance. While an optimal Maximum Likelihood (ML) detection using an exhaustive search method is prohibitively complex, we show that the Swarm Intelligence (SI) optimized MIMO detection algorithm gives near-optimal Bit Error Rate (BER) performance in fewer iterations, thereby reducing the ML computational complexity significantly. The simulation results suggest that the proposed detector gives an acceptable performance complexity trade-off in comparison with ML and VBLAST detector.|Adnan Ahmed Khan,Muhammad Naeem,Syed Ismail Shah","57975|GECCO|2007|Scalability of particle swarm algorithms|When dealing with complex optimisations problems, evolutionary computation techniques have proven tobe very helpful. Amongst optimisation algorithms driven by evolutionary computation techniques,particle swarm algorithms have proven to be a very good alternative to genetic algorithms because of their faster convergence. However they can still suffer from premature convergence to local optima. Premature convergence occurs when the particles of the swarm are too close to each other to enable further exploration of the space. To put it another way, the dispersion or distribution of the swarm throughout the search space has been localised to a small region with a consequent stagnation of the search process. Many strategies have been used to try to prevent convergence to a local optimum. However little work has been done on problems of high dimensions. By high dimensions, we mean dimensions of  and above.This paper focuses, therefore, on the limitations of classical particle swarm algorithms when dealing with high dimensional problems. We compare different versions of Particle Swarm Algorithms GBest, LBest with ringor random neighbourhood of size citeClerc, and GCPSOciteVanDenBergh. Those algorithmswere run twice, with a linearly decreasing inertia weight, and with the use of a constriction factor.We also used two repulsive-based algorithms Charged PSOciteBlackwell and Predator PreyciteSilva.Our main focus is problems of high dimensionality. In particular, we applied the different algorithmsto the benchmarks functions Ackley and Rosenbrock, in the following dimensions , , .Even though these represent problems of relatively small dimensionality in a real-world context,experiments on higher dimensions have not been necessary to show the limits of the algorithms studied.Each experiment was run  times. The swarm size was chosen from $    $ and so that it isnot greater than the problem size. Each experiment is  iterations long.A one-way analysis of variance (ANOVA) was used to compare the performance of each algorithms. We found that the LBest algorithms perform significantly better when used with the constriction factor.GBest and GCPSO perform better with linearly decreasing inertia with a small swarm size, but better with the constriction factorwith a big swarm size. The improvement of GCPSO on GBest is not statistically significant in our experiments. The LBest algorithms with the constriction factor seem to be the best algorithms to handle problems of high dimensionality. The LBest algorithm with fixed neighbourhood seems to be less sensitive to the size of the swarm than the LBest algorithm with randomneighbourhood. Especially, in the case of the Rosenbrock function of size , increasing the sizeof the swarm does not improve the performance of LBest with constricted factor and fixed neighbourhood.The algorithms based on repulsion between particles, i.e. Charged Swarm and Predator Prey, do not perform very well.Indeed, even if the predator prey algorithm gives quite good results, it is trapped in a local optimum, as the fitness value stagnates on a constant value for the last % of iterations. This may come from a too low levelof repulsion. Tuning the parameters used for repulsion seems to be very important for high dimensionality problems. Experiments show that almost all the algorithms managed to solve the problems for dimension  butnone of the algorithms managed to solve the problem in the case of problems of dimension  and . The LBest algorithm with random neighbourhood and constriction factor performed the best. Further work will be done on modelling the size of the swarm required to be able to solve the problems. Other particle swarm algorithms will also be included.|Sébastien Piccand,Michael O'Neill,Jacqueline Walker","58228|GECCO|2007|Adjacency list matchings an ideal genotype for cycle covers|We propose and analyze a novel genotype to represent walk and cycle covers in graphs, namely matchings in the adjacency lists. This representation admits the natural mutation operator of adding a random match and possibly also matching the former partners. To demonstrate the strength of this set-up, we use it to build a simple (+) evolutionary algorithm for the problem of finding an Eulerian cycle in a graph. We analyze several natural variants that stem from different ways to randomly choose the new match. Among other insight, we exhibit a (+) evolutionary algorithm that computes an Euler tour in a graph with $m$ edges in expected optimization time (m log m). This significantly improves the previous best evolutionary solution having expected optimization time (m log m) in the worst-case, but also compares nicely with the runtime of an optimal classical algorithm which is of order (m). A simple coupon collector argument indicates that our optimization time is asymptotically optimal for any randomized search heuristic.|Benjamin Doerr,Daniel Johannsen"],["16534|IJCAI|2007|An Efficient Protocol for Negotiation over Multiple Indivisible Resources|We study the problem of autonomous agents negotiating the allocation of multiple indivisible resources. It is difficult to reach optimal outcomes in bilateral or multi-lateral negotiations over multiple resources when the agents' preferences for the resources are not common knowledge. Self-interested agents often end up negotiating inefficient agreements in such situations. We present a protocol for negotiation over multiple indivisible resources which can be used by rational agents to reach efficient outcomes. Our proposed protocol enables the negotiating agents to identify efficient solutions using systematic distributed search that visits only a subspace of the whole solution space.|Sabyasachi Saha,Sandip Sen","16608|IJCAI|2007|Gossip-Based Aggregation of Trust in Decentralized Reputation Systems|Decentralized Reputation Systems have recently emerged as a prominent method of establishing trust among self-interested agents in online environments. A key issue is the efficient aggregation of data in the system several approaches have been proposed, but they are plagued by major shortcomings. We put forward a novel, decentralized data management scheme grounded in gossip-based algorithms. Rumor mongering is known to possess algorithmic advantages, and indeed, our framework inherits many of their salient features scalability, robustness, globality, and simplicity. We also demonstrate that our scheme motivates agents to maintain a sparkling clean reputation, and is inherently impervious to certain kinds of attacks.|Ariel D. Procaccia,Yoram Bachrach,Jeffrey S. Rosenschein","16505|IJCAI|2007|Market Based Resource Allocation with Incomplete Information|Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).|Bo An,Chunyan Miao,Zhiqi Shen","16637|IJCAI|2007|An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource Allocation Mechanism|Multiagent distributed resource allocation requires that agents act on limited, localized information with minimum communication overhead in order to optimize the distribution of available resources. When requirements and constraints are dynamic, learning agents may be needed to allow for adaptation. One way of accomplishing learning is to observe past outcomes, using such information to improve future decisions. When limits in agents' memory or observation capabilities are assumed, one must decide on how large should the observation window be. We investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems, namely dispersion games. We show by numerical experiments over a specific dispersion game (the Minority Game) that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. We then provide an information-theoretic explanation for the observed behaviors, showing that a downward causation effect takes place.|Ricardo M. Araujo,Luís C. Lamb","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","16799|IJCAI|2007|Control of Agent Swarms Using Generalized Centroidal Cyclic Pursuit Laws|One of the major tasks in swarm intelligence is to design decentralized but homogenoeus strategies to enable controlling the behaviour of swarms of agents. It has been shown in the literature that the point of convergence and motion of a swarm of autonomous mobile agents can be controlled by using cyclic pursuit laws. In cyclic pursuit, there exists a predefined cyclic connection between agents and each agent pursues the next agent in the cycle. In this paper we generalize this idea to a case where an agent pursues a point which is the weighted average of the positions of the remaining agents. This point correspond to a particular pursuit sequence. Using this concept of centroidal cyclic pursuit, the behavior of the agents is analyzed such that, by suitably selecting the agents' gain, the rendezvous point of the agents can be controlled, directed linear motion of the agents can be achieved, and the trajectories of the agents can be changed by switching between the pursuit sequences keeping some of the behaviors of the agents invariant. Simulation experiments are given to support the analytical proofs.|Arpita Sinha,Debasish Ghose","58209|GECCO|2007|Evolving distributed agents for managing air traffic|Air traffic management offers an intriguing real world challenge to designing large scale distributed systems using evolutionary computation. The ability to evolve effective air traffic flow strategies depends not only on evolving good local strategies, but also on ensuring that those local strategies result in good global solutions. While traditional, direct evolutionary strategies can be highly effective in certain combinatorial domains, they are not well-suited to complex air traffic flow problems because of the large interdependencies among the local subsystems. In this paper, we propose an evolutionary agent-based solution to the air traffic flow problem. In this approach, we evolve agents both to learn the right local flow strategies to alleviate congestion in their immediate surroundings, and to prevent the creation of congestion \"downstream\" from their local areas. The agent-based approach leads to better and more fault-tolerant solutions. To validate this approach, we use FACET, an air traffic simulator developed at NASA and used extensively by the FAA and industry. On a scenario composed of three hundred aircraft and two points of congestion, our results show that an agent based evolutionary computation method, where each agent uses the system evaluation function, achieves % improvement over a direct evolutionary algorithm. In addition by creating agent-specific \"difference evaluation functions\" we achieve an additional % improvement over agents using the system evaluation.|Adrian K. Agogino,Kagan Tumer","16375|IJCAI|2007|Reaching Envy-Free States in Distributed Negotiation Settings|Mechanisms for dividing a set of goods amongst a number of autonomous agents need to balance efficiency and fairness requirements. A common interpretation of fairness is envy-freeness, while efficiency is usually understood as yielding maximal overall utility. We show how to set up a distributed negotiation framework that will allow a group of agents to reach an allocation of goods that is both efficient and envy-free.|Yann Chevaleyre,Ulle Endriss,Sylvia Estivie,Nicolas Maudet","16781|IJCAI|2007|Providing a Recommended Trading Agent to a Population A Novel Approach|This paper presents a novel approach for providing automated trading agents to a population, focusing on bilateral negotiation with unenforceable agreements. A new type of agents, called semicooperative (SC) agents is proposed for this environment. When these agents negotiate with each other they reach a pareto-optimal solution that is mutually beneficial. Through extensive experiments we demonstrate the superiority of providing such agents for humans over supplying equilibrium agents or letting people design their own agents. These results are based on our observation that most people do not modify SC agents even though they are not in equilibrium. Our findings introduce a new factor -human response to provided agents - that should be taken into consideration when developing agents that are provided to a population.|Efrat Manisterski,Ron Katz,Sarit Kraus","58069|GECCO|2007|SwarmArchitect a swarm framework for collaborative construction|Computer game development has become increasingly popular in the field of autonomous systems. One of the main topics studies the building of various architectures in computer games. A realistic human-like architecture is expected in a thematic computer game, since it strongly motivates the game players in an intuitive way. However, the task of building a human-like architecture is non-trivial since the construction is a real time process without human supervision. In this paper, we present a collective building algorithm inspired by social insects for intelligent construction based on multiple agents. A swarm of virtual agents indirectly design edifications, which resemble basic features in human-like architecture by using a stigmergic mechanism along with branching rules. The main idea of the algorithm is to map sensory information to appropriate building actions.|Yifeng Zeng,Jorge Cordero Hernandez,Dennis Plougman Buus"],["80816|VLDB|2007|Probabilistic Skylines on Uncertain Data|Uncertain data are inherent in some important applications. Although a considerable amount of research has been dedicated to modeling uncertain data and answering some types of queries on uncertain data, how to conduct advanced analysis on uncertain data remains an open problem at large. In this paper, we tackle the problem of skyline analysis on uncertain data. We propose a novel probabilistic skyline model where an uncertain object may take a probability to be in the skyline, and a p-skyline contains all the objects whose skyline probabilities are at least p. Computing probabilistic skylines on large uncertain data sets is challenging. We develop two efficient algorithms. The bottom-up algorithm computes the skyline probabilities of some selected instances of uncertain objects, and uses those instances to prune other instances and uncertain objects effectively. The top-down algorithm recursively partitions the instances of uncertain objects into subsets, and prunes subsets and objects aggressively. Our experimental results on both the real NBA player data set and the benchmark synthetic data sets show that probabilistic skylines are interesting and useful, and our two algorithms are efficient on large data sets, and complementary to each other in performance.|Jian Pei,Bin Jiang,Xuemin Lin,Yidong Yuan","80814|VLDB|2007|Semi-Automatic Schema Integration in Clio|Schema integration is the problem of finding a unified representation, called the integrated schema, from a set of source schemas that are related to each other. The relationships between the source schemas can be represented via correspondences between schema elements or via some other forms of schema mappings such as constraints or views. The integrated schema can be viewed as a means for dealing with the heterogeneity in the source schemas, by providing a standard representation of the data. Schema integration has received much of attention in the research literature , , , ,  and still remains a challenge in practice. Existing approaches require substantial amount of human feedback during the integration process and moreover, the outcome of these approaches is a single integrated schema. In general, however, there can be multiple possible schemas that integrate data in different ways and each may be valuable in a given scenario.|Laura Chiticariu,Mauricio A. Hernández,Phokion G. Kolaitis,Lucian Popa","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80809|VLDB|2007|A Bayesian Method for Guessing the Extreme Values in a Data Set|For a large number of data management problems, it would be very useful to be able to obtain a few samples from a data set, and to use the samples to guess the largest (or smallest) value in the entire data set. Minmax online aggregation, top-k query processing, outlier detection, and distance join are just a few possible applications. This paper details a statistically rigorous, Bayesian approach to attacking this problem. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.|Mingxi Wu,Chris Jermaine","80795|VLDB|2007|HiSbase Histogram-based PP Main Memory Data Management|Many e-science communities, e. g., medicine, climatology, and astrophysics, are overwhelmed by the exponentially growing data volumes that need to be accessible by collaborating researchers. Nowadays, new scientific results are often obtained by exploring and cross-correlating data from different distributed sources . However, neither centralized data processing by shipping the data to the processing site on demand nor a centralized data warehouse approach scale sufficiently to handle the huge data volumes and processing demands of future e-science communities and applications. The former suffers from high transmission costs while the latter cannot scale to the large amounts of data in combination with the growing number of queries.|Tobias Scholl,Bernhard Bauer,Benjamin Gufler,Richard Kuntschke,Daniel Weber,Angelika Reiser,Alfons Kemper","80772|VLDB|2007|Unifying Data and Domain Knowledge Using Virtual Views|The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB  PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.|Lipyeow Lim,Haixun Wang,Min Wang","80840|VLDB|2007|Data Integration with Uncertainty|This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels, and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, queries to the system may be posed with keywords rather than in a structured form. Third, the data from the sources may be extracted using information extraction techniques and so may yield imprecise data. As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings by-table semantics assumes that there exists a correct mapping but we don't know what it is by-tuple semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of approximate schema mappings, and we describe an algorithm for efficiently computing the top-k answers to queries in such a setting.|Xin Luna Dong,Alon Y. Halevy,Cong Yu","80803|VLDB|2007|Request Window an Approach to Improve Throughput of RDBMS-based Data Integration System by Utilizing Data Sharing Across Concurrent Distributed Queries|This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently issue data requests and fetch data over the network. This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a .x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.|Rubao Lee,Minghong Zhou,Huaming Liao","80747|VLDB|2007|Model Management and Schema Mappings Theory and Practice|We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.|Philip A. Bernstein,Howard Ho","80846|VLDB|2007|iTrails Pay-as-you-go Information Integration in Dataspaces|Dataspace management has been recently identified as a new agenda for information management ,  and information integration . In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration , as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.|Marcos Antonio Vaz Salles,Jens-Peter Dittrich,Shant Kirakos Karakashian,Olivier René Girard,Lukas Blunschi"],["57951|GECCO|2007|An estimation of distribution algorithm with guided mutation for a complex flow shop scheduling problem|An Estimation of Distribution Algorithm (EDA) is proposed toapproach the Hybrid Flow Shop with Sequence Dependent Setup Times and Uniform Machines in parallel (HFS-SDST-UM) problem. The latter motivated by the needs of a real world company. The proposed EDA implements a fairly new mechanism to improve the search of more traditional EDAs. This is the Guided Mutation (GM). EDA-GM generates new solutions by using the information from a probability model, as all EDAs, and the local information from a good known solution. The approach is tested on several instances of HFS-SDST-UM and compared with adaptations of meta-heuristics designed for very similarproblems. Encouraging results are reported.|Abdellah Salhi,José Antonio Vázquez Rodríguez,Qingfu Zhang","16738|IJCAI|2007|Quantified Coalition Logic|We add a limited but useful form of quantification to Coalition Logic, a popular formalism for reasoning about cooperation in game-like multi-agent systems. The basic constructs of Quantified Coalition Logic (QCL) allow us to express properties as \"there exists a coalition C satisfying property P such that C can achieve . We give an axiomatization of QCL, and show that while it is no more expressive than Coalition Logic, it is exponentially more succinct. The time complexity of QCL model checking for symbolic and explicit state representations is shown to be no worse than that of Coalition Logic. We illustrate the formalism by showing how to succinctly specify such social choice mechanisms as majority voting, which in Coalition Logic require specifications that are exponentially long in the number of agents.|Thomas \u2026gotnes,Wiebe van der Hoek,Michael Wooldridge","58030|GECCO|2007|Addressing sampling errors and diversity loss in UMDA|Estimation of distribution algorithms replace the typical crossover and mutation operators by constructing a probabilistic model and generating offspring according to this model. In previous studies, it has been shown that this generally leads to diversity loss due to sampling errors. In this paper, for the case of the simple Univariate Marginal Distribution Algorithm (UMDA), we propose and test several methods for counteracting diversity loss. The diversity loss can come in two phases sampling from the probability model (offspring generation) and selection. We show that it is possible to completely remove the sampling error during offspring generation. Furthermore, we examine several plausible model construction variants which counteract diversity loss during selection and demonstrate that these update rules work better than the standard update on a variety of simple test problems.|Jürgen Branke,Clemens Lode,Jonathan L. Shapiro","58031|GECCO|2007|Option pricing model calibration using a real-valued quantum-inspired evolutionary algorithm|Quantum effects are a natural phenomenon and just like evolution, or immune processes, can serve as an inspiration for the design of computing algorithms. This study illustrates how a real-valued quantum-inspired evolutionary algorithm(QEA) can be constructed and examines the utility of the resulting algorithm on an important real-world problem, namely the calibration of an Option Pricing model. The results from the algorithm are shown to be robust and sensitivity analysis is carried out on the algorithm parameters, suggesting that there is useful potential to apply QEA to this domain.|Kai Fan,Anthony Brabazon,Conall O'Sullivan,Michael O'Neill","16461|IJCAI|2007|Topological Mapping through Distributed Passive Sensors|In this paper we address the problem of inferring the topology, or inter-node navigability, of a sensor network given non-discriminating observations of activity in the environment. By exploiting motion present in the environment, our approach is able to recover a probabilistic model of the sensor network connectivity graph and the underlying traffic trends. We employ a reasoning system made up of a stochastic Expectation Maximization algorithm and a higher level search strategy employing the principle of Occam's Razor to look for the simplest solution explaining the data. The technique is assessed through numerical simulations and experiments conducted on a real sensor network.|Dimitri Marinakis,Gregory Dudek","57887|GECCO|2007|An application of EDA and GA to dynamic pricing|E-commerce has transformed the way firms develop their pricing strategies, producing shift away from fixed pricing to dynamic pricing. In this paper, we use two different Estimation of distribution algorithms (EDAs), a Genetic Algorithm (GA) and a Simulated Annealing (SA) algorithm for solving two different dynamic pricing models. Promising results were obtained for an EDA confirming its suitability for resource management in the proposed model. Our analysis gives interesting insights into the application of population based optimization techniques for dynamic pricing.|Siddhartha Shakya,Fernando Oliveira,Gilbert Owusu","57952|GECCO|2007|Screening the parameters affecting heuristic performance|This research screens the tuning parameters of a combinatorial optimization heuristic. Specifically, it presents a Design of Experiments (DOE) approach that uses a Fractional Factorial Design to screen the tuning parameters of Ant Colony System (ACS) for the Travelling Sales person problem. Screening is a preliminary step towards building a full Response Surface Model (RSM) . It identifies parametersthat have little influence on performance and can be omittedfrom the RSM design. This reduces the complexity andexpense of the RSM design.  algorithm parameters and  problem characteristics are considered. Open questionson the effect of  parameters on performance are answered.A further parameter, sometimes assumed important, was shown to have no effect on performance. A new problem characteristic that effects performance was identified. A full version of this paper is available .|Enda Ridge,Daniel Kudenko","16715|IJCAI|2007|Planning for Gene Regulatory Network Intervention|Modeling the dynamics of cellular processes has recently become a important research area of many disciplines. One of the most important reasons to model a cellular process is to enable highthroughput in-silico experiments that attempt to predict or intervene in the process. These experiments can help accelerate the design of therapies through their cheap replication and alteration. While some techniques exist for reasoning with cellular processes, few take advantage of the flexible and scalable algorithms popularized in AI research. In this domain, where scalability is crucial for feasible application, we apply AI planning based search techniques and demonstrate their advantage over existing enumerative methods.|Daniel Bryce,Seungchan Kim","58204|GECCO|2007|Population sizing for entropy-based model building in discrete estimation of distribution algorithms|This paper proposes a population-sizing model for entropy-based model building in discrete estimation of distribution algorithms. Specifically, the population size required for building an accurate model is investigated. The effect of selection pressure on population sizing is also preliminarily incorporated. The proposed model indicates that the population size required for building an accurate model scales as (m log m), where m is the number of substructures of the given problem and is proportional to the problem size. Experiments are conducted to verify the derivations, and the results agree with the proposed model.|Tian-Li Yu,Kumara Sastry,David E. Goldberg,Martin Pelikan","58154|GECCO|2007|Environment as a spatial constraint on the growth of structural form|We explore the use of the developmental environment as a spatial constraint on a model of Artificial Embryogeny, applied to the growth of structural forms. A Deva model is used to translate genotype to phenotype, allowing a Genetic Algorithm to evolve Plane Trusses. Genomes are expressed in one of several developmental environments, and selected using a fitness function favouring stability, height, and distribution of pressure. Positive results are found in nearly all cases, demonstrating that environment can be used as an effective spatial constraint on development. Further experiments take genomes evolved in some environment and transplant them into different environments, or re-grow them at different phenotypic sizes It is shown that while some genomes are highly specialized for the particular environment in which they evolved, others may be re-used in a different context without significant re-design, retaining the majority of their original utility. This strengthens the notion that growth via Artificial Embryogeny can be resistant to perturbations in environment, and that good designs may be re-used in a variety of contexts.|Taras Kowaliw,Peter Grogono,Nawwaf N. Kharma"],["16757|IJCAI|2007|EQL-Lite Effective First-Order Query Processing in Description Logics|Querying Description Logic knowledge bases has received great attention in the last years. In such a problem, the need of coping with incomplete information is the distinguishing feature with respect to querying databases. Due to this feature, we have to deal with two conflicting needs on the one hand, we would like to query the knowledge base with sophisticated mechanisms provided by full first-order logic (FOL) on the other hand, the presence of incomplete information makes query answering a much more difficult task than in databases. In this paper we advocate the use of a nonmonotonic epistemic FOL query language as a means for expressing sophisticated queries over Description Logic knowledge bases. We show that through a controlled use of the epistemic operator, resulting in the language called EQL-Lite, we are able to formulate full FOL queries over Description Logic knowledge bases, while keeping computational complexity of query answering under control. In particular, we show that EQL-Lite queries over DL-Lite knowledge bases are FOL reducible (i.e., compilable into SQL) and hence can be answered in LOGSPACE through standard database technologies.|Diego Calvanese,Giuseppe De Giacomo,Domenico Lembo,Maurizio Lenzerini,Riccardo Rosati","16662|IJCAI|2007|Contextual Default Reasoning|In this paper we introduce a multi-context variant of Reiter's default logic. The logic provides a syntactical counterpart of Roelofsen and Serafini's information chain approach (IJCAI-), yet has several advantages it is closer to standard ways of representing nonmonotonic inference and a number of results from that area come \"for free\" it is closer to implementation, in particular the restriction to logic programming gives us a computationally attractive framework and it allows us to handle a problem with the information chain approach related to skeptical reasoning.|Gerhard Brewka,Floris Roelofsen,Luciano Serafini","16492|IJCAI|2007|Completing Description Logic Knowledge Bases Using Formal Concept Analysis|We propose an approach for extending both the terminological and the assertional part of a Description Logic knowledge base by using information provided by the knowledge base and by a domain expert. The use of techniques from Formal Concept Analysis ensures that, on the one hand, the interaction with the expert is kept to a minimum, and, on the other hand, we can show that the extended knowledge base is complete in a certain, well-defined sense.|Franz Baader,Bernhard Ganter,Baris Sertkaya,Ulrike Sattler","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","16542|IJCAI|2007|Using the Probabilistic Logic Programming Language P-log for Causal and Counterfactual Reasoning and Non-Naive Conditioning|P-log is a probabilistic logic programming language, which combines both logic programming style knowledge representation and probabilistic reasoning. In earlier papers various advantages of P-log have been discussed. In this paper we further elaborate on the KR prowess of P-log by showing that (i) it can be used for causal and counterfactual reasoning and (ii) it provides an elaboration tolerant way for non-naive conditioning.|Chitta Baral,Matt Hunsaker","16463|IJCAI|2007|Characterizing the NP-PSPACE Gap in the Satisfiability Problem for Modal Logic|There has been a great deal of work on characterizing the complexity of the satisfiability and validity problem for modal logics. In particular, Ladner showed that the satisfiability problem for all logics between K and S is PSPACE-hard, while for S it is NP-complete. We show that it is negative introspection, the axiom Kp  KKp, that causes the gap if we add this axiom to any modal logic between K and S, then the satisfiability problem becomes NP-complete. Indeed, the satisfiability problem is NP-complete for any modal logic that includes the negative introspection axiom.|Joseph Y. Halpern,Leandro Chaves Rêgo","16595|IJCAI|2007|Argumentation Based Contract Monitoring in Uncertain Domains|Few existing argumentation frameworks are designed to deal with probabilistic knowledge, and none are designed to represent possibilistic knowledge, making them unsuitable for many real world domains. In this paper we present a subjective logic based framework for argumentation which overcomes this limitation. Reasoning about the state of a literal in this framework can be done in polynomial time. A dialogue game making use of the framework and a utility based heuristic for playing the dialogue game are also presented. We then show how these components can be applied to contract monitoring. The dialogues that emerge bear some similarity to the dialogues that occur when humans argue about contracts, and our approach is highly suited to complex, partially observable domains with fallible sensors where determining environment state cannot be done for free.|Nir Oren,Timothy J. Norman,Alun D. Preece","16787|IJCAI|2007|A Faithful Integration of Description Logics with Logic Programming|Integrating description logics (DL) and logic programming (LP) would produce a very powerful and useful formalism. However, DLs and LP are based on quite different principles, so achieving a seamless integration is not trivial. In this paper, we introduce hybrid MKNF knowledge bases that faithfully integrate DLs with LP using the logic of Minimal Knowledge and Negation as Failure (MKNF) Lifschitz, . We also give reasoning algorithms and tight data complexity bounds for several interesting fragments of our logic.|Boris Motik,Riccardo Rosati","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman"],["16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","16538|IJCAI|2007|Change of Representation for Statistical Relational Learning|Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.|Jesse Davis,Irene M. Ong,Jan Struyf,Elizabeth S. Burnside,David Page,Vítor Santos Costa","16360|IJCAI|2007|Exploiting Sensorimotor Coordination for Learning to Recognize Objects|In this paper we present a system which learns to recognize objects through interaction by exploiting the principle of sensorimotor coordination. The system uses a learning architecture which is composed of reactive and deliberative layers. The reactive layer consists of a database of behaviors that aremodulated to produce a desired behavior. In this work we have implemented and installed in our architecture an object manipulation behavior inspired by the concept that infants learn about their environment through manipulation. While manipulating objects, both proprioceptive data and exteroceptive data are recorded. Both of these types of data are combined and statistically analyzed in order to extract important parameters that distinctively describe the object being manipulated. This data is then clustered using the standard k-means algorithm and the resulting clusters are labeled. The labeling is used to train a radial basis function network for classifying the clusters. The performance of the system has been tested on a kinematically complex walking robot capable of manipulating objects with two legs used as arms, and it has been found that the trained neural network is able to classify objects even when only partial sensory data is available to the system. Our preliminary results demonstrate that this method can be effectively used in a robotic system which learns from experience about its environment.|Yohannes Kassahun,Mark Edgington,Jose de Gea,Frank Kirchner","58221|GECCO|2007|The effect of learning on life history evolution|A series of evolutionary neural network simulations are presented which explore the hypothesis that learning factors can result in the evolution of long periods of parental protection and late onset of maturity. By evolving populations of neural networks to learn quickly to perform well on simple classification tasks, it is shown that better learned performance is obtained if protection from competition is provided during the network's early learning period. Moreover, if the length of the protection period is allowed to evolve, it does result in the emergence of relatively long protection periods, even if there are other costs involved, such as individuals not being allowed to reproduce during their protection phase, and the parents suffering increased risk of dying while protecting their offspring.|John A. Bullinaria","58197|GECCO|2007|Order or not does parallelization of model building in hBOA affect its scalability|It has been shown that model building in the hierarchical Bayesian optimization algorithm (hBOA) can be efficiently parallelized by randomly generating an ancestral ordering of the nodes of the network prior to learning the network structure and allowing only dependencies consistent with the generated ordering. However, it has not been thoroughly shown that this approach to restricting probabilistic models does not affect scalability of hBOA on important classes of problems. This paper demonstrates that although the use of a random ancestral ordering restricts the structure of considered models to allow efficient parallelization of model building, its effects on hBOA performance and scalability are negligible.|Martin Pelikan,James D. Laury Jr.","16745|IJCAI|2007|Analogical Learning in a Turn-Based Strategy Game|A key problem in playing strategy games is learning how to allocate resources effectively. This can be a difficult task for machine learning when the connections between actions and goal outputs are indirect and complex. We show how a combination of structural analogy, experimentation, and qualitative modeling can be used to improve performance in optimizing food production in a strategy game. Experimentation bootstraps a case library and drives variation, while analogical reasoning supports retrieval and transfer. A qualitative model serves as a partial domain theory to support adaptation and credit assignment. Together, these techniques can enable a system to learn the effects of its actions, the ranges of quantities, and to apply training in one city to other, structurally different cities. We describe experiments demonstrating this transfer of learning.|Thomas R. Hinrichs,Kenneth D. Forbus","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","16517|IJCAI|2007|Learning to Count by Think Aloud Imitation|Although necessary, learning to discover new solutions is often long and difficult, even for supposedly simple tasks such as counting. On the other hand, learning by imitation provides a simple way to acquire knowledge by watching other agents do. In order to learn more complex tasks by imitation than mere sequences of actions, a Think Aloud protocol is introduced, with a new neuro-symbolic network. The latter uses time in the same way as in a Time Delay Neural Network, and is added basic first order logic capacities. Tested on a benchmark counting task, learning is very fast, generalization is accurate, whereas there is no initial bias toward counting.|Laurent Orseau","57905|GECCO|2007|Automated alphabet reduction method with evolutionary algorithms for protein structure prediction|This paper focuses on automated procedures to reduce the dimensionality ofprotein structure prediction datasets by simplifying the way in which the primary sequence of a protein is represented. The potential benefits ofthis procedure are faster and easier learning process as well as the generationof more compact and human-readable classifiers.The dimensionality reduction procedure we propose consists on the reductionof the -letter amino acid (AA) alphabet, which is normally used to specify a protein sequence, into a lower cardinality alphabet. This reduction comes about by a clustering of AA types accordingly to their physical and chemical similarity. Our automated reduction procedure is guided by a fitness function based on the Mutual Information between the AA-based input attributes of the dataset and the protein structure featurethat being predicted. To search for the optimal reduction, the Extended Compact Genetic Algorithm (ECGA) was used, and afterwards the results of this process were fed into (and validated by) BioHEL, a genetics-based machine learningtechnique. BioHEL used the reduced alphabet to induce rules forprotein structure prediction features. BioHEL results are compared to two standard machine learning systems. Our results show that it is possible to reduce the size of the alphabet used for prediction fromtwenty to just three letters resulting in more compact, i.e. interpretable,rules. Also, a protein-wise accuracy performance measure suggests that the loss of accuracy acrued by this substantial alphabet reduction is not statistically significant when compared to the full alphabet.|Jaume Bacardit,Michael Stout,Jonathan D. Hirst,Kumara Sastry,Xavier Llorà,Natalio Krasnogor","16723|IJCAI|2007|Generalized Additive Bayesian Network Classifiers|Bayesian network classifiers (BNC) have received considerable attention in machine learning field. Some special structure BNCs have been proposed and demonstrate promise performance. However, recent researches show that structure learning in BNs may lead to a non-negligible posterior problem, i.e, there might be many structures have similar posterior scores. In this paper, we propose a generalized additive Bayesian network classifiers, which transfers the structure learning problem to a generalized additive models (GAM) learning problem. We first generate a series of very simple BNs, and put them in the framework of GAM, then adopt a gradient-based algorithm to learn the combining parameters, and thus construct a more powerful classifier. On a large suite of benchmark data sets, the proposed approach outperforms many traditional BNCs, such as naive Bayes, TAN, etc, and achieves comparable or better performance in comparison to boosted Bayesian network classifiers.|Jianguo Li,Changshui Zhang,Tao Wang,Yimin Zhang"],["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","16525|IJCAI|2007|Database-Text Alignment via Structured Multilabel Classification|This paper addresses the task of aligning a database with a corresponding text. The goal is to link individual database entries with sentences that verbalize the same information. By providing explicit semantics-to-text links, these alignments can aid the training of natural language generation and information extraction systems. Beyond these pragmatic benefits, the alignment problem is appealing from a modeling perspective the mappings between database entries and text sentences exhibit rich structural dependencies, unique to this task. Thus, the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. To this end, we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. In contrast to existing multilabel classifiers, our approach operates over arbitrary global features of inputs and proposed labels. We compare our model with a baseline classifier that makes locally optimal decisions. Our results show that the proposed model yields a % relative reduction in error, and compares favorably with human performance.|Benjamin Snyder,Regina Barzilay","16601|IJCAI|2007|An Axiomatic Approach to Personalized Ranking Systems|Personalized ranking systems and trust systems are an essential tool for collaboration in a multi-agent environment. In these systems, trust relations between many agents are aggregated to produce a personalized trust rating of the agents. In this paper we introduce the first extensive axiomatic study of this setting, and explore a wide array of well-known and new personalized ranking systems. We adapt several axioms (basic criteria) from the literature on global ranking systems to the context of personalized ranking systems, and prove strong properties implied by the combination of these axioms.|Alon Altman,Moshe Tennenholtz","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","16511|IJCAI|2007|Formal Trust Model for Multiagent Systems|Trust should be substantially based on evidence. Further, a key challenge for multiagent systems is how to determine trust based on reports from multiple sources, who might themselves be trusted to varying degrees. Hence an ability to combine evidence-based trust reports in a manner that discounts for imperfect trust in the reporting agents is crucial for multiagent systems. This paper understands trust in terms of belief and certainty A's trust in B is reflected in the strength of A's belief that B is trustworthy. This paper formulates certainty in terms of evidence based on a statistical measure defined over a probability distribution of the probability of positive outcomes. This novel definition supports important mathematical properties, including () certainty increases as conflict increases provided the amount of evidence is unchanged, and () certainty increases as the amount of evidence increases provided conflict is unchanged. Moreover, despite a more subtle definition than previous approaches, this paper () establishes a bijection between evidence and trust spaces, enabling robust combination of trust reports and () provides an efficient algorithm for computing this bijection.|Yonghong Wang,Munindar P. Singh","58128|GECCO|2007|Suppression based immune mechanism to find arepresentative training set in data classification tasks|This article proposes a new classifier inspired by a biolog-ical immune systems' characteristic which also belongs tothe class of k-nearest-neighbors algorithms. Its main fea-ture is a suppression mechanism used to reduce the size of the training set maintaining the most significative sampleswithout loosing much capability of generalization.|Grazziela Patrocinio Figueredo,Nelson F. F. Ebecken,Helio J. C. Barbosa","57905|GECCO|2007|Automated alphabet reduction method with evolutionary algorithms for protein structure prediction|This paper focuses on automated procedures to reduce the dimensionality ofprotein structure prediction datasets by simplifying the way in which the primary sequence of a protein is represented. The potential benefits ofthis procedure are faster and easier learning process as well as the generationof more compact and human-readable classifiers.The dimensionality reduction procedure we propose consists on the reductionof the -letter amino acid (AA) alphabet, which is normally used to specify a protein sequence, into a lower cardinality alphabet. This reduction comes about by a clustering of AA types accordingly to their physical and chemical similarity. Our automated reduction procedure is guided by a fitness function based on the Mutual Information between the AA-based input attributes of the dataset and the protein structure featurethat being predicted. To search for the optimal reduction, the Extended Compact Genetic Algorithm (ECGA) was used, and afterwards the results of this process were fed into (and validated by) BioHEL, a genetics-based machine learningtechnique. BioHEL used the reduced alphabet to induce rules forprotein structure prediction features. BioHEL results are compared to two standard machine learning systems. Our results show that it is possible to reduce the size of the alphabet used for prediction fromtwenty to just three letters resulting in more compact, i.e. interpretable,rules. Also, a protein-wise accuracy performance measure suggests that the loss of accuracy acrued by this substantial alphabet reduction is not statistically significant when compared to the full alphabet.|Jaume Bacardit,Michael Stout,Jonathan D. Hirst,Kumara Sastry,Xavier Llorà,Natalio Krasnogor","58017|GECCO|2007|Procreating V-detectors for nonself recognition an application to anomaly detection in power systems|The artificial immune system approach for self-nonself discrimination and its application to anomaly detection problems in engineering is showing great promise. A seminal contribution in this area is the V-detectors algorithm that can very effectively cover the nonself region of the feature space with a set of detectors. The detector set can be used to detect anomalous inputs. In this paper, a multistage approach to create an effective set of V-detectors is considered. The first stage of the algorithm generates an initial set of V-detectors. In subsequent stage, new detectors are grown from existing ones, by means of a mechanism called procreation. Procreating detectors can more effectively fill hard-to-reach interstices in the nonself region, resulting in better coverage. The effectiveness of the algorithm is first illustrated by applying it to a well-known fractal, the Koch curve. The algorithm is then applied to the problem of detecting anomalous behavior in power distribution systems, and can be of much use for maintenance-related decision-making in electrical utility companies.|Min Gui,Sanjoy Das,Anil Pahwa","58124|GECCO|2007|Stochastic training of a biologically plausible spino-neuromuscular system model|A primary goal of evolutionary robotics is to create systems that are as robust and adaptive as the human body. Moving toward this goal often involves training control systems that processes sensory information in a way similar to humans. Artificial neural networks have been an increasingly popular option for thisbecause they consist of processing units that approximate thesynaptic activity of biological signal processing units, i.e. neurons. In this paper we train a nonlinear recurrent spino-neuromuscular system model(SNMS) comparing the performance of genetic algorithms (GA)s, particle swarm optimizers (PSO)s, and GAPSO hybrids. This model includes several key features of the SNMS that have previously been modeled individually but have not been combined into a single model as is done here. The results show that each algorithm produces fit solutions and generates fundamental biological behaviors that are not directly trained for such as tonic tension behaviors and tricepsactivation patterns.|Stanley Phillips Gotshall,Terence Soule"],["58096|GECCO|2007|Diverse committees vote for dependable profits|Stock selection for hedge fund portfolios is a challenging problem for Genetic Programming (GP) because the markets (the environment in which the GP solution must survive) are dynamic, unpredictable and unforgiving. How can GP be improved so that solutions are produced that are robust to non-trivial changes in the environment We explore an approach that uses a voting committee of GP individualswith differing phenotypic behaviour.|Wei Yan,Christopher D. Clack","16615|IJCAI|2007|New Constraint Programming Approaches for the Computation of Leximin-Optimal Solutions in Constraint Networks|We study the problem of computing a leximin-optimal solution of a constraint network. This problem is highly motivated by fairness and efficiency requirements in many real-world applications implying human agents. We compare several generic algorithms which solve this problem in a constraint programming framework. The first one is entirely original, and the other ones are partially based on existing works adapted to fit with this problem.|Sylvain Bouveret,Michel Lemaître","58068|GECCO|2007|Solving the artificial ant on the Santa Fe trail problem in   fitness evaluations|In this paper, we provide an algorithm that systematically considers all small trees in the search space of genetic programming. These small trees are used to generate useful subroutines for genetic programming. This algorithm is tested on the Artificial Ant on the Santa Fe Trail problem, a venerable problem for genetic programming systems. When four levels of iteration are used, the algorithm presented here generates better results than any known published result by a factor of .|Steffen Christensen,Franz Oppacher","57968|GECCO|2007|An analysis of constructive crossover and selection pressure in genetic programming|A common problem in genetic programming search algorithms is destructive crossover in which the offspring of good parents generally has worse performance than the parents. Designing constructive crossover operators and integrating some local search techniques into the breeding process have been suggested as solutions. This paper reports on experiments demonstrating that premature convergence may happen more often when using these techniques in combination with standard parent selection. It shows that modifying the selection pressure in the parent selection process is necessary to obtain a significant performance improvement.|Huayang Xie,Mengjie Zhang,Peter Andreae","58036|GECCO|2007|Characteristic determination for solid state devices with evolutionary computation a case study|In this paper, we develop a new optimization framework that consists of the extended compact genetic algorithm (ECGA) and split-on-demand (SoD), an adaptive discretization technique, to tackle the characteristic determination problem for solid state devices. As most decision variables of characteristic determination problems are real numbers due to the modeling of physical phenomena, and ECGA is designed for handling discrete-type problems, a specific mechanism to transform the variable types of the two ends is in order. In the proposed framework, ECGA is used as a back-end optimization engine, and SoD is adopted as the interface between the engine and the problem. Moreover, instead of one mathematical model with various parameters, characteristic determination is in fact a set of problems of which the mathematical formulations may be very different. Therefore, in this study, we employ the proposed framework on three study cases to demonstrate that the technique proposed in the domain of evolutionary computation can provide not only the high quality optimization results but also the flexibility to handle problems of different formulations.|Ping-Chu Hung,Ying-Ping Chen,Hsiao Wen Zan","57920|GECCO|2007|A study of mutational robustness as the product of evolutionary computation|This paper investigates the ability of a tournament selection based genetic algorithm to find mutationally robust solutions to a simple combinatorial optimization problem. Two distinct algorithms (a stochastic hill climber and a tournament selection based GA) were used to search for optimal walks in several variants of the self avoiding walk problem. The robustness of the solutions obtained by the algorithms were compared, both with each other and with solutions obtained by a random sampling of the optimal solution space. The solutions found by the GA were, for most of the problem variants, significantly more robust than those found by either the hill climbing algorithm or random sampling. The solutions found by the hill climbing algorithm were often significantly less robust than those obtained through random sampling. .|Justin Schonfeld","58188|GECCO|2007|Evolving robust GP solutions for hedge fund stock selection in emerging markets|Stock selection for hedge fund portfolios is a challenging problem for Genetic Programming (GP) because the markets (the environment in which the GP solution must survive) are dynamic, unpredictable and unforgiving. How can GP be improved so that solutions are produced that are robust to non-trivial changes in the environment We explore an approach that uses subsets of extreme environments during training.|Wei Yan,Christopher D. Clack","58052|GECCO|2007|Hybrid multiobjective optimization genetic algorithms for graph drawing|In this paper we introduce an application of multiobjective optimization with genetic algorithms to the problem of graph drawing and explore the potential contribution of the genetic algorithms for this particular problem.|Dana Vrajitoru","58050|GECCO|2007|Real-coded ECGA for economic dispatch|In this paper, we propose a new approach that consists of the extended compact genetic algorithm (ECGA) and split-on-demand (SoD), an adaptive discretization technique, to economic dispatch (ED) problems with nonsmooth cost functions. ECGA is designed for handling problems with decision variables of the discrete type, while the decision variables of ED problems are oftentimes real numbers. Thus, in order to employ ECGA to tackle ED problems, SoD is utilized for discretizing the continuous decision variables and works as the interface between ECGA and the ED problem. Furthermore, ED problems in practice are usually hard for traditional mathematical programming methodologies because of the equality and inequality constraints. Hence, in addition to integrating ECGA and SoD, in this study, we devise a repair operator specifically for making the infeasible solutions to satisfy the equality constraint. To examine the performance and effectiveness, we apply the proposed framework to two different-sized ED problems with nonsmooth cost function considering the valve-point effects. The experimental results are compared to those obtained by various evolutionary algorithms and demonstrate that handling ED problems with the proposed framework is a promising research direction.|Chao-Hong Chen,Ying-Ping Chen","16645|IJCAI|2007|Generalized Interval Projection A New Technique for Consistent Domain Extension|This paper deals with systems of parametric equations over the reals, in the framework of interval constraint programming. As parameters vary within intervals, the solution set of a problem may have a non null volume. In these cases, an inner box (i.e., a box included in the solution set) instead of a single punctual solution is of particular interest, because it gives greater freedom for choosing a solution. Our approach is able to build an inner box for the problem starting with a single point solution, by consistently extending the domain of every variable. The key point is a new method called generalized projection. The requirements are that each parameter must occur only once in the system, variable domains must be bounded, and each variable must occur only once in each constraint. Our extension is based on an extended algebraic structure of intervals called generalized intervals, where improper intervals are allowed (e.g. ,).|Carlos Grandón,Gilles Chabert,Bertrand Neveu"],["16615|IJCAI|2007|New Constraint Programming Approaches for the Computation of Leximin-Optimal Solutions in Constraint Networks|We study the problem of computing a leximin-optimal solution of a constraint network. This problem is highly motivated by fairness and efficiency requirements in many real-world applications implying human agents. We compare several generic algorithms which solve this problem in a constraint programming framework. The first one is entirely original, and the other ones are partially based on existing works adapted to fit with this problem.|Sylvain Bouveret,Michel Lemaître","16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","16689|IJCAI|2007|On Modeling Multiagent Task Scheduling as a Distributed Constraint Optimization Problem|This paper investigates how to represent and solve multiagent task scheduling as a Distributed Constraint Optimization Problem (DCOP). Recently multiagent researchers have adopted the CTMS language as a standard for multiagent task scheduling. We contribute an automated mapping that transforms CTMS into a DCOP. Further, we propose a set of representational compromises for CTMS that allow existing distributed algorithms for DCOP to be immediately brought to bear on CTMS problems. Next, we demonstrate a key advantage of a constraint based representation is the ability to leverage the representation to do efficient solving. We contribute a set of pre-processing algorithms that leverage existing constraint propagation techniques to do variable domain pruning on the DCOP. We show that these algorithms can result in % reduction in state space size for a given set of CTMS problems. Finally, we demonstrate up to a % increase in the ability to optimally solve CTMS problems in a reasonable amount of time and in a distributed manner as a result of applying our mapping and domain pruning algorithms.|Evan Sultanik,Pragnesh Jay Modi,William C. Regli","16700|IJCAI|2007|Constraint Partitioning for Solving Planning Problems with Trajectory Constraints and Goal Preferences|The PDDL specifications include soft goals and trajectory constraints for distinguishing highquality plans among the many feasible plans in a solution space. To reduce the complexity of solving a large PDDL planning problem, constraint partitioning can be used to decompose its constraints into subproblems of much lower complexity. However, constraint locality due to soft goals and trajectory constraints cannot be effectively exploited by existing subgoal-partitioning techniques developed for solving PDDL. problems. In this paper, we present an improved partition-andresolve strategy for supporting the new features in PDDL. We evaluate techniques for resolving violated global constraints, optimizing goal preferences, and achieving subgoals in a multivalued representation. Empirical results on the th International Planning Competition (IPC) benchmarks show that our approach is effective and significantly outperforms other competing planners.|Chih-Wei Hsu,Benjamin W. Wah,Ruoyun Huang,Yixin Chen","16535|IJCAI|2007|Combining Topological and Directional Information for Spatial Reasoning|Current research on qualitative spatial representation and reasoning usually focuses on one single aspect of space. However, in real world applications, several aspects are often involved together. This paper extends the well-known RCC constraint language to deal with both topological and directional information, and then investigates the interaction between the two kinds of information. Given a topological (RCC) constraint network and a directional constraint network, we ask when the joint network is satisfiable. We show that when the topological network is over one of the three maximal tractable subclasses of RCC, the problem can be reduced into satisfiability problems in the RCC algebra and the rectangle algebra (RA). Therefore, reasoning techniques developed for RCC and RA can be used to solve the satisfiability problem of a joint network.|Sanjiang Li","58186|GECCO|2007|A multi-objective imaging scheduling approach for earth observing satellites|EOSs (Earth Observing Satellites) circle the earth to take shotswhich are requested by customers. To make replete use of resourcesof EOSs, it is required to deal with the problem of united imagingscheduling of EOSs in a given scheduling horizon, which is acomplicated multi-objective combinatorial optimization problem. Inthis paper, we construct a mathematical model for the problem byabstracting imaging constraints of different EOSs. Then we propose anovel multi-objective EOSs imaging scheduling method, which is basedon the Strength Pareto Evolutionary Algorithm . The specialencoding technique and imaging constraint control are applied toguarantee feasibility of solutions. The approach is tested upon fourreal application problems of CBERS EOSs series. From the results, itis confirmed that the proposed approach is effective in solvingmulti-objective EOSs imaging scheduling problems.|Jun Wang,Ning Jing,Jun Li,Zhong Hui Chen","58101|GECCO|2007|An online implementable differential evolution tuned optimal guidance law|This paper proposes a novel application of differential evolution to solve a difficult dynamic optimisation or optimal control problem. The miss distance in a missile-target engagement is minimised using differential evolution. The difficulty of solving it by existing conventional techniques in optimal control theory is caused by the nonlinearity of the dynamic constraint equation, inequality constraint on the control input and inequality constraint on another parameter that enters problem indirectly. The optimal control problem of finding the minimum miss distance has an analytical solution subject to several simplifying assumptions. In the approach proposed in this paper, the initial population is generated around the seed value given by this analytical solution. Thereafter, the algorithm progresses to an acceptable final solution within a few generations, satisfying the constraints at every iteration. Since this solution or the control input has to be obtained in real time to be of any use in practice, the feasibility of online implementation is also illustrated.|Raghunathan Thangavelu,S. Pradeep","16722|IJCAI|2007|Query-Driven Constraint Acquisition|The modelling and reformulation of constraint networks are recognised as important problems. The task of automatically acquiring a constraint network formulation of a problem from a subset of its solutions and non-solutions has been presented in the literature. However, the choice of such a subset was assumed to be made independently of the acquisition process. We present an approach in which an interactive acquisition system actively selects a good set of examples. We show that the number of examples required to acquire a constraint network is significantly reduced using our approach.|Christian Bessière,Remi Coletta,Barry O'Sullivan,Mathias Paulin","16459|IJCAI|2007|Quality Guarantees on k-Optimal Solutions for Distributed Constraint Optimization Problems|A distributed constraint optimization problem (DCOP) is a formalism that captures the rewards and costs of local interactions within a team of agents. Because complete algorithms to solve DCOPs are unsuitable for some dynamic or anytime domains, researchers have explored incomplete DCOP algorithms that result in locally optimal solutions. One type of categorization of such algorithms, and the solutions they produce, is k- optimality a k-optimal solution is one that cannot be improved by any deviation by k or fewer agents. This paper presents the first known guarantees on solution quality for k-optimal solutions. The guarantees are independent of the costs and rewards in the DCOP, and once computed can be used for any DCOP of a given constraint graph structure.|Jonathan P. Pearce,Milind Tambe","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao"]]},"title":{"entropy":6.2048317787008544,"topics":["for learning, model for, learning, reinforcement learning, support vector, gaussian process, mobile robot, learning from, transfer learning, for query, learning and, model and, model, for machine, machine learning, probabilistic and, feature selection, using extraction, method for, learning the","algorithm for, particle swarm, genetic algorithm, genetic programming, the, genetic for, evolutionary algorithm, for optimization, for problems, algorithm optimization, for the, the problems, the effects, particle optimization, swarm optimization, genetic and, using genetic, for, using algorithm, for and","neural networks, for networks, genetic networks, data using, word disambiguation, data, word sense, sense disambiguation, gene expression, for data, automatic generation, genetic programming, artificial immune, association rule, rule mining, association mining, test generation, networks programming, for classification, data classification","for planning, heuristic search, description logic, framework for, random fields, the web, the logic, decision processes, solving problems, for reasoning, markov processes, reasoning about, logic programs, for recognition, decision tree, markov decision, fast algorithm, for logic, markov model, using conditional","support vector, mobile robot, for machine, efficient for, for selection, modeling and, feature selection, for multiple, and efficient, for nonlinear, feature for, inference for, for, for robot, efficient, for and, diagnosis, matching, xcs, user","for learning, learning and, learning, reinforcement learning, learning from, transfer learning, learning the, using learning, method for, supervised learning, networks learning, semi-supervised learning, relational learning, exploiting learning, classifiers, dynamic, plan, map, simulation, observation","evolutionary algorithm, evolutionary for, algorithm for, and evolutionary, algorithm the, using algorithm, multiobjective optimization, algorithm with, using evolutionary, for design, estimation distribution, with evolutionary, the using, algorithm optimization, evolutionary optimization, selection evolutionary, for multiobjective, the design, algorithm and, evolutionary approach","genetic algorithm, algorithm for, genetic for, for and, the evolution, genetic the, local search, evolution strategies, for evolution, building block, operator for, and local, for the, differential evolution, dynamic algorithm, evolution using, evolution and, for selection, algorithm and, algorithm the","word disambiguation, for based, word sense, sense disambiguation, automatic generation, test generation, generation for, automatic for, based, for pattern, for clustering, for streams, for processing, automatic, for, optimal, processing, control, large, recognition","genetic programming, genetic networks, networks programming, for rule, association rule, rule mining, association mining, using programming, genetic for, algorithm networks, programming for, mining for, rule with, with fuzzy, genetic with, algorithm with, genetic rule, genetic mining, for with, using genetic","for recognition, description logic, the logic, for logic, random fields, logic programs, using conditional, for programs, recognition and, logic, using, under, interaction, mutual, decentralized, anytime, object, semantic","framework for, for reasoning, and reasoning, and complexity, for and, with and, complexity the, with preferences, updates for, for bayesian, framework and, complexity, bayesian, and, with, games, strategy, online, tracking, auctions"],"ranking":[["16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16405|IJCAI|2007|Detection of Cognitive States from fMRI Data Using Machine Learning Techniques|Over the past decade functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful technique to locate activity of human brain while engaged in a particular task or cognitive state. We consider the inverse problem of detecting the cognitive state of a human subject based on the fMRI data. We have explored classification techniques such as Gaussian Naive Bayes, k-Nearest Neighbour and Support Vector Machines. In order to reduce the very high dimensional fMRI data, we have used three feature selection strategies. Discriminating features and activity based features were used to select features for the problem of identifying the instantaneous cognitive state given a single fMRI scan and correlation based features were used when fMRI data from a single time interval was given. A case study of visuo-motor sequence learning is presented. The set of cognitive states we are interested in detecting are whether the subject has learnt a sequence, and if the subject is paying attention only towards the position or towards both the color and position of the visual stimuli. We have successfully used correlation based features to detect position-color related cognitive states with % accuracy and the cognitive states related to learning with .% accuracy.|Vishwajeet Singh,Krishna P. Miyapuram,Raju S. Bapi","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","16661|IJCAI|2007|An Experts Algorithm for Transfer Learning|A long-lived agent continually faces new tasks in its environment. Such an agent may be able to use knowledge learned in solving earlier tasks to produce candidate policies for its current task. There may, however, be multiple reasonable policies suggested by prior experience, and the agent must choose between them potentially without any a priori knowledge about their applicability to its current situation. We present an \"experts\" algorithm for efficiently choosing amongst candidate policies in solving an unknown Markov decision process task. We conclude with the results of experiments on two domains in which we generate candidate policies from solutions to related tasks and use our experts algorithm to choose amongst them.|Erik Talvitie,Satinder Singh","16679|IJCAI|2007|A Machine Learning Approach for Statistical Software Testing|Some Statistical Software Testing approaches rely on sampling the feasible paths in the control flow graph of the program the difficulty comes from the tiny ratio of feasible paths. This paper presents an adaptive sampling mechanismcalled EXIST for Exploration eXploitation Inference for Software Testing, able to retrieve distinct feasible paths with high probability. EXIST proceeds by alternatively exploiting and updating a distribution on the set of program paths. An original representation of paths, accommodating long-range dependencies and data sparsity and based on extended Parikh maps, is proposed. Experimental validation on real-world and artificial problems demonstrates dramatic improvements compared to the state of the art.|Nicolas Baskiotis,Michèle Sebag,Marie-Claude Gaudel,Sandrine-Dominique Gouraud","16450|IJCAI|2007|Using Focal Point Learning to Improve Tactic Coordination in Human-Machine Interactions|We consider an automated agent that needs to coordinate with a human partner when communication between them is not possible or is undesirable (tactic coordination games). Specifically, we examine situations where an agent and human attempt to coordinate their choices among several alternatives with equivalent utilities. We use machine learning algorithms to help the agent predict human choices in these tactic coordination domains. Learning to classify general human choices, however, is very difficult. Nevertheless, humans are often able to coordinate with one another in communication-free games, by using focal points, \"prominent\" solutions to coordination problems. We integrate focal points into the machine learning process, by transforming raw domain data into a new hypothesis space. This results in classifiers with an improved classification rate and shorter training time. Integration of focal points into learning algorithms also results in agents that are more robust to changes in the environment.|Inon Zuckerman,Sarit Kraus,Jeffrey S. Rosenschein","16684|IJCAI|2007|Constructing New and Better Evaluation Measures for Machine Learning|Evaluation measures play an important role in machine learning because they are used not only to compare different learning algorithms, but also often as goals to optimize in constructing learning models. Both formal and empirical work has been published in comparing evaluation measures. In this paper, we propose a general approach to construct new measures based on the existing ones, and we prove that the new measures are consistent with, and finer than, the existing ones. We also show that the new measure is more correlated to RMS (Root Mean Square error) with artificial datasets. Finally, we demonstrate experimentally that the greedy-search based algorithm (such as artificial neural networks) trained with the new and finer measure usually can achieve better prediction performance. This provides a general approach to improve the predictive performance of existing learning algorithms based on greedy search.|Jin Huang,Charles X. Ling","16494|IJCAI|2007|Building Portable Options Skill Transfer in Reinforcement Learning|The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.|George Konidaris,Andrew G. Barto","16426|IJCAI|2007|Machine Learning for On-Line Hardware Reconfiguration|As computer systems continue to increase in complexity, the need for AI-based solutions is becoming more urgent. For example, high-end servers that can be partitioned into logical subsystems and repartitioned on the fly are now becoming available. This development raises the possibility of reconfiguring distributed systems online to optimize for dynamically changing workloads. However, it also introduces the need to decide when and how to reconfigure. This paper presents one approach to solving this online reconfiguration problem. In particular, we learn to identify, from only low-level system statistics, which of a set of possible configurations will lead to better performance under the current unknown workload. This approach requires no instrumentation of the system's middleware or operating systems. We introduce an agent that is able to learn this model and use it to switch configurations online as the workload varies. Our agent is fully implemented and tested on a publicly available multi-machine, multi-process distributed system (the online transaction processing benchmark TPC-W). We demonstrate that our adaptive configuration is able to outperform any single fixed configuration in the set over a variety of workloads, including gradual changes and abrupt workload spikes.|Jonathan Wildstrom,Peter Stone,Emmett Witchel,Michael Dahlin"],["58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58021|GECCO|2007|GARS an improved genetic algorithm with reserve selection for global optimization|This paper investigates how genetic algorithms (GAs) can be improved to solve large-scale and complex problems more efficiently. First of all, we review premature convergence, one of the challenges confronted with when applying GAs to real-world problems. Next, some of the methods now available to prevent premature convergence and their intrinsic defects are discussed. A qualitative analysis is then done on the cause of premature convergence that is the loss of building blocks hosted in less-fit individuals during the course of evolution. Thus, we propose a new improver - GAs with Reserve Selection (GARS), where a reserved area is set up to save potential building blocks and a selection mechanism based on individual uniqueness is employed to activate the potentials. Finally, case studies are done in a few standard problems well known in the literature, where the experimental results demonstrate the effectiveness and robustness of GARS in suppressing premature convergence, and also an enhancement is found in global optimization capacity.|Yang Chen,Jinglu Hu,Kotaro Hirasawa,Songnian Yu","58135|GECCO|2007|Genetic optimization for yacht design|This paper introduces a procedure for using genetic multi-objective optimization in yacht design. The problem described consists on the optimization of a bulb shape to improve the performance of the yacht. The two objectives considered are the minimization of the drag in calm water together with the minimization of the Vertical Center of Gravity (VCG), all the configurations should satisfy length and volume constraints. Since there is no a single optimum to be found, the MOGA-II was used as multi-objective genetic algorithm. The distributed optimization search exploited the parallelization capabilities of the MOGA-II algorithm which allowed the evaluation of several designs configurations by running concurrent threads of the flow analysis solver. Three bulb shapes of different length are selected between the non-dominated solutions. Using these three solutions, seakeeping tests of a fully appended scale model have been carried out at the towing tank of the University of Trieste. A single hull has been tested for each bulb configurations to check the influence of the bulb shape on the performance of the yacht in waves. The results obtained are very satisfactory, and the procedure described can be applied to even more complex yacht design problems.|Paolo Geremia,Mauro Poian,Silvia Poles","58156|GECCO|2007|A genetic algorithm with exon shuffling crossover for hard bin packing problems|A novel evolutionary approach for the bin packing problem (BPP) is presented. A simple steady-state genetic algorithm is developed that produces results comparable to other approaches in the literature, without the need for any additional heuristics. The algorithm's design makes maximum use of the principle of natural selection to evolve valid solutions without the explicit need to verify constraint violations. Our algorithm is based upon a biologically inspired group encoding which allows for a modularisation of the search space in which individual sub-solutions may be assigned independent cost values. These values are subsequently utilised in a crossover event modelled on the theory of exon shuffling to produce a single offspring that inherits the most promising segments from its parents. The algorithm is tested on a set of hard benchmark problems and the results indicate that the method has a very high degree of accuracy and reliability compared to other approaches in the literature.|Philipp Rohlfshagen,John A. Bullinaria","58077|GECCO|2007|Honey bee foraging algorithm for multimodal  dynamic optimization problems|We present a new swarm based algorithm called Honey Bee Foraging (HBF). This algorithm is modeled after the food foraging behavior of the honey bees and performs a swarm based collective foraging for fitness in promising neighborhoods in combination with individual scouting searches in other areas. The strength of the algorithm lies in its continuous monitoring of the whole scouting and foraging process with dynamic relocation of the bees if more promising regions are found. The algorithm has the potential to be useful for optimization problems of multi-modal and dynamic nature.|Abdul Rauf Baig,M. Rashid"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","57904|GECCO|2007|Association rule mining for continuous attributes using genetic network programming|Most association rule mining algorithms make use of discretization algorithms for handling continuous attributes. However, by means of methods of discretization, it is difficult to get highest attribute interdependency and at the same time to get lowest number of intervals. We propose a method using a new graph-based evolutionary algorithm named \"Genetic Network Programming (GNP)\" that can deal with continues values directly, that is, without using any discretization method as a preprocessing step. GNP is one of the evolutionary optimization techniques, which uses directed graph structures as solutions and is composed of three kinds of nodes start node, judgment node and processing node. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the judgment and connection from the current activated node. The features of GNP are described as follows. First, it is possible to reuse nodes because of this, the structure is compact. Second, GNP can find solutions of problems without bloat, which can be sometimes found in Genetic Programming (GP), because of the fixed number of nodes in GNP. Third, nodes that are not used at the current program executions will be used for future evolution. Fourth, GNP is able to cope with partially observable Markov processes. In this paper, we propose a method that can deal with continuous attributes, where attributes in databases correspond to judgment nodes in GNP and each continuous attribute is checked whether its value is greater than a threshold value and the association rules are represented as the connections of the judgment nodes. Threshold ai is firstly determined by calculating the mean i and standard deviation si of all attribute values of Ai. Then, initial threshold ai is selected randomly between the interval i - aisi, i + aisi where ai is a parameter to determine the range of the interval. Once the threshold ai is selected for all attributes, each value of the attribute Ai is checked if it is greater than the threshold ai in the judgment nodes of the proposed method. In addition to that, the threshold ai is also evolved by mutation between i - aisi, i + aisi in every generation in order to obtain as many association rules as possible. The features of the proposed method are as follows compared with other methods ) Extracts rules without identifying frequent itemsets used in Apriori-like mining methods. ) Stores extracted important association rules in a pool all together through generations. ) Measures the significance of associations via the chi-squared test. ) Extracts important rules sufficient enough for user's purpose in a short time. ) The pool is updated in every generation and only important association rules with higher chi-squared value are stored when the identical rules are stored. We have evaluated the proposed method by doing two simulations. Simulation  uses fixed threshold values that is, they remain fixed at initial thresholds during evolution. In simulation ,thresholds are evolved by mutation in every generation. Fig.  shows the number of rules extracted in the pool in simulation . It is found that the number of rules extracted has been increased, which means simulation  outperforms simulation .|Karla Taboada,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","58034|GECCO|2007|Clustering gene expression data via mining ensembles of classification rules evolved using moses|A novel approach, model-based clustering, is described foridentifying complex interactions between genes or gene-categories based on static gene expression data. The approach deals with categorical data, which consists of a set of gene expressionprofiles belonging to one category, and a set belonging to anothercategory. An evolutionary algorithm (Meta-Optimizing Semantic Evolutionary Search, or MOSES) is used to learn an ensemble of classification models distinguishing the two categories, based on inputs that are features corresponding to gene expression values. Each feature is associated with a model-based vector, which encodes quantitative information regarding the utilization of the feature across the ensembles of models. Two different ways of constructing these vectors are explored. These model-based vectors are then clustered using a variant of hierarchical clustering called Omniclust. The result is a set of model-based clusters, in which features are gathered together if they are often considered together by classification models -- which may be because they're co-expressed, or may be for subtler reasons involving multi-gene interactions. The method is illustrated by applying it to two datasets regarding human gene expression, one drawn from brain cells and pertinent to the neurogenetics of aging, and the other drawn from blood cells and relating to differentiating between types of lymphoma. We find that, compared to traditional expression-based clustering, the new method often yields clusters that have higher mathematical quality (in the sense of homogeneity and separation) and also yield novel and meaningful insights into the underlying biological processes.|Moshe Looks,Ben Goertzel,Lúcio de Souza Coelho,Mauricio Mudado,Cassio Pennachin","57884|GECCO|2007|Genetic network programming with parallel processing for association rule mining in large and dense databases|Several methods of extracting association rules have been reported. A new evolutionary computation method named Genetic Network Programming (GNP) has also been developed recently and its efectiveness is shown for small datasets. However, it has not been tested for large datasets, particularly in datasets with a large number of attributes. The aim of this paper is to extract association rules from large and dense datasets using GNP considering a real world database with a huge number of attributes. We propose a new method where a large database is divided into many small datasets, then each GNP deals with one dataset having attributes with appropiate size, which was selected randomly from a large dataset and generated genetically. These GNPs are processed in parallel. We then propose some new genetic operations to improve the number of rules extracted and their quality as well. The proposed method improves remarkably on simulations. Fig.  shows the architecture of the proposed method. We use the CLIENTSERVER model. CLIENT side carries out preprocessing of large database, assignment of files to each server, rule checking, and genetic operations on files. SERVER side carries out processing of each file using conventional GNP based mining method independently. The features and advantages of the proposed method are the following Rule extraction is done in parallel. Each file generates its local pool of the rules. Files or datasets are treated as individuals in order to do new genetic operations over them and improve the rule extraction. Extracted rules are stored in a global pool. The rules are verified to avoid redundancy among them and it is assured that only new rules are stored.|Eloy Gonzales,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","16613|IJCAI|2007|Learning by Analogy A Classification Rule for Binary and Nominal Data|This paper deals with learning to classify by using an approximation of the analogical proportion between four objects. These objects are described by binary and nominal attributes. Firstly, the paper recalls what is an analogical proportion between four objects, then it introduces a measure called \"analogical dissimilarity\", reflecting how close four objects are from being in an analogical proportion. Secondly, it presents an analogical instance-based learning method and describes a fast algorithm. Thirdly, a technique to assign a set of weights to the attributes of the objects is given a weight is chosen according to the type of the analogical proportion involved. The weights are obtained from the learning sample. Then, some results of the method are presented. They compare favorably to standard classification techniques on six benchmarks. Finally, the relevance and complexity of the method are discussed.|Sabri Bayoudh,Laurent Miclet,Arnaud Delhay","80822|VLDB|2007|Security in Outsourcing of Association Rule Mining|Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.|Wai Kit Wong,David W. Cheung,Edward Hung,Ben Kao,Nikos Mamoulis","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","57934|GECCO|2007|Evolutionary selection of minimum number of features for classification of gene expression data using genetic algorithms|Selecting the most relevant factors from genetic profiles that can optimally characterize cellular states is of crucial importance in identifying complex disease genes and biomarkers for disease diagnosis and assessing drug efficiency. In this paper, we present an approach using a genetic algorithm for a feature subset selection problem that can be used in selecting the near optimum set of genes for classification of cancer data. In substantial improvement over existing methods, we classified cancer data with high accuracy with less features.|Alper Küçükural,Reyyan Yeniterzi,Süveyda Yeniterzi,Osman Ugur Sezerman"],["16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","16593|IJCAI|2007|A Fully Connectionist Model Generator for Covered First-Order Logic Programs|We present a fully connectionist system for the learning of first-order logic programs and the generation of corresponding models Given a program and a set of training examples, we embed the associated semantic operator into a feed-forward network and train the network using the examples. This results in the learning of first-order knowledge while damaged or noisy data is handled gracefully.|Sebastian Bader,Pascal Hitzler,Steffen Hölldobler,Andreas Witzel","16786|IJCAI|2007|An Extension to Conformant Planning Using Logic Programming|In this paper we extend the logic programming based conformant planner described in Son et al., a to allow it to work on planning problems with more complex descriptions of the initial states. We also compare the extended planner with other concurrent conformant planners.|A. Ricardo Morales,Phan Huy Tu,Tran Cao Son","16542|IJCAI|2007|Using the Probabilistic Logic Programming Language P-log for Causal and Counterfactual Reasoning and Non-Naive Conditioning|P-log is a probabilistic logic programming language, which combines both logic programming style knowledge representation and probabilistic reasoning. In earlier papers various advantages of P-log have been discussed. In this paper we further elaborate on the KR prowess of P-log by showing that (i) it can be used for causal and counterfactual reasoning and (ii) it provides an elaboration tolerant way for non-naive conditioning.|Chitta Baral,Matt Hunsaker","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","16367|IJCAI|2007|A Fast Analytical Algorithm for Solving Markov Decision Processes with Real-Valued Resources|Agents often have to construct plans that obey deadlines or, more generally, resource limits for real-valued resources whose consumption can only be characterized by probability distributions, such as execution time or battery power. These planning problems can be modeled with continuous state Markov decision processes (MDPs) but existing solution methods are either inefficient or provide no guarantee on the quality of the resulting policy. We therefore present CPH, a novel solution method that solves the planning problems by first approximating with any desired accuracy the probability distributions over the resource consumptions with phasetype distributions, which use exponential distributions as building blocks. It then uses value iteration to solve the resulting MDPs by exploiting properties of exponential distributions to calculate the necessary convolutions accurately and efficiently while providing strong guarantees on the quality of the resulting policy. Our experimental feasibility study in a Mars rover domain demonstrates a substantial speedup over Lazy Approximation, which is currently the leading algorithm for solving continuous state MDPs with quality guarantees.|Janusz Marecki,Sven Koenig,Milind Tambe","16759|IJCAI|2007|Using Linear Programming for Bayesian Exploration in Markov Decision Processes|A key problem in reinforcement learning is finding a good balance between the need to explore the environment and the need to gain rewards by exploiting existing knowledge. Much research has been devoted to this topic, and many of the proposed methods are aimed simply at ensuring that enough samples are gathered to estimate well the value function. In contrast, Bellman and Kalaba,  proposed constructing a representation in which the states of the original system are paired with knowledge about the current model. Hence, knowledge about the possible Markov models of the environment is represented and maintained explicitly. Unfortunately, this approach is intractable except for bandit problems (where it gives rise to Gittins indices, an optimal exploration method). In this paper, we explore ideas for making this method computationally tractable. We maintain a model of the environment as a Markov Decision Process. We sample finite-length trajectories from the infinite tree using ideas based on sparse sampling. Finding the values of the nodes of this sparse subtree can then be expressed as an optimization problem, which we solve using Linear Programming. We illustrate this approach on a few domains and compare it with other exploration algorithms.|Pablo Samuel Castro,Doina Precup","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman"],["58105|GECCO|2007|Modeling selection pressure in XCS for proportionate and tournament selection|In this paper, we derive models of the selection pressure in XCS for proportionate (roulette wheel) selection and tournament selection. We show that these models can explain the empirical results that have been previously presented in the literature. We validate the models on simple problems showing that, (i) when the model assumptions hold, the theory perfectly matches the empirical evidence (ii) when the model assumptions do not hold, the theory can still provide qualitative explanations of the experimental results.|Albert Orriols-Puig,Kumara Sastry,Pier Luca Lanzi,David E. Goldberg,Ester Bernadó-Mansilla","16534|IJCAI|2007|An Efficient Protocol for Negotiation over Multiple Indivisible Resources|We study the problem of autonomous agents negotiating the allocation of multiple indivisible resources. It is difficult to reach optimal outcomes in bilateral or multi-lateral negotiations over multiple resources when the agents' preferences for the resources are not common knowledge. Self-interested agents often end up negotiating inefficient agreements in such situations. We present a protocol for negotiation over multiple indivisible resources which can be used by rational agents to reach efficient outcomes. Our proposed protocol enables the negotiating agents to identify efficient solutions using systematic distributed search that visits only a subspace of the whole solution space.|Sabyasachi Saha,Sandip Sen","16495|IJCAI|2007|Graph Decomposition for Efficient Multi-Robot Path Planning|In my previous paper (Ryan, ) I introduced the concept of subgraph decomposition as a means of reducing the search space in multi-robot planning problems. I showed how partitioning a roadmap into subgraphs of known structure allows us to first plan at a level of abstraction and then resolve these plans into concrete paths without the need for further search so we can solve significantly harder planning tasks with the same resources. However the subgraph types I introduced in that paper, stacks and cliques, are not likely to occur often in realistic planning problems and so are of limited usefulness. In this paper I describe a new kind of subgraph called a hall, which can also be used for planning and which occurs much more commonly in real problems. I explain its formal properties as a planning component and demonstrate its use on a map of the Patrick's container yard at the Port of Brisbane in Queensland Australia.|Malcolm R. K. Ryan","16487|IJCAI|2007|A New Approach for Stereo Matching in Autonomous Mobile Robot Applications|We propose a new approach for stereo matching in Autonomous Mobile Robot applications. In this framework an accurate but slow reconstruction of the D scene is not needed rather, it is more important to have a fast localization of the obstacles to avoid them. All the methods in the literature are based on a punctual correspondence, but they are inefficient in realistic contexts for the presence of uniform patterns, or some perturbations between the two images of the stereo pair. Our idea is to face the stereo matching problem as a matching between homologous regions, instead of a point matching. The stereo images are represented as graphs and a graph matching is computed to find homologous regions. We present some results on a standard stereo database and also on a more realistic stereo sequence acquired from a robot moving in an indoor environment, and a performance comparison with other approaches in the literature is reported and discussed. Our method is strongly robust in case of some fluctuations of the stereo pair, homogeneous and repetitive regions, and is fast. The result is a semi-dense disparity map, leaving only a few regions in the scene unmatched.|Pasquale Foggia,Jean-Michel Jolion,Alessandro Limongiello,Mario Vento","16612|IJCAI|2007|Efficient Planning of Informative Paths for Multiple Robots|In many sensing applications, including environmental monitoring, measurement systems must cover a large space with only limited sensing resources. One approach to achieve required sensing coverage is to use robots to convey sensors within this space. Planning the motion of these robots - coordinating their paths in order to maximize the amount of information collected while placing bounds on their resources (e.g., path length or energy capacity) - is aNP-hard problem. In this paper, we present an efficient path planning algorithm that coordinates multiple robots, each having a resource constraint, to maximize the \"informativeness\" of their visited locations. In particular, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to characterize the amount of information collected. We provide strong theoretical approximation guarantees for our algorithm by exploiting the submodularity property of mutual information. In addition, we improve the efficiency of our approach by extending the algorithm using branch and bound and a region-based decomposition of the space. We provide an extensive empirical analysis of our algorithm, comparing with existing heuristics on datasets from several real world sensing applications.|Amarjeet Singh 0003,Andreas Krause,Carlos Guestrin,William J. Kaiser,Maxim A. Batalin","80734|VLDB|2007|Example-driven design of efficient record matching queries|Record matching is the task of identifying records that match the same real world entity. This is a problem of great significance for a variety of business intelligence applications. Implementations of record matching rely on exact as well as approximate string matching (e.g., edit distances) and use of external reference data sources. Record matching can be viewed as a query composed of a small set of primitive operators. However, formulating such record matching queries is difficult and depends on the specific application scenario. Specifically, the number of options both in terms of string matching operations as well as the choice of external sources can be daunting. In this paper, we exploit the availability of positive and negative examples to search through this space and suggest an initial record matching query. Such queries can be subsequently modified by the programmer as needed. We ensure that the record matching queries our approach produces are () efficient these queries can be run on large datasets by leveraging operations that are well-supported by RDBMSs, and () explainable the queries are easy to understand so that they may be modified by the programmer with relative ease. We demonstrate the effectiveness of our approach on several real-world datasets.|Surajit Chaudhuri,Bee-Chung Chen,Venkatesh Ganti,Raghav Kaushik","16581|IJCAI|2007|Using a Mobile Robot for Cognitive Mapping|When animals (including humans) first explore a new environment, what they remember is fragmentary knowledge about the places visited. Yet, they have to use such fragmentary knowledge to find their way home. Humans naturally use more powerful heuristics while lower animals have shown to develop a variety of methods that tend to utilize two key pieces of information, namely distance and orientation information. Their methods differ depending on how they sense their environment. Could a mobile robot be used to investigate the nature of such a process, commonly referred to in the psychological literature as cognitive mapping What might be computed in the initial explorations and how is the resulting \"cognitive map\" be used to return home In this paper, we presented a novel approach using a mobile robot to do cognitive mapping. Our robot computes a \"cognitive map\" and uses distance and orientation information to find its way home. The process developed provides interesting insights into the nature of cognitive mapping and encourages us to use a mobile robot to do cognitive mapping in the future, as opposed to its popular use in robot mapping.|Chee K. Wong,Jochen Schmidt,Wai K. Yeap","16349|IJCAI|2007|A Subspace Kernel for Nonlinear Feature Extraction|Kernel based nonlinear Feature Extraction (KFE) or dimensionality reduction is a widely used preprocessing step in pattern classification and data mining tasks. Given a positive definite kernel function, it is well known that the input data are implicitly mapped to a feature space with usually very high dimensionality. The goal of KFE is to find a low dimensional subspace of this feature space, which retains most of the information needed for classification or data analysis. In this paper, we propose a subspace kernel based on which the feature extraction problem is transformed to a kernel parameter learning problem. The key observation is that when projecting data into a low dimensional subspace of the feature space, the parameters that are used for describing this subspace can be regarded as the parameters of the kernel function between the projected data. Therefore current kernel parameter learning methods can be adapted to optimize this parameterized kernel function. Experimental results are provided to validate the effectiveness of the proposed approach.|Mingrui Wu,Jason D. R. Farquhar","80845|VLDB|2007|On Efficient Spatial Matching|This paper proposes and solves a new problem called spatial matching (SPM). Let P and O be two sets of objects in an arbitrary metric space, where object distances are defined according to a norm satisfying the triangle inequality. Each object in O represents a customer, and each object in P indicates a service provider, which has a capacity corresponding to the maximum number of customers that can be supported by the provider. SPM assigns each customer to herhis nearest provider, among all the providers whose capacities have not been exhausted in serving other closer customers. We elaborate the applications where SPM is useful, and develop algorithms that settle this problem with a linear number O(P + O) of nearest neighbor queries. We verify our theoretical findings with extensive experiments, and show that the proposed solutions outperform alternative methods by a factor of orders of magnitude.|Raymond Chi-Wing Wong,Yufei Tao,Ada Wai-Chee Fu,Xiaokui Xiao","16386|IJCAI|2007|Feature Selection and Kernel Design via Linear Programming|The definition of object (e.g., data point) similarity is critical to the performance of many machine learning algorithms, both in terms of accuracy and computational efficiency. However, it is often the case that a similarity function is unknown or chosen by hand. This paper introduces a formulation that given relative similarity comparisons among triples of points of the form object i is more like object j than object k, it constructs a kernel function that preserves the given relationships. Our approach is based on learning a kernel that is a combination of functions taken from a set of base functions (these could be kernels as well). The formulation is based on defining an optimization problem that can be solved using linear programming instead of a semidefinite program usually required for kernel learning. We show how to construct a convex problem from the given set of similarity comparisons and then arrive to a linear programming formulation by employing a subset of the positive definite matrices. We extend this formulation to consider representationevaluation efficiency based on formulating a novel form of feature selection using kernels (that is not much more expensive to solve). Using publicly available data, we experimentally demonstrate how the formulation introduced in this paper shows excellent performance in practice by comparing it with a baseline method and a related state-of-the art approach, in addition of being much more efficient computationally.|Glenn Fung,Rómer Rosales,R. Bharat Rao"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","16660|IJCAI|2007|Selective Supervision Guiding Supervised Learning with Decision-Theoretic Active Learning|An inescapable bottleneck with learning from large data sets is the high cost of labeling training data. Unsupervised learning methods have promised to lower the cost of tagging by leveraging notions of similarity among data points to assign tags. However, unsupervised and semi-supervised learning techniques often provide poor results due to errors in estimation. We look at methods that guide the allocation of human effort for labeling data so as to get the greatest boosts in discriminatory power with increasing amounts of work. We focus on the application of value of information to Gaussian Process classifiers and explore the effectiveness of the method on the task of classifying voice messages.|Ashish Kapoor,Eric Horvitz,Sumit Basu","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","16661|IJCAI|2007|An Experts Algorithm for Transfer Learning|A long-lived agent continually faces new tasks in its environment. Such an agent may be able to use knowledge learned in solving earlier tasks to produce candidate policies for its current task. There may, however, be multiple reasonable policies suggested by prior experience, and the agent must choose between them potentially without any a priori knowledge about their applicability to its current situation. We present an \"experts\" algorithm for efficiently choosing amongst candidate policies in solving an unknown Markov decision process task. We conclude with the results of experiments on two domains in which we generate candidate policies from solutions to related tasks and use our experts algorithm to choose amongst them.|Erik Talvitie,Satinder Singh","16494|IJCAI|2007|Building Portable Options Skill Transfer in Reinforcement Learning|The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.|George Konidaris,Andrew G. Barto","16376|IJCAI|2007|Semi-Supervised Learning for Multi-Component Data Classification|This paper presents a method for designing a semisupervised classifier for multi-component data such as web pages consisting of text and link information. The proposed method is based on a hybrid of generative and discriminative approaches to take advantage of both approaches. With our hybrid approach, for each component, we consider an individual generative model trained on labeled samples and a model introduced to reduce the effect of the bias that results when there are few labeled samples. Then, we construct a hybrid classifier by combining all the models based on the maximum entropy principle. In our experimental results using three test collections such as web pages and technical papers, we confirmed that our hybrid approach was effective in improving the generalization performance of multi-component data classification.|Akinori Fujino,Naonori Ueda,Kazumi Saito","58158|GECCO|2007|Learning noise|In this paper we propose a genetic programming approach to learning stochastic models with unsymmetrical noise distributions. Most learning algorithms try to learn from noisy data by modeling the maximum likelihood output or least squared error, assuming that noise effects average out. While this process works well for data with symmetrical noise distributions (such as Gaussian observation noise), many real-life sources of noise are not symmetrically distributed, thus this approach does not hold. We suggest improved learning can be obtained by including noise sources explicitly in the model as a stochastic element. A stochastic element is a random sub-process or latent variable of a hidden system that can propagate nonlinear noise to the observable outputs. Stochastic elements can skew and distort output features making regression of analytical models particularly difficult and error minimizing approaches inhibiting. We introduce a new method to infer the analytical model of a system by decomposing non-uniform noise observed at the outputs into uniform stochastic elements appearing symbolically inside the system. Results demonstrate the ability to regress exact analytical models where stochastic elements are embedded inside nonlinear and polynomial hidden systems.|Michael D. Schmidt,Hod Lipson","16732|IJCAI|2007|Utile Distinctions for Relational Reinforcement Learning|We introduce an approach to autonomously creating state space abstractions for an online reinforcement learning agent using a relational representation. Our approach uses a tree-based function approximation derived from McCallum's  UTree algorithm. We have extended this approach to use a relational representation where relational observations are represented by attributed graphs McGovern et al., . We address the challenges introduced by a relational representation by using stochastic sampling to manage the search space Srinivasan,  and temporal sampling to manage autocorrelation Jensen and Neville, . Relational UTree incorporates Iterative Tree Induction Utgoff et al.,  to allow it to adapt to changing environments. We empirically demonstrate that Relational UTree performs better than similar relational learning methods Finney et al.,  Driessens et al.,  in a blocks world domain. We also demonstrate that Relational UTree can learn to play a sub-task of the game of Go called Tsume-Go Ramon et al., .|William Dabney,Amy McGovern","16676|IJCAI|2007|Semi-Supervised Learning of Attribute-Value Pairs from Product Descriptions|We describe an approach to extract attribute-value pairs from product descriptions. This allows us to represent products as sets of such attribute-value pairs to augment product databases. Such a representation is useful for a variety of tasks where treating a product as a set of attribute-value pairs is more useful than as an atomic entity. Examples of such applications include product recommendations, product comparison, and demand forecasting. We formulate the extraction as a classification problem and use a semi-supervised algorithm (co-EM) along with (Nave Bayes). The extraction system requires very little initial user supervision using unlabeled data, we automatically extract an initial seed list that serves as training data for the supervised and semi-supervised classification algorithms. Finally, the extracted attributes and values are linked to form pairs using dependency information and co-location scores. We present promising results on product descriptions in two categories of sporting goods.|Katharina Probst,Rayid Ghani,Marko Krema,Andrew E. Fano,Yan Liu 0002"],["58086|GECCO|2007|A hybrid evolutionary programming algorithm for spread spectrum radar polyphase codes design|This paper presents a hybrid evolutionary programming algorithm to solve the spread spectrum radar polyphase code design problem. The proposed algorithm uses an Evolutionary Programming (EP) approach as global search heuristic. This EP is hybridized with a gradient-based local search procedure which includes a dynamic step adaptation procedure to perform accurate and efficient local search for better solutions. Numerical examples demonstrate that the algorithm outperforms existing approaches for this problem.|?ngel M. Pérez-Bellido,Sancho Salcedo-Sanz,Emilio G. Ortíz-García,Antonio Portilla-Figueras","58031|GECCO|2007|Option pricing model calibration using a real-valued quantum-inspired evolutionary algorithm|Quantum effects are a natural phenomenon and just like evolution, or immune processes, can serve as an inspiration for the design of computing algorithms. This study illustrates how a real-valued quantum-inspired evolutionary algorithm(QEA) can be constructed and examines the utility of the resulting algorithm on an important real-world problem, namely the calibration of an Option Pricing model. The results from the algorithm are shown to be robust and sensitivity analysis is carried out on the algorithm parameters, suggesting that there is useful potential to apply QEA to this domain.|Kai Fan,Anthony Brabazon,Conall O'Sullivan,Michael O'Neill","58092|GECCO|2007|Guided hyperplane evolutionary algorithm|A new evolutionary technique for multicriteria optimization called Guiding Hyper-plane Evolutionary Algorithm (GHEA) is proposed. The originality of the approach consists in the fact that the fitness assignment is realized by using a guiding hyperplane and a new non Pareto optimality concept. Numerical experiments illustrate the performance of GHEA compared with the popular NSGA-II and SPEA.|Corina Rotar,D. Dumitrescu,Rodica Ioana Lung","58004|GECCO|2007|Dimensionality reduction in evolutionary multiobjective design case study|Real-world applications of Pareto-based optimisation commonly involve many objectives. It causes difficulties because of reduced selection pressure for better solutions. Dimensionality Reduction (DR) is a very appealing approach to overcome this problem. A case study of multiobjective Electric Machine (EM) design based on DR of the novel model  is considered.|Piotr Wozniak","57984|GECCO|2007|Discrimination of metabolic flux profiles using a hybrid evolutionary algorithm|Studying metabolic fluxes is a crucial aspect of understanding biological phenotypes. However, it is often not possible to measure these fluxes directly. As an alternative, fluxome profiling provides indirect information about fluxes in a high-throughput setting. In this paper, we consider a scenario where fluxome profiling is used to investigate characteristic differences between a number of bacterial mutant strains. The goal is to identify groups of mutants that show maximally different fluxome profiles. We propose an evolutionary algorithm for this optimization problem and demonstrate that it outperforms alternative methods based on principle component analysis and independent component analysis on both real and synthetic data sets.|Stefan Bleuler,Eckart Zitzler","58244|GECCO|2007|A self-adaptive multiagent evolutionary algorithm for electrical machine design|This paper presents a self-adaptive algorithm that hybridises evolutionary and multiagent concepts. Each evolutionary individual is implemented as a simple agent capable of re-production and predation. The transitions between these two states depend on the agent's local environment. Thus, no explicit global process is defined to select neither the mates nor the preys. The convergence of the algorithm emerges from the behaviour of the agents. This brings interesting properties, such as population size self-regulation. Two sets of experimental results are provided a comparison with Saw-Tooth Algorithm and micro-GA using four classical functions and an optimisation of the efficiency and the weight of an electrical motor. Some possible evolutions and prospects are finally proposed.|Jean-Laurent Hippolyte,Christelle Bloch,Pascal Chatonnay,Christophe Espanet,Didier Chamagne","58088|GECCO|2007|Carbon-friendly travel plan construction using an evolutionary algorithm|This paper discusses the use of an evolutionary algorithm to design workplace travel plans, to promote of car sharing and reduce carbon emissions from single-occupancy motor vehicles.|Neil Urquhart","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang","16516|IJCAI|2007|Relevance Estimation and Value Calibration of Evolutionary Algorithm Parameters|The main objective of this paper is to present and evaluate a method that helps to calibrate the parameters of an evolutionary algorithm in a systematic and semi-automated manner. The method for Relevance Estimation and Value Calibration of EA parameters (REVAC) is empirically evaluated in two different ways. First, we use abstract test cases reflecting the typical properties of EA parameter spaces. Here we observe that REVAC is able to approximate the exact (hand-coded) relevance of parameters and it works robustly with measurement noise that is highly variable and not normally distributed. Second, we use REVAC for calibrating GAs for a number of common objective functions. Here we obtain a common sense validation, REVAC finds mutation rate pm much more sensitive than crossover rate pc and it recommends intuitively sound values pm between . and ., and .  pc  ..|Volker Nannen,A. E. Eiben","58167|GECCO|2007|Disburdening the species conservation evolutionary algorithm of arguing with radii|The present paper investigates the hybridization of two well-known multimodal optimization methods, i.e. species conservation and multinational algorithms. The topological species conservation algorithm embraces the vision of the existence of subpopulations around seeds (the best local individuals) and the preservation of these dominating individuals from one generation to another, but detects multimodality by means of the hill-valley mechanism employed by multinational algorithms. The aim is to inherit the strengths of both parent techniques and at the same time overcome their flaws. The species conservation algorithm efficiently keeps track of several good search space regions at once, but is difficult to parametrize without prior problem knowledge. Conversely, the multinational algorithms use many functionevaluations to establish subpopulations, but do not depend onprovided radius parameter values. Experiments with all threealgorithms are made on a wide range of test problems in order toinvestigate their advantages and shortcomings.|Catalin Stoean,Mike Preuss,Ruxandra Stoean,Dumitru Dumitrescu"],["57879|GECCO|2007|Adopting dynamic operators in a genetic algorithm|Genetic Algorithms have been used to solve difficult optimization problems in a number of fields. However, in order to solve a problem with GA, the user has to specify a number of parameters.allThis parameter tuning is a difficult task as different genetic operators are suitable in different application areas. This paper proposes a scheme for genetic algorithms where the genetic operators are changed randomly. The information of gender and age is also incorporated in this approach to maintain population diversity. The experimental result of the proposed algorithm based on a mechanical design problem shows promising result.|Khadiza Tahera,Raafat N. Ibrahim,Paul B. Lochert","58076|GECCO|2007|On the scalability of evolution strategies in the optimization of dynamic molecular alignment|We consider the numerical optimization of dynamic molecular alignment by shaped femtosecond pulses, and study the scalability of the electric field subject to optimization by Evolution Strategies.The trade-off between fine-tuning of the electric field versus the evolutionary optimization feasibility is investigated.|Ofer M. Shir,Thomas Bäck,Marc J. J. Vrakking","58000|GECCO|2007|Dynamic populations and length evolution key factors for analyzing fault tolerance on parallel genetic programming|This paper presents an experimental research on the size of individuals when fixed and dynamic size populationsare employed with Genetic Programming (GP). We propose an improvement to the Plague operator (PO), that we have called Random Plague (RPO). Then by further studies based on the RPO results we analyzed the Fault Tolerance onParallel Genetic Programming.|Daniel Lombraña Gonzalez,Francisco Fernández de Vega","57985|GECCO|2007|Differential evolution and non-separability using selective pressure to focus search|Recent results show that the Differential Evolution algorithm has significant difficulty on functions that are not linearly separable. On such functions, the algorithm must rely primarily on its differential mutation procedure which, unlike its recombination strategy, is rotationally invariant. We conjecture that this mutation strategy lacks sufficient selective pressure when appointing parent and donor vectors to have satisfactory exploitative power on non-separable functions. We find that imposing pressure in the form of rank-based differential mutation results in a significant improvement of exploitation on rotated benchmarks.|Andrew M. Sutton,Monte Lunacek,L. Darrell Whitley","58024|GECCO|2007|A discrete differential evolution algorithm for the permutation flowshop scheduling problem|In this paper, a novel discrete differential evolution (DDE) algorithm is presented to solve the permutation flowhop scheduling problem with the makespan criterion. The DDE algorithm is simple in nature such that it first mutates a target population to produce the mutant population. Then the target population is recombined with the mutant population in order to generate a trial population. Finally, a selection operator is applied to both target and trial populations to determine who will survive for the next generation based on fitness evaluations. As a mutation operator in the discrete differential evolution algorithm, a destruction and construction procedure is employed to generate the mutant population. We propose a referenced local search, which is embedded in the discrete differential evolution algorithm to further improve the solution quality. Computational results show that the proposed DDE algorithm with the referenced local search is very competitive to the iterated greedy algorithm which is one of the best performing algorithms for the permutation flowshop scheduling problem in the literature.|Quan-Qe Pan,Mehmet Fatih Tasgetiren,Yun-Chia Liang","57877|GECCO|2007|An extended mutation concept for the local selection based differential evolution algorithm|A new mutation concept is proposed to generalize local selection based Differential Evolution algorithm to work in general multi-modal problems. Three variations of the proposed method are compared with classic Differential Evolution algorithm using a set of five well known test functions and their variants. The general idea of the new mutation operation is to divide the mutation into two parts the local and global mutation. The global mutation works as a migration operator allowing the algorithm perform global search efficiently, while the local mutation improves the efficiency of local search. The results show that the concept of global mutation is able to generalize the good performance of local selection based Differential Evolution from convex uni-modal functions to general non-convex and multi-modal problems. Among the tested functions, the new method was able to outperform the classic Differential Evolution in all butone. A limited analysis of the effects of control parameters to the performance of the algorithm is also done.|Jani Rönkkönen,Jouni Lampinen","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","58061|GECCO|2007|Genetic evolution of hierarchical behavior structures|The development of coherent and dynamic behaviors for mobile robots is an exceedingly complex endeavor ruled by task objectives, environmental dynamics and the interactions within the behavior structure. This paper discusses the use of genetic programming techniques and the unified behavior framework to develop effective control hierarchies using interchangeable behaviors and arbitration components. Given the number of possible variations provided by the framework, evolutionary programming is used to evolve the overall behavior design. Competitive evolution of the behavior population incrementally develops feasible solutions for the domain through competitive ranking. By developing and implementing many simple behaviors independently and then evolving a complex behavior structure suited to the domain, this approach allows for the reuse of elemental behaviors and eases the complexity of development for a given domain. Additionally, this approach has the ability to locate a behavior structure which a developer may not have previously considered, and whose ability exceeds expectations. The evolution of the behavior structure is demonstrated using agents in the Robocode environment, with the evolved structures performing up to  percent better than one crafted by an expert.|Brian G. Woolley,Gilbert L. Peterson","16437|IJCAI|2007|GUNSAT A Greedy Local Search Algorithm for Unsatisfiability|Local search algorithms for satisfiability testing are still the best methods for a large number of problems, despite tremendous progresses observed on complete search algorithms over the last few years. However, their intrinsic limit does not allow them to address UNSAT problems. Ten years ago, this question challenged the community without any answer was it possible to use local search algorithm for UNSAT formulae We propose here a first approach addressing this issue, that can beat the best resolution-based completemethods. We define the landscape of the search by approximating the number of filtered clauses by resolution proof. Furthermore, we add high-level reasoning mechanism, based on Extended Resolution and Unit Propagation Look-Ahead to make this new and challenging approach possible. Our new algorithm also tends to be the first step on two other challenging problems obtaining short proofs for UNSAT problems and build a real local-search algorithm for QBF.|Gilles Audemard,Laurent Simon","58078|GECCO|2007|TFBS identification by position- and consensus-led genetic algorithm with local filtering|Identification of Transcription Factor Binding Site (TFBS) motifs in multiple DNA upstream sequences is important in understanding the mechanism of gene regulation. This identification problem is challenging because such motifs are usually weakly conserved due to evolutionary variation. Exhaustive search is intractable for finding long motifs because the combinatorial growth of the search space is exponential, thus heuristic methods are preferred. In this paper, we propose the Genetic Algorithm with Local Filtering (GALF) to address the problem, which combines and utilizes both position-led and consensus-led representations in present GA approaches. While position-led representation provides flexibility to move around the search space, it is likely to contain some \"false positive\" sites within an individual. This problem can be overcome by our local filtering operator, which employs consensus-led representation, while it needs less computation than alignments used in conventional consensus-led approaches. Thus both efficiency and accuracy can be achieved. The experimental results on real biological data show that our method can identify TFBSs more accurately and efficiently than other methods including GA-based ones, and is able to deal with relaxed motif widths with superior correctness.|Tak-Ming Chan,Kwong-Sak Leung,Kin-Hong Lee"],["16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","16726|IJCAI|2007|Combining Learning and Word Sense Disambiguation for Intelligent User Profiling|Understanding user interests from text documents can provide support to personalized information recommendation services. Typically, these services automatically infer the user profile, a structured model of the user interests, from documents that were already deemed relevant by the user. Traditional keyword-based approaches are unable to capture the semantics of the user interests. This work proposes the integration of linguistic knowledge in the process of learning semantic user profiles that capture concepts concerning user interests. The proposed strategy consists of two steps. The first one is based on a word sense disambiguation technique that exploits the lexical database WordNet to select, among all the possible meanings (senses) of a polysemous word, the correct one. In the second step, a nave Bayes approach learns semantic sense-based user profiles as binary text classifiers (user-likes and user-dislikes) from disambiguated documents. Experiments have been conducted to compare the performance obtained by keyword-based profiles to that obtained by sense-based profiles. Both the classification accuracy and the effectiveness of the ranking imposed by the two different kinds of profile on the documents to be recommended have been considered. The main outcome is that the classification accuracy is increased with no improvement on the ranking. The conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those documents whose classification score is close to the likesdislikes threshold (the items for which the classification is highly uncertain).|Giovanni Semeraro,Marco Degemmis,Pasquale Lops,Pierpaolo Basile","58066|GECCO|2007|Automatic generation of benchmarks for plagiarism detection tools using grammatical evolution|Student plagiarism is a mayor problem in universities worldwide. In this paper,we focus on plagiarism in answers to computer programming assignments,where student mix andor modify one or more original solutions to obtain counterfeits. Although several software tools have been implemented to help the tedious and time consuming task of detecting plagiarism, little has been done to assess their quality, because, in fact, determining the original subset of the whole solutionset is practically impossible for graders. In this article we present a Grammatical Evolution technique which generates benchmarks. Given a programming language, our technique generates a set of original solutions to an assignment, together with a set of plagiarisms of the former set which mimic the way in which students act. The phylogeny of the coded solutions is predefined, providing a base for the evaluationof the performance of copy-catching tools. We give empirical evidence of the suitability of our approach by studying the behavior of one state-of-the-art detection tool (AC) on four benchmarks coded in APL, generated with this technique.|Manuel Cebrián,Manuel Alfonseca,Alfonso Ortega","58215|GECCO|2007|Coupling EA and high-level metrics for the automatic generation of test blocks for peripheral cores|Test of peripheral modules has not been deeply investigated by the research community. When embedded in a system on chip, however, peripherals pose accessibility problems that may make traditional test approaches ineffective. In this paper an evolutionary methodology, based upon coverage metrics at high-level, is described to automatically generate test sets for peripheral modules in a SoC. A general-purpose evolutionary tool, able to cultivate composite individuals, has been developed and isused for the test set generation. This tool is described and its basic concepts explained. The method compares favorably with results obtained by hand.|Leticia Maria Veiras Bolzani,Ernesto Sánchez,Massimiliano Schillaci,Giovanni Squillero","16701|IJCAI|2007|Fault-Model-Based Test Generation for Embedded Software|Testing embedded software systems on the control units of vehicles is a safety-relevant task, and developing the test suites for performing the tests on test benches is time-consuming. We present the foundations and results of a case study to automate the generation of tests for control software of vehicle control units based on a specification of requirements in terms of finite state machines. This case study builds upon our previous work on generation of tests for physical systems based on relational behavior models. In order to apply the respective algorithms, the finite state machine representation is transformed into a relational model. We present the transformation, the application of the test generation algorithm to a real example, and discuss the results and some specific challenges regarding software testing.|Michael Esser,Peter Struss","58081|GECCO|2007|A multi-objective approach to search-based test data generation|There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage.The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade-offs between the two simultaneous objectives.|Kiran Lakhotia,Mark Harman,Phil McMinn","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","58059|GECCO|2007|Automatic mutation test input data generation via ant colony|Fault-based testing is often advocated to overcome limitations ofother testing approaches however it is also recognized as beingexpensive. On the other hand, evolutionary algorithms have beenproved suitable for reducing the cost of data generation in the contextof coverage based testing. In this paper, we propose a newevolutionary approach based on ant colony optimization for automatictest input data generation in the context of mutation testingto reduce the cost of such a test strategy. In our approach the antcolony optimization algorithm is enhanced by a probability densityestimation technique. We compare our proposal with otherevolutionary algorithms, e.g., Genetic Algorithm. Our preliminaryresults on JAVA testbeds show that our approach performed significantlybetter than other alternatives.|Kamel Ayari,Salah Bouktif,Giuliano Antoniol","58099|GECCO|2007|Automatic analog IC layout generation based on a evolutionary computation approach|This paper describes an innovative analog IC layout generation approach based on evolutionary computation techniques.|Nuno C. Lourenço,Nuno C. G. Horta"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","57904|GECCO|2007|Association rule mining for continuous attributes using genetic network programming|Most association rule mining algorithms make use of discretization algorithms for handling continuous attributes. However, by means of methods of discretization, it is difficult to get highest attribute interdependency and at the same time to get lowest number of intervals. We propose a method using a new graph-based evolutionary algorithm named \"Genetic Network Programming (GNP)\" that can deal with continues values directly, that is, without using any discretization method as a preprocessing step. GNP is one of the evolutionary optimization techniques, which uses directed graph structures as solutions and is composed of three kinds of nodes start node, judgment node and processing node. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the judgment and connection from the current activated node. The features of GNP are described as follows. First, it is possible to reuse nodes because of this, the structure is compact. Second, GNP can find solutions of problems without bloat, which can be sometimes found in Genetic Programming (GP), because of the fixed number of nodes in GNP. Third, nodes that are not used at the current program executions will be used for future evolution. Fourth, GNP is able to cope with partially observable Markov processes. In this paper, we propose a method that can deal with continuous attributes, where attributes in databases correspond to judgment nodes in GNP and each continuous attribute is checked whether its value is greater than a threshold value and the association rules are represented as the connections of the judgment nodes. Threshold ai is firstly determined by calculating the mean i and standard deviation si of all attribute values of Ai. Then, initial threshold ai is selected randomly between the interval i - aisi, i + aisi where ai is a parameter to determine the range of the interval. Once the threshold ai is selected for all attributes, each value of the attribute Ai is checked if it is greater than the threshold ai in the judgment nodes of the proposed method. In addition to that, the threshold ai is also evolved by mutation between i - aisi, i + aisi in every generation in order to obtain as many association rules as possible. The features of the proposed method are as follows compared with other methods ) Extracts rules without identifying frequent itemsets used in Apriori-like mining methods. ) Stores extracted important association rules in a pool all together through generations. ) Measures the significance of associations via the chi-squared test. ) Extracts important rules sufficient enough for user's purpose in a short time. ) The pool is updated in every generation and only important association rules with higher chi-squared value are stored when the identical rules are stored. We have evaluated the proposed method by doing two simulations. Simulation  uses fixed threshold values that is, they remain fixed at initial thresholds during evolution. In simulation ,thresholds are evolved by mutation in every generation. Fig.  shows the number of rules extracted in the pool in simulation . It is found that the number of rules extracted has been increased, which means simulation  outperforms simulation .|Karla Taboada,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","16663|IJCAI|2007|QuantMiner A Genetic Algorithm for Mining Quantitative Association Rules|In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers \"good\" intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive data mining tool.|Ansaf Salleb-Aouissi,Christel Vrain,Cyril Nortet","58113|GECCO|2007|Best SubTree genetic programming|The result of the program encoded into a Genetic Programming(GP) tree is usually returned by the root of that tree. However, this is not a general strategy. In this paper we present and investigate a new variant where the best subtree is chosen to provide the solution of the problem. The other nodes (not belonging to the best subtree) are deleted. This will reduce the size of the chromosome in those cases where its best subtree is different from the entire tree. We have tested this strategy on a wide range of regression and classification problems. Numerical experiments have shown that the proposed approach can improve both the search speed and the quality of results.|Oana Muntean,Laura Diosan,Mihai Oltean","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57884|GECCO|2007|Genetic network programming with parallel processing for association rule mining in large and dense databases|Several methods of extracting association rules have been reported. A new evolutionary computation method named Genetic Network Programming (GNP) has also been developed recently and its efectiveness is shown for small datasets. However, it has not been tested for large datasets, particularly in datasets with a large number of attributes. The aim of this paper is to extract association rules from large and dense datasets using GNP considering a real world database with a huge number of attributes. We propose a new method where a large database is divided into many small datasets, then each GNP deals with one dataset having attributes with appropiate size, which was selected randomly from a large dataset and generated genetically. These GNPs are processed in parallel. We then propose some new genetic operations to improve the number of rules extracted and their quality as well. The proposed method improves remarkably on simulations. Fig.  shows the architecture of the proposed method. We use the CLIENTSERVER model. CLIENT side carries out preprocessing of large database, assignment of files to each server, rule checking, and genetic operations on files. SERVER side carries out processing of each file using conventional GNP based mining method independently. The features and advantages of the proposed method are the following Rule extraction is done in parallel. Each file generates its local pool of the rules. Files or datasets are treated as individuals in order to do new genetic operations over them and improve the rule extraction. Extracted rules are stored in a global pool. The rules are verified to avoid redundancy among them and it is assured that only new rules are stored.|Eloy Gonzales,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","58168|GECCO|2007|An experimental evaluation of genetic process mining|This paper aims to ascertain the optimum values for two fitness function parameters within a process mining genetic algorithm the o parameter, which reduces the likelihood of process models with extra behaviour being selected and the parameter, which restricts the selection of models containing duplicate tasks. The experiments conducted in this research also include the use of a decaying rate for the mutation operator in order to promote greater accuracy in the mined process models. The paper concludes that the optimum setting of the fitness function parameters will in fact vary depending on the constructs found in each process model. This paper finds that a higher value for one of the fitness function parameters allows for simple process constructs to be mined with greater accuracy. The use of a decaying rate of mutation is also found to be beneficial in the correct mining of simple processe.|Chris J. Turner,Ashutosh Tiwari","80822|VLDB|2007|Security in Outsourcing of Association Rule Mining|Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.|Wai Kit Wong,David W. Cheung,Edward Hung,Ben Kao,Nikos Mamoulis","58106|GECCO|2007|Linear genetic programming of metaheuristics|We suggest a flavour of linear Genetic Programming indomain-specific languages that acts as a hyperheuristic (HH).|Robert E. Keller,Riccardo Poli","58144|GECCO|2007|Gene finding and rule discovery with a multi-objective neural-genetic hybrid|In this paper, we describe a multi-objective neural-genetic gene finding technique.|Ed Keedwell,Ajit Narayanan"],["16682|IJCAI|2007|Conjunctive Query Answering for the Description Logic SHIQ|Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, it was an open problem whether conjunctive query answering over DL knowledge bases is decidable if transitive roles are admitted in the query. In this paper, we consider conjunctive queries over knowledge bases formulated in the popular DL SHIQ and allow transitive roles in both the query and the knowledge base. We show that query answering is decidable and establish the following complexity bounds regarding combined complexity, we devise a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query. Regarding data complexity, we prove co-NP-completeness.|Birte Glimm,Ian Horrocks,Carsten Lutz,Ulrike Sattler","16492|IJCAI|2007|Completing Description Logic Knowledge Bases Using Formal Concept Analysis|We propose an approach for extending both the terminological and the assertional part of a Description Logic knowledge base by using information provided by the knowledge base and by a domain expert. The use of techniques from Formal Concept Analysis ensures that, on the one hand, the interaction with the expert is kept to a minimum, and, on the other hand, we can show that the extended knowledge base is complete in a certain, well-defined sense.|Franz Baader,Bernhard Ganter,Baris Sertkaya,Ulrike Sattler","16593|IJCAI|2007|A Fully Connectionist Model Generator for Covered First-Order Logic Programs|We present a fully connectionist system for the learning of first-order logic programs and the generation of corresponding models Given a program and a set of training examples, we embed the associated semantic operator into a feed-forward network and train the network using the examples. This results in the learning of first-order knowledge while damaged or noisy data is handled gracefully.|Sebastian Bader,Pascal Hitzler,Steffen Hölldobler,Andreas Witzel","16786|IJCAI|2007|An Extension to Conformant Planning Using Logic Programming|In this paper we extend the logic programming based conformant planner described in Son et al., a to allow it to work on planning problems with more complex descriptions of the initial states. We also compare the extended planner with other concurrent conformant planners.|A. Ricardo Morales,Phan Huy Tu,Tran Cao Son","16787|IJCAI|2007|A Faithful Integration of Description Logics with Logic Programming|Integrating description logics (DL) and logic programming (LP) would produce a very powerful and useful formalism. However, DLs and LP are based on quite different principles, so achieving a seamless integration is not trivial. In this paper, we introduce hybrid MKNF knowledge bases that faithfully integrate DLs with LP using the logic of Minimal Knowledge and Negation as Failure (MKNF) Lifschitz, . We also give reasoning algorithms and tight data complexity bounds for several interesting fragments of our logic.|Boris Motik,Riccardo Rosati","16356|IJCAI|2007|Embedding Non-Ground Logic Programs into Autoepistemic Logic for Knowledge-Base Combination|In the context of the Semantic Web, several approaches to the combination of ontologies, given in terms of theories of classical first-order logic, and rule bases have been proposed. They either cast rules into classical logic or limit the interaction between rules and ontologies. Autoepistemic logic (AEL) is an attractive formalismwhich allows to overcome these limitations, by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it. For the latter, so far only the propositional setting has been considered. In this paper, we present several embeddings of normal and disjunctive non-ground logic programs under the stable-model semantics into first-order AEL, and compare them in combination with classical theories, with respect to stable expansions and autoepistemic consequences. Our results reveal differences and correspondences of the embeddings and provide a useful guidance in the choice of a particular embedding for knowledge combination.|Jos de Bruijn,Thomas Eiter,Axel Polleres,Hans Tompits","16739|IJCAI|2007|Complexity Results for Checking Equivalence of Stratified Logic Programs|Recent research in nonmonotonic logic programming under the answer-set semantics focuses on different notions of program equivalence. However, previous results do not address the important classes of stratified programs and its subclass of acyclic (i.e., recursion-free) programs, although they are recognized as important tools for knowledge representation and reasoning. In this paper, we consider such programs, possibly augmented with constraints. Our results show that in the propositional setting, where reasoning is well-known to be polynomial, deciding strong and uniform equivalence is as hard as for arbitrary normal logic programs (and thus coNP-complete), but is polynomial in some restricted cases. Nonground programs behave similarly. However, exponential lower bounds already hold for small programs (i.e., with constantly many rules). In particular, uniform equivalence is undecidable even for small Horn programs plus a single negative constraint.|Thomas Eiter,Michael Fink,Hans Tompits,Stefan Woltran","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman","58063|GECCO|2007|Swarming with logic|A robot swarm is a distributed entity that can sense andperform many actions simultaneously at different spatial locations. But how, with all this sensory information and capacity for action, can the individuals unite for a common purpose, coordinating their actions in space and time Inthis preliminary simulation study, we suggest that a swarmcan be designed as an engineering control system characterised by a problem specific inputoutput (IO) relationship that can be implemented through the simple (socialinsect inspired) indirect transfer of information between individuals. For such a system, the inputs represent sensory information acquired by one or more agents and the outputs are used to trigger actions that agents perform.|Robert L. Stewart,Michael Kirley"],["16550|IJCAI|2007|A General Framework for Reasoning about Inconsistency|Numerous logics have been developed for reasoning about inconsistency which differ in (i) the logic to which they apply, and (ii) the criteria used to draw inferences. In this paper, we propose a general framework for reasoning about inconsistency in a wide variety of logics including ones for which inconsistency resolution methods have not yet been studied (e.g. various temporal and epistemic logics). We start with Tarski and Scott's axiomatization of logics, but drop their monotonicity requirements that we believe are too strong for AI. For such a logic L, we define the concept of an option. Options are sets of formulas in L that are closed and consistent according to the notion of consequence and consistency in L. We show that by defining an appropriate preference relation on options, we can capture several existing works such as Brewka's subtheories. We also provide algorithms to compute most preferred options.|V. S. Subrahmanian,Leila Amgoud","16620|IJCAI|2007|Infeasibility Certificates and the Complexity of the Core in Coalitional Games|This paper characterizes the complexity of the core in coalitional games. There are different proposals for representing coalitional games in a compact way, where the worths of coalitions may be computed in polynomial time. In all those frameworks, it was shown that core non-emptiness is a co-NP-hard problem. However, for the most general of them, it was left as an open problem whether it belongs to co-NP or it actually is an harder problem. We solve this open problem in a positive way indeed, we are able to show that, for the case of transferable payoffs, the problem belongs to co-NP for any compact representation of the game where the worths of coalitions may be computed in polynomial time (also, non-deterministic polynomial time), encompassing all previous proposals of this kind. This is proved by showing that games with empty cores have small infeasibility certificates. The picture is completed by looking at coalitional games with non-transferable payoffs. We propose a compact representation based on marginal contribution nets. Also in this case, we are able to settle the precise complexity of core non-emptiness, which turns out to be p-complete.|Enrico Malizia,Luigi Palopoli,Francesco Scarcello","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","16545|IJCAI|2007|Complexity of Pure Equilibria in Bayesian Games|In this paper we make a comprehensive study of the complexity of the problem of deciding the existence of equilibria in strategic games with incomplete information, in case of pure strategies. In particular, we show that this is NP-complete in general Bayesian Games in Standard Normal Form, and that it becomes PP-hard (and, in fixed-precision scenarios, PP-complete), when the game is represented succinctly in General Normal Form. Suitable restrictions in case of graphical games that make the problem tractable are also discussed.|Georg Gottlob,Gianluigi Greco,Toni Mancini","16350|IJCAI|2007|Counting Complexity of Propositional Abduction|Abduction is an important method of non-monotonic reasoning with many applications in AI and related topics. In this paper, we concentrate on propositional abduction, where the background knowledge is given by a propositional formula. Decision problems of great interest are the existence and the relevance problems. The complexity of these decision problems has been systematically studied while the counting complexity of propositional abduction has remained obscure. The goal of this work is to provide a comprehensive analysis of the counting complexity of propositional abduction in various classes of theories.|Miki Hermann,Reinhard Pichler","57896|GECCO|2007|Reducing the space-time complexity of the CMA-ES|A limited memory version of the covariance matrix adaptation evolution strategy (CMA-ES) is presented. This algorithm, L-CMA-ES, improves the space and time complexity of the CMA-ES algorithm. The L-CMA-ES uses the $m$ eigenvectors and eigenvalues spanning the m-dimensional dominant subspace of the n-dimensional covariance matrix, C, describing the mutation distribution. The algorithm avoids explicit computation and storage of $C$ resulting in space and time savings. The L-CMA-ES algorithm has a space complexity of mathcalO(nm) and a time complexity of mathcalO(nm). The algorithm is evaluated on a number of standard test functions. The results show that while the number of objective function evaluations needed to find a solution is often increased by using mn the increase in computational efficiency leads to a lower overall run time.|James N. Knight,Monte Lunacek","16503|IJCAI|2007|Reducing Accidental Complexity in Planning Problems|Although even propositional STRIPS planning is a hard problem in general, many instances of the problem, including many of those commonly used as benchmarks, are easy. In spite of this, they are often hard to solve for domain-independent planners, because the encoding of the problem into a general problem specification formalism such as STRIPS hides structure that needs to be exploited to solve problems easily. We investigate the use of automatic problem transformations to reduce this \"accidental\" problem complexity. The main tool is abstraction we identify a new, weaker, condition under which abstraction is \"safe\", in the sense that any solution to the abstracted problem can be refined to a concrete solution (in polynomial time, for most cases) and also show how different kinds of problem reformulations can be applied to create greater opportunities for such safe abstraction.|Patrik Haslum","16572|IJCAI|2007|On Reversing Actions Algorithms and Complexity|Reversing actions is the following problem After executing a sequence of actions, which sequence of actions brings the agent back to the state just before this execution (an action reversal). Notably, this problem is different from a vanilla planning problem since the state we have to get back to is in general unknown. It emerges, for example, if an agent needs to find out which action sequences are undoable, and which ones are committed choices. It has applications related to plan execution and monitoring in nondeterministic domains, such as recovering from a failed execution by partially undoing the plan, dynamically switching from one executed plan to another, or restarting plans. We formalize action reversal in a logic-based action framework and characterize its computational complexity. Since unsurprisingly, the problem is intractable in general, we present a knowledge compilation approach that constructs offline a reverse plan library for efficient (in some cases, linear time) on-line computation of action reversals. Our results for the generic framework can be easily applied for expressive action languages such as C+ or .|Thomas Eiter,Esra Erdem,Wolfgang Faber","58238|GECCO|2007|A simple genetic algorithm for reducible complexity|Challenges are often lofted to explain how gradualistic evolution can evolve systems where function ceases with the removal of any of their multiple parts. We present a genetic algorithm (GA) example using a dynamic fitness function. Given clear definitions of relevant terms, the GA produces such complex systems.|Lee K. Graham,Steffen Christensen,Franz Oppacher","58117|GECCO|2007|Comparison of tree and graph encodings as function of problem complexity|In this paper, we analyze two general-purpose encoding types, trees and graphs systematically, focusing on trends over increasingly complex problems. Tree and graph encodings are similar in application but offer distinct advantages and disadvantages in genetic programming. We describe two implementations and discuss their evolvability. We then compare performance using symbolic regression on hundreds of random nonlinear target functions of both -dimensional and -dimensional cases. Results show the graph encoding has less bias for bloating solutions but is slower to converge and deleterious crossovers are more frequent. The graph encoding however is found to have computational benefits, suggesting it to be an advantageous trade-off between regression performance and computational effort.|Michael D. Schmidt,Hod Lipson"]]}}