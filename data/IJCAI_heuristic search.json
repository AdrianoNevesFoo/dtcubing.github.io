{
  "sentence": {
    "entropy": 5.5887586743547,
    "topics": [
      "search algorithm, heuristic search, search problem, heuristic problem, approach search, paper search, planning search, heuristic algorithm, search tree, present search, results search, use search, search domain, performance search, heuristic domain, search strategy, show search, game tree, present algorithm, using search",
      "knowledge search, web search, use knowledge, knowledge disambiguation, user search, knowledge base, analysis knowledge, system search, search engines, knowledge representation, heuristic knowledge, access direct, system knowledge, top-down bottom-up, knowledge similarity, information search, data base, integer programming, heuristic system, information disambiguation",
      "search space, search problem, search algorithm, local search, solving problem, model search, search time, search solution, method search, constraint search, proof search, constraint satisfaction, search solving, search procedure, beam search, used search, techniques search, search paths, reduce search, problem space",
      "research, artificial intelligence, research system, research intelligence, real world, machine learning, research artificial, real voice, paper research, world voice, research problem, recent research, research learning, research area, natural language, paper describes, display voice, research domain, research effort, processing voice",
      "heuristic search, search problem, variety search, search time, search selection, local search, search algorithm, search operators, version search, search paradigm, first search, search ida, forward search, search actions, search space, forward planning, integration search, use search, runs search, bidirectional search",
      "planning search, present search, search domain, search plans, heuristic planning, search strategy, planner search, heuristic domain, application search, temporal search, paper search, solve problem, effects search, control search, planning domain, based search, planning problem, search graph, shown search, systematic search",
      "search process, top-down bottom-up, control knowledge, integer programming, general search, control search, paper search, knowledge programming, search generation, paper describes, use search, system search, implemented system, handle dependencies, integrated knowledge, describes search, framework search, knowledge search, general knowledge, representing knowledge",
      "user search, direct search, access direct, access search, entity search, access news, fast search, news search, search clusters, news video, entities entity, access video, search video, access time, hierarchical search, providing search, topics search, system news, access nearly, entity multimedia",
      "search solution, search time, search paths, constraint satisfaction, search memory, search algorithm, beam search, reduce search, search find, search optimal, search space, solution problem, search cost, search given, complete search, optimal solution, important search, search available, consistency search, size search",
      "proof search, search space, using search, means search, techniques search, theorem search, method search, discrepancy search, search step, search computing, exploit search, combines search, performs search, search clause, limited search, search prover, points search, model search, theorem prover, transformation search",
      "research learning, machine learning, paper research, paper describes, research robot, describes research, current research, research understanding, research machine, research system, research environment, research program, paper system, present research, learning system, goal research, speech understanding, research model, learning domain, research concept",
      "real world, real voice, natural language, world voice, research natural, display voice, research world, processing voice, information voice, natural voice, research real, world processing, real display, research recognition, world display, processing real, research processing, world objects, world information, computational natural"
    ],
    "ranking": [
      [
        "13388|IJCAI|1973|The Bandwidth Heuristic Search|By placing various restrictions on the heuristic estimator it is possible to constrain the heuristic search process to fit specific needs. This paper introduces a new restriction upon the heuristic, called the \"bandwidth\" condition, that enables the ordered search to better cope with time and space difficulties. In particular, the effect of error within the heuristic is considered in detail. Beyond this the bandwidth condition quite naturally allows for the extension of the heuristic search to MINMAX trees. The resulting game playing algorithm affords many desirable practical features not found in minimax based techniques, as well as maintaining the theoretical framework of ordered searchs. The development of this algorithm provides some additional insight to the general problem of searching game trees by showing that certain, somewhat surprising changes in the cost estimates are required to properly search the tree. Furthermore, the use of an ordered search of MINMAX trees brings about a rather provocative departure from the conventional approach to computer game playing.|Larry R. Harris",
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "13475|IJCAI|1975|The Heuristic Search And The Game Of Chess - A Study Of Quiescene Sacrifices And Plan Oriented Play|This paper describes the results of applying the formal heurisitic search algorithm to the game of chess, and the impact of this work on the theory of heuristic search. It is not that the application of the heuristic search can by itself solve the problems at the heart of the computer chess, but that representing these problems within the formalism of the heuristic search will further their common solution. A separate search heuristic is proposed that does offer a common solution to the problems of quiescence, sacrifices, and plan oriented play.|Larry R. Harris",
        "16829|IJCAI|2009|Minimum Proof Graphs and Fastest-Cut-First Search Heuristics|Alpha-Beta is the most common game tree search algorithm, due to its high-performance and straightforward implementation. In practice one must find the best trade-off between heuristic evaluation time and bringing the subset of nodes explored closer to a minimum proof graph. In this paper we present a series of structural properties of minimum proof graphs that help us to prove that finding such graphs is NP-hard for arbitrary DAG inputs, but can be done in linear time for trees. We then introduce the class of fastest-cut-first search heuristics that aim to approximate minimum proof graphs by sorting moves based on approximations of sub-DAG values and sizes. To explore how various aspects of the game tree (such as branching factor and distribution of move values) affect the performance of Alpha-Beta we introduce the class of \"Prefix Value Game Trees\" that allows us to label interior nodes with true minimax values on the fly without search. Using these trees we show that by explicitly attempting to approximate a minimum game tree we are able to achieve performance gains over Alpha-Beta with common extensions.|Timothy Furtak,Michael Buro",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "15700|IJCAI|2001|Keyword Spices A New Method for Building Domain-Specific Web Search Engines|This paper presents a new method for building domain-specific web search engines. Previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. Accordingly, they are hard to build and can not be applied to other domains. The keyword spice method, in contrast, improves search performance by adding domain-specific keywords, called keyword spices, to the user's input query the modified query is then forwarded to a general-purpose search engine. Keyword spices can be effectively discovered automatically from web documents allowing us to build high quality domain-specific search engines in various domains without requiring the collection of heuristic knowledge. We describe a machine learning algorithm, which is a type of decision-tree learning algorithm, that can extract keyword spices. To demonstrate the value of the proposed approach, we conduct experiments in the domain of cooking. The results confirm the excellent performance of our method in terms of both precision and recall.|Satoshi Oyama,Takashi Kokubo,Toru Ishida,Teruhiro Yamada,Yasuhiko Kitamura",
        "15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer",
        "16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar",
        "15158|IJCAI|1995|Diagnosing Tree-Decomposable Circuits|This paper describes a diagnosis algorithm called structure-based abduction (SAB) which was developed in the framework of constraint networks . The algorithm exploits the structure of the constraint network and is most efficient for near-tree problem domains. By analyzing the structure of the problem domain, the performance of such algorithms can be bounded in advance. We present empirical results comparing SAB with two modelbased algorithms, MBD and MBD, for the task of finding one or all minimal-cardinality diagnoses. MBD uses the same computing strategy as algorithm GDE . MBD adopts a breadth-first search strategy similar to the algorithm DIAGNOSE . The main conclusion is that for nearly acyclic circuits, such as the N-bit adder, the performance of SAB being linear provides definite advantages as the size of the circuit increases.|Yousri El Fattah,Rina Dechter",
        "15722|IJCAI|2001|Incomplete Tree Search using Adaptive Probing|When not enough time is available to fully explore a search tree, different algorithms will visit different leaves. Depth-first search and depth-bounded discrepancy search, for example, make opposite assumptions about the distribution of good leaves. Unfortunately, it is rarely clear a priori which algorithm will be most appropriate for a particular problem. Rather than fixing strong assumptions in advance, we propose an approach in which an algorithm attempts to adjust to the distribution of leaf costs in the tree while exploring it. By sacrificing completeness, such flexible algorithms can exploit information gathered during the search using only weak assumptions. As an example, we show how a simple depth-based additive cost model of the tree can be learned on-line. Empirical analysis using a generic tree search problem shows that adaptive probing is competitive with systematic algorithms on a variety of hard trees and outperforms them when the node-ordering heuristic makes many mistakes. Results on boolean satisfiability and two different representations of number partitioning confirm these observations. Adaptive probing combines the flexibility and robustness of local search with the ability to take advantage of constructive heuristics.|Wheeler Ruml"
      ],
      [
        "15432|IJCAI|1999|Generalized Physical Networks for Automated Model Building|We present a new knowledge representation and reasoning framework for modeling nonlinear dynamical systems. The goals of this framework are to smoothly incorporate varying levels of domain knowledge and to tailor the reasoning methods - and hence the search space -- accordingly. Our solution exploits generalized physical networks (GPN), a rneta-level representation of idealized two-terminal elements, together with a hierarchy of qualitative and quantitative analysis tools, to produce a dynamic modeling domain whose complexity naturally adapts to the amount of available information about the target system.|Matthew Easley,Elizabeth Bradley",
        "13629|IJCAI|1977|Knowledge Base Management for Experiment Planning in Molecular Genetics|The use of a representation language involving schemata and associated derived models has been extended to include all aspects of domain knowledge and strategy and heuristic problem solving knowledge. This uniform representation will allow the extension of knowledge base management techniques for acquisition and retrieval of procedural knowledge.|Nancy Martin,Peter Friedland,Jonathan King,Mark Stefik",
        "15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin",
        "15542|IJCAI|1999|Combining Weak Knowledge Sources for Sense Disambiguation|There has been a tradition of combining different knowledge sources in Artificial Intelligence research. We apply this methodology to word sense disambiguation (WSD), a long-standing problem in Computational Linguistics. We report on an implemented sense tagger which uses a machine readable dictionary to provide both a set of senses and associated forms of information on which to base disambiguation decisions. The system is based on an architecture which makes use of different sources of lexical knowledge in two ways and optimises their combination using a learning algorithm. Tested accuracy of our approach on a general corpus exceeds %, demonstrating the viability of allword disambiguation as opposed to restricting oneself to a small sample.|Mark Stevenson,Yorick Wilks",
        "14310|IJCAI|1985|A Predicate Connection Graph Based Logic with Flexible Control|The FDE has been designed to support multiple search strategies for logic programs. This machine represents the knowledge base in a strategy independent fashion as a predicate connection graph which encodes potential unifications between predicates. It facilitates knowledge representation in the language of full first order predicate calculus Immediate developments include implementation of various database access strategies and addition of evaluable predicates and functions to the language. Long-term research will focus on exploration of search strategies, especially for parallel logic machines.|Richard Whitney,Darrel J. Van Buer,Donald P. McKay,Dan Kogan,Lynette Hirschman,Rebecca Davis",
        "13575|IJCAI|1977|Levels of Complexity in Discourse for Anaphora Disambiguation and Speech Act Interpretation|This paper presents a discussion of means of describing the discourse and its components which makes speech act interpretation and anaphora disambiguation possible with minimal search of the knowledge in the database. A portion of this paper will consider how a frames representation of sentences and common sense knowledge provides a mechanism for representing the postulated discourse components. Finally some discussion of the use of the discourse model and of frames in a discourse understanding program for a personal assistant will be presented.|C. Bullwinkle",
        "16590|IJCAI|2007|Semantic Indexing of a Competence Map to Support Scientific Collaboration in a Research Community|This paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. The taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences, called Knowledge Map. The KMap is a system to support research collaborations and sharing of results within and beyond a European Network of Excellence. The methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members - and to use this model for improving accessibility of data and knowledge repositories.|Paola Velardi,Roberto Navigli,Micha\u00ebl Petit",
        "14283|IJCAI|1985|Representation and Use of Explicit Justifications for Knowledge Base Refinements|We discuss the representation and use of justification structures as an aid to knowledge base refinement We show how justifications can be used by a system to generate explanations - for its own use-of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.|Reid G. Smith,Howard A. Winston,Tom M. Mitchell,Bruce G. Buchanan",
        "13604|IJCAI|1977|Focus of Attention in the Hearsay-II Speech Understanding System|Using the concepts of stimulus and response frames of scheduled Knowledge source instantiations, competition among alternative responses, goals, and the desirability of a knowledge source instantiation, a general attentional control mechanism is developed. This general focusing mechanism facilitates the experimental evaluation of a variety of specific attentional control policies (such as best-first, bottom-up, and top-down search strategies) and allows the modular addition of specialized heuristics for the speech understanding task. Empirical results demonstrate the effectiveness of the focusing principles, and possible directions for future research are considered.|Frederick Hayes-Roth,Victor R. Lesser",
        "14434|IJCAI|1987|From Application Shell to Knowledge Acquisition System|The TEST (Troubleshooting Expert System Tool) architecture greatly aided the development of TDE (TEST Development Environment). In particular, the choice of a schematic as opposed to rule-based representation led to a knowledge base characterized by the use of domain-familiar concepts, and sufficient conceptual structure to facilitate several TDE features, including knowledge base development through both directecfinterviews and the direct manipulation of icons multiple knowledge-based browsing strategies heuristic error analysis and easily-unaerstooa debugging techniques.|Gary S. Kahn"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "15045|IJCAI|1993|Understanding the Role of Negotiation in Distributed Search Among Heterogereous Agents|In our research, we explore the role of negotiation for conflict resolution in distributed search among heterogeneous and reusable agents. We present negotiated search, an algorithm that explicitly recognizes and exploits conflict to direct search activity across a set of agents. In negotiated search, loosely coupled agents interleave the tasks of ) local search for a solution to some subproblem ) integration of local subproblem solutions into a shared solution ) information exchange to define and refine the shared search space of the agents and ) assessment and reassessment of emerging solutions. Negotiated search is applicable to diverse application areas and problem-solving environments. It requires only basic search operators and allows maximum flexibility in the distribution of those operators. These qualities make the algorithm particularly appropriate for the integration of heterogeneous agents into application systems. The algorithm is implemented in a multi-agent framework, TEAM, that provides the infrastructure required for communication and cooperation.|Susan E. Lander,Victor R. Lesser",
        "15835|IJCAI|2003|Making the Breakout Algorithm Complete Using Systematic Search|Local search algorithms have been very successful for solving constraint satisfaction problems (CSP). However, a major weakness has been that local search is unable to detect unsolvability and is thus not suitable for tightly and overconstrained problems. We present a hybrid solving scheme where we combine a local search algorithm - the breakout algorithm, with a systematic search algorithm - backtracking. The breakout algorithm is used for identifying hard or unsolvable subproblems and the backtracking algorithm proves the solvability of these subproblems. The resulting hybrid algorithm is complete and is tested on randomly generated graph -colouring problems. The algorithm performs extremely well for all areas of the phase transition and outperforms the individual methods by several orders of magnitude.|Carlos Eisenberg,Boi Faltings",
        "16124|IJCAI|2005|Limited Discrepancy Beam Search|Beam search reduces the memory consumption of best-first search at the cost of finding longer paths but its memory consumption can still exceed the given memory capacity quickly. We therefore develop BULB (Beam search Using Limited discrepancy Backtracking), a complete memory-bounded search method that is able to solve more problem instances of large search problems than beam search and does so with a reasonable runtime. At the same time, BULB tends to find shorter paths than beam search because it is able to use larger beam widths without running out of memory. We demonstrate these properties of BULB experimentally for three standard benchmark domains.|David Furcy,Sven Koenig",
        "15786|IJCAI|2003|Propagate the Right Thing How Preferences Can Speed-Up Constraint Solving|We present an algorithm Pref-AC that limits arc consistency (AC) to the preferred choices of a tree search procedure and that makes constraint solving more efficient without changing the pruning and shape of the search tree. Arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problems such as configuration and scheduling. Moreover, Pref-AC directly computes a preferred solution for treelike constraint satisfaction problems.|Christian Bessi\u00e8re,Ana\u00efs Fabre,Ulrich Junker",
        "13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel",
        "15585|IJCAI|2001|Backtracking Through Biconnected Components of a Constraint Graph|The algorithm presented here, BCC, is an enhancement of the well known Backtrack used to solve constraint satisfaction problems. Though most backtrack improvements rely on propagation of local informations, BCC uses global knowledge of the constraint graph structure (and in particular its biconnected components) to reduce search space, permanently removing values and compiling partial solutions during exploration. This algorithm performs well by itself, without any filtering, when the biconnected components are small, achieving optimal time complexity in case of a tree. Otherwise, it remains compatible with most existing techniques, adding only a negligible overhead cost.|Jean-Fran\u00e7ois Baget,Yannic S. Tognetti",
        "14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan",
        "16361|IJCAI|2007|Arc Consistency during Search|Enforcing arc consistency (AC) during search has proven to be a very effective method in solving Constraint Satisfaction Problems and it has been widely-used in many Constraint Programming systems. Although much effort has been made to design efficient standalone AC algorithms, there is no systematic study on how to efficiently enforce AC during search, as far as we know. The significance of the latter is clear given the fact that AC will be enforced millions of times in solving hard problems. In this paper, we propose a framework for enforcing AC during search (ACS) and complexity measurements of ACS algorithms. Based on this framework, several ACS algorithms are designed to take advantage of the residual data left in the data structures by the previous invocation(s) of ACS. The algorithms vary in the worst-case time and space complexity and other complexity measurements. Empirical study shows that some of the new ACS algorithms perform better than the conventional implementation of AC algorithms in a search procedure.|Chavalit Likitvivatanavong,Yuanlin Zhang,Scott Shannon,James Bowen,Eugene C. Freuder",
        "13402|IJCAI|1973|The Q Algorithm - A Search Strategy for a Deduclive Question-Answering System|An approach for bringing semantic, as well as syntactic, information to bear on the problem of theorem-proving search for Question-Answering (QA) Systems is descrilsed. The approach is embodied in a search algorithm, the Q* search algorithm, developed to control deductive searches in an experimental system. The Q* algorithm is part of a system, termed the Maryland Refutation Proof Procedure System (MRPPS), which incorporates both the Q* algorithm, which performs the search required to answer a query, and an inferential component, which performs the logical manipulations necessary to deduce a clause from one or two other clauses. The inferential component includes many refinements of resolution. The Q* algorithm generates nodes in the search space, applying semantic and syntactic information to direct the search. The use of semantics permits paths to be terminated and fruitful paths to be explored. The paper is restricted to a description of the use of syntactic and semantic information in the Q* algorithm.|Jack Minker,Daniel H. Fishman,James R. McSkimin"
      ],
      [
        "16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone",
        "13651|IJCAI|1977|Language Access to Distributed Data with Error Recovery|This paper discusses an effort in the application of artificial intelligence to the access of data from a large, distributed data base over a computer network. A running system is described that provides real-time access over the ARPANET to a data base distributed over several machines. The system accepts a rather wide range of natural language questions about the data, plans a sequence of appropriate queries to the data base management system to answer the question, determines on which machine(s) to carry out the queries, establishes links to those machines over the ARPANET, monitors the prosecution of the queries and recovers from certain errors in execution, and prepares a relevant answer. In addition to the components that make up the demonstration system, more sophisticated functionally equivalent components are discussed and proposed. The work described in this paper represents the joint efforts of an integrated, energetic group at SRI. Members of this group include Rich Fikes (now at Xerox PARC), Koichi Furukawa (now at ETL). Gary Hendrix, Paul Morris (now at UC Irvine), Nils Nilsson, Bill Paxton, Jane Robinsonr Daniel Sagalowicz, Jonathan Slocum, and Mike Wilber. The research reported herein, other than the development of the LIFER system, was supported by the Advanced Research Projects Agency of the Department of Defense under contract DAAG--C- with the U.S. Army Research Office.|Earl D. Sacerdoti",
        "14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,Fran\u00e7ois Felix Ingrand",
        "13787|IJCAI|1981|Summarizing Narratives|Most research on narrative text summarisation has been conducted within the paradigm of experimental psychology. But recent language processing research in artificial intelligence suggests that the predominant theory of text summarisation requires further examination. Seemingly minor structural modifications of a story can result in significant alterations of summary behavior. In this paper, highlights of summary data from  subjects are presented and analyzed in terms of two competing summarization models () the story grammar model of psychology, and () the plot unit model developed in artificial intelligence. We will show how selected story grammar predictions compare to plot unit predictions for short term summarization and then identify two complicating factors that have a major impact on summarisation behavior.|Wendy G. Lehnert,John B. Black,Brian J. Reiser",
        "14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz",
        "14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue",
        "13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson",
        "13395|IJCAI|1973|Artificial Intelligence and Automatic Programming in CAI|This paper discusses generative computer-assisted instruction (CAI) and its relationship to Artificial Intelligence Research. Systems which have a limited capability for natural language communication are described. In addition, potential areas in which Artificial Intelligence could be applied are outlined. These include individualization of instruction, determining the degree of accuracy of a student response, and problem-solving. A CAI system which is capable of writing computer programs is described in detail. Techniques are given for generating meaningful programming problems. These problems are represented as a sequence of primitive tasks each of which can be coded in several ways. The manner in which the system designs its own solution program and monitors the student solution is also described.|Elliot B. Koffman,Sumner E. Blount",
        "15248|IJCAI|1995|Ubiquitous Talker Spoken Language Interaction with Real World Objects|Augmented reality is a research area that tries to embody an electronic information space within the real world through computational devices A crucial issue within thus area is the recognition of real world objects or situations In natural language processing, it is much racier to determine interpietations of utterances even if they art in formed WHEN the context or situation is fixed We therefore introduce robust natural language processing in to a system of augmented reality with situation awareness Based on this idea we have developed a portable system called the Ubiquitous Talker This consists of an LCD display that reflects the scene at which a user is looking as if it is a transparent glass a CCD camera for reognizing real world objects with color bar ID codes a microphone for recognizing a human voice and a speaker which outputs a synthesized voice The Ubiquitous Talker provides its user with some information related to a recognized object by using the display and voice It also accepts requests or questions as voice inputs The user feels as if heshe is talking with the object itself through the system.|Katashi Nagao,Jun Rekimoto",
        "15612|IJCAI|2001|Behavior Planning for a Reflexive Agent|The aim of our research is to build a Reflexive Agent, that is able to either manifest an emotion it is feeling or to hide it. If the Agent decides to manifest its emotion, it can establish what verbal or nonverbal signals to employ in its communication and how to combine and synchronize them. In the decision of whether to express an emotion in a given context, a number of factors are considered, such as the Agent's own personality and goals, the Interlocutor's characteristics and the context. In planning how to communicate an emotion, various factors are considered as well the available modalities (face, gaze, voice etc) the cognitive ease in producing and processing the various signals the expressiveness of every signal in communicating specific meanings and, finally, the appropriateness of signals to social situations.|Berardina De Carolis,Catherine Pelachaud,Isabella Poggi,Fiorella de Rosis"
      ],
      [
        "14750|IJCAI|1989|Fast Recursive Formulations for Best-First Search That Allow Controlled Use of Memory|MREC is a new recursive best-first search algorithm which combines the good features of A* and IDA*. It is closer in operation to IDA*, and does not use an OPEN list. In order to execute, all MREC needs is sufficient memory for its implicit stack. But it can also be fed at runtime a parameter M which tells it how much additional memory is available for use. In this extra memory, MREC stores as much as possible of the explicit graph. When M  , MREC is identical to IDA*. But when M  , it can make far fewer node expansions than IDA*. This can be advantageous for problems where the time to expand a node is significant. Extensive runs on a variety of search problems, involving search graphs that may or may not be trees, indicate that MREC with M   is as good as IDA* on problems such as the - puzzle for which IDA* is suitable, while MREC with large M is as fast as A* on problems for which node expansion time is not negligible.|Anup K. Sen,Amitava Bagchi",
        "16024|IJCAI|2003|Using Available Memory to Transform Graphplans Search|We present a major variant of the Graphplan algorithm that employs available memory to transform the depth-first nature of Graphplan's search into an iterative state space view in which heuristics can be used to traverse the search space. When the planner, PEGG, is set to conduct exhaustive search, it produces guaranteed optimal parallel plans  to  times faster than a version of Graphplan enhanced with CSP speedup methods. By heuristically pruning this search space PEGG produces plans comparable to Graphplan's in makespan, at speeds approaching state-of-the-art heuristic serial planners.|Terry Zimmerman,Subbarao Kambhampati",
        "16949|IJCAI|2009|Efficient Incremental Search for Moving Target Search|Incremental search algorithms reuse information from previous searches to speed up the current search and are thus often able to find shortest paths for series of similar search problems faster than by solving each search problem independently from scratch. However, they do poorly on moving target search problems, where both the start and goal cells change over time. In this paper, we thus develop Fringe-Retrieving A* (FRA*), an incremental version of A* that repeatedly finds shortest paths for moving target search in known gridworlds. We demonstrate experimentally that it runs up to one order of magnitude faster than a variety of state-of-the-art incremental search algorithms applied to moving target search in known gridworlds.|Xiaoxun Sun,William Yeoh,Sven Koenig",
        "15806|IJCAI|2003|Lookahead Pathologies for Single Agent Search|Admissible and consistent heuristic functions are usually preferred in single-agent heuristic search as they guarantee optimal solutions with complete search methods such as A* and IDA*. Larger problems, however, frequently make a complete search intractable due to space andor time limitations. In particular, a path-planning agent in a real-time strategy game may need to take an action before its complete search has the time to finish. In such cases, incomplete search techniques (such as RTA*, SRTA*, RTDP, DTA*) can be used. Such algorithms conduct a limited ply lookahead and then evaluate the states envisioned using a heuristic function. The action selected on the basis of such evaluations can be suboptimal due to the incompleteness of search and inaccuracies in the heuristic. It is usually believed that deeper lookahead increases the chances of taking the optimal action. In this paper, we demonstrate that this is not necessarily the case, even when admissible and consistent heuristic functions are used.|Vadim Bulitko,Lihong Li,Russell Greiner,Ilya Levner",
        "15045|IJCAI|1993|Understanding the Role of Negotiation in Distributed Search Among Heterogereous Agents|In our research, we explore the role of negotiation for conflict resolution in distributed search among heterogeneous and reusable agents. We present negotiated search, an algorithm that explicitly recognizes and exploits conflict to direct search activity across a set of agents. In negotiated search, loosely coupled agents interleave the tasks of ) local search for a solution to some subproblem ) integration of local subproblem solutions into a shared solution ) information exchange to define and refine the shared search space of the agents and ) assessment and reassessment of emerging solutions. Negotiated search is applicable to diverse application areas and problem-solving environments. It requires only basic search operators and allows maximum flexibility in the distribution of those operators. These qualities make the algorithm particularly appropriate for the integration of heterogeneous agents into application systems. The algorithm is implemented in a multi-agent framework, TEAM, that provides the infrastructure required for communication and cooperation.|Susan E. Lander,Victor R. Lesser",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "16987|IJCAI|2009|Monte-Carlo Exploration for Deterministic Planning|Search methods based on Monte-Carlo simulation have recently led to breakthrough performance improvements in difficult game-playing domains such as Go and General Game Playing. Monte-Carlo Random Walk (MRW) planning applies Monte-Carlo ideas to deterministic classical planning. In the forward chaining planner ARVAND, Monte-Carlo random walks are used to explore the local neighborhood of a search state for action selection. In contrast to the stochastic local search approach used in the recent planner Identidem, random walks yield a larger and unbiased sample of the search neighborhood, and require state evaluations only at the endpoints of each walk. On IPC- competition problems, the performance of ARVAND is competitive with state of the art systems.|Hootan Nakhost,Martin M\u00fcller 0003",
        "15464|IJCAI|1999|Switching from Bidirectional to Unidirectional Search|Recently, we showed that for traditional bidirectional search with \"front-to-end\" evaluations, it is not the meeting of search fronts but the cost of proving the optimality of a solution that is problematic. Using our improved understanding of the problem, we developed a new approach to improving this kind of search switching to unidirectional search after the search frontiers meet for the first time (with the first solution found). This new approach shows improvements over previous bidirectional search approaches and (partly) also over the corresponding unidirectional search approaches in different domains. Together with a special-purpose improvement for the TSP, this approach showed better results than the standard search algorithms using the same knowledge.|Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda",
        "16118|IJCAI|2005|Abstraction-based Action Ordering in Planning|Many planning problems contain collections of symmetric objects, actions and structures which render them difficult to solve efficiently. It has been shown that the detection and exploitation of symmetric structure in planning problems can dramatically reduce the size of the search space and the time taken to find a solution. We present the idea of using an abstraction of the problem domain to reveal symmetric structure and guide the navigation of the search space. We show that this is effective even in domains in which there is little accessible symmetric structure available for pruning. Proactive exploitation represents a flexible and powerful alternative to the symmetry-breaking strategies exploited in earlier work in planning and CSPs. The notion of almost symmetry is defined and results are presented showing that proactive exploitation of almost symmetry can improve the performance of a heuristic forward search planner.|Maria Fox,Derek Long,Julie Porteous"
      ],
      [
        "14902|IJCAI|1991|Localized Search for Multiagent Planning|This paper describes the localized search mechanism of the GEMPLAN multiagent planner. Both formal complexity results and empirical results are provided, demonstrating the benefits of localized search, A localized domain description is one that decomposes domain activities and requirements into a set of regions. This description is used to infer how domain requirements are semantically localized and, as a result, to enable the decomposition of the planning search space into a set of spaces, one for each domain region. Benefits of localization include a smaller and cheaper overall search space as well as heuristic guidance in controlling search. Such benefits are critical if current planning technologies and other types of reasoning are to be scaled up to large, complex domains.|Amy L. Lansky",
        "16832|IJCAI|2009|Stratified Planning|Most planning problems have strong structures. They can be decomposed into subdomains with causal dependencies. The idea of exploiting the domain decomposition has motivated previous work such as hierarchical planning and factored planing. However, these algorithms require extensive backtracking and lead to few efficient general-purpose planners. On the other hand, heuristic search has been a successful approach to automated planning. The domain decomposition of planning problems, unfortunately, is not directly and fully exploited by heuristic search. We propose a novel and general framework to exploit domain decomposition. Based on a structure analysis on the SAS+ planning formalism, we stratify the sub-domains of a planning problem into dependency layers. By recognizing the stratification of a planning structure, we propose a space reduction method that expands only a subset of executable actions at each state. This reduction method can be combined with state-space search, allowing us to simultaneously employ the strength of domain decomposition and high-quality heuristics. We prove that the reduction preserves completeness and optimality of search and experimentally verify its effectiveness in space reduction.|Yixin Chen,You Xu,Guohui Yao",
        "15594|IJCAI|2001|Planning in Nondeterministic Domains under Partial Observability via Symbolic Model Checking|Planning under partial observability is one of the most significant and challenging planning problems. It has been shown to be hard, both theoretically and experimentally. In this paper, we present a novel approach to the problem of planning under partial observability in non-deterministic domains. We propose an algorithm that searches through a (possibly cyclic) and-or graph induced by the domain. The algorithm generates conditional plans that are guaranteed to achieve the goal despite of the uncertainty in the initial condition, the uncertain effects of actions, and the partial observability of the domain. We implement the algorithm by means of BDD-based, symbolic model checking techniques, in order to tackle in practice the exponential blow up of the search space. We show experimentally that our approach is practical by evaluating the planner with a set of problems taken from the literature and comparing it with other state of the art planners for partially observable domains.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri,Paolo Traverso",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "16725|IJCAI|2007|Domain Independent Approaches for Finding Diverse Plans|In many planning situations, a planner is required to return a diverse set of plans satisfying the same goals which will be used by the external systems collectively. We take a domain-independent approach to solving this problem. We propose different domain independent distance functions among plans that can provide meaningful insights about the diversity in the plan set. We then describe how two representative state-of-the-art domain independent planning approaches - one based on compilation to CSP, and the other based on heuristic local search - can be adapted to produce diverse plans. We present empirical evidence demonstrating the effectiveness of our approaches.|Biplav Srivastava,Tuan A. Nguyen,Alfonso Gerevini,Subbarao Kambhampati,Minh Binh Do,Ivan Serina",
        "15219|IJCAI|1995|Planning with Sharable Resource Constraints|When planning systems deal with realistic domains, they must cope with a large variety of constraints imposed by the environment such as temporal or resource constraints The robustness of the generated plan is a direct consequence of a correct handling of these constraints We argue that increasing the expressiveness of a representation can be achieved without fundamentally affecting the global efficiency of the search This paper presents a temporal planner, LxTeT, which integrates sharable resource management into the process of plan generation In LxTeT, planning operators are described as temporal structures of conditions, effects and sharable resource uses During the search, pending subgoals, protection threats and resource conflicts are detected by three flaw analysis modules The detection of sharable resource conflicts is performed thanks to an efficient clique-search algorithm on a possible intersection graph The control of the search is based on a least-commitment opportunistic strategy Our approach has been implemented, tested and shown to be satisfactory in various application domains.|Philippe Laborie,Malik Ghallab",
        "15551|IJCAI|1999|On the Use of Integer Programming Models in AI Planning|Recent research has shown the promise of using propositional reasoning and search to solve AI planning problems. In this paper, we further explore this area by applying Integer Programming to solve AI planning problems. The application of Integer Programming to AI planning has a potentially significant advantage, as it allows quite naturally for the incorporation of numerical constraints and objectives into the planning domain. Moreover, the application of Integer Programming to AI planning addresses one of the challenges in propositional reasoning posed by Kautz and Selman, who conjectured that the principal technique used to solve Integer Programs--the linear programming (LP) relaxation--is not useful when applied to propositional search. We discuss various IP formulations for the class of planning problems based on STRIPS-style planning operators. Our main objective is to show that a carefully chosen IP formulation significantly improves the \"strength\" of the LP relaxation, and that the resultant LPs are useful in solving the IP and the associated planning problems. Our results clearly show the importance of choosing the \"right\" representation, and more generally the promise of using Integer Programming techniques in the AI planning domain.|Thomas Vossen,Michael O. Ball,Amnon Lotem,Dana S. Nau",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "16449|IJCAI|2007|Edge Partitioning in External-Memory Graph Search|There is currently much interest in using external memory, such as disk storage, to scale up graph-search algorithms. Recent work shows that the local structure of a graph can be leveraged to substantially improve the efficiency of external-memory graph search. This paper introduces a technique, called edge partitioning, which exploits a form of local structure that has not been considered in previous work. The new technique improves the scalability of structured approaches to external-memory graph search, and also guarantees the applicability of these approaches to any graph-search problem. We show its effectiveness in an external-memory graph-search algorithm for domain-independent STRIPS planning.|Rong Zhou,Eric A. Hansen",
        "16118|IJCAI|2005|Abstraction-based Action Ordering in Planning|Many planning problems contain collections of symmetric objects, actions and structures which render them difficult to solve efficiently. It has been shown that the detection and exploitation of symmetric structure in planning problems can dramatically reduce the size of the search space and the time taken to find a solution. We present the idea of using an abstraction of the problem domain to reveal symmetric structure and guide the navigation of the search space. We show that this is effective even in domains in which there is little accessible symmetric structure available for pruning. Proactive exploitation represents a flexible and powerful alternative to the symmetry-breaking strategies exploited in earlier work in planning and CSPs. The notion of almost symmetry is defined and results are presented showing that proactive exploitation of almost symmetry can improve the performance of a heuristic forward search planner.|Maria Fox,Derek Long,Julie Porteous"
      ],
      [
        "14690|IJCAI|1989|Extending Reflective Architectures|The overhead incurred by reasoning in knowledgebased systems can be considerable when it is forced to rely on search. Even problems that are known to have tractable solutions can expend large amounts of computation when the inference method is too general. As discussed in this paper, reflective architectures provide a well-motivated framework for integrating specialized control with general reasoning in knowledge-based systems. However, progress in developing reflective architectures for more expressive languages such as first-order logic has encountered several problems of its own. Briefly, this paper considers a reflective architecture for general declarative languages, and describes how declarative and procedural requirements can be combined in a reflective system for first-order logic. As part of this example, two kinds of control, in the form control strategies and subsidiary deduction rules, are identified.|Timothy M. Lownie",
        "14402|IJCAI|1987|Learning General Search Control from Outside Guidance|The system presented here shows how Soar, an architecture for general problem solving and learning, can acquire general search-control knowledge from outside guidance. The guidance can be either direct advice about what the system should do, or a problem that illustrates a relevant idea. The system makes use of the guidance by first formulating an appropriate goal for itself. In the process of achieving this goal, it learns general search-control chunks. In the case of learning from direct advice, the goal is to verify that the advice is correct. The verification allows the system to obtain general conditions of applicability of the advice, and to protect itself from erroneous advice. The system learns from illustrative problems by setting the goal of solving the problem provided. It can then transfer the lessons it learns along the way to its original problem. This transfer constitutes a rudimentary form of analogy.|Andrew R. Golding,Paul S. Rosenbloom,John E. Laird",
        "13413|IJCAI|1973|The Hearsay Speech Understanding System An Example of the Recognition Process|This paper describes the structure and operation of the Hearsay speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of Knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its Operation in a particular task situation Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.|D. R. Reddy,Lee D. Erman,R. D. Fenneli,Richard B. Neely",
        "15443|IJCAI|1999|Lemma Generation for Model Elimination by Combining Top-Down and Bottom-Up Inference|A very promising approach for integrating top-down and bottom-up proof search is the use of bottom-up generated lemmas in top-down provers. When generating lemmas, however) the currently used lemma generation procedures suffer from the well-known problems of forward reasoning methods, e.g., the proof goal is ignored. In order to overcome these problems we propose two relevancy-based lemma generation methods for top-down provers. The first approach employs a bottom-up level saturation procedure controlled by top-down generated patterns which represent promising subgoals. The second approach uses evolutionary search and provides a self-adaptive control of lemma generation and goal decomposition.|Marc Fuchs",
        "15662|IJCAI|2001|A-System Problem Solving through Abduction|This paper presents a new system, called the A- System, performing abductive reasoning within the framework of Abductive Logic Programming. It is based on a hybrid computational model that implements the abductive search in terms of two tightly coupled processes a reduction process of the highlevel logical representation to a lower-level constraint store and a lower-level constraint solving process. A set of initial \"proof of principle\" experiments demonstrate the versatility of the approach stemming from its declarative representation of problems and the good underlying computational behaviour of the system. The approach offers a general methodology of declarative problem solving in AI where an incremental and modular refinement of the high-level representation with extra domain knowledge can improve and scale the computational performance of the framework.|Antonis C. Kakas,Bert Van Nuffelen,Marc Denecker",
        "16590|IJCAI|2007|Semantic Indexing of a Competence Map to Support Scientific Collaboration in a Research Community|This paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. The taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences, called Knowledge Map. The KMap is a system to support research collaborations and sharing of results within and beyond a European Network of Excellence. The methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members - and to use this model for improving accessibility of data and knowledge repositories.|Paola Velardi,Roberto Navigli,Micha\u00ebl Petit",
        "13604|IJCAI|1977|Focus of Attention in the Hearsay-II Speech Understanding System|Using the concepts of stimulus and response frames of scheduled Knowledge source instantiations, competition among alternative responses, goals, and the desirability of a knowledge source instantiation, a general attentional control mechanism is developed. This general focusing mechanism facilitates the experimental evaluation of a variety of specific attentional control policies (such as best-first, bottom-up, and top-down search strategies) and allows the modular addition of specialized heuristics for the speech understanding task. Empirical results demonstrate the effectiveness of the focusing principles, and possible directions for future research are considered.|Frederick Hayes-Roth,Victor R. Lesser",
        "15139|IJCAI|1995|Control Structures for Incorporating Picture-Specific Context in Image Interpretation|This paper describes an efficient control mechanism for incorporating picture-specific context in the task of image interpretation. Although other knowledge-based vision systems use general domain context in reducing the computational burden of image interpretation, to our knowledge, this is the first effort in exploring picture-specific collateral information. We assume that constraints on the picture are generated from a natural language understanding module which processes descriptive text accompanying the pictures. We have developed a unified framework for exploiting these constraints both in the object location and identification (labeling) stage. In particular, we describe a technique for incorporating constrained search in context-based vision. Finally, we demonstrate the effectiveness of this approach in PICTION, a system that uses captions to label human faces in newspaper photographs.|Rajiv Chopra,Rohini K. Srihari",
        "16733|IJCAI|2007|Planning with Goal Utility Dependencies|Work in partial satisfaction planning (PSP) has hitherto assumed that goals are independent thus implying that they have additive utility values. In many real-world problems, we cannot make this assumption. In this paper, we motivate the need for handling various types of goal utility dependence in PSP. We provide a framework for representing them using the General Additive Independence model and investigate two different approaches to handle this problem () compiling PSP with utility dependencies to Integer Programming () extending forward heuristic search planning to handle PSP goal dependencies. To guide the forward planning search, we introduce a novel heuristic framework that combines costpropagation and Integer Programming to select beneficial goals to find an informative heuristic estimate. The two implemented planners, iPUD and SPUDS, using the approaches discussed above, are compared empirically on several benchmark domains. While iPUD is more readily amendable to handle goal utility dependencies and can provide bounded optimality guarantees, SPUDS scales much better.|Minh Binh Do,J. Benton,Menkes van den Briel,Subbarao Kambhampati",
        "13691|IJCAI|1977|Model-Building in the Visions System|The semantic Interpretation system of VISIONS receives a segmentation containing symbolically encoded regions, boundaries and endpoints. The model-builder consists of four major components ) multiple levels of representation for both the image-specific model and long-term general knowledge, ) a set of modular knowledge sources, ) a hierarchical modular strategy to control application of the knowledge sources, and ) a tree representing the current state of search through the space of possible models.|T. Williams,John D. Lowrance,Allen R. Hanson,Edward M. Riseman"
      ],
      [
        "17070|IJCAI|2009|Improving Search in Social Networks by Agent Based Mining|The popularity of social networks have burgeoned in recent years. Users share and access large volumes of information on social networking sites like Facebook, Flickr, del.icio.us, etc. Whereas a few of these sites have generic, impersonal searching mechanisms, we have developed an agent-based framework that mines the social network of a user to improve search results. Our Social Network-based Item Search (SNIS) system uses agents that utilize the connections of a user in the social network to facilitate the search for items of interest. Our approach generates targeted search results that can improve the precision of the result returned from a user's query. We have implemented the SNIS agent-based framework in Flickr, a photo-sharing social network, for searching for photos by using tag lists as search queries. We discuss the architecture of SNIS, motivate the searching scheme used, and demonstrate the effectiveness of the SNIS approach by presenting results. We also show how SNIS can be utilized for expertise location.|Anil G\u00fcrsel,Sandip Sen",
        "16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan",
        "15077|IJCAI|1993|Case Retrieval through Multiple Indexing and Heuristic Search|We discuss the indexing of cases for use in precedent-based argument. Our focus is on how multiple, related indices into a case base of legal precedents are exploited by an argument-generation program called BankXX. This system's architecture and control scheme are rooted in a conceptualization of legal argument as heuristic search. Although our framing argument as search is not discussed in detail in this paper, we describe the main features of this view to provide context for a discussion of an indexing scheme that facilitates argument creation. We describe five inter-related index types--citation, prototypical story, factor, family resemblance, and legal theory indices-- and show how they can be used to access, view, widen, or filter a set of cases. The application domain is a U.S. Federal statute that governs the approval of bankruptcy plans.|Edwina L. Rissland,David B. Skalak,M. Timur Friedman",
        "13744|IJCAI|1981|Use of Data Representation Mapping in Automatic Generation of Data Base Access Procedures|Formal definitions of data structure mappings are given and are used to transform relational algebraic formula into data base access procedures which search a formally defined data structure. Especially, a hierarcical data structure is introduced to represent a set of relations which are inherently hierarchical. A retrieval procedure using data accesses along a two level hierarchy is obtained from an original formula on flat relations by applying equivalence transformation rules and data representation mappings.|Koichi Furukawa",
        "15927|IJCAI|2003|Intelligent Multimodal Stream Processing|This poster describes methods to enable intelligent access to multimodal information streams. We illustrate these methods in two integrated systems the Broadcast News Editor (BNE) which incorporates image, speech, and language processing and the Broadcast News Navigator (BNN) which provides search, visualization and personalized access to broadcast news video. BNN enables users to perform keyword and named entity search, temporally and geospatially visualize entities and stories, cluster stories, discover entity relations, and obtain personalized multimedia summaries. By transforming access from sequential to direct search and providing hierarchical hyperlinked summaries, BNE and BNN enable users to access topics and entity news clusters nearly three times as fast as direct search of video.|Mark T. Maybury",
        "14310|IJCAI|1985|A Predicate Connection Graph Based Logic with Flexible Control|The FDE has been designed to support multiple search strategies for logic programs. This machine represents the knowledge base in a strategy independent fashion as a predicate connection graph which encodes potential unifications between predicates. It facilitates knowledge representation in the language of full first order predicate calculus Immediate developments include implementation of various database access strategies and addition of evaluable predicates and functions to the language. Long-term research will focus on exploration of search strategies, especially for parallel logic machines.|Richard Whitney,Darrel J. Van Buer,Donald P. McKay,Dan Kogan,Lynette Hirschman,Rebecca Davis",
        "13707|IJCAI|1981|A General Semantic Analyser for Data Base Access|The paper discusses the design principles and current status of a natural language front end for access to data bases. This Is based on the use, first, of a semantically-oriented question analyser exploiting general, language-wide semantic categories and patterns, rather than data base-specific ones and, second, of a data base-oriented translation component for obtaining search specifications from the meaning representations for questions derived by the analyser. This approach is motivated by the desire to reduce the effort of providing data base-specific material for the front end, by the belief that a general analyser is well suited to the \"casual\" data base user, and by the assumption that the rich semantic apparatus used will be both adequate as a means of analysis and appropriate as a tool for linking the characterisations of input and data language items. The paper describes this approach in more detail, with emphasis on the existing, tested, analyser.|Branimir Boguraev,Karen Sparck Jones",
        "16274|IJCAI|2005|An Heuristic Search based Approach for Moving Objects Tracking|Fast and accurate tracking of moving objects in video streams is a critical process in computer vision. This problem can be formulated as exploration problems and thus can be expressed as a search into a state space based representation approach. However, these search problems are hard to solve because they involve search through a high dimensional space. In this paper, we describe an A* heuristic search for computing efficient search through a space of transformations corresponding to the D motion of the object, where most promising search alternatives are computed by means of integrating target dynamics into the search process, and ideas from information theory are used to guide the search. The paper includes evaluations with video streams that illustrate the efficiency and suitability for real-time vision tasks on general purpose hardware. Moreover, the computational cost to carry out the tracking task is smaller than real time requirements ( ms).|Elena S\u00e1nchez-Nielsen,Mario Hern\u00e1ndez-Tejera",
        "15928|IJCAI|2003|Broadcast News Navigator BNN Demonstration|The Broadcast News Navigator (BNN) is a fully implemented system that incorporates image, speech, and language processing together with visualization and user preference modeling to support intelligent, personalized access to broadcast news video. The demonstration will illustrate the use of the system's underlying machine learning enabled story segmentation and processing, called the Broadcast News Editor (BNE). A live, scenario-based demonstration will illustrate the use of named entity search, temporal visualization of entities, story clustering and geospatial story visualization, discovery of entity relations, and personalized multimedia summary generation. By transforming access from sequential to direct search and providing hierarchical hyperlinked summaries, we will demonstrate how users can access topics and entity specific news clusters nearly three times as fast as direct search of digital video. In short, we will demonstrate intelligent news on demand enabled by a suite of AI technologies.|Mark T. Maybury",
        "13593|IJCAI|1977|A Deductive Question Answering System on Relational Data Bases|This paper describes a new formalization of a deductive question answering system on a relational data base using a theorem proving technique. A theorem proving procedure for a finite domain is investigated and a direct proof procedure based on substitutions of equivalent formulas which employs the breadth first search is introduced. The search strategy is then expanded to set operations of the relational algebra which are in corporated into the proof procedure in order to increase the data base search efficiency. Virtual relations are realized by means of introducing several axioms and utilizing the deductive capability of the logical system. Furthermore, a conditional domain is, introduced as one of the virtual domains and is used to give a relational view to a pseudo relational data base which can represent exceptional cases using some link information. A query transformation system called DBAP (Data Base Access Planner) which embodies those features is implemented in QJJSP.|Koichi Furukawa"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "14726|IJCAI|1989|Single-Agent Parallel Window Search A Summary of Results|We show how node ordering can be combined with parallel window search to quickly find a nearly optimal solution to single-agent problems. First, we show how node ordering by maximum g among nodes with equal I  g + h values can improve the performance of IDA*. We then consider a window search where different processes perform IDA* simultaneously on the same problem but with different cost thresholds. Finally, we combine the two ideas to produce a parallel window search algorithm in which node ordering information is shared among the different processes. Parallel window search can be used to find a nearly optimal solution quickly, improve the solution until it is optimal, and then finally guarantee optimality, depending on the amount of time available.|Curt Powley,Richard E. Korf",
        "16498|IJCAI|2007|Best-First Utility-Guided Search|In many shortest-path problems of practical interest, insufficient time is available to find a provably optimal solution. One can only hope to achieve a balance between search time and solution cost that respects the user's preferences, expressed as a utility function over time and cost. Current stateof-the-art approaches to this problem rely on anytime algorithms such as Anytime A* or ARA*. These algorithms require the use of extensive training data to compute a termination policy that respects the user's utility function. We propose a more direct approach, called BUGSY, that incorporates the utility function directly into the search, obviating the need for a separate termination policy. Experiments in several challenging problem domains, including sequence alignment and temporal planning, demonstrate that this direct approach can surpass anytime algorithms without requiring expensive performance profiling.|Wheeler Ruml,Minh Binh Do",
        "16124|IJCAI|2005|Limited Discrepancy Beam Search|Beam search reduces the memory consumption of best-first search at the cost of finding longer paths but its memory consumption can still exceed the given memory capacity quickly. We therefore develop BULB (Beam search Using Limited discrepancy Backtracking), a complete memory-bounded search method that is able to solve more problem instances of large search problems than beam search and does so with a reasonable runtime. At the same time, BULB tends to find shorter paths than beam search because it is able to use larger beam widths without running out of memory. We demonstrate these properties of BULB experimentally for three standard benchmark domains.|David Furcy,Sven Koenig",
        "15705|IJCAI|2001|Heuristic Search in Infinite State Spaces Guided by Lyapunov Analysis|In infinite state spaces, many standard heuristic search algorithms do not terminate if the problem is unsolvable. Under some conditions, they can fail to terminate even when there are solutions. We show how techniques from control theory, in particular Lyapunov stability analysis, can be employed to prove the existence of solution paths and provide guarantees that search algorithms will find those solutions. We study both optimal search algorithms, such as A*, and suboptimalreal-time search methods. A Lyapunov framework is useful for analyzing infinite-state search problems, and provides guidance for formulating search problems so that they become tractable for heuristic search. We illustrate these ideas with experiments using a simulated robot arm.|Theodore J. Perkins,Andrew G. Barto",
        "15786|IJCAI|2003|Propagate the Right Thing How Preferences Can Speed-Up Constraint Solving|We present an algorithm Pref-AC that limits arc consistency (AC) to the preferred choices of a tree search procedure and that makes constraint solving more efficient without changing the pruning and shape of the search tree. Arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problems such as configuration and scheduling. Moreover, Pref-AC directly computes a preferred solution for treelike constraint satisfaction problems.|Christian Bessi\u00e8re,Ana\u00efs Fabre,Ulrich Junker",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "13935|IJCAI|1983|A - An Efficient Near Admissible Heuristic Search Algorithm|The algorithm A* (Nilsson, ) presents two significant drawbacks. First, in seeking strict optimal solution paths it necessarily has high order of complexity. Second, the algorithm does not explicitly descriminate between the cost of a solution path and the cost of finding the solution path. To confront these problems we propose the algorithm AE, a generalization of A*. Instead of seeking an optimal solution, it seeks one which is within a factor (+e) of optimum (e  ). The basic idea is to avoid doing any search at all on most near optimal partial solutions by sticking to a small number of most fruitful paths. Various strategies for searching for near optimal partial solutions are discussed. Experimental results are presented indicating that A e has average complexity of lower order than A* and compares favorably to the related algorithm Af* (Pearl and Kim, ).|Malik Ghallab,Dennis G. Allard",
        "17094|IJCAI|2009|Completeness and Optimality Preserving Reduction for Planning|Traditional AI search methods search in a state space typically modelled as a directed graph. Prohibitively large sizes of state space graphs make complete or optimal search expensive. A key observation, as exemplified by the SAS+ formalism for planning, is that most commonly a state-space graph can be decomposed into subgraphs, linked by constraints. We propose a novel space reduction algorithm that exploits such structure. The result reveals that standard search algorithms may explore many redundant paths. Our method provides an automatic way to remove such redundancy. At each state, we expand only the subgraphs within a dependency closure satisfying certain sufficient conditions instead of all the subgraphs. Theoretically we prove that the proposed algorithm is completeness-preserving as well as optimality-preserving. We show that our reduction method can significantly reduce the search cost on a collection of planning domains.|Yixin Chen,Guohui Yao",
        "16199|IJCAI|2005|A Greedy Approach to Establish Singleton Arc Consistency|In this paper, we propose a new approach to establish Singleton Arc Consistency (SAC) on constraint networks. While the principle of existing SAC algorithms involves performing a breadth-first search up to a depth equal to , the principle of the two algorithms introduced in this paper involves performing several runs of a greedy search (where at each step, arc consistency is maintained). It is then an original illustration of applying inference (i.e. establishing singleton arc consistency) by search. Using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. Further-more, both space and time complexities are quite competitive.|Christophe Lecoutre,St\u00e9phane Cardon"
      ],
      [
        "13873|IJCAI|1983|TERMINATOR|The Markgraf Karl Refutation Procedure (MKR-Procedure) is an automated theorem prover for sorted logic, based on an extended clause graph calculus, currently under development at the University of Karlsruhe. This paper describes the TERMINATOR module, a component of the MKR-Procedure, which is essentially a very fast algorithm for the search for unit refutations. The TERMINATOR is used as a fast pre-theorem prover as well as an integral component of the system, and is called for different tasks during the search for a proof.|Grigoris Antoniou,Hans J\u00fcrgen Ohlbach",
        "14269|IJCAI|1985|Prolog Extensions Based on Tableau Calculus|The intention of this paper is to help bridging the gap between logic programming and theorem proving. It presents the design of a Gentzen type proof search procedure, based on classical tableau calculus, for knowledge bases consisting of arbitrary first order formulas. At each proof search step, when a new formula is to be chosen from the knowledge base, the procedure chooses in such a way that the search space is small. When applied to a Horn clause knowledge base and an atomic goal, it performs the same proof search steps as any PROLOG interpreter would do. Hence, PROLOG can be viewed as a special Gentzen type procedure just as it is a special (namely, linear input) resolution procedure.|Wolfgang Sch\u00f6nfeld",
        "14855|IJCAI|1991|Consolution and its Relation with Resolution|In this paper the method of consolution for clause form theorem proving is introduced. Consolution is based on the connection method. This means that in a consolution derivation the paths through the input formula are checked for complementarity. In contrast to resolution this checking can be done in a systematic way as in any connection calculus. It is proved that the consolution calculus presented here is sound and complete and that it can simulate resolution step by step and is a generalization of resolution. It combines the advantages of the connection method such as the directedness of search with the advantages of resolution such as the possibility of the use of lemmata.|Elmar Eder",
        "13441|IJCAI|1975|On the Structure Of An Important Class Of Exhaustive Problems And On Ways Of Search Reduction For Them|The paper discusses necessity of structuring a search tree. A theorem is stated that - the procedure is the only search reduction procedure for non-structured minimay problems. For a class of problems structure in some way a non-trivial search reduction method is described.|G. M. Adelson-Velskiy,V. L. Arlazarov,M. V. Donskoy",
        "13887|IJCAI|1983|Equality Reasoning in Clause Graphics|A method to control the application of equality derivation rules in an automatic theorem proving system is presented. When handling equality in the search for a proof of a theorem two main problems arise ) to obtain a control mechanism for the search and application of useful equality derivation steps in order to support global strategies which plan and control the whole proof, thus conducing to an efficient and complete proof procedure. ) to find proper equations rendering two terms unifiable. These problems are solved by combining the clause graph method and the Mparamodulation-if-needed\" idea by introducing Morris' E-resolution into the clause graph proof procedure. The necessary equations to form possible E-resolvents are searched for in the initial graph and are inherited afterwards. The search space for possible E-resolutions will be reduced by exploiting constraints using the information in the clause graph.|Karl-Hans Bl\u00e4sius",
        "15255|IJCAI|1995|Towards Efficient Default Reasoning|A decision method for Reiter's default logic is developed. It can determine whether a default theory has an extension, whether a formula is in some extension of a default theory and whether a formula is in every extension of a default theory. The method handles full propositional default logic. It can be implemented to work in polynomial space and by using only a theorem prover for the underlying propositional logic as a subroutine. The method divides default reasoning into two major subtasks the search task of examining every alternative for extensions, which is solved by backtracking search, and the classical reasoning task, which can be implemented by a theorem prover for the underlying classical logic. Special emphasis is given to the search problem. The decision method employs a new compact representation of extensions which reduces the search space. Efficient techniques for pruning the search space further are developed.|Ilkka Niemel\u00e4",
        "15383|IJCAI|1997|Depth-bounded Discrepancy Search|Many search trees are impractically large to explore exhaustively. Recently, techniques like limited discrepancy search have been proposed for improving the chance of finding a goal in a limited amount of search. Depth-bounded discrepancy search offers such a hope. The motivation behind depth-bounded discrepancy search is that branching heuristics are more likely to be wrong at the top of the tree than at the bottom. We therefore combine one of the best features of limited discrepancy search-the ability to undo early mistakes-with the completeness of iterative deepening search. We show theoretically and experimentally that this novel combination outperforms existing techniques.|Toby Walsh",
        "13748|IJCAI|1981|A Feature-Based Scene Matcher|A method is presented for matching two scene descriptions, each of which consists of a set of measured feature vectors with estimated uncertainties. The two scenes differ by a transformation that depends on a few unknown parameters. The method performs a search by sequentially matching features of one scene to those of the other scene, solving for the transformation parameters by means of a generalized least-squares adjustment, computing the probabilities of these matches by means of Bayes theorem, and using these probabilities to prune the search. An example is given using scene descriptions of the Martian surface in which the features are rocks approximated by ellipsoids.|Donald B. Gennery",
        "13402|IJCAI|1973|The Q Algorithm - A Search Strategy for a Deduclive Question-Answering System|An approach for bringing semantic, as well as syntactic, information to bear on the problem of theorem-proving search for Question-Answering (QA) Systems is descrilsed. The approach is embodied in a search algorithm, the Q* search algorithm, developed to control deductive searches in an experimental system. The Q* algorithm is part of a system, termed the Maryland Refutation Proof Procedure System (MRPPS), which incorporates both the Q* algorithm, which performs the search required to answer a query, and an inferential component, which performs the logical manipulations necessary to deduce a clause from one or two other clauses. The inferential component includes many refinements of resolution. The Q* algorithm generates nodes in the search space, applying semantic and syntactic information to direct the search. The use of semantics permits paths to be terminated and fruitful paths to be explored. The paper is restricted to a description of the use of syntactic and semantic information in the Q* algorithm.|Jack Minker,Daniel H. Fishman,James R. McSkimin",
        "15088|IJCAI|1993|SCOTT A Model-Guided Theorem Prover|SCOTT (Semantically Constrained Otter) is a resolution-based automatic theorem prover for first order logic. It is based on the high performance prover OTTER by W. McCune and also incorporates a model generator. This finds finite models which SCOTT is able to use in a variety of ways to direct its proof search. Clauses generated by the prover are in turn used as axioms of theories to be modelled. Thus prover and model generator inform each other dynamically. This paper describes the algorithm and some sample results.|John K. Slaney"
      ],
      [
        "15563|IJCAI|1999|Automatic Diagnosis of Student Programs in Programming Learning Environments|This paper describes a method to automate the diagnosis of students' programming errors in programming learning environments. In order to recognize correct students' programs as well as to identify errors in incorrect student programs, programs are represented using an improved dependence graph representation. The student program is compared with a specimen program (also called a model program) at the semantic level after both are standardized by program transformations. The method is implemented using Smalltalk in SIPLeS-II, an automatic program diagnosis system for Samlltalk programming learning environments. The system has been tested on approximately  student programs for various tasks. Experimental results show that, using the method, semantic errors in a student program can be identified rigorously and safely. Semantics-preserving variations in a student program can be eliminated or accommodated. The tests also show that the system can identify a wide range of errors as well as produce indications of the corrections needed. This method is essential for the development of programming learning environments. The techniques of the improved program dependence graph representation, program standardization by transformations, and semantic level program comparison are also useful in other research fields including program understanding and software maintenance.|Songwen Xu,Yam San Chee",
        "14251|IJCAI|1985|A Prototypical Approach to Machine Learning|This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization. A structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.|R. I. Phelps,Peter B. Musgrove",
        "13384|IJCAI|1973|System Organizations for Speech Understanding Implications of Network and Multiprocessor Computer Architecture for AI|This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented control of pro|Lee D. Erman,R. D. Fenneli,Victor R. Lesser,D. R. Reddy",
        "15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore",
        "14385|IJCAI|1987|Interactive Vocabulary Acquisition in XTRA|This paper describes a practical solution to on-line dictionary update in XTRA, a machine translation system developed by Xiuming Huang at the Computing Research Laboratory of New Mexico State University. The focus of the discussion in on IVES - an Interactive Vocabulary Enrichment System built by this writer for XTRA, It reflects an on-going effort at the laboratory to build embedded learning mechanisms in machine translation systems. Two types of learning are discussed, word learning and word sense learning. Each type of learning undergoes three routine processes detection, acquisition, and evaluation. The emphasis of this paper is on the use of semantic preference violations in the detection of the need to learn new word senses.|Cheng-ming Guo",
        "15217|IJCAI|1995|Determining What to Learn Through Component-Task Modeling|Research in machine learning has typically addressed the problem of how and when to learn, and ignored the problem of formulating learning tasks in the first place. This paper addresses this issue in the context of the CASTLE system, that dynamically formulates learning tasks for a given situation. Our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. CASTLE can then focus the learning process on the issues involved in improving the performance of the particular component.|Bruce Krulwich,Lawrence Birnbaum,Gregg Collins",
        "13434|IJCAI|1973|Speech Understanding Through Syntactic and Semantic Analysis|Stanford Research Institute (SRI) is participating in a major program of research on the analysis of continuous speech by computer. The goal is the development of a speech understanding system capable of engaging a human operator in a natural conversation about a specific problem domain. The approach being taken is distinctive in the extent to which it depends on syntactic and semantic processing to guide the acoustic analysis. This correspondence provides a description of the first version of the system, emphasizing the kinds of information that need to be added for effective results.|Donald E. Walker",
        "13409|IJCAI|1973|A Parser for a Speech Understanding System|This paper describes a parsing system specifically designed for spoken rather than written input. The parser is part of a project in progress at Stanford Research Institute to develop a computer system for understanding speech. The approach described uses as much heuristic knowledge as possible in order to minimize the demands on acoustic analysis.|William H. Paxton,Ann E. Robinson",
        "13680|IJCAI|1977|Understanding a Simple Cartoon Film by a Computer Vision System|This paper describes a primitive intelligent system that analyzes and understands simple cartoon films of a dynamic mini-world each film contains a dynamic line image in which an actor, a personified frog named Besi, and objects such as a tree or a rock exist. The first goal of our research is to give the machine vision system the capability of understanding what the frog is doing or meanings of its actions.|Saburo Tsuji,A. Morizono,S. Kuroda",
        "14938|IJCAI|1991|Towards a Model of Grounded Concept Formation|In most research on concept formation within machine learning and cognitive psychology, the features from which concepts are built are assumed to be provided as elementary vocabulary. In this paper, we argue that this is an unnecessarily limited paradigm within which to examine concept formation. Based on evidence from psychology and machine learning, we contend that a principled account of the origin of features can only be given with a grounded model of concept formation, i.e., with a model that incorporates direct access to the world via sensors and manipulators. We discuss the domain of process control as a suitable framework for research into such models, and present a first approach to the problem of developing elementary vocabularies from perceptual sensor data.|Stefan Wrobel"
      ],
      [
        "13689|IJCAI|1977|Planning in the World of the Air Traffic Controller|An enroute air traffic control (ATC) simulation has provided the basis for research into the marriage of discrete simulation and artificial intelligence techniques. A program which simulates, using real world data, the movement of aircraft in an ATC environment forms a robot's world model. Using a production system to respond to events in the simulated world, the robot is able to look ahead and form a plan of instructions which guarantees safe, expedient aircraft transit. A distinction is made between the real world, where pilots can make mistakes, change their minds, etc., and an idealized plan-ahead world which the robot uses the over-all simulation alternates between updating the real world and planning in the idealized one to investigate the robot's ability to plan in the face of uncertainty.|Robert B. Wesson",
        "15306|IJCAI|1995|Adaptable Planner Primitives for Real-World Robotic Applications|With increased processor speed and improved robotic and AI technology, researchers are beginning to design programs that can behave intelligently and interact in the real world. A large increase in processing power has come from parallel machines, but taking advantage of this power is challenging. In this paper we address the issues in designing planners for real-time AI and robotic applications, and provide guiding principles. These principles were designed to minimize the difference between the new real-time model and the standard off-line model. Applying these principles yields a better-structured application, easier design and implementation, and improved performance. The focus of the paper is on a design methodology for implementing effective planners in real-world applications. Using Ephor (our runtime environment), and applying the described planner principles, we demonstrate improved performance in a real-world shepherding application.|Robert W. Wisniewski,Christopher M. Brown",
        "15345|IJCAI|1997|Remote-Brained Robots|We introduce our research approach to investigating real world intelligence by building 'Remote-Brained Robots'. The key idea is that of interfacing AI systems with real-world behaviors through wireless technology. In this approach the robot system is designed to have the brain and body separate, both conceptually and physically. It allows us to tie AI directly to the world, enabling the verification of high-level AI techniques which could previously only be used in simulation. For robotics research, this approach opens the way to the use of large-scale powerful parallel computers. For AI, this approach allows experiments with realistic agents, an essential step to the application of AI in the real world. In this presentation we introduce the remote-brained approach, describe some remote-brained robots and discuss experiments.|Masayuki Inaba",
        "13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris",
        "13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson",
        "14953|IJCAI|1991|Natural Object Recognition A Theoretical Framework and Its Implementation|Most work in visual recognition by computer has focused on recognizing objects by their geometric shape, or by the presence or absence of some prespecified collection of locally measurable attributes (e.g., spectral reflectance, texture, or distinguished markings). On the other hand, most entities in the natural world defy compact description of their shapes, and have no characteristic features with discriminatory power. As a result, image-understanding research has achieved little success toward recognition in natural scenes. We offer a fundamentally new approach to visual recognition that avoids these limitations and has been used to recognize trees, bushes, grass, and trails in ground-level scenes of a natural environment.|Thomas M. Strat,Martin A. Fischler",
        "14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue",
        "13398|IJCAI|1973|Planning Considerations for a Roving Robot with Arm|The Jet Propulsion Laboratory is engaged in a robot research program. The program is aimed at the development and demonstration of technology required to integrate a variety of robntic functions (locomotion, manipulation, sensing and perception, decision making, and man-robot interaction) into a working robot unit operating in a real world environment and dealing with both man-made and natural objects. This paper briefly describes the hardware and software system architecture of the robot breadboard and summarizes the developments to date. The content of the paper is focused on the unique planning considerations involved in incorporating a manipulator as part of an autonomous robot system. In particular, the effects of system architecture, arm trajectory calculations, and arm dynamics and control are discussed in the context of planning arm motion in complex and changing sensory and workspace environments.|Richard A. Lewis,Antal K. Bejczy",
        "15248|IJCAI|1995|Ubiquitous Talker Spoken Language Interaction with Real World Objects|Augmented reality is a research area that tries to embody an electronic information space within the real world through computational devices A crucial issue within thus area is the recognition of real world objects or situations In natural language processing, it is much racier to determine interpietations of utterances even if they art in formed WHEN the context or situation is fixed We therefore introduce robust natural language processing in to a system of augmented reality with situation awareness Based on this idea we have developed a portable system called the Ubiquitous Talker This consists of an LCD display that reflects the scene at which a user is looking as if it is a transparent glass a CCD camera for reognizing real world objects with color bar ID codes a microphone for recognizing a human voice and a speaker which outputs a synthesized voice The Ubiquitous Talker provides its user with some information related to a recognized object by using the display and voice It also accepts requests or questions as voice inputs The user feels as if heshe is talking with the object itself through the system.|Katashi Nagao,Jun Rekimoto",
        "15612|IJCAI|2001|Behavior Planning for a Reflexive Agent|The aim of our research is to build a Reflexive Agent, that is able to either manifest an emotion it is feeling or to hide it. If the Agent decides to manifest its emotion, it can establish what verbal or nonverbal signals to employ in its communication and how to combine and synchronize them. In the decision of whether to express an emotion in a given context, a number of factors are considered, such as the Agent's own personality and goals, the Interlocutor's characteristics and the context. In planning how to communicate an emotion, various factors are considered as well the available modalities (face, gaze, voice etc) the cognitive ease in producing and processing the various signals the expressiveness of every signal in communicating specific meanings and, finally, the appropriateness of signals to social situations.|Berardina De Carolis,Catherine Pelachaud,Isabella Poggi,Fiorella de Rosis"
      ]
    ]
  },
  "abstract": {
    "entropy": 4.8311421358436,
    "topics": [
      "search problem, search algorithm, search using, paper search, planning search, search space, search approach, method search, search show, heuristic search, search results, used search, learning knowledge, local search, different search, search time, use search, solution search, conceptual clustering, present search",
      "search problem, search algorithm, paper search, search using, search space, planning search, search approach, search show, method search, heuristic search, used search, search results, learning knowledge, different search, local search, use search, search time, solution search, present search, knowledge system",
      "search problem, search algorithm, search using, paper search, search space, search approach, planning search, method search, search show, heuristic search, used search, search results, use search, different search, learning knowledge, solution search, local search, search time, present search, knowledge system",
      "search problem, search algorithm, search using, paper search, search space, planning search, search approach, method search, search show, heuristic search, search results, used search, different search, use search, local search, learning knowledge, solution search, search time, present search, conceptual clustering",
      "learn learning, automated reasoning, inductive learning, learning induction, search learning, paper learning, general learning, learning approach, learning results, learning training, theorem search, example learning, resolution search, proof search, learning concept, matching search, learning multiple, learning show, research learning, paper search",
      "planning search, search problem, search approach, paper search, knowledge search, search using, learning knowledge, planning approach, used search, problem planning, search learning, planning domain, planning planner, temporal planning, causal knowledge, paper planning, search domain, learning approach, search algorithm, use search",
      "learning knowledge, web search, learning approach, conceptual clustering, knowledge acquisition, search learning, paper learning, learn learning, research learning, learned learning, explanation-based learning, learning data, learning system, knowledge search, information web, learning training, information search, learning method, learning results, machine learning",
      "artificial intelligence, knowledge system, research system, intelligence research, research paper, knowledge base, knowledge representation, paper system, paper describes, research knowledge, research artificial, knowledge domain, reasoning knowledge, paper knowledge, specific knowledge, research reasoning, knowledge used, describes system, data base, natural language",
      "artificial intelligence, intelligence research, information data, web search, research paper, research system, research information, research data, information web, paper information, paper data, information search, research artificial, queries search, information system, data using, query search, system data, information processing, entities social",
      "knowledge acquisition, knowledge system, automated reasoning, knowledge search, problem knowledge, knowledge base, paper knowledge, knowledge representation, knowledge domain, knowledge used, solving knowledge, reasoning knowledge, specific knowledge, theorem search, heuristic knowledge, proof search, analysis knowledge, resolution search, use knowledge, important knowledge",
      "artificial intelligence, research system, research paper, intelligence research, paper system, paper describes, knowledge domain, paper model, knowledge base, plan planning, knowledge system, control search, knowledge representation, data base, describes system, research artificial, system problem, task learning, specific knowledge, described system",
      "planning search, search problem, search approach, search space, search algorithm, search using, heuristic search, search memory, search domain, state search, problem planning, different search, planning approach, search results, paper search, divide-and-conquer covering, optimal search, planning planner, search show, solution search"
    ],
    "ranking": [
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "14338|IJCAI|1987|An Investigation of Opportunistic Constraint Satisfaction in Space Planning|We are investigating constraint directed heuristic search as a means for performing design in the field of space planning. Space planning is selecting, dimensioning, locating and shaping design units to create two dimensional layouts based on functional, topological and geometrical considerations. Search is carried out using operators at different abstraction levels and design objects at different levels of detail. Constraints are used to represent domain knowledge, to define the search space by specifying operators in means ends analysis manner, and to rate the partial candidate solutions using importances associated with each constraint. Search is carried out opportunistically. The philosophy behind opportunism is that understanding the approximate topology of the search space will lead to efficient search. Uncertainty associated with constraints is derived and used to identify islands of certainty in the search space, which are used as starting points and anchors for search. The knowledge that enables us to identify opportunistic decisions are interactions between constraints and the usefulness of a constraint in different situations. The resulting uncertainty measure will be tested by observing the problem solving behavior it causes in different search spaces.|Can A. Baykan,Mark S. Fox",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji",
        "15464|IJCAI|1999|Switching from Bidirectional to Unidirectional Search|Recently, we showed that for traditional bidirectional search with \"front-to-end\" evaluations, it is not the meeting of search fronts but the cost of proving the optimality of a solution that is problematic. Using our improved understanding of the problem, we developed a new approach to improving this kind of search switching to unidirectional search after the search frontiers meet for the first time (with the first solution found). This new approach shows improvements over previous bidirectional search approaches and (partly) also over the corresponding unidirectional search approaches in different domains. Together with a special-purpose improvement for the TSP, this approach showed better results than the standard search algorithms using the same knowledge.|Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "15722|IJCAI|2001|Incomplete Tree Search using Adaptive Probing|When not enough time is available to fully explore a search tree, different algorithms will visit different leaves. Depth-first search and depth-bounded discrepancy search, for example, make opposite assumptions about the distribution of good leaves. Unfortunately, it is rarely clear a priori which algorithm will be most appropriate for a particular problem. Rather than fixing strong assumptions in advance, we propose an approach in which an algorithm attempts to adjust to the distribution of leaf costs in the tree while exploring it. By sacrificing completeness, such flexible algorithms can exploit information gathered during the search using only weak assumptions. As an example, we show how a simple depth-based additive cost model of the tree can be learned on-line. Empirical analysis using a generic tree search problem shows that adaptive probing is competitive with systematic algorithms on a variety of hard trees and outperforms them when the node-ordering heuristic makes many mistakes. Results on boolean satisfiability and two different representations of number partitioning confirm these observations. Adaptive probing combines the flexibility and robustness of local search with the ability to take advantage of constructive heuristics.|Wheeler Ruml",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "14338|IJCAI|1987|An Investigation of Opportunistic Constraint Satisfaction in Space Planning|We are investigating constraint directed heuristic search as a means for performing design in the field of space planning. Space planning is selecting, dimensioning, locating and shaping design units to create two dimensional layouts based on functional, topological and geometrical considerations. Search is carried out using operators at different abstraction levels and design objects at different levels of detail. Constraints are used to represent domain knowledge, to define the search space by specifying operators in means ends analysis manner, and to rate the partial candidate solutions using importances associated with each constraint. Search is carried out opportunistically. The philosophy behind opportunism is that understanding the approximate topology of the search space will lead to efficient search. Uncertainty associated with constraints is derived and used to identify islands of certainty in the search space, which are used as starting points and anchors for search. The knowledge that enables us to identify opportunistic decisions are interactions between constraints and the usefulness of a constraint in different situations. The resulting uncertainty measure will be tested by observing the problem solving behavior it causes in different search spaces.|Can A. Baykan,Mark S. Fox",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji",
        "14431|IJCAI|1987|Subgoal Ordering and Goal Augmentation for Heuristic Problem Solving|In order to improve the performance of heuristic search for finding optimal solutions, two high level problem solving strategies, namely, subgoal ordering and goal augmentation, have been developed. The essence of these two strategies is to make explicit the knowledge embedded in a general problem formulation which can be used to constrain the solution search space. These two strategies have been incorporated into a methodology which we previously developed for automatically generating admissible search heuristics. The effectiveness of these strategies is demonstrated by the application to robot optimal task planning problems.|Keki B. Irani,Jie Cheng",
        "15464|IJCAI|1999|Switching from Bidirectional to Unidirectional Search|Recently, we showed that for traditional bidirectional search with \"front-to-end\" evaluations, it is not the meeting of search fronts but the cost of proving the optimality of a solution that is problematic. Using our improved understanding of the problem, we developed a new approach to improving this kind of search switching to unidirectional search after the search frontiers meet for the first time (with the first solution found). This new approach shows improvements over previous bidirectional search approaches and (partly) also over the corresponding unidirectional search approaches in different domains. Together with a special-purpose improvement for the TSP, this approach showed better results than the standard search algorithms using the same knowledge.|Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "14338|IJCAI|1987|An Investigation of Opportunistic Constraint Satisfaction in Space Planning|We are investigating constraint directed heuristic search as a means for performing design in the field of space planning. Space planning is selecting, dimensioning, locating and shaping design units to create two dimensional layouts based on functional, topological and geometrical considerations. Search is carried out using operators at different abstraction levels and design objects at different levels of detail. Constraints are used to represent domain knowledge, to define the search space by specifying operators in means ends analysis manner, and to rate the partial candidate solutions using importances associated with each constraint. Search is carried out opportunistically. The philosophy behind opportunism is that understanding the approximate topology of the search space will lead to efficient search. Uncertainty associated with constraints is derived and used to identify islands of certainty in the search space, which are used as starting points and anchors for search. The knowledge that enables us to identify opportunistic decisions are interactions between constraints and the usefulness of a constraint in different situations. The resulting uncertainty measure will be tested by observing the problem solving behavior it causes in different search spaces.|Can A. Baykan,Mark S. Fox",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji",
        "14431|IJCAI|1987|Subgoal Ordering and Goal Augmentation for Heuristic Problem Solving|In order to improve the performance of heuristic search for finding optimal solutions, two high level problem solving strategies, namely, subgoal ordering and goal augmentation, have been developed. The essence of these two strategies is to make explicit the knowledge embedded in a general problem formulation which can be used to constrain the solution search space. These two strategies have been incorporated into a methodology which we previously developed for automatically generating admissible search heuristics. The effectiveness of these strategies is demonstrated by the application to robot optimal task planning problems.|Keki B. Irani,Jie Cheng",
        "15464|IJCAI|1999|Switching from Bidirectional to Unidirectional Search|Recently, we showed that for traditional bidirectional search with \"front-to-end\" evaluations, it is not the meeting of search fronts but the cost of proving the optimality of a solution that is problematic. Using our improved understanding of the problem, we developed a new approach to improving this kind of search switching to unidirectional search after the search frontiers meet for the first time (with the first solution found). This new approach shows improvements over previous bidirectional search approaches and (partly) also over the corresponding unidirectional search approaches in different domains. Together with a special-purpose improvement for the TSP, this approach showed better results than the standard search algorithms using the same knowledge.|Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "14338|IJCAI|1987|An Investigation of Opportunistic Constraint Satisfaction in Space Planning|We are investigating constraint directed heuristic search as a means for performing design in the field of space planning. Space planning is selecting, dimensioning, locating and shaping design units to create two dimensional layouts based on functional, topological and geometrical considerations. Search is carried out using operators at different abstraction levels and design objects at different levels of detail. Constraints are used to represent domain knowledge, to define the search space by specifying operators in means ends analysis manner, and to rate the partial candidate solutions using importances associated with each constraint. Search is carried out opportunistically. The philosophy behind opportunism is that understanding the approximate topology of the search space will lead to efficient search. Uncertainty associated with constraints is derived and used to identify islands of certainty in the search space, which are used as starting points and anchors for search. The knowledge that enables us to identify opportunistic decisions are interactions between constraints and the usefulness of a constraint in different situations. The resulting uncertainty measure will be tested by observing the problem solving behavior it causes in different search spaces.|Can A. Baykan,Mark S. Fox",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15194|IJCAI|1995|How to Use Limited Memory in Heuristic Search|Traditional best-first search for optimal solutions quickly runs out of space even for problem instances of moderate size, and linear-space search has unnecessarily long running times since it cannot make use of available memory. For using available memory effectively, we developed a new generic approach to heuristie search. It integrates various strategies and includes ideas from bidirectional search. Due to insights into different utilizations of available memory, it allows the search to use limited memory effectively. Instantiations of this approach for two different benchmark domains showed excellent results that are statistically significant improvements over previously reported results for finding optimal solutions in the -Puzzle we achieved the fastest searches of all those using the Manhattan distance heuristic as the only knowledge source, and for a scheduling domain our approach can solve much more difficult problems than the best competitor. The most important lessons we learned from the experiments are first, that also in domains with symmetric graph topology selecting the right search direction can be very important, and second, that memory can-- under certain conditions--be used much more effectively than by traditional best-first search.|Hermann Kaindl,Gerhard Kainz,Angelika Leeb,Harald Smetana",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji",
        "15464|IJCAI|1999|Switching from Bidirectional to Unidirectional Search|Recently, we showed that for traditional bidirectional search with \"front-to-end\" evaluations, it is not the meeting of search fronts but the cost of proving the optimality of a solution that is problematic. Using our improved understanding of the problem, we developed a new approach to improving this kind of search switching to unidirectional search after the search frontiers meet for the first time (with the first solution found). This new approach shows improvements over previous bidirectional search approaches and (partly) also over the corresponding unidirectional search approaches in different domains. Together with a special-purpose improvement for the TSP, this approach showed better results than the standard search algorithms using the same knowledge.|Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "15722|IJCAI|2001|Incomplete Tree Search using Adaptive Probing|When not enough time is available to fully explore a search tree, different algorithms will visit different leaves. Depth-first search and depth-bounded discrepancy search, for example, make opposite assumptions about the distribution of good leaves. Unfortunately, it is rarely clear a priori which algorithm will be most appropriate for a particular problem. Rather than fixing strong assumptions in advance, we propose an approach in which an algorithm attempts to adjust to the distribution of leaf costs in the tree while exploring it. By sacrificing completeness, such flexible algorithms can exploit information gathered during the search using only weak assumptions. As an example, we show how a simple depth-based additive cost model of the tree can be learned on-line. Empirical analysis using a generic tree search problem shows that adaptive probing is competitive with systematic algorithms on a variety of hard trees and outperforms them when the node-ordering heuristic makes many mistakes. Results on boolean satisfiability and two different representations of number partitioning confirm these observations. Adaptive probing combines the flexibility and robustness of local search with the ability to take advantage of constructive heuristics.|Wheeler Ruml",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "16247|IJCAI|2005|Phase Transitions within Grammatical Inference|It is now well-known that the feasibility of inductive learning is ruled by statistical properties linking the empirical risk minimization principle and the \"capacity\" of the hypothesis space. The discovery, a few years ago, of a phase transition phenomenon in inductive logic programming proves that other fundamental characteristics of the learning problems may similarly affect the very possibility of learning under very general conditions. Our work examines the case of grammatical inference. We show that while there is no phase transition when considering the whole hypothesis space, there is a much more severe \"gap\" phenomenon affecting the effective search space of standard grammatical induction algorithms for deterministic finite automata (DFA). Focusing on the search heuristics of the RPNI and RED-BLUE algorithms, we show that they overcome this problem to some extent, but that they are subject to overgeneralization. The paper last suggests some directions for new generalization operators, suited to this Phase Transition phenomenon.|Nicolas Pernot,Antoine Cornu\u00e9jols,Mich\u00e8le Sebag",
        "13864|IJCAI|1981|Concept Learning by Structured Examples - An Algebraic Approach|A system learning concepts from training samples consisting of structured objects is described. It is based on descriptions invariant under isomorphism. In order to get a unified mathematical formalism recent graph theoretic results are used- The structures are transformed into feature vectors and after that a concept learning algorithm developing decision trees is applied which is an extension of algorithms found in psychological experiments. It corresponds to a general-to-specific depth-first search with reexamination of past events. The generalization ability is demonstrated by means of the blocks world example and it is shown that the algorithm can successfully handle practical problems with samples of about one hundred of relatively complicated structures in a reasonable time. Additionally, the problem of representation and learning context dependent concepts is discussed in the paper.|Fritz Wysotzki,Werner Kolbe,Joachim Selbig",
        "15798|IJCAI|2003|Simultaneous Adversarial Multi-Robot Learning|Multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. There has been a great deal of recent research on multiagent reinforcement learning in stochastic games, which is the intuitive extension of MDPs to multiple agents. This recent work, although general, has only been applied to small games with at most hundreds of states. On the other hand robot tasks have continuous, and often complex, state and action spaces. Robot learning tasks demand approximation and generalization techniques, which have only received extensive attention in single-agent learning. In this paper we introduce GraWoLF, a general-purpose, scalable, multiagent learning algorithm. It combines gradient-based policy learning techniques with the WoLF (\"Win or Learn Fast\") variable learning rate. We apply this algorithm to an adversarial multi-robot task with simultaneous learning. We show results of learning both in simulation and on the real robots. These results demonstrate that GraWoLF can learn successful policies, overcoming the many challenges in multi-robot learning.|Michael H. Bowling,Manuela M. Veloso",
        "14874|IJCAI|1991|Theoretical Underpinnings of Version Spaces|Mitchell's version-space approach to inductive concept learning has been highly influential in machine learning, as it formalizes inductive concept learning as a search problem-to identify some concept definition out of a space of possible definitions. This paper lays out some theoretical underpinnings of version spaces. It presents the conditions under which an arbitrary set of concept definitions in a concept description language can be represented by boundary sets, which is a necessary condition for a set of concept definitions to be a version space. Furthermore, although version spaces can be intersected and unioned (version spaces are simply sets, albeit with special structure), the result need not be a version space this paper also presents the conditions under which such intersection and union of two version spaces yields a version space (i.e., representable by boundary sets). Finally, the paper shows how the resulting boundary sets after intersections and unions can be computed from the initial boundary sets, and proves the algorithms correct.|Haym Hirsh",
        "15730|IJCAI|2001|Learning on the Phase Transition Edge|A previous research has shown that most learning strategies fail to learn relational concepts when descriptions involving more than three variables are required. The reason resides in the emergence of a phase transition in the covering test. After an in depth analysis of this aspect, this paper proposes an alternative learning strategy, combining a Monte Carlo stochastic search with local deterministic search. This approach offers two main benefits on the one hand, substantial advantages over more traditional search algorithms, in terms of increased learning ability, and, on the other, the possibility of an a-priori estimation of the cost for solving a learning problem, under specific assumptions about the target concept.|Alessandro Serra,Attilio Giordana,Lorenza Saitta",
        "14499|IJCAI|1987|Layered Concept-Learning and Dynamically Variable Bias Management|Concept learning is inherently complex. Without severe constraint or inductive \"bias,\" the general problem is intractable. While most learning systems have been designed with built-in biases, these systems typically work well only in narrowly circumscribed problem domains. Here we present a model of concept formation that views learning as a simultaneous optimization problem at three different levels, with dynamically chosen biases guiding the search for satisfactory hypotheses. In this model, the partitioning of events into classes occurs through dynamic interactions among three layers event space, hypothesis space, and bias space. This view of the induction process may help clarify the problem of learning and lead to more general and efficient induction systems. To test this model of meta-knowledge, a variable bias management system (VBMS) has been designed and partly implemented. The system will dynamically alter evolving hypotheses, concept representation languages, and concept formation algorithms by monitoring progress and selecting biases based on characteristics of the particular induction problems presented. VBMS is designed to learn the best biases for different types of induction problems. Thus it is robust (effective and efficient in many domains). The system can learn incrementally despite noisy data at any level.|Larry A. Rendell,Raj Sheshu,David K. Tcheng",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell",
        "17066|IJCAI|2009|Learning Probabilistic Hierarchical Task Networks to Capture User Preferences|While much work on learning in planning focused on learning domain physics (i.e., action models), and search control knowledge, little attention has been paid towards learning user preferences on desirable plans. Hierarchical task networks (HTN) are known to provide an effective way to encode user prescriptions about what constitute good plans. However, manual construction of these methods is complex and error prone. In this paper, we propose a novel approach to learning probabilistic hierarchical task networks that capture user preferences by examining user-produced plans given no prior information about the methods (in contrast, most prior work on learning within the HTN framework focused on learning \"method preconditions\"--i.e., domain physics--assuming that the structure of the methods is given as input). We will show that this problem has close parallels to the problem of probabilistic grammar induction, and describe how grammar induction methods can be adapted to learn task networks. We will empirically demonstrate the effectiveness of our approach by showing that task networks we learn are able to generate plans with a distribution close to the distribution of the user-preferred plans.|Nan Li,Subbarao Kambhampati,Sung Wook Yoon",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney"
      ],
      [
        "16251|IJCAI|2005|Automated Composition of Web Services by Planning at the Knowledge Level|In this paper, we address the problem of the automated composition of web services by planning on their \"knowledge level\" models. We start from descriptions of web services in standard process modeling and execution languages, like BPELWS, and automatically translate them into a planning domain that models the interactions among services at the knowledge level. This allows us to avoid the explosion of the search space due to the usually large and possibly infinite ranges of data values that are exchanged among services, and thus to scale up the applicability of state-of-the-art techniques for the automated composition of web services. We present the theoretical framework, implement it, and provide an experimental evaluation that shows the practical advantage of our approach w.r.t. techniques that are not based on a knowledgelevel representation.|Marco Pistore,Annapaola Marconi,Piergiorgio Bertoli,Paolo Traverso",
        "16218|IJCAI|2005|Planning with Continuous Resources in Stochastic Domains|We consider the problem of optimal planning in stochastic domains with resource constraints, where resources are continuous and the choice of action at each step may depend on the current resource level. Our principal contribution is the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. The search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. We claim that this approach is especially effective when resource limitations contribute to reachability constraints. Experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.|Mausam,Emmanuel Benazera,Ronen I. Brafman,Nicolas Meuleau,Eric A. Hansen",
        "16832|IJCAI|2009|Stratified Planning|Most planning problems have strong structures. They can be decomposed into subdomains with causal dependencies. The idea of exploiting the domain decomposition has motivated previous work such as hierarchical planning and factored planing. However, these algorithms require extensive backtracking and lead to few efficient general-purpose planners. On the other hand, heuristic search has been a successful approach to automated planning. The domain decomposition of planning problems, unfortunately, is not directly and fully exploited by heuristic search. We propose a novel and general framework to exploit domain decomposition. Based on a structure analysis on the SAS+ planning formalism, we stratify the sub-domains of a planning problem into dependency layers. By recognizing the stratification of a planning structure, we propose a space reduction method that expands only a subset of executable actions at each state. This reduction method can be combined with state-space search, allowing us to simultaneously employ the strength of domain decomposition and high-quality heuristics. We prove that the reduction preserves completeness and optimality of search and experimentally verify its effectiveness in space reduction.|Yixin Chen,You Xu,Guohui Yao",
        "15594|IJCAI|2001|Planning in Nondeterministic Domains under Partial Observability via Symbolic Model Checking|Planning under partial observability is one of the most significant and challenging planning problems. It has been shown to be hard, both theoretically and experimentally. In this paper, we present a novel approach to the problem of planning under partial observability in non-deterministic domains. We propose an algorithm that searches through a (possibly cyclic) and-or graph induced by the domain. The algorithm generates conditional plans that are guaranteed to achieve the goal despite of the uncertainty in the initial condition, the uncertain effects of actions, and the partial observability of the domain. We implement the algorithm by means of BDD-based, symbolic model checking techniques, in order to tackle in practice the exponential blow up of the search space. We show experimentally that our approach is practical by evaluating the planner with a set of problems taken from the literature and comparing it with other state of the art planners for partially observable domains.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri,Paolo Traverso",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "15582|IJCAI|2001|Planning with Resources and Concurrency A Forward Chaining Approach|Recently tremendous advances have been made in the performance of AI planning systems. However increased performance is only one of the prerequisites for bringing planning into the realm of real applications advances in the scope of problems that can be represented and solved must also be made. In this paper we address two important representational features, concurrently executable actions with varying durations, and metric quantities like resources, both essential for modeling real applications. We show how the forward chaining approach to planning can be extended to allow it to solve planning problems with these two features. Forward chaining using heuristics or domain specific information to guide search has shown itself to be a very promising approach to planning, and it is sensible to try to build on this success. In our experiments we utilize the TLPLAN approach to planning, in which declaratively represented control knowledge is used to guide search. We show that this extra knowledge can be intuitive and easy to obtain, and that with it impressive planning performance can be achieved.|Fahiem Bacchus,Michael Ady",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "15720|IJCAI|2001|Robot Weightlifting By Direct Policy Search|This paper describes a method for structuring a robot motor learning task. By designing a suitably parameterized policy, we show that a simple search algorithm, along with biologically motivated constraints, offers an effective means for motor skill acquisition. The framework makes use of the robot counterparts to several elements found in human motor learning imitation, equilibrium-point control, motor programs, and synergies. We demonstrate that through learning, coordinated behavior emerges from initial, crude knowledge about a difficult robot weightlifting task.|Michael T. Rosenstein,Andrew G. Barto",
        "16373|IJCAI|2007|Learning User Clicks in Web Search|Machine learning for predicting user clicks in Web-based search offers automated explanation of user activity. We address click prediction in the Web search scenario by introducing a method for click prediction based on observations of past queries and the clicked documents. Due to the sparsity of the problem space, commonly encountered when learning for Web search, new approaches to learn the probabilistic relationship between documents and queries are proposed. Two probabilistic models are developed, which differ in the interpretation of the query-document co-occurrences. A novel technique, namely, conditional probability hierarchy, flexibly adjusts the level of granularity in parsing queries, and, as a result, leverages the advantages of both models.|Ding Zhou,Levent Bolelli,Jia Li,C. Lee Giles,Hongyuan Zha",
        "14251|IJCAI|1985|A Prototypical Approach to Machine Learning|This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization. A structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.|R. I. Phelps,Peter B. Musgrove",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "14487|IJCAI|1987|Using Prior Learning to Facilitate the Learning of New Causal Theories|We present an approach to learning causal knowledge which lies in between two extremely different approaches to learning  empirical methods (e.g., ,) which detect similarities and differences between between examples to reveal regularities.  explanation-based methods (e.g., ,) which derive a causal explanation for a single event from existing causal knowledge. The event and the causal explanation are generalized to create a new \"chunk\" of causal knowledge by retaining only those features of the event which were needed to produce the explanation. In the approach to learning presented in this paper and implemented in a program called OCCAM, prior knowledge indicating what sort of distinctions have proven useful in the past influences the search for causal hypotheses. Our approach to learning snares a goal with explanation-based learning to allow existing knowledge to facilitate future learning so that fewer examples are required. However, it does not share one shortcoming of explanation-based learning since it can create causal theories which are not implications of existing causal theories.|Michael J. Pazzani,Michael G. Dyer,Margot Flowers",
        "17066|IJCAI|2009|Learning Probabilistic Hierarchical Task Networks to Capture User Preferences|While much work on learning in planning focused on learning domain physics (i.e., action models), and search control knowledge, little attention has been paid towards learning user preferences on desirable plans. Hierarchical task networks (HTN) are known to provide an effective way to encode user prescriptions about what constitute good plans. However, manual construction of these methods is complex and error prone. In this paper, we propose a novel approach to learning probabilistic hierarchical task networks that capture user preferences by examining user-produced plans given no prior information about the methods (in contrast, most prior work on learning within the HTN framework focused on learning \"method preconditions\"--i.e., domain physics--assuming that the structure of the methods is given as input). We will show that this problem has close parallels to the problem of probabilistic grammar induction, and describe how grammar induction methods can be adapted to learn task networks. We will empirically demonstrate the effectiveness of our approach by showing that task networks we learn are able to generate plans with a distribution close to the distribution of the user-preferred plans.|Nan Li,Subbarao Kambhampati,Sung Wook Yoon",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney",
        "14706|IJCAI|1989|The Effect of Rule Use on the Utility of Explanation-Based Learning|The utility problem in explanation-based learning concerns the ability of learned rules or plans to actually improve the performance of a problem solving system. Previous research on this problem has focused on the amount, content, or form of learned information. This paper examines the effect of the use of learned information on performance. Experiments and informal analysis show that unconstrained use of learned rules eventually leads to degraded performance. However, constraining the use of learned rules helps avoid the negative effect of learning and lead to overall performance improvement. Search strategy is also shown to have a substantial effect on the contribution of learning to performance by affecting the manner in which learned rules arc used. These effects help explain why previous experiments have obtained a variety of different results concerning the impact of explanation-based learning on performance.|Raymond J. Mooney"
      ],
      [
        "13781|IJCAI|1981|Time-Oriented Features for Medical Consultation Systems|Medical consultation system is one of the major application of artificial intelligence research, and in this field, it is important to treat time-oriented data. The authors developed a system named MECS-Al (MEdical Consultation System by means of Artificial Intelligence), which is designed as a general purpose tool for constructing systems with capability to treat time-oriented data. The basic idea of this system is to describe the time flow as a chain of discrete events. As a general purpose tool, the system consists of an inference-engine and a knowledge-base editor, so that consultation systems in any field can be easily defined and tested.|T. Koyama,S. Kaihara,T. Minamikawa,T. Kurokawa",
        "13971|IJCAI|1983|A Deductive Model of Belief|Representing and reasoning about the knowledge an agent (human or computer) must have to accomplish some task is becoming an increasingly important issue in artificial intelligence (AI) research. To reason about an agent's beliefs, an AI system must assume some formal model of those beliefs. An attractive candidate is the Deductive Belief model an agent's beliefs are described as a set of sentences in some formal language (the base sentences), together with a deductive process for deriving consequences of those beliefs. In particular, a Deductive Belief model can account for the effect of resource limitations on deriving consequences of the base set an agent need not believe all the logical consequences of his beliefs. In this paper we develop a belief model based on the notion of deduction, and contrast it with current AI formalisms for belief derived from HintikkaKripke possible-worlds semantics for knowledge.|Kurt Konolige",
        "14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,Fran\u00e7ois Felix Ingrand",
        "13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico",
        "14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz",
        "14496|IJCAI|1987|The Classification Detection and Handling of Imperfect Theory Problems|In recent years knowledge-based techniques like explanation-based learning, qualitative reasoning and case-based reasoning have been gaining considerable popularity in AI. Such knowledge-based methods face two difficult problems ) the performance of the system is fundamentally limited by the knowledge initially encoded into its domain theory ) the encoding of just the right knowledge to enable the system to function properly over a wide range of tasks and situations is virtually impossible for a complex domain. This paper describes research directed towards the construction of a system that will detect and correct problems with domain theories. This will enable knowledge-based systems to operate with imperfect domain theories and automatically correct the imperfections whenever they pose problems. This paper discusses the classification of imperfect theory problems, strategies for their detection and an approach based on experiment design to handle different types of imperfect theory problems.|Shankar A. Rajamoney,Gerald DeJong",
        "13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris",
        "16590|IJCAI|2007|Semantic Indexing of a Competence Map to Support Scientific Collaboration in a Research Community|This paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. The taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences, called Knowledge Map. The KMap is a system to support research collaborations and sharing of results within and beyond a European Network of Excellence. The methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members - and to use this model for improving accessibility of data and knowledge repositories.|Paola Velardi,Roberto Navigli,Micha\u00ebl Petit",
        "14283|IJCAI|1985|Representation and Use of Explicit Justifications for Knowledge Base Refinements|We discuss the representation and use of justification structures as an aid to knowledge base refinement We show how justifications can be used by a system to generate explanations - for its own use-of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.|Reid G. Smith,Howard A. Winston,Tom M. Mitchell,Bruce G. Buchanan",
        "16591|IJCAI|2007|Case Base Mining for Adaptation Knowledge Acquisition|In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.|Mathieu d'Aquin,Fadi Badra,Sandrine Lafrogne,Jean Lieber,Amedeo Napoli,Laszlo Szathmary"
      ],
      [
        "13525|IJCAI|1975|A Database For AI|It is argued that progress in A.I. research requires reference data concerning cognitive processing. It is proposed that such information relevant to the needs of A.I. can be made available by the controlled study of manmachine interactive problem solving, a paradigm we have called \"The Cognitive Test-Bed\". The system currently implemented at Reading is described and the methods of results analysis discussed.|J. Quinton,A. Andrew",
        "15087|IJCAI|1993|Retrieving Cases from Relational Data-Bases Another Stride Towards Corporate-Wide Case-Base Systems|Vital information for corporate activities is generally stored in large databases. While conventional data-base management systems offer limited query flexibility, systems capable of generating similarity-based queries, such as those seen in case-based reasoning research, would certainly enhance the utility of data resources. This paper describes a method for building case-based systems using a conventional relational data-base (RDB). The core of the algorithm is a novel approach to similarity computing in which database query form similarities, rather than similarities of individual cases, are computed. The method uses Standard Query Language (SQL) to achieve nearest neighbor matching, thus allowing similarity-based database retrieval. It has been implemented as a part of the CARET case retrieval tool and evaluated through the use of a newly developed corporate-wide case-based system for a software quality control domain. Experiments have shown the proposed method to provide retrieval results equivalent to those of non-RDB implementation at a sufficiently fast response time.|Hideo Shimazu,Hiroaki Kitano,Akihiro Shibata",
        "15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore",
        "16443|IJCAI|2007|Improving Author Coreference by Resource-Bounded Information Gathering from the Web|Accurate entity resolution is sometimes impossible simply due to insufficient information. For example, in research paper author name resolution, even clever use of venue, title and coauthorship relations are often not enough to make a confident coreference decision. This paper presents several methods for increasing accuracy by gathering and integrating additional evidence from the web. We formulate the coreference problem as one of graph partitioning with discriminatively-trained edge weights, and then incorporate web information either as additional features or as additional nodes in the graph. Since the web is too large to incorporate all its data, we need an efficient procedure for selecting a subset of web queries and data. We formally describe the problem of resource bounded information gathering in each of these contexts, and show significant accuracy improvement with low cost.|Pallika Kanani,Andrew McCallum,Chris Pal",
        "16847|IJCAI|2009|Web-Scale N-gram Models for Lexical Disambiguation|Web-scale data has been used in a diverse range of language research. Most of this research has used web counts for only short, fixed spans of context. We present a unified view of using web counts for lexical disambiguation. Unlike previous approaches, our supervised and unsupervised systems combine information from multiple and overlapping segments of context. On the tasks of preposition selection and context-sensitive spelling correction, the supervised system reduces disambiguation error by -% over the current state-of-the-art.|Shane Bergsma,Dekang Lin,Randy Goebel",
        "15907|IJCAI|2003|Web Intelligence WI What Makes Wisdom Web|Web Intelligence (WI) sheds new light on directions for scientific research and development which explores the fundamental roles as well as practical impacts of Artificial Intelligence (AI) and advanced Information Technology (IT) on the next generation of Web-empowered products, systems, services, and activities. This paper gives new perspectives on the future WI research and highlights some of the research challenges and initiatives.|Jiming Liu",
        "16730|IJCAI|2007|Exploiting Image Contents in Web Search|Web search is a challenging task. Previous research mainly exploits texts on the Web pages or link information between the pages, while multimedia information is largely ignored. This paper proposes a new framework for Web search, which exploits image contents to help improve the search performance. In this framework, candidate images are retrieved at first by considering their associated text information. Then, images related to the query are identified by analyzing the density of the visual feature space. After that, an image-based rank of the Web pages is generated, which is combined with the traditional keyword-based search result to produce the final search result. Experiments demonstrate the promise of the proposed framework.|Zhi-Hua Zhou,Hong-Bin Dai",
        "17005|IJCAI|2009|Relation Regularized Matrix Factorization|In many applications, the data, such as web pages and research papers, contain relation (link) structure among entities in addition to textual content information. Matrix factorization (MF) methods, such as latent semantic indexing (LSI), have been successfully used to map either content information or relation information into a lower-dimensional latent space for subsequent processing. However, how to simultaneously model both the relation information and the content information effectively with an MF framework is still an open research problem. In this paper, we propose a novel MF method called relation regularized matrix factorization (RRMF) for relational data analysis. By using relation information to regularize the content MF procedure, RRMF seamlessly integrates both the relation information and the content information into a principled framework. We propose a linear-time learning algorithm with convergence guarantee to learn the parameters of RRMF. Extensive experiments on real data sets show that RRMF can achieve state-of-the-art performance.|Wu-Jun Li,Dit-Yan Yeung",
        "13614|IJCAI|1977|Ghosts in the Machine An AI Treatment of Medieval History|This paper gives a generalized overview of RESEDA, an interactive question answering system designed primarily for use by historians. Its data base consists of historical information, which attemps to describe the attitudes, political, religious and interpersonal, of the chief characters of the period. Question answering is done by search of the data base and by inference on the information therein. The difficulties of representing this type of data and of formulating inference rules dealing with human motivations and attitudes is also discussed.|Margaret King,Monique Ornato,Gian Piero Zarri,L. Zarri-Baldi,A. Zwiebel",
        "13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson"
      ],
      [
        "14269|IJCAI|1985|Prolog Extensions Based on Tableau Calculus|The intention of this paper is to help bridging the gap between logic programming and theorem proving. It presents the design of a Gentzen type proof search procedure, based on classical tableau calculus, for knowledge bases consisting of arbitrary first order formulas. At each proof search step, when a new formula is to be chosen from the knowledge base, the procedure chooses in such a way that the search space is small. When applied to a Horn clause knowledge base and an atomic goal, it performs the same proof search steps as any PROLOG interpreter would do. Hence, PROLOG can be viewed as a special Gentzen type procedure just as it is a special (namely, linear input) resolution procedure.|Wolfgang Sch\u00f6nfeld",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "15841|IJCAI|2003|The Knowledge Required to Interpret Noun Compounds|Noun compound interpretation is the task of determining the semantic relations among the constituents of a noun compound. For example, \"concrete floor\" means a floor made of concrete, while \"gymnasium floor\" is the floor region of a gymnasium. We would like to enable knowledge acquisition systems to interpret noun compounds, as part of their overall task of translating imprecise and incomplete information into formal representations that support automated reasoning. However, if interpreting noun compounds requires detailed knowledge of the constituent nouns, then it may not be worth doing the cost of acquiring this knowledge may outweigh the potential benefit. This paper describes an empirical investigation of the knowledge required to interpret noun compounds. It concludes that the axioms and ontological distinctions important for this task are derived from the top levels of a hierarchical knowledge base (KB) detailed knowledge of specific nouns is less important. This is good news, not only for our work on knowledge acquisition systems, but also for research on text understanding, where noun compound interpretation has a long history. A more detailed version of this paper can be found in Fan et al, .|James Fan,Ken Barker,Bruce W. Porter",
        "13413|IJCAI|1973|The Hearsay Speech Understanding System An Example of the Recognition Process|This paper describes the structure and operation of the Hearsay speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of Knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its Operation in a particular task situation Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.|D. R. Reddy,Lee D. Erman,R. D. Fenneli,Richard B. Neely",
        "13629|IJCAI|1977|Knowledge Base Management for Experiment Planning in Molecular Genetics|The use of a representation language involving schemata and associated derived models has been extended to include all aspects of domain knowledge and strategy and heuristic problem solving knowledge. This uniform representation will allow the extension of knowledge base management techniques for acquisition and retrieval of procedural knowledge.|Nancy Martin,Peter Friedland,Jonathan King,Mark Stefik",
        "14326|IJCAI|1987|Compiling Design Plans from Descriptions of Artifacts and Problem Solving Heuristics|An analysis of the design plans in the Pride expert system shows that they integrate knowledge about structure and functionality of artifacts as well as problem-solving heuristics A method is presented by which such plans can be automatically generated by compiling knowledge about artifacts, problem solving heuristics, and characteristics of specific problems. Knowledge compilation allows the creation of plans tailored to particular problems and offers potential benefits in maintaining a knowledge base, in reusing the same knowledge for different purposes, and in providing a framework for more systematic knowledge acquisition.|Agustin A. Araya,Sanjay Mittal",
        "15388|IJCAI|1999|Integrating Problem-Solving Methods into CYC|This paper argues that the reuse of domain knowledge must be complemented by the reuse of problem-solving methods. Problem-solving methods (PSMs) provide a means to structure search, and can provide tractable solutions to reasoning with a very large knowledge base. We show that PSMs can be used in a way which complements large-scale representation techniques, and optimisations such as those for taxonornie reasoning found in Cyc. Our approach illustrates the advantages of task-oriented knowledge modelling and we demonstrate that the resulting ontologies have both task-dependent and task-independent elements. Further, we show how the task ontology can be organised into conceptual levels to reflect knowledge typing principles.|James S. Aitken,Dimitrios Sklavakis",
        "16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann",
        "16590|IJCAI|2007|Semantic Indexing of a Competence Map to Support Scientific Collaboration in a Research Community|This paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. The taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences, called Knowledge Map. The KMap is a system to support research collaborations and sharing of results within and beyond a European Network of Excellence. The methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members - and to use this model for improving accessibility of data and knowledge repositories.|Paola Velardi,Roberto Navigli,Micha\u00ebl Petit",
        "16591|IJCAI|2007|Case Base Mining for Adaptation Knowledge Acquisition|In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.|Mathieu d'Aquin,Fadi Badra,Sandrine Lafrogne,Jean Lieber,Amedeo Napoli,Laszlo Szathmary"
      ],
      [
        "13781|IJCAI|1981|Time-Oriented Features for Medical Consultation Systems|Medical consultation system is one of the major application of artificial intelligence research, and in this field, it is important to treat time-oriented data. The authors developed a system named MECS-Al (MEdical Consultation System by means of Artificial Intelligence), which is designed as a general purpose tool for constructing systems with capability to treat time-oriented data. The basic idea of this system is to describe the time flow as a chain of discrete events. As a general purpose tool, the system consists of an inference-engine and a knowledge-base editor, so that consultation systems in any field can be easily defined and tested.|T. Koyama,S. Kaihara,T. Minamikawa,T. Kurokawa",
        "13971|IJCAI|1983|A Deductive Model of Belief|Representing and reasoning about the knowledge an agent (human or computer) must have to accomplish some task is becoming an increasingly important issue in artificial intelligence (AI) research. To reason about an agent's beliefs, an AI system must assume some formal model of those beliefs. An attractive candidate is the Deductive Belief model an agent's beliefs are described as a set of sentences in some formal language (the base sentences), together with a deductive process for deriving consequences of those beliefs. In particular, a Deductive Belief model can account for the effect of resource limitations on deriving consequences of the base set an agent need not believe all the logical consequences of his beliefs. In this paper we develop a belief model based on the notion of deduction, and contrast it with current AI formalisms for belief derived from HintikkaKripke possible-worlds semantics for knowledge.|Kurt Konolige",
        "13967|IJCAI|1983|Knowledge Based Error Recovery in Industrial Robots|This paper describes the main aims of a new research project concerned with the implementation of automatic error recovery facilities in industrial robotics. An approach is discussed in which an existing manufacturing work cell is to be enhanced by the addition of a task event model contained in an error recovery knowledge base. This paper outlines the main design issues involved in this work. Reasons why conventional fault tolerance techniques are inadequate are given and the industrial application is explained. A particular approach to sensory monitoring and error diagnosis is described. The proposed system has more similarities with sensory driven expert systems than with body modelling contingency planners.|M. H. Lee,David P. Barnes,N. W. Hardy",
        "14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,Fran\u00e7ois Felix Ingrand",
        "13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico",
        "14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz",
        "16590|IJCAI|2007|Semantic Indexing of a Competence Map to Support Scientific Collaboration in a Research Community|This paper describes a methodology to semiautomatically acquire a taxonomy of terms and term definitions in a specific research domain. The taxonomy is then used for semantic search and indexing of a knowledge base of scientific competences, called Knowledge Map. The KMap is a system to support research collaborations and sharing of results within and beyond a European Network of Excellence. The methodology is general and can be applied to model any web community - starting from the documents shared and exchanged among the community members - and to use this model for improving accessibility of data and knowledge repositories.|Paola Velardi,Roberto Navigli,Micha\u00ebl Petit",
        "14283|IJCAI|1985|Representation and Use of Explicit Justifications for Knowledge Base Refinements|We discuss the representation and use of justification structures as an aid to knowledge base refinement We show how justifications can be used by a system to generate explanations - for its own use-of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.|Reid G. Smith,Howard A. Winston,Tom M. Mitchell,Bruce G. Buchanan",
        "16591|IJCAI|2007|Case Base Mining for Adaptation Knowledge Acquisition|In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.|Mathieu d'Aquin,Fadi Badra,Sandrine Lafrogne,Jean Lieber,Amedeo Napoli,Laszlo Szathmary",
        "14383|IJCAI|1987|An Examination of the Third Stage in the Analogy Process Verification-based Analogical Learning|Many studies of analogy in Artificial Intelligence have focused on analogy as a heuristic mechanism to guide search and simplify problem solving or as a basis for forming generalizations. This paper examines analogical learning, where analogy is used to conjecture new knowledge about some domain. A theory of Verification-Based Analogical Learning is presented which addresses the tenuous nature of analogically inferred concepts and describes procedures that can be used to increase confidence in the inferred knowledge. The theory describes how analogy may be used to discover and refine scientific models of the physical world. Examples are taken from an implemented system, which discovers qualitative models of processes such as liquid flow and heat flow.|Brian Falkenhainer"
      ],
      [
        "14902|IJCAI|1991|Localized Search for Multiagent Planning|This paper describes the localized search mechanism of the GEMPLAN multiagent planner. Both formal complexity results and empirical results are provided, demonstrating the benefits of localized search, A localized domain description is one that decomposes domain activities and requirements into a set of regions. This description is used to infer how domain requirements are semantically localized and, as a result, to enable the decomposition of the planning search space into a set of spaces, one for each domain region. Benefits of localization include a smaller and cheaper overall search space as well as heuristic guidance in controlling search. Such benefits are critical if current planning technologies and other types of reasoning are to be scaled up to large, complex domains.|Amy L. Lansky",
        "16218|IJCAI|2005|Planning with Continuous Resources in Stochastic Domains|We consider the problem of optimal planning in stochastic domains with resource constraints, where resources are continuous and the choice of action at each step may depend on the current resource level. Our principal contribution is the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. The search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. We claim that this approach is especially effective when resource limitations contribute to reachability constraints. Experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.|Mausam,Emmanuel Benazera,Ronen I. Brafman,Nicolas Meuleau,Eric A. Hansen",
        "16832|IJCAI|2009|Stratified Planning|Most planning problems have strong structures. They can be decomposed into subdomains with causal dependencies. The idea of exploiting the domain decomposition has motivated previous work such as hierarchical planning and factored planing. However, these algorithms require extensive backtracking and lead to few efficient general-purpose planners. On the other hand, heuristic search has been a successful approach to automated planning. The domain decomposition of planning problems, unfortunately, is not directly and fully exploited by heuristic search. We propose a novel and general framework to exploit domain decomposition. Based on a structure analysis on the SAS+ planning formalism, we stratify the sub-domains of a planning problem into dependency layers. By recognizing the stratification of a planning structure, we propose a space reduction method that expands only a subset of executable actions at each state. This reduction method can be combined with state-space search, allowing us to simultaneously employ the strength of domain decomposition and high-quality heuristics. We prove that the reduction preserves completeness and optimality of search and experimentally verify its effectiveness in space reduction.|Yixin Chen,You Xu,Guohui Yao",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "15594|IJCAI|2001|Planning in Nondeterministic Domains under Partial Observability via Symbolic Model Checking|Planning under partial observability is one of the most significant and challenging planning problems. It has been shown to be hard, both theoretically and experimentally. In this paper, we present a novel approach to the problem of planning under partial observability in non-deterministic domains. We propose an algorithm that searches through a (possibly cyclic) and-or graph induced by the domain. The algorithm generates conditional plans that are guaranteed to achieve the goal despite of the uncertainty in the initial condition, the uncertain effects of actions, and the partial observability of the domain. We implement the algorithm by means of BDD-based, symbolic model checking techniques, in order to tackle in practice the exponential blow up of the search space. We show experimentally that our approach is practical by evaluating the planner with a set of problems taken from the literature and comparing it with other state of the art planners for partially observable domains.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri,Paolo Traverso",
        "16725|IJCAI|2007|Domain Independent Approaches for Finding Diverse Plans|In many planning situations, a planner is required to return a diverse set of plans satisfying the same goals which will be used by the external systems collectively. We take a domain-independent approach to solving this problem. We propose different domain independent distance functions among plans that can provide meaningful insights about the diversity in the plan set. We then describe how two representative state-of-the-art domain independent planning approaches - one based on compilation to CSP, and the other based on heuristic local search - can be adapted to produce diverse plans. We present empirical evidence demonstrating the effectiveness of our approaches.|Biplav Srivastava,Tuan A. Nguyen,Alfonso Gerevini,Subbarao Kambhampati,Minh Binh Do,Ivan Serina",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "16449|IJCAI|2007|Edge Partitioning in External-Memory Graph Search|There is currently much interest in using external memory, such as disk storage, to scale up graph-search algorithms. Recent work shows that the local structure of a graph can be leveraged to substantially improve the efficiency of external-memory graph search. This paper introduces a technique, called edge partitioning, which exploits a form of local structure that has not been considered in previous work. The new technique improves the scalability of structured approaches to external-memory graph search, and also guarantees the applicability of these approaches to any graph-search problem. We show its effectiveness in an external-memory graph-search algorithm for domain-independent STRIPS planning.|Rong Zhou,Eric A. Hansen",
        "16118|IJCAI|2005|Abstraction-based Action Ordering in Planning|Many planning problems contain collections of symmetric objects, actions and structures which render them difficult to solve efficiently. It has been shown that the detection and exploitation of symmetric structure in planning problems can dramatically reduce the size of the search space and the time taken to find a solution. We present the idea of using an abstraction of the problem domain to reveal symmetric structure and guide the navigation of the search space. We show that this is effective even in domains in which there is little accessible symmetric structure available for pruning. Proactive exploitation represents a flexible and powerful alternative to the symmetry-breaking strategies exploited in earlier work in planning and CSPs. The notion of almost symmetry is defined and results are presented showing that proactive exploitation of almost symmetry can improve the performance of a heuristic forward search planner.|Maria Fox,Derek Long,Julie Porteous",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ]
    ]
  },
  "title": {
    "entropy": 6.4146548049509,
    "topics": [
      "problems solving, heuristic search, constraint satisfaction, local search, problems, constraint problems, constraint, satisfaction problems, distributed optimization, theorem proving, state space, constraint optimization, search, search methods, search space, admissible heuristic, scheduling problems, search problems, heuristic problems, search techniques",
      "learning, web search, reinforcement learning, using learning, neural networks, bayesian networks, rule learning, concept formation, neural learning, learning approach, learning examples, web, heuristic program, explanation-based learning, inductive learning, program, learning networks, induction learning, concept, networks",
      "case study, path planning, game tree, search algorithm, robot planning, algorithm, symmetry breaking, using search, quantified boolean, planning, tree search, search game, graphical model, moving search, model, search application, anytime algorithm, planning uncertainty, using algorithm, best-first search",
      "speech understanding, natural language, artificial intelligence, data base, system, fault diagnosis, representation knowledge, arc consistency, knowledge base, data access, knowledge, understanding system, natural base, semantic analysis, logic programming, robot navigation, base access, mobile robot, speech system, language base",
      "local search, window search, structure search, temporal, approaches, sat, efficiency, sentence, analysis",
      "heuristic search, search space, heuristic, state space, admissible search, admissible heuristic, machine learning, heuristic approach, planning plans, learning heuristic, heuristic theory, heuristic planning, plans, heuristic space, machine, theory, proof, constrained, partitioning, planning",
      "reasoning, approach, induction, structured, time, bayesian, acquisition, constructive, explanation-based",
      "web search, concept formation, web, methods, concept, clustering, mining, information, conceptual, examples, learned, integrating, descriptions",
      "planning, model, path planning, robot planning, graphical model, planning uncertainty, information, resources, model information, domains, negotiation, costs, multi-agent, incomplete, partial, model planning, strategies, checking, symbolic",
      "case study, game tree, search, tree search, search game, search application, best-first search, parallel search, search study, case search, searching, space, branch, bound, error, indexing, limited, decomposition, flexible",
      "representation knowledge, logic programming, recognition, object recognition, programming, object, reasoning, belief, visual, spatial, interaction, implementation, environment, memory, edge",
      "robot navigation, mobile robot, system, search system, system robot, robot, distributed, vision, based, use, process, networks"
    ],
    "ranking": [
      [
        "15911|IJCAI|2003|Solving Constraint Optimization Problems in Anytime Contexts|This paper presents a new hybrid method for solving constraint optimization problems in anytime contexts. Discrete optimization problems are modelled as Valued CSP. Our method (VNSLDS+CP) combines a Variable Neighborhood Search and Limited Discrepancy Search with Constraint Propagation to efficiently guide the search. Experiments on the CELAR benchmarks demonstrate significant improvements over other competing methods. VNSLDS+CP has been successfully applied to solve a real-life anytime resource allocation problem in computer networks.|Samir Loudni,Patrice Boizumault",
        "13881|IJCAI|1983|A Wrinkle on Satisficing Search Problems|The problem of optimally ordering the execution of independent disjuncts is explored. Only a single answer is sought, not necessarily the best one. By definition, this is called satisfying search. Since the disjuncts are independent, the total combined probability that a solution is found does not depend on the execution order. However, the ordering does affect the total expected execution time because execution ceases as soon as any solution is discovered. Therefore, the optimal ordering is the one that minimizes the total expected work. The new result is an algorithm to find this optimal ordering when the effects of executing a disjunct must be undone before another one can be tried. The algorithm is shown to have time complexity O(n log n), where n is the number of disjuncts. This is the same complexity as for the original problem where undo times are ignored.|Jeffrey A. Barnett,Don Cohen",
        "15032|IJCAI|1993|Exploiting Interchangeabilities in Constraint-Satisfaction Problems|Constraint satisfaction - a method for representing and solving many AI problems in a very elegant manner - is a well-studied research area of recent years. Freuder observed that some constraint satisfaction problems are fashioned so that certain domain values of constraint variables are interchangeable. The use of such knowledge can increase search efficiency drastically by reducing the problem. In this paper we carry on these considerations and give a formal foundation of interchangeabilities by the notion of domain partitions induced by equivalence relations. We show how these domain partitions can be used in a very accurate manner by the majority of existing constraint propagation algorithms and introduce a novel backtrack procedure exploiting such interchangeabilities of domain values. Both theoretical analysis and experiments indicate that our proposed approach is an improvement of Freuder's use of neighborhood interchangeability and has very good behavior for certain problem types.|Alois Haselb\u00f6ck",
        "15069|IJCAI|1993|Genetic State-Space Search for Constrained Optimization Problems|This paper introduces GSSS (Genetic State-Space Search). The integration of two general search paradigms--genetic search and state-space-search - provides a general framework which can be applied to a large variety of search problems. Here, we show how GSSS solves constrained optimization problems (COPs). Basically, it searches for \"promising search states\" from which good solutions can be easily found. Domain knowledge in the form of constraints is used to limit the space to be searched. Interestingly, our approach allows the handling of constraints within genetic search at a general domain independent level. First, we introduce a genetic representation of search states. Next, we provide empirical results which compare the relative merit of the introduction of constraints during the generation of the initial population, during the fitness calculation, and during the application of genetic operators. Finally, we describe some extensions to our method which came about when applying it to factory floor scheduling problems.|Jan Paredis",
        "16153|IJCAI|2005|Optimal Refutations for Constraint Satisfaction Problems|Variable ordering heuristics have long been an important component of constraint satisfaction search algorithms. In this paper we study the behaviour of standard variable ordering heuristics when searching an insoluble (sub)problem. We employ the notion of an optimal refutation of an insoluble (sub)problem and describe an algorithm for obtaining it. We propose a novel approach to empirically looking at problem hardness and typical-case complexity by comparing optimal refutations with those generated by standard search heuristics. It is clear from our analysis that the standard variable orderings used to solve CSPs behave very differently on real-world problems than on random problems of comparable size. Our work introduces a potentially useful tool for analysing the causes of the heavy-tailed phenomenon observed in the runtime distributions of backtrack search procedures.|Tudor Hulubei,Barry O'Sullivan",
        "13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel",
        "14989|IJCAI|1993|Diagnosing and Solving Over-Determined Constraint Satisfaction Problems|Constraint relaxation is a frequently used technique for managing over-determined constraint satisfaction problems. A problem in constraint relaxation is the selection of the appropriate constraints. We show that methods developed in model-based diagnosis solve this problem. The resulting method, DOC, an abbreviation for Diagnosis of Over-determined Constraint Satisfaction Problems, identifies the set of least important constraints that should be relaxed to solve the remaining constraint satisfaction problem. If the solution is not acceptable for a user, DOC selects next-best sets of least-important constraints until an acceptable solution has been generated. The power of DOC is illustrated by a case study of scheduling the Dutch major league soccer competition. The current schedule is made using human insight and Operations Research methods. Using DOC, the - schedule has been improved by reducing the number and importance of the violated constraints by %. The case study revealed that efficiency improvement is a major issue in order to apply this method to large-scale over-determined scheduling and constraint satisfaction problems.|R. R. Bakker,F. Dikker,F. Tempelman,P. M. Wognum",
        "14591|IJCAI|1989|Experimental Evaluation of Preprocessing Techniques in Constraint Satisfaction Problems|This paper presents an evaluation of two orthogonal schemes for improving the efficiency of solving constraint satisfaction problems (CSPs). The first scheme involves a class of pre-processing techniques designed to make the representation of the CSP more explicit, including directional-arc-consistency, directional-path-consistency and adaptive-consistency. The second scheme aims at improving the order in which variables are chosen for evaluation during the search. In the first part of the experiment we tested the performance of backtracking (and its common enhancement - backjumping) with and without each of the preprocessings techniques above. The results show that directional arc-consistency, a scheme which embodies the simplest form of constraint recording, outperforms all other preprocessing techniques. The results of the second part of the experiment suggest that the best variable ordering is achieved by the fixed max-cardinality search order.|Rina Dechter,Itay Meiri",
        "14737|IJCAI|1989|An Algebraic Approach to Constraint Satisfaction Problems|A constraint satisfaction problem, or CSP, can be reformulated as an integer linear programming problem. The reformulated problem can be solved via polynomial multiplication. If the CSP has n variables whose domain size is m, and if the equivalent programming problem involves M equations, then the number of solutions can be determined in time O(nmM-n). This surprising link between search problems and algebraic techniques allows us to show improved bounds for several constraint satisfaction problems, including new simply exponential bounds for determining the number of solutions to the n-queens problem. We also address the problem of minimizing M for a particular CSP.|Igor Rivin,Ramin Zabih",
        "15377|IJCAI|1997|Combining Local Search and Look-Ahead for Scheduling and Constraint Satisfaction Problems|We propose a solution technique for scheduling and constraint satisfaction problems that combines backtracking-free constructive methods and local search techniques. Our technique incrementally constructs the solution, performing a local search on partial solutions each time the construction reaches a dead-end. Local search on the space of partial solutions is guided by a cost function based on three components the distance to feasibility of the partial solution, a look-ahead factor, and (for optimization problems) a lower bound of the objective function. In order to improve search effectiveness, we make use of an adaptive relaxation of constraints and an interleaving of different lookahead factors. The new technique has been successfully experimented on two real-life problems university course scheduling and sport tournament scheduling.|Andrea Schaerf"
      ],
      [
        "16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir",
        "16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan",
        "13864|IJCAI|1981|Concept Learning by Structured Examples - An Algebraic Approach|A system learning concepts from training samples consisting of structured objects is described. It is based on descriptions invariant under isomorphism. In order to get a unified mathematical formalism recent graph theoretic results are used- The structures are transformed into feature vectors and after that a concept learning algorithm developing decision trees is applied which is an extension of algorithms found in psychological experiments. It corresponds to a general-to-specific depth-first search with reexamination of past events. The generalization ability is demonstrated by means of the blocks world example and it is shown that the algorithm can successfully handle practical problems with samples of about one hundred of relatively complicated structures in a reasonable time. Additionally, the problem of representation and learning context dependent concepts is discussed in the paper.|Fritz Wysotzki,Werner Kolbe,Joachim Selbig",
        "16373|IJCAI|2007|Learning User Clicks in Web Search|Machine learning for predicting user clicks in Web-based search offers automated explanation of user activity. We address click prediction in the Web search scenario by introducing a method for click prediction based on observations of past queries and the clicked documents. Due to the sparsity of the problem space, commonly encountered when learning for Web search, new approaches to learn the probabilistic relationship between documents and queries are proposed. Two probabilistic models are developed, which differ in the interpretation of the query-document co-occurrences. A novel technique, namely, conditional probability hierarchy, flexibly adjusts the level of granularity in parsing queries, and, as a result, leverages the advantages of both models.|Ding Zhou,Levent Bolelli,Jia Li,C. Lee Giles,Hongyuan Zha",
        "15518|IJCAI|1999|A Neural Reinforcement Learning Approach to Learn Local Dispatching Policies in Production Scheduling|Finding optimal solutions for job shop scheduling problems requires high computational effort, especially under consideration of uncertainty and frequent replanning. In contrast to computational solutions, domain experts are often able to derive good local dispatching heuristics by looking at typical problem instances. They can be efficiently applied by looking at few relevant features. However, these rules are usually not optimal, especially in complex decision situations. Here we describe an approach that tries to combine both worlds. A neural network based agent autonomously optimizes its local dispatching policy with respect to a global optimization goal, defined for the overall plant. On two benchmark scheduling problems, we show both learning and generalization abilities of the proposed approach.|Simone C. Riedmiller,Martin A. Riedmiller",
        "15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik",
        "15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell",
        "14272|IJCAI|1985|Learning Concept Descriptions from Examples with Errors|This paper presents a scheme for learning complex descriptions, such as logic formulas, from examples with errors. The basis for learning is provided by a selection criterion which minimizes a combined measure of discrepancy of a description with training data, and complexity of a description. Learning rules for two types of descriptors are derived one for finding descriptors with good average discrimination over a set of concepts, second for selecting the best descriptor for a specific concept. Once these descriptors are found, an unknown instance can be identified by a search using the descriptors of the first type for a fast screening of candidate concepts, and the second for the final selection of the closest concept.|Jakub Segen",
        "17066|IJCAI|2009|Learning Probabilistic Hierarchical Task Networks to Capture User Preferences|While much work on learning in planning focused on learning domain physics (i.e., action models), and search control knowledge, little attention has been paid towards learning user preferences on desirable plans. Hierarchical task networks (HTN) are known to provide an effective way to encode user prescriptions about what constitute good plans. However, manual construction of these methods is complex and error prone. In this paper, we propose a novel approach to learning probabilistic hierarchical task networks that capture user preferences by examining user-produced plans given no prior information about the methods (in contrast, most prior work on learning within the HTN framework focused on learning \"method preconditions\"--i.e., domain physics--assuming that the structure of the methods is given as input). We will show that this problem has close parallels to the problem of probabilistic grammar induction, and describe how grammar induction methods can be adapted to learn task networks. We will empirically demonstrate the effectiveness of our approach by showing that task networks we learn are able to generate plans with a distribution close to the distribution of the user-preferred plans.|Nan Li,Subbarao Kambhampati,Sung Wook Yoon",
        "15151|IJCAI|1995|Rule Induction and Instance-Based Learning A Unified Approach|This paper presents a new approach to inductive learning that combines aspects of instancebased learning and rule induction in a single simple algorithm. The RISE system searches for rules in a specific-to-general fashion, starting with one rule per training example, and avoids some of the difficulties of separate-and-eonquer approaches by evaluating each proposed induction step globally, i e, through an efficient procedure that is equivalent to checking the accuracy of the rule set as a whole on every training example. Classification is performed using a best-match strategy, and reduces to nearest-neighbor if all generalizations of instances were rejected. An extensive empirical study shows that RISE consistently achieves higher accuracies than state-of-the-art representatives of its \"parent\" paradigms (PEBLS and CN), and also outperforms a decision-tree learner (C ) in  out of  test domains (in  with % confidence).|Pedro Domingos"
      ],
      [
        "15119|IJCAI|1995|Experimenting with Revisits in Game Tree Search|The oldest known game tree search algorithm Alpha-Beta is still the most popular one. All other algorithms in this area fall short of Alpha-Beta in one or more of the following three desired characteristics - high pruning power, low storage requirement and low execution time. This paper discusses how revisit of nodes can be used effectively in game tree search. A few strategies of introducing revisits in game tree search are presented. It is demonstrated that for any shape and ordering of the game tree to be searched, there always exists one strategy that, on an average, consistently evaluates less number of terminals than Alpha-Beta in comparable memory and time.|Subir Bhattacharya",
        "14517|IJCAI|1987|CYPRESS-Soar A Case Study in Search and Learning in Algorithm Design|This paper describes a partial reimplementation of Doug Smith's CYPRESS algorithm design system within the Soar problem-solving architecture. The system, CYPRESS-SOAR, reproduces most of CYPRESS' behavior in the synthesis of three divide-and-conquer sorting algorithms from formal specifications. CYPRESS-Soar is based on heuristic search of problem spaces, and uses search to compensate for missing knowledge in some instances. CYPRESS-Soar also learns as it designs algorithms, exhibiting significant transfer of learned knowledge, both within a single design run, and across designs of several different algorithms. These results were produced by reimplementing just the high-level synthesis control of CYPRESS, simulating the results of calls to CYPRESS deduction engine. Thus after only two months of effort, we had a surprisingly effective research vehicle for investigating the roles of search, knowledge, and learning in this domain.|David M. Steier",
        "17057|IJCAI|2009|Search Strategies for an Anytime Usage of the Branch and Prune Algorithm|When applied to numerical CSPs, the branch and prune algorithm (BPA) computes a sharp covering of the solution set. The BPA is therefore impractical when the solution set is large, typically when it has a dimension larger than four or five which is often met in underconstrained problems. The purpose of this paper is to present a new search tree exploration strategy for BPA that hybridizes depth-first and breadth-first searches. This search strategy allows the BPA discovering potential solutions in different areas of the search space in early stages of the exploration, hence allowing an anytime usage of the BPA. The merits of the proposed search strategy are experimentally evaluated.|Rapha\u00ebl Chenouard,Alexandre Goldsztejn,Christophe Jermann",
        "15835|IJCAI|2003|Making the Breakout Algorithm Complete Using Systematic Search|Local search algorithms have been very successful for solving constraint satisfaction problems (CSP). However, a major weakness has been that local search is unable to detect unsolvability and is thus not suitable for tightly and overconstrained problems. We present a hybrid solving scheme where we combine a local search algorithm - the breakout algorithm, with a systematic search algorithm - backtracking. The breakout algorithm is used for identifying hard or unsolvable subproblems and the backtracking algorithm proves the solvability of these subproblems. The resulting hybrid algorithm is complete and is tested on randomly generated graph -colouring problems. The algorithm performs extremely well for all areas of the phase transition and outperforms the individual methods by several orders of magnitude.|Carlos Eisenberg,Boi Faltings",
        "16522|IJCAI|2007|A Multiobjective Frontier Search Algorithm|The paper analyzes the extension of frontier search to the multiobjective framework. A frontier multiobjective A* search algorithm is developed, some formal properties are presented, and its performance is compared to those of other multiobjective search algorithms. The new algorithm is adequate for both monotone and non-monotone heuristics.|Lawrence Mandow,Jos\u00e9-Luis P\u00e9rez-de-la-Cruz",
        "17024|IJCAI|2009|Monte Carlo Tree Search Techniques in the Game of Kriegspiel|Monte Carlo tree search has brought significant improvements to the level of computer players in games such as Go, but so far it has not been used very extensively in games of strongly imperfect information with a dynamic board and an emphasis on risk management and decision making under uncertainty. In this paper we explore its application to the game of Kriegspiel (invisible chess), providing three Monte Carlo methods of increasing strength for playing the game with little specific knowledge. We compare these Monte Carlo agents to the strongest known minimax-based Kriegspiel player, obtaining significantly better results with a considerably simpler logic and less domain-specific knowledge.|Paolo Ciancarini,Gian Piero Favini",
        "14204|IJCAI|1985|A Hybrid SSSAlpha-Beta Algorithm for Parallel Search of Game Trees|This paper explores the issues that arise when SSS*-like search algorithms are implemented in parallel. There is an important implicit assumption regarding the OPEN list of SSS* (and A*-like algorithms) those states which are guaranteed never to become part of an optimal solution are forced down into the OPEN list and never rise to the top for expansion. However. when multiple processors are introduced in a parallel version of SSS*. These buried states become subject to expansion despite their provable suboptimality. In such states are not identified and purged, they may exert an enormous drag on the parallel algorithm because considerable processor effort will be wasted, However, the pruning mechanisms of alpha-beta can be adapted by a parallel SSS* the resulting algorithm HYBRID is suitable for searching game trees and general ANDOR trees in parallel.|Daniel B. Leifker,Laveen N. Kanal",
        "16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar",
        "15262|IJCAI|1995|Best-First Fixed-Depth Game-Tree Search in Practice|We present a new paradigm for minimax search algorithms MT, a memory-enhanced version of Pearl's Test procedure. By changing the way MT is called, a number of practical best-first search algorithms can be simply constructed. Reformulating SSS* as an instance of MT eliminates all its perceived implementation drawbacks. Most assessments of minimax search performance are based on simulations that do not address two key ingredients of high performance game-playing programs iterative deepening and memory usage. Instead, we use experimental data gathered from tournament checkers, Othello and chess programs. The use of iterative deepening and memory makes our results differ significantly from the literature. One new instance of our framework, MTD(I), out-performs our best Alpha-Beta searcher on leaf nodes, total nodes and execution time. To our knowledge, these are the first reported results that compare both depth-first and best-first algorithms given the same amount of memory.|Aske Plaat,Jonathan Schaeffer,Wim Pijls,Arie de Bruin",
        "14742|IJCAI|1989|On Optimal Game-Tree Search using Rational Meta-Reasoning|In this paper we outline a general approach to the study of problem-solving, in which search steps are considered decisions in the same sense as actions in the world. Unlike other metrics in the literature, the value of a search step is defined as a real utility rather than as a quasi-utility, and can therefore be computed directly from a model of the base-level problem-solver. We develop a formula for the expected value of a search step in a game-playing context using the single-step assumption, namely that a computation step can be evaluated as it was the last to be taken. We prove some meta-level theorems that enable the development of a low-overhead algorithm, MGSS*, that chooses search steps in order of highest estimated utility. Although we show that the single-step assumption is untenable in general, a program implemented for the game of Othello soundly beats an alpha-beta search while expanding significantly fewer nodes, even though both programs use the same evaluation function.|Stuart J. Russell,Eric Wefald"
      ],
      [
        "13744|IJCAI|1981|Use of Data Representation Mapping in Automatic Generation of Data Base Access Procedures|Formal definitions of data structure mappings are given and are used to transform relational algebraic formula into data base access procedures which search a formally defined data structure. Especially, a hierarcical data structure is introduced to represent a set of relations which are inherently hierarchical. A retrieval procedure using data accesses along a two level hierarchy is obtained from an original formula on flat relations by applying equivalence transformation rules and data representation mappings.|Koichi Furukawa",
        "13546|IJCAI|1975|Natural Language Access To A Large Data Base An Engineering Approach|An intelligent program which accepts natural language queries can allow anon-technical user to easily obtain information from a large non-uniform data base. This paper discusses the design of a program which will tolerate a wide variety of requests including ones with pronouns and referential phrases. The system embodies a certain amount of common sense, so that for example, it \"knows when it does or does not understand a particular request and it can bypass actual data base search in answering unreasonable requests. The system is conceptually simple and could be easily adapted to other data bases.|D. Walts",
        "14404|IJCAI|1987|Understanding System Specifications Written in Natural Language|This paper describes research in understanding system specifications written in natural language. This research involves the implementation of a natural language interface, PHRANSPAN, for specifying the abstract behavior of digital systems in restricted English text. A neutral formal representation for the behavior is described using the USC Design Data Structure. A small set of concepts that characterize digital system behavior are presented using this representation. An intermediate representation loosely based on Conceptual Dependency is presented. Its use with a semantic-based parser to translate from English to the formal representation is illustrated by examples.|John J. Granacki Jr.,Alice C. Parker,Yigal Arens",
        "13932|IJCAI|1983|Automatic Construction of a Knowledge Base by Analysing Texts in Natural Language|We present a system which translates sentences from a subset of German into a database. This data-base will function as the basis for a question-answering-systern. The system is applied to a complete text and not to isolated sentences. As an intermediate stage between the German text and the database we use the Discourse Representation Structures (DRS) invented by Hans Kamp. Karnp's system has been chosen because it handles intrasentential and intersentential relations uniformly. Within Kamp's system one can account for certain types of anaphoric relations for which no other linguistic theory has provided a solution. The input to our system is analysed by a parser which is based on lexical functional grammar. This is the first attempt to combine research on discourse representation with lexical functional grammar with the help of the formalism of Definite Clause Grammar. For the construction of the database out of the DRS's, two solutions arc proposed. First, a translation of the DRS's into a set of PROLOG clauses enriched with some additional deductive principles. Second, the formulation of inference rules which operate directly on the DRS. So far we have implemented the following components parser of German, translation rules which map syntactic trees into DRS's and rules which translate DRS's into PROLOG-clauses.|Werner Frey,Uwe Reyle,Christian Rohrer",
        "13678|IJCAI|1977|Knowledge-Base Driven Analysis of Cinecardioangiograms|This short note describes research into the application of AI techniques to the analysis of cinecardioangiograms. These are X-ray films of the heart taken while a radiopaque dye is injected into the heart cavity. The film shows the opacified blood inside the left ventricle, thus outlining the inside wall of the cavity. The problem is to build a knowledge base which can guide the analysis of the motion of the walls in these films, determine the various parameters which physicians use in their diagnoses and recognize abnormalities of heart wall motion.|John K. Tsotsos",
        "13262|IJCAI|1969|Experiments with a Search Algorithm for the Data Base of a Human Belief System|A large data base was collected from a human informant. The data consisted of beliefs regarding parent-child relations. A variety of factors in searching the data base were manipulated in an attempt to discover which were the more important in contributing to estimates of credibility. Problems of data collection, data representation and a searching algorithm are discussed in detail.|Kenneth Mark Colby,Lawrence G. Tesler,Horace Enea",
        "13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico",
        "13707|IJCAI|1981|A General Semantic Analyser for Data Base Access|The paper discusses the design principles and current status of a natural language front end for access to data bases. This Is based on the use, first, of a semantically-oriented question analyser exploiting general, language-wide semantic categories and patterns, rather than data base-specific ones and, second, of a data base-oriented translation component for obtaining search specifications from the meaning representations for questions derived by the analyser. This approach is motivated by the desire to reduce the effort of providing data base-specific material for the front end, by the belief that a general analyser is well suited to the \"casual\" data base user, and by the assumption that the rich semantic apparatus used will be both adequate as a means of analysis and appropriate as a tool for linking the characterisations of input and data language items. The paper describes this approach in more detail, with emphasis on the existing, tested, analyser.|Branimir Boguraev,Karen Sparck Jones",
        "13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris",
        "14283|IJCAI|1985|Representation and Use of Explicit Justifications for Knowledge Base Refinements|We discuss the representation and use of justification structures as an aid to knowledge base refinement We show how justifications can be used by a system to generate explanations - for its own use-of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.|Reid G. Smith,Howard A. Winston,Tom M. Mitchell,Bruce G. Buchanan"
      ],
      [
        "16765|IJCAI|2007|Opinion Sentence Search Engine on Open-Domain Blog|We have introduced a search engine that can extract opinion sentences relevant to an open-domain query from Japanese blog pages. The engine identifies opinions based not only on positive or negative measurements but also on neutral opinions, requests, advice, and thoughts. To retrieve a number of opinion sentences that a user could reasonably be expected to read, we attempted to extract only explicitly stated writer's opinions at the sentence-level and to exclude quoted or implicational opinions. In our search engine, opinion sentences are identified based on features such as opinion clue expressions, and then, the relevance to the query of each identified opinion sentence is checked. The experimental results for various topics, obtained by comparing the output of the proposed opinion search engine with that of human judgments as to whether the sentences were opinions, showed that the proposed engine has promise as a practical application.|Osamu Furuse,Nobuaki Hiroshima,Setsuo Yamada,Ryoji Kataoka",
        "15457|IJCAI|1999|SAT-Encodings Search Space Structure and Local Search Performance|Stochastic local search (SLS) algorithms for prepositional satisfiability testing (SAT) have become popular and powerful tools for solving suitably encoded hard combinatorial from different domains like, e.g., planning. Consequently, there is a considerable interest in finding SAT-encodings which facilitate the efficient application of SLS algorithms. In this work, we study how two encodings schemes for combinatorial problems, like the well-known Constraint Satisfaction or Hamilton Circuit Problem, affect SLS performance on the SAT-encoded instances. To explain the observed performance differences, we identify features of the induces search spaces which affect SLS performance. We furthermore present initial results of a comparitive analysis of the performance of the SAT-encoding and-solving approach versus that of native SLS algorithms directly applied to the unencoded problem instances.|Holger H. Hoos",
        "16630|IJCAI|2007|Building Structure into Local Search for SAT|Local search procedures for solving satisfiability problems have attracted considerable attention since the development of GSAT in . However, recentwork indicates that for many real-world problems, complete search methods have the advantage, because modern heuristics are able to effectively exploit problem structure. Indeed, to develop a local search technique that can effectively deal with variable dependencies has been an open challenge since . In this paper we show that local search techniques can effectively exploit information about problem structure producing significant improvements in performance on structured problem instances. Building on the earlier work of Ostrowski et al. we describe how information about variable dependencies can be built into a local search, so that only independent variables are considered for flipping. The cost effect of a flip is then dynamically calculated using a dependency lattice that models dependent variables using gates (specifically and, or and equivalence gates). The experimental study on hard structured benchmark problems demonstrates that our new approach significantly outperforms the previously reported best local search techniques.|Duc Nghia Pham,John Thornton,Abdul Sattar",
        "16899|IJCAI|2009|SATenstein Automatically Building Local Search SAT Solvers from Components|Designing high-performance algorithms for computationally hard problems is a difficult and often time-consuming task. In this work, we demonstrate that this task can be automated in the context of stochastic local search (SLS) solvers for the propositional satisfiability problem (SAT). We first introduce a generalised, highly parameterised solver framework, dubbed SATenstein, that includes components gleaned from or inspired by existing high-performance SLS algorithms for SAT. The parameters of SATenstein control the selection of components used in any specific instantiation and the behaviour of these components. SATenstein can be configured to instantiate a broad range of existing high-performance SLS-based SAT solvers, and also billions of novel algorithms. We used an automated algorithm configuration procedure to find instantiations of SATenstein that perform well on several well-known, challenging distributions of SAT instances. Overall, we consistently obtained significant improvements over the previously best-performing SLS algorithms, despite expending minimal manual effort.|Ashiqur R. KhudaBukhsh,Lin Xu,Holger H. Hoos,Kevin Leyton-Brown",
        "14256|IJCAI|1985|Information Acquisition in Minimal Window Search|The alpha-beta tree search algorithm can be improved through the use of minimal windows. Branches are searched with a minimal window a,a+l with the expectancy that this will show the sub-tree to be inferior. If not, then that sub-tree must be re-searched. In this paper, several methods are discussed to minimize the cost of the re-search. Two new algorithms, INS and PNS, are introduced and their performance on practical trees is shown to be comparable to SSS*, but with considerably smaller overhead.|Alexander Reinefeld,Jonathan Schaeffer,T. Anthony Marsland",
        "16226|IJCAI|2005|Applying Local Search to Disjunctive Temporal Problems|We present a method for applying local search to overconstrained instances of the Disjunctive Temporal Problem (DTP). Our objective is to generate high quality solutions (i.e., solutions that violate few constraints) in as little time as possible. The technique presented here differs markedly from previous work on DTPs, as it operates within the total assignment space of the underlying CSP rather than the partial assignment space of the related meta-CSP. We provide experimental results demonstrating that the use of local search leads to substantially improved performance over systematic methods.|Michael D. Moffitt,Martha E. Pollack",
        "14498|IJCAI|1987|A Quantitative Analysis of Minimal Window Search|Use of minimal windows enhances the an algorithm in practical applications as well as in the search of artificially constructed game trees. Nevertheless, there exists no theoretical model to measure the strengths and weaknesses of minimal window search. In particular, it is not known which tree ordering properties are favorable for minimal window search. This paper presents a quantitative analysis of minimal window search based on recursive equations which assess the influence of static node values on the dynamic search process. The analytical model is computationally simple, easily extendible and gives a realistic estimate of the expected search time for averagely ordered game trees.|Alexander Reinefeld,T. Anthony Marsland",
        "15962|IJCAI|2003|Combining Two Local Search Approaches to Hypergraph Partitioning|We study leading-edge local search heuristics for balanced hypergraph partitioning and Boolean satisfiability, intending the generalization of such heuristics beyond their original domains. We adapt the Fiduccia Mattheyses (FM) hypergraph partitioning heuristic to Boolean Satisfiability (SAT), and the WalkSAT SAT solver to hypergraph partitioning. Focusing on balanced hypergraph partitioning, we propose a combination of the classical FM heuristic and our \"cross-over\" heuristic WalkPart, and empirically show that it is more powerful than each component alone. Empirically, we show a % improvement in net cut and a % improvement in runtime over a leading-edge implementation of the FM heuristic.|Arathi Ramani,Igor L. Markov",
        "16573|IJCAI|2007|Information-Theoretic Approaches to Branching in Search|Deciding what question to branch on at each node is a key element of search algorithms. We introduce the information-theoretic paradigm for branching question selection. The idea is to drive the search to reduce uncertainty (entropy) in the current subproblem. We present four families of methods that fall within this paradigm. In the first, a variable to branch on is selected based on lookahead. This method performs comparably to strong branching on MIPLIB, and better than strong branching on hard real-world procurement optimization instances on which CPLEX's default strong branching outperforms CPLEX's default branching strategy. The second family combines this idea with strong branching. The third family does not use lookahead, but instead exploits the tie between indicator variables and the other variables they govern. This significantly outperforms the state-of-the-art branching strategies. The fourth family is about branching using carefully constructed linear inequality constraints over sets of variables.|Andrew Gilpin,Tuomas Sandholm",
        "15652|IJCAI|2001|Local Search Topology in Planning Benchmarks An Empirical Analysis|Many state-of-the-art heuristic planners derive their heuristic function by relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. Looking at a collection of planning benchmarks, we measure topological properties of state spaces with respect to that relaxation. The results suggest that, given the heuristic based on the relaxation, many planning benchmarks are simple in structure. This sheds light on the recent success of heuristic planners employing local search.|J\u00f6rg Hoffmann"
      ],
      [
        "13818|IJCAI|1981|Heuristic Search Theory Survey of Recent Results|This paper summarizes recent analytical Investigations of the mathematical properties of heuristics and their Influence on the performance of common search techniques. The results are reported without proofs, together with discussions of motivations and Interpretations. Highlights include the following relations between the precision of the heuristic estimates and the average complexity of the search, comparisons of the average complexities of A* and BACKTRACKING, procedures for comparing and combining non-admissible heuristic functions, the influence of the weight u (l-u)g  *h on the complexity of A*, determination of the branching factors of alpha-beta and SSS*, and the effects of successor ordering on the complexity of alpha-beta and of search depth on the quality of decisions.|Judea Pearl",
        "16753|IJCAI|2007|A Heuristic Search Approach to Planning with Temporally Extended Preferences|Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.|Jorge A. Baier,Fahiem Bacchus,Sheila A. McIlraith",
        "16803|IJCAI|2007|Using Learned Policies in Heuristic-Search Planning|Many current state-of-the-art planners rely on forward heuristic search. The success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. Such estimates are effective in guiding search for many domains, but there remain many other domains where current heuristics are inadequate to guide forward search effectively. In some of these domains, it is possible to learn reactive policies from example plans that solve many problems. However, due to the inductive nature of these learning techniques, the policies are often faulty, and fail to achieve high success rates. In this work, we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. We propose a simple approach that uses the policy to augment the states expanded during each search step. In particular, during each search node expansion, we add not only its neighbors, but all the nodes along the trajectory followed by the policy from the node until some horizon. Empirical results show that our proposed approach benefits both of the leveraged automated techniques, learning and heuristic search, outperforming the state-of-the-art in most benchmark planning domains.|Sung Wook Yoon,Alan Fern,Robert Givan",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "13306|IJCAI|1971|Heuristic Algorithms for Automated Space Planning|This paper reviews the major characteristics of existing programs for automatically solving space planning problems. It outlines general features of this class of problem and introduces a set of heuristic decision rules called Constraint Structured Planning, which in preliminary tests have efficiently solved a variety of problems. Developments allowing even greater efficiencies are also outlined.|Charles M. Eastman",
        "14655|IJCAI|1989|Parallel Iterative A Search An Admissible Distributed Heuristic Search Algorithm|In this paper, a distributed heuristic search algorithm is presented. We show that the algorithm is admissible and give an informal analysis of its load balancing, scalability, and speedup. A flow-shop scheduling problem has been implemented on a BBN Butterfly Multicomputer using up to  processors to empirically test this algorithm. From our experiments, this algorithm is capable of achieving almost linear speedup on a large number of processors with a relatively small problem size.|Shie-rei Huang,Larry S. Davis",
        "13935|IJCAI|1983|A - An Efficient Near Admissible Heuristic Search Algorithm|The algorithm A* (Nilsson, ) presents two significant drawbacks. First, in seeking strict optimal solution paths it necessarily has high order of complexity. Second, the algorithm does not explicitly descriminate between the cost of a solution path and the cost of finding the solution path. To confront these problems we propose the algorithm AE, a generalization of A*. Instead of seeking an optimal solution, it seeks one which is within a factor (+e) of optimum (e  ). The basic idea is to avoid doing any search at all on most near optimal partial solutions by sticking to a small number of most fruitful paths. Various strategies for searching for near optimal partial solutions are discussed. Experimental results are presented indicating that A e has average complexity of lower order than A* and compares favorably to the related algorithm Af* (Pearl and Kim, ).|Malik Ghallab,Dennis G. Allard",
        "15794|IJCAI|2003|Faster Heuristic Search Algorithms for Planning with Uncertainty and Full Feedback|Recent algorithms like RTDP and LAO* combine the strength of Heuristic Search (HS) and Dynamic Programming (DP) methods by exploiting knowledge of the initial state and an admissible heuristic function for producing optimal policies without evaluating the entire space. In this paper, we introduce and analyze three new HSDP algorithms. A first general algorithm schema that is a simple loop in which 'inconsistent' reachable states (i.e., with residuals greater than a given c) are found and updated until no such states are found, and serves to make explicit the basic idea underlying HSDP algorithms, leaving other commitments aside. A second algorithm, that builds on the first and adds a labeling mechanism for detecting solved states based on Tarjan's strongly-connected components procedure, which is very competitive with existing approaches. And a third algorithm, that approximates the latter by enforcing the consistency of the value function over the likely' reachable states only, and leads to great time and memory savings, with no much apparent loss in quality, when transitions have probabilities that differ greatly in value.|Blai Bonet,Hector Geffner",
        "14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan",
        "15415|IJCAI|1999|Genetic Heuristic for Search Space Exploration|This paper deals with the way dual genetic algorithms (DGA), an extension of the standard ones, explore the search space. After a brief introduction presenting genetic algorithms and dualism, the fitness distance correlation is discussed in the context of dualism. From this discussion, a conjecture is made about the genetic heuristic used by dual genetic algorithms to explore the search space. This conjecture is reinforced by the visualization of the population centroid trajectories in the plane fitness distance. These trajectories help to point out \"leg-up\" behaviors, which allow the dual genetic algorithm to reach the global optimum from walks on deceptive paths.|Manuel Clergue,Philippe Collard"
      ],
      [
        "13864|IJCAI|1981|Concept Learning by Structured Examples - An Algebraic Approach|A system learning concepts from training samples consisting of structured objects is described. It is based on descriptions invariant under isomorphism. In order to get a unified mathematical formalism recent graph theoretic results are used- The structures are transformed into feature vectors and after that a concept learning algorithm developing decision trees is applied which is an extension of algorithms found in psychological experiments. It corresponds to a general-to-specific depth-first search with reexamination of past events. The generalization ability is demonstrated by means of the blocks world example and it is shown that the algorithm can successfully handle practical problems with samples of about one hundred of relatively complicated structures in a reasonable time. Additionally, the problem of representation and learning context dependent concepts is discussed in the paper.|Fritz Wysotzki,Werner Kolbe,Joachim Selbig",
        "14258|IJCAI|1985|Substantial Constructive Induction Using Layered Information Compression Tractable Feature Formation in Search|This paper addresses a problem of induction (generalization learning) which is more difficult than any comparable work in AI. The subject of the present research is a hard problem of new terms, a task of realistic constructive induction. While the approach is quite general, the system is analyzed and tested in an environment of heuristic search where noise management and incremental learning are necessary. Here constructive induction becomes feature formation from data represented in elementary form. A high-level attribute or feature such as \"piece advantage\" in checkers is much more abstract than an elementary descriptor or primitive such as contents of a checkerboard square. Features have often been used in evaluation functions primitives are usually too detailed for this. To create abstract features from primitives (i.e. to restructure data descriptions), a new form of clustering is used which involves layering of knowledge and invariance of utility relationships related to data primitives and task goals. The scheme, which is both model- and data-driven, requires little background, domain-specific knowledge, but rather constructs it. The method achieves considerable generality with superior noise management and low computational complexity. Although the domains addressed are difficult, initial experimental results are encouraging.|Larry A. Rendell",
        "14508|IJCAI|1987|An Explanation-based Approach to Generalizing Number|An approach to generalizing number in explanation-based learning is presented. Generalizing number can involve generalizing such things as the number of entities involved in a concept or the number of times some action is performed. This issue has been largely ignored in previous explanation-based learning research. Instead, other research has focused on changing constants into variables and determining the general constraints on those variables. In the approach presented, generalization to N is triggered by the detection of inference rules of a specified syntactic form. When one is found, it is extended into the rule that results from an arbitrary number of repeated applications of the original rule. If the preconditions of the extended rule are met, the results of multiple applications of the original rule are immediately determined. There is no need to apply the underlying rule successively, each time checking if the preconditions for the next application are satisfied.|Jude W. Shavlik,Gerald DeJong",
        "14736|IJCAI|1989|Combining Case-Based and Rule-Based Reasoning A Heuristic Approach|In this paper we discuss a heuristically controlled approach to combining reasoning with cases and reasoning with rules. Our task is interpretation of under-defined terms that occur in legal statutes (like the Internal Revenue Code) where certain terms must be applied to particular cases even though their meanings are not defined by the statute and the statutory rules are unclear as to scope and meaning. We describe this problem, known as statutory interpretation, provide examples of it, describe the need for melding case-based and rule-based reasoning, and discuss heuristics used in guiding reasoning on such problems. We conclude with a discussion of our on-going work to model this mode of expert reasoning.|Edwina L. Rissland,David B. Skalak",
        "15356|IJCAI|1997|ILP with Noise and Fixed Example Size A Bayesian Approach|Current inductive logic programming systems are limited in their handling of noise, as they employ a greedy covering approach to constructing the hypothesis one clause at a time. This approach also causes difficulty in learning recursive predicates. Additionally, many current systems have an implicit expectation that the cardinality of the positive and negative examples reflect the \"proportion\" of the concept to the instance space. A framework for learning from noisy data and fixed example size is presented. A Bayesian heuristic for finding the most probable hypothesis in this general framework is derived. This approach evaluates a hypothesis as a whole rather than one clause at a time. The heuristic, which has nice theoretical properties, is incorporated in an ILP system, LIME. Experimental results show that LIME handles noise better than FOIL and PROGOL. It is able to learn recursive definitions from noisy data on which other systems do not perform well. LIME is also capable of learning from only positive data and also from only negative data.|Eric McCreath,Arun Sharma",
        "14700|IJCAI|1989|Principled Constructive Induction|A framework for the construction of new features for hard classification tasks is discussed. The approach brings together ideas from the fields of machine learning, computational geometry, and pattern recognition. Two heuristics for evaluation of newly-constructed features are proposed, and their statistical significance verified. Finally, it is shown how the proposed framework can be used to combine techniques for selection of representative examples with techniques for construction of new features, in order to solve difficult problems in learning from examples.|Pankaj Mehra,Larry A. Rendell,Benjamin W. Wah",
        "14534|IJCAI|1987|Guiding Constructive Induction for Incremental Learning from Examples|LAIR is a system that incrementally learns conjunctive concept descriptions from positive and negative examples. These concept descriptions are used to create and extend a domain theory that is applied, by means of constructive induction, to later learning tasks. Important issues for constructive induction are when to do it and how to control it LAIR demonstrates how constructive induction can be controlled by () reducing it to simpler operations, () constraining the simpler operations to preserve relative correctness, () doing deductive inference on an as-needed basis to meet specific information requirements of learning subtasks, and () constraining the search space by subtask-dependent constraints.|Larry Watanabe,Renee Elio",
        "14693|IJCAI|1989|Qualitative Reasoning of Bayesian Belief Using Meta-knowledge|Ordering composite hypotheses in a Bayesian network based on its associated a posteriori probabilities can be exponentially hard. This paper discusses a qualitative reasoning approach which reduces the computational complexity of deriving a partial ordering of composite hypotheses. Such a reasoning makes use of the meta-knowledge about the causal relationships among individual hypotheses to deduce qualitative conclusions about the ordering of local composite hypotheses. By doing so, we can employ \"divide and conquer\" strategy to derive the global ordering of the composite hypotheses from a set of local ordering in which consistencies are guaranteed. We view the contribution of this research is on the integration of qualitative reasoning with the use of local computations to find not only the most likely composite hypotheses, but also the partial ordering of the composite hypotheses.|Bon K. Sy",
        "17092|IJCAI|2009|Ranking Structured Documents A Large Margin Based Approach for Patent Prior Art Search|We propose an approach for automatically ranking structured documents applied to patent prior art search. Our model, SVM Patent Ranking (SVMPR) incorporates margin constraints that directly capture the specificities of patent citation ranking. Our approach combines patent domain knowledge features with meta-score features from several different general Information Retrieval methods. The training algorithm is an extension of the Pegasos algorithm with performance guarantees, effectively handling hundreds of thousands of patent-pair judgements in a high dimensional feature space. Experiments on a homogeneous essential wireless patent dataset show that SVMPR performs on average %-% better than many other state-of-the-art general-purpose Information Retrieval methods in terms of the NDCG measure at different cut-off positions.|Yunsong Guo,Carla P. Gomes",
        "15151|IJCAI|1995|Rule Induction and Instance-Based Learning A Unified Approach|This paper presents a new approach to inductive learning that combines aspects of instancebased learning and rule induction in a single simple algorithm. The RISE system searches for rules in a specific-to-general fashion, starting with one rule per training example, and avoids some of the difficulties of separate-and-eonquer approaches by evaluating each proposed induction step globally, i e, through an efficient procedure that is equivalent to checking the accuracy of the rule set as a whole on every training example. Classification is performed using a best-match strategy, and reduces to nearest-neighbor if all generalizations of instances were rejected. An extensive empirical study shows that RISE consistently achieves higher accuracies than state-of-the-art representatives of its \"parent\" paradigms (PEBLS and CN), and also outperforms a decision-tree learner (C ) in  out of  test domains (in  with % confidence).|Pedro Domingos"
      ],
      [
        "15509|IJCAI|1999|Adaptive Web Sites Conceptual Cluster Mining|The creation of a complex web site is a thorny problem in. user interface design. In IJCAI ', we challenged the AI community to address this problem by creating adaptive web sites. In response, we investigate the problem of index page synthesis - the automatic creation of pages that facilitate a visitor's navigation of a Web site. Previous work has employed statistical methods to generate candidate index pages that are of limited value because they do not correspond to concepts or topics that are intuitive to people. In this paper we formalize index page synthesis as a conceptual clustering problem and introduce a novel approach which we call conceptual cluster mining we search for a small number of cohesive clusters that correspond to concepts in a given concept description language L. Next, we present SGML, an algorithm schema that combines a statistical clustering algorithm with a concept learning algorithm. The clustering algorithm is used to generate seed clusters, and the concept learning algorithm to describe these seed clusters using expressions in L. Finally, we offer preliminary experimental evidence that instantiations of SGML outperform existing algorithms (e.g., COBWEB) in this domain.|Mike Perkowitz,Oren Etzioni",
        "13864|IJCAI|1981|Concept Learning by Structured Examples - An Algebraic Approach|A system learning concepts from training samples consisting of structured objects is described. It is based on descriptions invariant under isomorphism. In order to get a unified mathematical formalism recent graph theoretic results are used- The structures are transformed into feature vectors and after that a concept learning algorithm developing decision trees is applied which is an extension of algorithms found in psychological experiments. It corresponds to a general-to-specific depth-first search with reexamination of past events. The generalization ability is demonstrated by means of the blocks world example and it is shown that the algorithm can successfully handle practical problems with samples of about one hundred of relatively complicated structures in a reasonable time. Additionally, the problem of representation and learning context dependent concepts is discussed in the paper.|Fritz Wysotzki,Werner Kolbe,Joachim Selbig",
        "16451|IJCAI|2007|Web Page Clustering Using Heuristic Search in the Web Graph|Effective representation of Web search results remains an open problem in the Information Retrieval community. For ambiguous queries, a traditional approach is to organize search results into groups (clusters), one for each meaning of the query. These groups are usually constructed according to the topical similarity of the retrieved documents, but it is possible for documents to be totally dissimilar and still correspond to the same meaning of the query. To overcome this problem, we exploit the thematic locality of the Web--relevant Web pages are often located close to each other in the Web graph of hyperlinks. We estimate the level of relevance between each pair of retrieved pages by the length of a path between them. The path is constructed using multi-agent beam search each agent starts with one Web page and attempts to meet as many other agents as possible with some bounded resources. We test the system on two types of queries ambiguous English words and people names. The Web appears to be tightly connected about % of the agents meet with each other after only three iterations of exhaustive breadth-first search. However, when heuristics are applied, the search becomes more focused and the obtained results are substantially more accurate. Combined with a content-driven Web page clustering technique, our heuristic search system significantly improves the clustering results.|Ron Bekkerman,Shlomo Zilberstein,James Allan",
        "14974|IJCAI|1991|Concept Formation over Explanations and Problem-Solving Experience|Recent research suggests the utility of performing induction over explanations. This process identifies commonalities across explanations that cannot be extracted solely by explanation-based techniques. This has important implications for the 'correctness' of learned knowledge Flann and Dietterich,  and, as we show, on the efficiency with which learned knowledge can be reused. Specifically, we illustrate that inductive concept formation can abstract and organize explanatory knowledge for efficient reuse in a domain of algebra story problems.|Jungsoon P. Yoo,Douglas H. Fisher",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "15417|IJCAI|1999|Automatic Concept Formation in Pure Mathematics|The HR program forms concepts and makes conjectures in domains of pure mathematics and uses theorem prover OTTER and model generator MACE to prove or disprove the conjectures. HR measures properties of concepts and assesses the theorems and proofs involving them to estimate the interestingness of each concept and employ a best first search. This approach has led HR to the discovery of interesting new mathematics and enables it to build theories from just the axioms of finite algebras.|Simon Colton,Alan Bundy,Toby Walsh",
        "14272|IJCAI|1985|Learning Concept Descriptions from Examples with Errors|This paper presents a scheme for learning complex descriptions, such as logic formulas, from examples with errors. The basis for learning is provided by a selection criterion which minimizes a combined measure of discrepancy of a description with training data, and complexity of a description. Learning rules for two types of descriptors are derived one for finding descriptors with good average discrimination over a set of concepts, second for selecting the best descriptor for a specific concept. Once these descriptors are found, an unknown instance can be identified by a search using the descriptors of the first type for a fast screening of candidate concepts, and the second for the final selection of the closest concept.|Jakub Segen",
        "16483|IJCAI|2007|Learning Semantic Descriptions of Web Information Sources|The Internet is full of information sources providing various types of data from weather forecasts to travel deals. These sources can be accessed via web-forms, Web Services or RSS feeds. In order to make automated use of these sources, one needs to first model them semantically. Writing semantic descriptions for web sources is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions for web sources, in which we actively invoke sources and compare the data they produce with that of known sources of information. We perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. The paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.|Mark James Carman,Craig A. Knoblock",
        "14938|IJCAI|1991|Towards a Model of Grounded Concept Formation|In most research on concept formation within machine learning and cognitive psychology, the features from which concepts are built are assumed to be provided as elementary vocabulary. In this paper, we argue that this is an unnecessarily limited paradigm within which to examine concept formation. Based on evidence from psychology and machine learning, we contend that a principled account of the origin of features can only be given with a grounded model of concept formation, i.e., with a model that incorporates direct access to the world via sensors and manipulators. We discuss the domain of process control as a suitable framework for research into such models, and present a first approach to the problem of developing elementary vocabularies from perceptual sensor data.|Stefan Wrobel",
        "15992|IJCAI|2003|Coherent Keyphrase Extraction via Web Mining|Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using web mining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).|Peter D. Turney"
      ],
      [
        "16218|IJCAI|2005|Planning with Continuous Resources in Stochastic Domains|We consider the problem of optimal planning in stochastic domains with resource constraints, where resources are continuous and the choice of action at each step may depend on the current resource level. Our principal contribution is the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. The search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. We claim that this approach is especially effective when resource limitations contribute to reachability constraints. Experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.|Mausam,Emmanuel Benazera,Ronen I. Brafman,Nicolas Meuleau,Eric A. Hansen",
        "16915|IJCAI|2009|Information-Lookahead Planning for AUV Mapping|Exploration for robotic mapping is typically handled using greedy entropy reduction. Here we show how to apply information lookahead planning to a challenging instance of this problem in which an Autonomous Underwater Vehicle (AUV) maps hydrothermal vents. Given a simulation of vent behaviour we derive an observation function to turn the planning for mapping problem into a POMDP. We test a variety of information state MDP algorithms against greedy, systematic and reactive search strategies. We show that directly rewarding the AUV for visiting vents induces effective mapping strategies. We evaluate the algorithms in simulation and show that our information lookahead method outperforms the others.|Zeyn A. Saigol,Richard Dearden,Jeremy L. Wyatt,Bramley J. Murton",
        "16495|IJCAI|2007|Graph Decomposition for Efficient Multi-Robot Path Planning|In my previous paper (Ryan, ) I introduced the concept of subgraph decomposition as a means of reducing the search space in multi-robot planning problems. I showed how partitioning a roadmap into subgraphs of known structure allows us to first plan at a level of abstraction and then resolve these plans into concrete paths without the need for further search so we can solve significantly harder planning tasks with the same resources. However the subgraph types I introduced in that paper, stacks and cliques, are not likely to occur often in realistic planning problems and so are of limited usefulness. In this paper I describe a new kind of subgraph called a hall, which can also be used for planning and which occurs much more commonly in real problems. I explain its formal properties as a planning component and demonstrate its use on a map of the Patrick's container yard at the Port of Brisbane in Queensland Australia.|Malcolm R. K. Ryan",
        "15594|IJCAI|2001|Planning in Nondeterministic Domains under Partial Observability via Symbolic Model Checking|Planning under partial observability is one of the most significant and challenging planning problems. It has been shown to be hard, both theoretically and experimentally. In this paper, we present a novel approach to the problem of planning under partial observability in non-deterministic domains. We propose an algorithm that searches through a (possibly cyclic) and-or graph induced by the domain. The algorithm generates conditional plans that are guaranteed to achieve the goal despite of the uncertainty in the initial condition, the uncertain effects of actions, and the partial observability of the domain. We implement the algorithm by means of BDD-based, symbolic model checking techniques, in order to tackle in practice the exponential blow up of the search space. We show experimentally that our approach is practical by evaluating the planner with a set of problems taken from the literature and comparing it with other state of the art planners for partially observable domains.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri,Paolo Traverso",
        "15593|IJCAI|2001|Heuristic Search  Symbolic Model Checking  Efficient Conformant Planning|Planning in nondeterministic domains has gained more and more importance. Conformant planning is the problem of finding a sequential plan that guarantees the achievement of a goal regardless of the initial uncertainty and of nondeterministic action effects. In this paper, we present a new and efficient approach to conformant planning. The search paradigm, called heuristic-symbolic search, relies on a tight integration of symbolic techniques, based on the use of Binary Decision Diagrams, and heuristic search, driven by selection functions taking into account the degree of uncertainty. An extensive experimental evaluation of our planner HSCP against the state of the art conformant planners shows that our approach is extremely effective. In terms of search time, HSCP gains up to three orders of magnitude over the breadth-first, symbolic approach of CMBP, and up to five orders of magnitude over the heuristic search of GPT, requiring, at the same time, a much lower amount of memory.|Piergiorgio Bertoli,Alessandro Cimatti,Marco Roveri",
        "15707|IJCAI|2001|Planning as Model Checking for Extended Goals in Non-deterministic Domains|Recent research has addressed the problem of planning in non-deterministic domains. Classical planning has also been extended to the case of goals that can express temporal properties. However, the combination of these two aspects is not trivial. In non-deterministic domains, goals should take into account the fact that a plan may result in many possible different executions and that some requirements can be enforced on all the possible executions, while others may be enforced only on some executions. In this paper we address this problem. We define a planning algorithm that generates automatically plans for extended goals in nondeterministic domains. We also provide preliminary experimental results based on an implementation of the planning algorithm that uses symbolic model checking techniques.|Marco Pistore,Paolo Traverso",
        "14648|IJCAI|1989|A Model of Planning for Plan Efficiency Taking Advantage of Operator Overlap|Many realistic planning problems (such as those in manufacturing) place a high premium on plan efficiency. However, classical planning theory does not offer much insight into ways of obtaining efficient plans. The objective of this paper is to present a model of planning for plan efficiency that has been drawn from a case study in the machining domain. Some of the important features of this domain are that problems are stated as sets of conjunctive goals, and operators that achieve those goals have a high degree of overlap. Operators can be said to overlap when they can share work. Because of this overlap, the cost of the operators is dependent on their order in the plan, (for example, it is less time consuming to buy vegetables if your last action was also done at the grocery store) However, looking for a near optimal set of overlapping operators can lead a very expensive search. The methods that human machinists were found to be using reduced the complexity of the search for good operators by using cues and patterns in the problem specification, gained from experience, to tell them when it might be useful to explore an operator.|Caroline C. Hayes",
        "16916|IJCAI|2009|Tractable Multi-Agent Path Planning on Grid Maps|Multi-agent path planning on grid maps is a challenging problem and has numerous real-life applications. Running a centralized, systematic search such as A* is complete and cost-optimal but scales up poorly in practice, since both the search space and the branching factor grow exponentially in the number of mobile units. Decentralized approaches, which decompose a problem into several subproblems, can be faster and can work for larger problems. However, existing decentralized methods offer no guarantees with respect to completeness, running time, and solution quality. To address such limitations, we introduce MAPP, a tractable algorithm for multi-agent path planning on grid maps. We show that MAPP has low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions. As it runs in low-polynomial time, MAPP is incomplete in the general case. We identify a class of problems for which our algorithm is complete. We believe that this is the first study that formalises restrictions to obtain a tractable class of multi-agent path planning problems.|Ko-Hsin Cindy Wang,Adi Botea",
        "15940|IJCAI|2003|A heuristic model for concurrent bi-lateral negotiations in incomplete information settings|Bi-lateral negotiations represent an important class of encounter in agent-based systems. To this end, this paper develops and evaluates a heuristic model that enables an agent to participate in multiple, concurrent bi-lateral encounters in competitive situations in which there is information uncertainty and deadlines.|Thuc Duong Nguyen,Nicholas R. Jennings",
        "16902|IJCAI|2009|Optimal Symbolic Planning with Action Costs and Preferences|This paper studies the solving of finite-domain action planning problems with discrete action costs and soft constraints. For sequential optimal planning, a symbolic perimeter database heuristic is addressed in a bucket implementation of A*. For computing net-benefits, we propose symbolic branch-and-bound search together with some search refinements. The net-benefit we optimize is the total benefit of satisfying the goals, minus the total action cost to achieve them. This results in an objective function to be minimized that is a linear expression over the violation of the preferences added to the action cost total.|Stefan Edelkamp,Peter Kissmann"
      ],
      [
        "16919|IJCAI|2009|Russian Doll Search with Tree Decomposition|Optimization in graphical models is an important problem which has been studied in many AI frameworks such as weighted CSP, maximum satisfiability or probabilistic networks. By identifying conditionally independent subproblems, which are solved independently and whose optimum is cached, recent Branch and Bound algorithms offer better asymptotic time complexity. But the locality of bounds induced by decomposition often hampers the practical effects of this result because subproblems are often uselessly solved to optimality. Following the Russian Doll Search (RDS) algorithm, a possible approach to overcome this weakness is to (inductively) solve a relaxation of each subproblem to strengthen bounds. The algorithm obtained generalizes both RDS and tree-decomposition based algorithms such as BTD or AND-OR Branch and Bound. We study its efficiency on different problems, closing a very hard frequency assignment instance which has been open for more than  years.|Mart\u00ed S\u00e1nchez,David Allouche,Simon de Givry,Thomas Schiex",
        "15486|IJCAI|1999|Improving Search Using Indexing A Study with Temporal CSPs|Most studies concerning constraint satisfaction problems (CSPs) involve variables that take values from small domains. This paper deals with an alternative form of temporal CSPs the number of variables is relatively small and the domains are large collections of intervals. Such situations may arise in temporal databases where several types of queries can be modeled and processed as CSPs. For these problems, systematic CSP algorithms can take advantage of temporal indexing to accelerate search. Directed search versions of chronological backtracking and forward checking are presented and tested. Our results show that indexing can drastically improve search performance.|Nikos Mamoulis,Dimitris Papadias",
        "15119|IJCAI|1995|Experimenting with Revisits in Game Tree Search|The oldest known game tree search algorithm Alpha-Beta is still the most popular one. All other algorithms in this area fall short of Alpha-Beta in one or more of the following three desired characteristics - high pruning power, low storage requirement and low execution time. This paper discusses how revisit of nodes can be used effectively in game tree search. A few strategies of introducing revisits in game tree search are presented. It is demonstrated that for any shape and ordering of the game tree to be searched, there always exists one strategy that, on an average, consistently evaluates less number of terminals than Alpha-Beta in comparable memory and time.|Subir Bhattacharya",
        "15077|IJCAI|1993|Case Retrieval through Multiple Indexing and Heuristic Search|We discuss the indexing of cases for use in precedent-based argument. Our focus is on how multiple, related indices into a case base of legal precedents are exploited by an argument-generation program called BankXX. This system's architecture and control scheme are rooted in a conceptualization of legal argument as heuristic search. Although our framing argument as search is not discussed in detail in this paper, we describe the main features of this view to provide context for a discussion of an indexing scheme that facilitates argument creation. We describe five inter-related index types--citation, prototypical story, factor, family resemblance, and legal theory indices-- and show how they can be used to access, view, widen, or filter a set of cases. The application domain is a U.S. Federal statute that governs the approval of bankruptcy plans.|Edwina L. Rissland,David B. Skalak,M. Timur Friedman",
        "17024|IJCAI|2009|Monte Carlo Tree Search Techniques in the Game of Kriegspiel|Monte Carlo tree search has brought significant improvements to the level of computer players in games such as Go, but so far it has not been used very extensively in games of strongly imperfect information with a dynamic board and an emphasis on risk management and decision making under uncertainty. In this paper we explore its application to the game of Kriegspiel (invisible chess), providing three Monte Carlo methods of increasing strength for playing the game with little specific knowledge. We compare these Monte Carlo agents to the strongest known minimax-based Kriegspiel player, obtaining significantly better results with a considerably simpler logic and less domain-specific knowledge.|Paolo Ciancarini,Gian Piero Favini",
        "13475|IJCAI|1975|The Heuristic Search And The Game Of Chess - A Study Of Quiescene Sacrifices And Plan Oriented Play|This paper describes the results of applying the formal heurisitic search algorithm to the game of chess, and the impact of this work on the theory of heuristic search. It is not that the application of the heuristic search can by itself solve the problems at the heart of the computer chess, but that representing these problems within the formalism of the heuristic search will further their common solution. A separate search heuristic is proposed that does offer a common solution to the problems of quiescence, sacrifices, and plan oriented play.|Larry R. Harris",
        "15494|IJCAI|1999|Decomposition Search A Combinatorial Games Approach to Game Tree Search with Applications to Solving Go Endgames|We develop a new method called decomposition search for computing minimax solutions to games that can be partitioned into independent subgames. The method does not use traditional minimax search algorithms such as alpha-beta, but relies on concepts from combinatorial game theory to do locally restricted searches. This divide-and-conquer approach allows the exact solution of much larger problems than is possible with alpha-beta. We show an application of decomposition search to the game of Go, which has been traditionally regarded as beyond the range of exact search-based solution methods. Our experiments with solving endgames show that alpha-beta searches already become impractical in positions with about  remaining moves. However, an endgame solver based on decomposition search can solve a much larger class of endgame problems with solution lengths exceeding  moves.|Martin M\u00fcller 0003",
        "13254|IJCAI|1969|Search for a Solution A Case Study|This paper describes a series of attempts at the solution of a conceptually tough problem, the Firing Squad Synchronization Problem. These attempts demonstrate an increasing reliance on man-machine symbiosis and decreasing reliance on powerful heurisics and preplanning. These attempts consist of a clerical checking program, and four attempts utilizing a basic backtracking program for searching the solution space. The first two attempts, Serial Definition of Productions and Symbolic Definition of Productions were non - interactiveentirely computer directed attempts at solution. The second two, Functional Planning and Constraint Satisfaction were man-machine symbiotic attempts designed to allow the human to control and direct the computer search of the solution space. The benefits of these symbiotic attempts and the problems encountered with them are discussed.|Robert Balzer",
        "15262|IJCAI|1995|Best-First Fixed-Depth Game-Tree Search in Practice|We present a new paradigm for minimax search algorithms MT, a memory-enhanced version of Pearl's Test procedure. By changing the way MT is called, a number of practical best-first search algorithms can be simply constructed. Reformulating SSS* as an instance of MT eliminates all its perceived implementation drawbacks. Most assessments of minimax search performance are based on simulations that do not address two key ingredients of high performance game-playing programs iterative deepening and memory usage. Instead, we use experimental data gathered from tournament checkers, Othello and chess programs. The use of iterative deepening and memory makes our results differ significantly from the literature. One new instance of our framework, MTD(I), out-performs our best Alpha-Beta searcher on leaf nodes, total nodes and execution time. To our knowledge, these are the first reported results that compare both depth-first and best-first algorithms given the same amount of memory.|Aske Plaat,Jonathan Schaeffer,Wim Pijls,Arie de Bruin",
        "14742|IJCAI|1989|On Optimal Game-Tree Search using Rational Meta-Reasoning|In this paper we outline a general approach to the study of problem-solving, in which search steps are considered decisions in the same sense as actions in the world. Unlike other metrics in the literature, the value of a search step is defined as a real utility rather than as a quasi-utility, and can therefore be computed directly from a model of the base-level problem-solver. We develop a formula for the expected value of a search step in a game-playing context using the single-step assumption, namely that a computation step can be evaluated as it was the last to be taken. We prove some meta-level theorems that enable the development of a low-overhead algorithm, MGSS*, that chooses search steps in order of highest estimated utility. Although we show that the single-step assumption is untenable in general, a program implemented for the game of Othello soundly beats an alpha-beta search while expanding significantly fewer nodes, even though both programs use the same evaluation function.|Stuart J. Russell,Eric Wefald"
      ],
      [
        "14333|IJCAI|1987|Integrating Declarative Knowledge Programming Styles and Tools in a Structured Object AI Environment|XRL is an integrated knowledge programming environment whose major research theme is the investigation of declarative knowledge programming styles and tools, and of the way they can be effectively integrated and used to support AI programming. This investigation is carried out in the context of the structured-object representation paradigm which provides the glue keeping XRL components together. The paper describes several declarative programming styles and associated support tools currently available in XRL.|Mihai Barbuceanu,Stefan Trausan-Matu,Balint Molnar",
        "13814|IJCAI|1981|Object Recognition Using Three-Dimensional Information|This paper describes an approach to the recognition of stacked objects with planar and curved surfaces. The range data of a scene are obtained by a range finder. The system works In two phases. In a learning phase, a scene containing a single object Is described In terms of properties of regions and relations between them. This description Is stored as an object model. In a recognition phase, an unknown scene Is described In the same way as In the learning phase. And then the description is matched to the object models so that stacked objects are recognized one by one. Efficient matching is achieved by a combination of data-driven and model-driven search process. Experimental results for blocks and machine parts are shown.|Masaki Oshima,Yoshiaki Shirai",
        "15049|IJCAI|1993|The Intelligent Hand An Experimental Approach to Human Object Recognition and Implications for Robotic Design|The scientific study of biological systems offers an approach to the development of sensor-based robots that is complementary to the more formal analytic methods currently favoured by roboticists. I initially propose several general lessons from the biological field. Next, I consider a specific example selected from the work of Lederman & Klatzky, which focuses on human haptic object processing. An empirical base and recent theoretical developments from our research program on this topic are described. The human haptic system is an information-processing system that combines inputs from sensors in skin, muscles, tendons, and joints with motor capabilities to extract different object properties. A general model of human haptic object identification, which emphasises how object exploration is controlled, is presented. The model describes major architectural elements, including representations of haptically accessible object properties and exploratory procedures (EPs), which are dedicated movement patterns specialized to extract particular properties. These architectural units are related in processing-specific ways. The resulting architecture is treated as a system of constraints, which guide the exploration of an object during the course of identification. Empirical support for the model is also examined. To conclude, I show how this scientifically-based approach might be applied to developing strategies for active manual robotic exploration of unstructured environments.|Susan J. Lederman",
        "15792|IJCAI|2003|From Logic Programming Semantics to the Consistency of Syntactical Treatments of Knowledge and Belief|This paper concerns formal theories for reasoning about the knowledge and belief of agents. It has seemed attractive to researchers in artificial intelligence to formalise these propositional attitudes as predicates of first-order predicate logic. This allows the agents to express stronger introspective beliefs and engage in stronger meta-reasoning than in the classical modal operator approach. Results by Montague  and Thomason  show, however, that the predicate approach is prone to inconsistency. More recent results by des Rivieres & Levesque  and Morreau & Kraus  show that we can maintain the predicate approach if we make suitable restrictions to our set of epistemic axioms. Their results are proved by careful translations from corresponding modal formalisms. In the present paper we show that their results fit nicely into the framework of logic programming semantics, in that we show their results to be corollaries of well-known results in this field. This does not only allow us to demonstrate a close connection between consistency problems in the syntactic treatment of propositional attitudes and problems in semantics for logic programs, but it also allows us to strengthen the results of des Rivieres & Levesque  and Morreau & Kraus .|Thomas Bolander",
        "15300|IJCAI|1995|Kanervas Sparse Distributed Memory An Object-Oriented Implementation on the Connection Machine|This paper reports on an implementation of Kanerva's Sparse Distributed Memory for the Connection Machine. In order to accomplish a modular and adaptive software library we applied a plain object-oriented programming style to the Common Lisp extension ltsp. Some variations of the original model, the selected coordinate design, the hyperplane design, and a new general design, as well as the folded SDM due to Kanerva are realized. It has been necessary to elaborate a uniform presentation of the theoretical foundations the different designs are based on. We demonstrate the simulator's functionality with some simple applications. Runtime comparisons are given. We encourage the use of our simulation tool when outlining research topics of special interest to SDM.|Andreas Turk,G\u00fcnther G\u00f6rz",
        "14429|IJCAI|1987|Extending Logic Programming to Object Programming The System Lap|Object oriented programming aims at code lisibility and conciseness via abstract data type declarations and takes advantage from setting the objects (ie data) in the center of the application, instead of procedures or demonstrations. Just as in Logic programming, where describing the properties of the solution of some problem is (theoreticaly) enough to allow the program to compute the solution, Object programming proceeds from the fact that purely declarative information leads to more procedural behaviour. It appears that Logic programming and Object programming are complementary in the sense that the first is very suitable for expressing subtle reasoning, but rather weak as a formalism for describing complex structures, and conversely for the second. These considerations led us, at the IIRIAM, to combine Logic and Object programming in order to use the advantages of both. Our research effort resulted In the system LAP, an extension of Prolog to object programming, giving a new dimension to the latter, through logic programming concepts processing of partially known data and non determinism applied at the object level. The first part of this paper introduces the main characteristics of LAP, faced with the requirements of AI application development. The second part illustrates some of these capabilities through simple examples.|Herman Iline,Henry Kanoui",
        "14422|IJCAI|1987|Explanation-based Generalization in a Logic- Programming Environment|This paper describes a domain-independent implementation of explanation-based generalization (EBG) within a logic-programming environment. Explanation is interleaved with generalization, so that as the training instance is proven to be a positive example of the goal concept, the generalization is simultaneously created. All aspects of the EBG task are viewed in logic, which provides a clear semantics for EBG, and allows its integration into the logic-programming system. In this light operationally becomes a property requiring explicit reasoning. Additionally, viewing EBG in logic clarifies the relation of learning search-control to EBG, and suggests solutions for dealing with imperfect domain theories.|Haym Hirsh",
        "14207|IJCAI|1985|Visual Recognition from Spatial Correspondence and Perceptual Organization|Depth reconstruction from the two-dimensional image plays an important role in certain visual tasks and has been a major focus of computer vision research. However, in this paper we argue that most instances of recognition in human and machine vision can best be performed without the preliminary reconstruction of depth. Three other mechanisms are described that can be used to bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization can be used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Secondly, evidential reasoning can be used to combine evidence from these groupings and other sources of information to reduce the size of the search-space during model-based matching. Finally, a process of spatial correspondence can be used to bring the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. These methods have been combined in an experimental computer vision system named SCERPO. This system has demonstrated the use of these methods for the recognition of objects from unknown viewpoints in single gray-scale images.|David G. Lowe",
        "14953|IJCAI|1991|Natural Object Recognition A Theoretical Framework and Its Implementation|Most work in visual recognition by computer has focused on recognizing objects by their geometric shape, or by the presence or absence of some prespecified collection of locally measurable attributes (e.g., spectral reflectance, texture, or distinguished markings). On the other hand, most entities in the natural world defy compact description of their shapes, and have no characteristic features with discriminatory power. As a result, image-understanding research has achieved little success toward recognition in natural scenes. We offer a fundamentally new approach to visual recognition that avoids these limitations and has been used to recognize trees, bushes, grass, and trails in ground-level scenes of a natural environment.|Thomas M. Strat,Martin A. Fischler",
        "13590|IJCAI|1977|A Computer System for Visual Recognition Using Active Knowledge|This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N--C- . The report is based upon the author's Ph.D. dissertation for the Department of Electrical Engineering and Computer Science, M.I.T., . Current address Department of Mathematics and Computer Science, University of New Hampshire, Durham, N.H. .|Eugene C. Freuder"
      ],
      [
        "15360|IJCAI|1997|Vision-Motion Planning of a Mobile Robot considering Vision Uncertainty and Planning Cost|This paper proposes a planning method for a vision-guided mobile robot under vision uncertainty and limited computational resources. The method considers the following two tradeoffs () granularity in approximating a probabilistic distribution vs. plan quality, and () search depth vs. plan quality. The first tradeoff is managed by predicting the plan quality for a granularity using a learned relationship between them, and by adaptively selecting the best granularity. The second trade-off is managed by formulating the planning process as a search in the space of feasible plans, and by appropriately limiting the search considering the merit of each step of the search. Simulation results and experiments using a real robot show the feasibility of the method.|Jun Miura,Yoshiaki Shirai",
        "13675|IJCAI|1977|The Navigation System of the JPL Robot|The control structure of the JPL research robot and the operations of the navigation subsystem are discussed. The robot functions as a network of interacting concurrent processes distributed among several computers and coordinated by a central executive. The results of scene analysis are used to create a segmented terrain model in which surface regions are classified by traversibility. The model is used by a path-planning algorithm, PATH, which uses tree search methods to find the optimal path to a goal. In PATH, the search space is defined dynamically as a consequence of node testing. Maze-solving and the use of an associative data base for context-dependent node generation are also discussed. Execution of a planned path is accomplished by a feedback guidance process with automatic error recovery.|Alan M. Thompson",
        "15409|IJCAI|1999|A Context-Dependent Attention System for a Social Robot|This paper presents part of an on-going project to integrate perception, attention, drives, emotions, behavior arbitration, and expressive acts for a robot designed to interact socially with humans. We present the design of a visual attention system based on a model of human visual search behavior from Wolfe (). The attention system integrates perceptions (motion detection, color saliency, and face popouts) with habituation effects and influences from the robot's motivational and behavioral state to create a context-dependent attention activation map. This activation map is used to direct eye movements and to satiate the drives of the motivational system.|Cynthia Breazeal,Brian Scassellati",
        "16142|IJCAI|2005|Real-Time Path Planning for Humanoid Robot Navigation|We present a data structure and an algorithm for real-time path planning of a humanoid robot. Due to the many degrees of freedom, the robots shape and available actions are approximated for finding solutions efficiently. The resulting  dimensional configuration space is searched by the A* algorithm finding solutions in tenths of a second on lowperformance, embedded hardware. Experimental results demonstrate our solution for a robot in a world containing obstacles with different heights, stairs and a higher-level platform.|Jens-Steffen Gutmann,Masaki Fukuchi,Masahiro Fujita",
        "13360|IJCAI|1971|Grid Coding A Preprocessing Technique for Robot and Machine Vision|The problem of machine vision as evidenced In the various robot projects in existence is attacked by analogy with the supposed nature of human visual processing in that edges are enhanced, texture is examined and various heuristic approaches are studied. This paper describes a nonanthropomorphically based method of decomposing a scene subjected to a special form of illumination into elementary planar areas. The method consists in coding the various planar areas as the modulation on a spatial frequency carrier grid so that the extraction of the planar areas becomes a matter of linear frequency domain filtering. The paper also addresses the application of grid coding to other problems in recording and extracting information from -D images.|Peter M. Will,Keith S. Pennington",
        "16581|IJCAI|2007|Using a Mobile Robot for Cognitive Mapping|When animals (including humans) first explore a new environment, what they remember is fragmentary knowledge about the places visited. Yet, they have to use such fragmentary knowledge to find their way home. Humans naturally use more powerful heuristics while lower animals have shown to develop a variety of methods that tend to utilize two key pieces of information, namely distance and orientation information. Their methods differ depending on how they sense their environment. Could a mobile robot be used to investigate the nature of such a process, commonly referred to in the psychological literature as cognitive mapping What might be computed in the initial explorations and how is the resulting \"cognitive map\" be used to return home In this paper, we presented a novel approach using a mobile robot to do cognitive mapping. Our robot computes a \"cognitive map\" and uses distance and orientation information to find its way home. The process developed provides interesting insights into the nature of cognitive mapping and encourages us to use a mobile robot to do cognitive mapping in the future, as opposed to its popular use in robot mapping.|Chee K. Wong,Jochen Schmidt,Wai K. Yeap",
        "15281|IJCAI|1995|Probabilistic Robot Navigation in Partially Observable Environments|Autonomous mobile robots need very reliable navigation capabilities in order to operate unattended for long periods of time. This paper reports on first results of a research program that uses par tially observable Markov models to robustly track a robots location in office environments and to direct its goal-oriented actions. The approach explicitly maintains a probability distribution over the possible locations of the robot taking into account various sources of uncertainly including approximate knowledge of the environment and actuator and sensor uncertainty. A novel feature of our approach is its integration of topological map information with approximate metric information. We demonstrate the robustness of this approach in controlling an actual indoor mobile robot navigating corridors.|Reid G. Simmons,Sven Koenig",
        "14951|IJCAI|1991|Mobile Robot Navigation by an Active Control of the Vision System|In this paper, we argue that a mobile robot's environment can be determined by computing local maps surrounding feature points, called fixation points. These fixation points are obtained by searching the scene for points which present some interesting cue for robot navigation. This -D computation is based on a monocular active vision system composed of a camera, mounted on a rotating table accurately controlled by a computer, which gazes the fixation point as the robot moves. The system then computes the local map and updates it with each new observation in order to increase its accuracy and robustness. Real experimentation in a complex indoor scene illustrates that the -D scene coordinates can be obtained with a good accuracy by integrating several observations.|Patrick Stelmaszyk,Hiroshi Ishiguro,Saburo Tsuji",
        "13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris",
        "14702|IJCAI|1989|Robot Navigation|Robot navigation is a problem that encompasses most of the major areas of AI research. Machine architecture, search, knowledge acquisition and representation, planning, scheduling, reaction, perception, and of course robotics, all can play integral roles in a mobile robot navigation system. For this reason, a large part of the AI community is now interested (and has been since STRIPS was used to guide the Shakey robot) in mobile robot navigation.|David P. Miller"
      ]
    ]
  }
}