{"abstract":{"entropy":6.741231011380836,"topics":["machine learning, recent years, learning classifier, mining web, learning system, learning, data mining, data web, support vector, learning programming, learning problem, classifier system, knowledge learning, multi-agent system, service web, research learning, semi-supervised learning, recent research, explore learning, recent work","description logic, cognitive architectures, immune system, natural language, artificial immune, ontologies semantic, semantic web, representation data, automatically semantic, research continuous, important application, reasoning knowledge, system control, reasoning actions, use design, important task, important semantic, artificial system, artificial intelligence, present natural","xcs fault, novel approach, present approach, actions uncertainty, markov processes, markov decision, real world, develop theory, artificial intelligence, effects uncertainty, involves uncertainty, decision processes, technique planning, evolutionary computation, paper xcs, planning uncertainty, diagnosis problem, approach, problem given, system planning","evolutionary algorithm, genetic algorithm, algorithm, genetic programming, particle swarm, optimization problem, optimization algorithm, problem, describe algorithm, estimation distribution, hierarchical bayesian, swarm optimization, particle optimization, genetic problem, algorithm problem, multiobjective optimization, building blocks, search heuristics, consider problem, present algorithm","mining web, data mining, data web, web, service web, human, useful, virtual, key, technologies, text, scientific, dna, used, behavior, wide, classifier, based, factor, design","machine learning, learning classifier, support vector, learning, learning problem, learning programming, research learning, semi-supervised learning, genetic learning, introduces learning, learning support, support, complex, methods, multiple, digital, population, clustering, presented, developing","natural language, representation data, data, dynamic, explore, representation, present, space, various, physical, interactive, parameters, evolution, generalization, means, traditional, problem","immune system, artificial immune, ontologies semantic, semantic web, system control, important application, important semantic, artificial system, important, system, area, semantic, behavior, challenge, success, cell, computer, depends, issue, understand","artificial intelligence, application problem, application, time, interaction, software, analysis, data, focus, modern, real, problem, testing, novel","xcs fault, develop theory, paper xcs, first, introduces, analyze, test, case, important, students, structural, feature, strategies, generation, called, type, training","based algorithm, algorithm crossover, differential evolution, analyze algorithm, selection algorithm, algorithm solve, crossover, based, evolution, evolution algorithm, selection, class, existing, graph, binary, novel, introduced, approaches, random, strategy","genetic programming, genetic problem, evolutionary programming, problem programming, evolutionary problem, genetic network, challenging problem, genetic applied, applied problem, selection genetic, sequence, common, investigates, evolving, population, encoding, logic, application, market, find"],"ranking":[["57927|GECCO|2007|Improving the human readability of features constructed by genetic programming|The use of machine learning techniques to automatically analyse data for information is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to pre-process data before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract more information about the domain.|Matthew Smith,Larry Bull","66113|AAAI|2007|Biomind ArrayGenius and GeneGenius Web Services Offering Microarray and SNP Data Analysis via Novel Machine Learning Methods|Analysis of postgenomic biological data (such as microarray and SNP data) is a subtle art and science, and the statistical methods most commonly utilized sometimes prove inadequate. Machine learning techniques can provide superior understanding in many cases, but are rarely used due to their relative complexity and obscurity. A challenge, then, is to make machine learning approaches to data analysis available to the average biologist in a user-friendly way. This challenge is addressed by the Biomind ArrayGenius product, an easy-to-use Web-based system providing microarray analysis based on genetic prognunming, kernel methods, and incorporation of knowledge from biological ontologies and GeneGenius, its sister product for SNP data. This paper focuses on the obstacles faced and lessons learned in the course of creating, deploying, maintaining and selling ArrayGenius and GeneGenius - many of which are generic to any effort involving the creation of complex AI-based products addressing complex domain problems.|Ben Goertzel,Cassio Pennachin,LÃºcio de Souza Coelho,Leonardo Shikida,Murilo Saraiva de Queiroz","66166|AAAI|2007|Learning Large Scale Common Sense Models of Everyday Life|Recent work has shown promise in using large, publicly available, hand-contributed commonsense databases as joint models that can be used to infer human state from day-to-day sensor data. The parameters of these models are mined from the web. We show in this paper that learning these parameters using sensor data (with the mined parameters as priors) can improve performance of the models significantly. The primary challenge in learning is scale. Since the model comprises roughly , irregularly connected nodes in each time slice, it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice. We show how to solve the resulting semi-supervised learning problem by combining a variety of conventional approximation techniques and a novel technique for simplifying the model called context-based pruning. We show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various techniques contribute to the performance.|William Pentney,Matthai Philipose,Jeff A. Bilmes,Henry A. Kautz","58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","66139|AAAI|2007|Clustering with Local and Global Regularization|Clustering is an old research topic in data mining and machine learning communities. Most of the traditional clustering methods can be categorized local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the dataset is proposed. The method, Clustering with Local and Global Consistency (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We will show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently by some iterative methods. Finally the experimental results on several datasets are presented to show the effectiveness of our method.|Fei Wang,Changshui Zhang,Tao Li","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","57926|GECCO|2007|UCSpv principled voting in UCS rule populations|Credit assignment is a fundamental issue for the Learning Classifier Systems literature. We engage in a detailed investigation of credit assignment in one recent system called UCS, and in the process uncover two previously undocumented features. We draw on techniques from the classical pattern recognition literature, showing how to analytically derive an optimal credit assignment system, given certain assumptions. Our primary aim is not to improve accuracy, but to better understand the system and put it on a more solid theoreticalfoundation. Nonetheless, empirical results on benign data demonstrate our new system, called UCSpv (UCS with principled voting), can match or exceed the original UCS. Further, its fitness function is principled, and, unlike that of UCS, requires no tuning. However, on more difficult data it seems UCSpv does need some form of tuning or correction. We believe the framework we adopt offers a promising new direction for LCS research, providing principled methods for action selection and bringing LCS closer to the mainstream pattern recognition literature.|Gavin Brown,Tim Kovacs,James A. R. Marshall","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono"],["66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|LuÃ­s C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","66210|AAAI|2007|Approximating OWL-DL Ontologies|Efficient query answering over ontologies is one of the most useful and important services to support Semantic Web applications. Approximation has been identified as a potential way to reduce the complexity of query answering over OWL DL ontologies. Existing approaches are mainly based on syntactic approximation of ontological axioms and queries. In this paper, we propose to recast the idea of knowledge compilation into approximating OWL DL ontologies with DL-Lite ontologies, against which query answering has only polynomial data complexity. We identify a useful category of queries for which our approach guarantees also completeness. Furthermore, this paper reports on the implementation of our approach in the ONTOSEARCH system and preliminary, but encouraging, benchmark results which compare ONTOSEARCH's response times on a number of queries with those of existing ontology reasoning systems.|Jeff Z. Pan,Edward Thomas","66090|AAAI|2007|Repairing Ontology Mappings|Automatically discovering semantic relations between ontologies is an important task with respect to overcoming semantic heterogeneity on the semantic web. Existing ontology matching systems, however, often produce erroneous mappings. In this paper, we address the problem of errors in mappings by proposing a completely automatic debugging method for ontology mappings. The method uses logical reasoning to discover and repair logical inconsistencies caused by erroneous mappings. We describe the debugging method and report experiments on mappings submitted to the ontology alignment evaluation challenge that show that the proposed method actually improves mappings created by different matching systems without any human intervention.|Christian Meilicke,Heiner Stuckenschmidt,Andrei Tamilin","66257|AAAI|2007|On Possible Applications of Rough Mereology to Handling Granularity in Ontological Knowledge|Representing and reasoning about knowledge is critical in Artificial Intelligence. There is a distinction between factual and ontological knowledge. Factual knowledge represents a set of facts about individual objects that are known or believed whereas ontological (background) knowledge represents implicit concepts and relationships that are assumed to exist in the world. Ontological knowledge is often represented as a hierarchy of concepts because splitting things of the real world into categories and sub-categories is a natural way of human thinking. One example of conceptual hierarchies in AI is ontologies that are widely used in such areas as Natural Language Processing, Semantic Web, etc. Representation of both types of knowledge becomes difficult when the knowledge is imprecise. One example of imprecision is granularity i.e. inability to distinguish between the individual objects. In this case, knowledge cannot be represented precisely but can be approximated with respect to the granularity of the domain. Approximation of factual knowledge has been extensively researched and often employs Rough Set Theory (Pawlak ) for dealing with indiscernibility of objects. Similar approach has been applied to ontologies to approximate concepts in the hierarchy (Doherty et al. ). The open problem is the approximations of hierarchical relationships (such as \"is-a\", \"part-of') between concepts. This paper addresses this issue using Rough Mereology (Polkowski & Skowron ) complemented with Interval Analysis (Moore ). The principal contribution is to provide rough mereological functions that can be used for representation and reasoning with formal ontologies in approximation spaces. Specifically, approximate concept membership and approximate concept subsumption functions will be provided. It can be demonstrated that the interval based functions are free of the shortcomings of the previously suggested definitions (Cao, Sui, & Zhang ) (Klinov & Mazlack ).|Pavel Klinov,Lawrence J. Mazlack","66004|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet system is an attempt to automatically generate large scale semantic knowledge networks from natural language text. State-of-the-art language processing tools, including parsers and semantic analysers, are used to turn input sentences into fragments of semantic network. These network fragments are combined using spreading activation-based algorithms which utilise both lexical and semantic information. The emphasis of the system is on wide-coverage and speed of construction. In this paper we show how a network consisting of over . million nodes and . million edges, more than twice as large as any network currently available, can be created in less than  days. We believe that the methods proposed here will enable the construction of semantic networks on a scale never seen before, and in doing so reduce the knowledge acquisition bottleneck for AI.|Brian Harrington,Stephen Clark","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","66065|AAAI|2007|Semantic Inference at the Lexical-Syntactic Level|Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on complex logical representations. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, but lack a principled inference framework. We propose a generic semantic inference framework that operates directly on syntactic trees. New trees are infened by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, Covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical evaluation in a Relation Extraction setting supports the validity of our approach.|Roy Bar-Haim,Ido Dagan,Iddo Greental,Eyal Shnarch","66175|AAAI|2007|An Integrated Robotic System for Spatial Understanding and Situated Interaction in Indoor Environments|A major challenge in robotics and artificial intelligence lies in creating robots that are to cooperate with people in human-populated environments, e.g. for domestic assistance or elderly care. Such robots need skills that allow them to interact with the world and the humans living and working therein. In this paper we investigate the question of spatial understanding of human-made environments. The functionalities of our system comprise perception of the world, natural language, learning, and reasoning. For this purpose we integrate state-of-the-art components from different disciplines in AI, robotics and cognitive systems into a mobile robot system. The work focuses on the description of the principles we used for the integration, including cross-modal integration, ontology-based mediation, and multiple levels of abstraction of perception. Finally, we present experiments with the integrated \"CoSy Explorer\" system and list some of the major lessons that were learned from its design, implementation, and evaluation.|Hendrik Zender,Patric Jensfelt,Ã\u201Cscar MartÃ­nez Mozos,Geert-Jan M. Kruijff,Wolfram Burgard","66068|AAAI|2007|Learning by Reading A Prototype System Performance Baseline and Lessons Learned|A traditional goal of Artificial Intelligence research has been a system that can read unrestricted natural language texts on a given topic, build a model of that topic and reason over the model. Natural Language Processing advances in syntax and semantics have made it possible to extract a limited form of meaning from sentences. Knowledge Representation research has shown that it is possible to model and reason over topics in interesting areas of human knowledge. It is useful for these two communities to reunite periodically to see where we stand with respect to the common goal of text understanding. In this paper, we describe a coordinated effort among researchers from the Natural Language and Knowledge Representation and Reasoning communities. We routed the output of existing NL software into existing KR software to extract knowledge from texts for integration with engineered knowledge bases. We tested the system on a suite of roughly  small English texts about the form and function of the human heart, as well as a handful of \"confuser\" texts from other domains. We then manually evaluated the knowledge extracted from novel texts. Our conclusion is that the technology from these fields is mature enough to start producing unified machine reading systems. The results of our exercise provide a performance baseline for systems attempting to acquire models from text.|Ken Barker,Bhalchandra Agashe,Shaw Yi Chaw,James Fan,Noah S. Friedland,Michael Glass,Jerry R. Hobbs,Eduard H. Hovy,David J. Israel,Doo Soon Kim,Rutu Mulkar-Mehta,Sourabh Patwardhan,Bruce W. Porter,Dan Tecuci,Peter Z. Yeh"],["58146|GECCO|2007|A synthesis of optimal stopping time in compact genetic algorithm based on real options approach|This paper introduces the real options approach, which is an evaluation tool for investment under uncertainty, to analyze optimal stopping time in genetic algorithms. This paper focuses on the simple model of EDAs named the compact genetic algorithm. This algorithm employs the probability vector as a model that scales well with the problem size. We analyze optimal stopping time of trap problems and propose an optimal stopping criterion as a decision contour. The proposed criterion also provides a stopping boundary, where termination is optimal on one side and continuation is on the other. This region suggests when it is worth continuing the algorithm and helps save computational effort by stopping early. Moreover, when the reset method is applied, the algorithm can reach a higher solution quality. The proposed technique can also be applied to analyze other problems.|Sunisa Rimcharoen,Daricha Sutivong,Prabhas Chongstitvatana","66171|AAAI|2007|Authorial Idioms for Target Distributions in TTD-MDPs|In designing Markov Decision Processes (MDP), one must define the world, its dynamics, a set of actions, and a reward function. MDPs are often applied in situations where there is a clear choice of reward functions and in these cases significant care must be taken to construct a reward function that induces the desired behavior. In this paper, we consider an analogous design problem crafting a target distribution in Targeted Trajectory Distribution MDPs (TTD-MDPs). TTD-MDPs produce probabilistic policies that minimize divergence from a target distribution of trajectories from an underlying MDP. They are an extension of MDPs that provide variety of experience during repeated execution. Here, we present a brief overview of TTD-MDPs with approaches for constructing target distributions. Then we present a novel authorial idiom for creating target distributions using prototype trajectories. We evaluate these approaches on a drama manager for an interactive game.|David L. Roberts,Sooraj Bhat,Kenneth St. Clair,Charles Lee Isbell Jr.","66270|AAAI|2007|Dominance and Equivalence for Sensor-Based Agents|This paper describes recent results from the robotics community that develop a theory, similar in spirit to the theory of computation, for analyzing sensor-based agent systems. The central element to this work is a notion of dominance of one such system over another. This relation is formally based on the agents' progression through a derived information space, but may informally be understood as describing one agent's ability to \"simulate\" another. We present some basic properties of this dominance relation and demonstrate its usefulness by applying it to a basic problem in robotics. We argue that this work is of interest to a broad audience of artificial intelligence researchers for two main reasons. First, it calls attention to the possibility of studying belief spaces in way that generalizes both probabilistic and nondeterministic uncertainty models. Second, it provides a means for evaluating the information that an agent is able to acquire (via its sensors and via conformant actions), independent of any optimality criterion and of the task to be completed.|Jason M. O'Kane,Steven M. LaValle","57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","66250|AAAI|2007|MasDISPO A Multiagent Decision Support System for Steel Production and Control|In the majority of cases, steel production constitutes the inception of the Supply Chains they are involved Just as in automotive clusters or aerospace. Steel manufactunng companies are affected strongest by bull whip effects or other unpredictable influences along the production chain to the customers. Therefore, flexible planning and realisation as well as fast reorganisation after interferences are indispensable requirements for a competitive position on the market. In thiS paper, MasDISPO, an agent-based decision support system for production and control inside the steel works of Saarstahl AG, a globally respected steel manufacturer, is presented. It is based on a distributed online planning and online scheduling algorithm to calculate solutions supporting production and control inside the melting shop. It monitors the execution of their chosen solutions and responds to unpredicted changes during production by dynamically adapting the schedules. This paper gives an overview of the system, the approach for solving the complex problem of steel production and control, the development process, the main experiences as well as lessons learned.|Sven Jacobi,Esteban LeÃ³n-Soto,CristiÃ¡n Madrigal-Mora,Klaus Fischer","66205|AAAI|2007|Stochastic Optimization for Collision Selection in High Energy Physics|Artificial intelligence has begun to play a critical role in basic science research. In high energy physics, AI methods can aid precision measurements that elucidate the underlying structure of matter, such as measurements of the mass of the top quark. Top quarks can be produced only in collisions at high energy particle accelerators. Most collisions, however, do not produce top quarks and making precise measurements requires culling these collisions into a sample that is rich in collisions producing top quarks (signal) and spare in collisions producing other particles (background). Collision selection is typically performed with heuristics or supervised learning methods. However, such approaches are suboptimal because they assume that the selector with the highest classification accuracy will yield a mass measurement with the smallest statistical uncertainty. In practice, however, the mass measurement is more sensitive to some backgrounds than others. This paper presents a new approach that uses stochastic optimization techniques to directly search for selectors that minimize statistical uncertainty in the top quark mass measurement. Empirical results confirm that stochastically optimized selectors have much smaller uncertainty. This new approach contributes substantially to our knowledge of the top quark's mass, as the new selectors are currently in use selecting real collisions.|Shimon Whiteson,Daniel Whiteson","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|RÃ©gis Sabbadin,JÃ©rÃ´me Lang,Nasolo Ravoanjanahry","66010|AAAI|2007|A Unification of Extensive-Form Games and Markov Decision Processes|We describe a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization is that it places arbitrary convex optimization problems at decision nodes, in place of the finite action sets typically considered. The possibly-infinite action sets mean we must \"forget\" the exact action taken (feasible solution to the optimization problem), remembering instead only some statistic sufficient for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games.|H. Brendan McMahan,Geoffrey J. Gordon","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66016|AAAI|2007|Action-Space Partitioning for Planning|For autonomous artificial decision-makers to solve realistic tasks, they need to deal with searching through large state and action spaces under time pressure. We study the problem of planning in such domains and show how structured representations of the environment's dynamics can help partition the action space into a set of equivalence classes at run time. The partitioned action space is then used to produce a reduced set of actions. This technique speeds up search and can yield significant gains in planning efficiency.|Natalia Hernandez-Gardiol,Leslie Pack Kaelbling"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-SuÃ¡rez,Manuel Valenzuela-RendÃ³n,Hugo Terashima-MarÃ­n,Eduardo Uresti-Charre","58170|GECCO|2007|Parameter cross-validation and early-stopping in univariate marginal distribution algorithm|In this paper, a cross-validation and early-stopping algorithm is devised for parameter updating in the Univariate Marginal Distribution Algorithm (UMDA) to reduce overftting. Our hypothesis is that the well-known problem of diversity loss in UMDA is a consequence of overfitting during the parameter estimation step at each generation. It is tested by experiments on two different optimization problems.|Hao Wu,Jonathan L. Shapiro","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","58243|GECCO|2007|Analyzing probabilistic models in hierarchical BOA on traps and spin glasses|The hierarchical Bayesian optimization algorithm (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling probabilistic models of promising solutions. This paper analyzes probabilistic models in hBOA on two common test problems concatenated traps and D Ising spin glasses with periodic boundary conditions. We argue that although Bayesian networks with local structures can encode complex probability distributions, analyzing these models in hBOA is relatively straightforward and the results of such analyses may provide practitioners with useful information about their problems. The results show that the probabilistic models in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in subsequent iterations of BOA, and creating adequate probabilistic models by hand is not straightforward even with complete knowledge of the optimization problem.|Mark Hauschild,Martin Pelikan,ClÃ¡udio F. Lima,Kumara Sastry","58240|GECCO|2007|Symbiotic tabu search|Recombination in the Genetic Algorithm (GA) is supposed to extract the component characteristics from two parents and reassemble them in different combinations hopefully producing an offspring that has the good characteristics of both parents. Symbiotic Combination is formerly introduced as an alternative for sexual recombination operator to overcome the need of explicit design of recombination operators in GA all. This paper presents an optimization algorithm based on using this operator in Tabu Search. The algorithm is benchmarked on two problem sets and is compared with standard genetic algorithm and symbiotic evolutionary adaptation model, showing success rates higher than both cited algorithms.|Ramin Halavati,Saeed Bagheri Shouraki,Bahareh Jafari Jashmi,Mojdeh Jalali Heravi","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith","58016|GECCO|2007|An artificial immune system with partially specified antibodies|Artificial Immune System algorithms use antibodies which fully specify the solution of an optimization, learning, or pattern recognition problem. By being restricted to fully specified antibodies, an AIS algorithm can not make use of schemata or classes of partial solutions. This paper presents a symbiotic artificial immune system (SymbAIS) algorithm which is an extension of CLONALG algorithm. It uses partially specified antibodies and gradually builds up building blocks of suitable sub-antibodies. The algorithm is compared with CLONALG on multimodal function optimization and combinatorial optimization problems and it is shown that it can solve problems that CLONALG is unable to solve.|Ramin Halavati,Saeed Bagheri Shouraki,Mojdeh Jalali Heravi,Bahareh Jafari Jashmi"],["66195|AAAI|2007|The Virtual Solar-Terrestrial Observatory A Deployed Semantic Web Application Case Study for Scientific Research|The Virtual Solar-Terrestrial Observatory is a production semantic web data framework providing access to observational datasets from fields spanning upper atmospheric terrestrial physics to solar physics. The observatory allows virtual access to a highly distributed and heterogeneous set of data that appears as if all resources are organized, stored and retrievedused in a common way. The end-user community comprises scientists, students, data providers numbering over  out of an estimated community of . We present details on the case study, our technological approach including the semantic web languages, tools and infrastructure deployed, benefits of AI technology to the application, and our present evaluation after the initial nine months of use.|Deborah L. McGuinness,Peter Fox,Luca Cinquini,Patrick West,Jose Garcia,James L. Benedict,Don Middleton","57927|GECCO|2007|Improving the human readability of features constructed by genetic programming|The use of machine learning techniques to automatically analyse data for information is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to pre-process data before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract more information about the domain.|Matthew Smith,Larry Bull","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy LÃ©cuÃ©,Alexandre Delteil","66166|AAAI|2007|Learning Large Scale Common Sense Models of Everyday Life|Recent work has shown promise in using large, publicly available, hand-contributed commonsense databases as joint models that can be used to infer human state from day-to-day sensor data. The parameters of these models are mined from the web. We show in this paper that learning these parameters using sensor data (with the mined parameters as priors) can improve performance of the models significantly. The primary challenge in learning is scale. Since the model comprises roughly , irregularly connected nodes in each time slice, it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice. We show how to solve the resulting semi-supervised learning problem by combining a variety of conventional approximation techniques and a novel technique for simplifying the model called context-based pruning. We show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various techniques contribute to the performance.|William Pentney,Matthai Philipose,Jeff A. Bilmes,Henry A. Kautz","57947|GECCO|2007|A NSGA-II web-enabled parallel optimization framework for NLP and MINLP|Engineering design increasingly uses computer simulation models coupled with optimization algorithms to find the best design that meets the customer constraints within a time constrained deadline. The continued application of Moore's law combined with linear speedups of coarse grained parallelization will allow more designs to be evaluated in shorter periods of time. This paper presents a scalable, standards based framework that uses web services and grid services with a multiple objective genetic algorithm to solve continuous, mixed integer, single objective or multiple objective nonlinear, constrained design problems. Test data is provided to validate a linear speedup based on the number of processors and to show the robustness of the genetic algorithm on a set of  design problems.|David J. Powell,Joel K. Hollingsworth","66013|AAAI|2007|Towards Large Scale Argumentation Support on the Semantic Web|This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW) a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Users can also attack or support parts of existing arguments, use existing parts of an argument in the creation of new arguments, or create new argumentation schemes. As such, this initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.|Iyad Rahwan,Fouad Zablith,Chris Reed","66002|AAAI|2007|Towards Efficient Dominant Relationship Exploration of the Product Items on the Web|In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.|Zhenglu Yang,Lin Li,Botao Wang,Masaru Kitsuregawa","66041|AAAI|2007|A Deployed Semantically-Enabled Interdisciplinary Virtual Observatory|We have used semantic technologies to design, implement, and deploy an interdisciplinary virtual observatory. The Virtual Solar-Terrestrial Observatory is a production data framework providing access to observational datasets. It is in use by a community of scientists, students, and data providers interested in the middle and upper Earth's atmosphere, and the Sun. The data sets span upper atmospheric terrestrial physics to solar physics. The observatory allows virtual access to a highly distributed and heterogeneous set of data that appears as if all resources are organized, stored and accessible from a local machine. The system has been operational since the summer of  and has shown registered data access by over % of the active community (last count over  of the estimated  person active research community). This demonstration will highlight how semantic technologies are being used to support data integration and more efficient data access in a multi-disciplinary setting. A full paper on this work is being published in the IAAI  'deployed' paper track.|Deborah L. McGuinness,Peter Fox,Luca Cinquini,Patrick West,Jose Garcia,James L. Benedict,Don Middleton","66047|AAAI|2007|Freebase A Shared Database of Structured General Human Knowledge|Freebase is a practical, scalable, graph-shaped database of structured general human knowledge, inspired by Semantic Web research and collaborative data communities such as the Wikipedia. Freebase allows public read and write access through an HTTP-based graph-query API for research, the creation and maintenance of structured data, and application building. Access is free and all data in Freebase has a very open (e.g. Creative Commons, GFDL) license.|Kurt D. Bollacker,Robert P. Cook,Patrick Tufts"],["58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","57978|GECCO|2007|Controlling overfitting with multi-objective support vector machines|Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs.|Ingo Mierswa","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","66139|AAAI|2007|Clustering with Local and Global Regularization|Clustering is an old research topic in data mining and machine learning communities. Most of the traditional clustering methods can be categorized local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the dataset is proposed. The method, Clustering with Local and Global Consistency (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We will show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently by some iterative methods. Finally the experimental results on several datasets are presented to show the effectiveness of our method.|Fei Wang,Changshui Zhang,Tao Li","66151|AAAI|2007|Machine Learning for Automatic Mapping of Planetary Surfaces|We describe an application of machine learning to the problem of geomorphic mapping of planetary surfaces. Mapping landforms on planetary surfaces is an important task and the first step to deepen our understanding of many geologic processes. Until now such maps have heen manually drawn by a domain expert. We describe a framework to automate the mapping process by meam of segmentation and classification of landscape datasets. We propose and implement a number of extensions to the existing methodology with particular emphasis on the incorporation of machine learning techniques. These extensions result in a robust and practical mapping system that we apply on six sites on Mars. Support Vector Machines show the best mapping results with an accuracy rate of  %. The resultant maps reflect the geomorphology of the sites and have appearance reminiscent of traditional, manually drawn maps. The system is capable of mapping numerous sites using a limited training set. Immediate and eventual applications of this automated mapping system are discussed in the context of planetary science and other domains.|Tomasz F. Stepinski,Soumya Ghosh,Ricardo Vilalta","58139|GECCO|2007|Ensemble learning for free with evolutionary algorithms|Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.|Christian GagnÃ©,MichÃ¨le Sebag,Marc Schoenauer,Marco Tomassini","66197|AAAI|2007|Refining Rules Incorporated into Knowledge-Based Support Vector Learners Via Successive Linear Programming|Knowledge-based classification and regression methods are especially powerful forms of learning. They allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm, combining that knowledge with data to produce accurate models. A limitation of the use of prior knowledge occurs when the provided knowledge is incorrect. Such knowledge likely still contains useful information, but knowledge-based learners might not be able to fully exploit such information. In fact, incorrect knowledge can lead to poorer models than result from knowledge-free learners. We present a support-vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge, but also suggests changes to the provided knowledge. Our approach is built on the knowledge-based classification and regression methods presented by Fung, Mangasarian, & Shavlik ( ) and by Mangasarian, Shavlik, & Wild (). Experiments on artificial data sets with known properties, as well as on a real-world data set, demonstrate that our method learns more accurate models while also adjusting the provided rules in intuitive ways. Our new algorithm provides an appealing extension to knowledge-based, support-vector learning that is not only able to combine knowledge from rules with data, but is also able to use the data to modify and change those rules to better fit the data.|Richard Maclin,Edward W. Wild,Jude W. Shavlik,Lisa Torrey,Trevor Walker","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","66093|AAAI|2007|Evolutionary and Lifetime Learning in Varying NK Fitness Landscape Changing Environments An Analysis of Both Fitness and Diversity|This paper examines the effects of lifetime learning on populations evolving genetically in a series of changing environmets. The analysis of both fitness and diversity of the populations provides an insight into the improved performance provided by lifetime learning. The NK fitness landscape model is employed as the problem task, which has the advantage of being able to generate a variety of fitness landscapes of varying difficulty. Experiments observe the response of populations in an environment where problem difficulty increases and decreases with varying frequency. Results show that lifetime learning is capable of overall higher fitness levels and, in addition, that lifetime learning stimulates the diversity of the population. This increased diversity allows lifetime learning a greater level of recovery and stability than evolutionary learning alone.|Dara Curran,Colm O'Riordan,Humphrey Sorensen","58029|GECCO|2007|Hybrid coevolutionary algorithms vs SVM algorithms|As a learning method support vector machine is regarded as one of the best classifiers with a strong mathematical foundation. On the other hand, evolutionary computational technique is characterized as a soft computing learning method with its roots in the theory of evolution. During the past decade, SVM has been commonly used as a classifier for various applications. The evolutionary computation has also attracted a lot of attention in pattern recognition and has shown significant performance improvement on a variety of applications. However, there has been no comparison of the two methods. In this paper, first we propose an improvement of a coevolutionary computational classification algorithm, called Improved Coevolutionary Feature Synthesized EM (I-CFS-EM) algorithm. It is a hybrid of coevolutionary genetic programming and EM algorithm applied on partially labeled data. It requires less labeled data and it makes the test in a lower dimension, which speeds up the testing. Then, we provide a comprehensive comparison between SVM with different kernel functions and I-CFS-EM on several real datasets. This comparison shows that I-CFS-EM outperforms SVM in the sense of both the classification performance and the computational efficiency in the testing phase. We also give an intensive analysis of the pros and cons of both approaches.|Rui Li,Bir Bhanu,Krzysztof Krawiec"],["65965|AAAI|2007|Manifold Denoising as Preprocessing for Finding Natural Representations of Data|A natural representation of data is given by the parameters which generated the data. If the space of parameters is continuous, then we can regard it as a manifold. In practice, we usually do not know this manifold but we just have some representation of the data, often in a very high-dimensional feature space. Since the number of internal parameters does not change with the representation, the data will effectively lie on a low-dimensional submanifold in feature space. However, the data is usually corrupted by noise, which particularly in high-dimensional feature spaces makes it almost impossible to find the manifold structure. This paper reviews a method called Manifold Denoising, which projects the data onto the submanifold using a diffusion process on a graph generated by the data. We will demonstrate that the method is capable of dealing with non-trival high-dimensional noise. Moreover, we will show that using the denoising method as a preprocessing step, one can significantly improve the results of a semi-supervised learning algorithm.|Matthias Hein,Markus Maier","57938|GECCO|2007|Acquiring evolvability through adaptive representations|Adaptive representations allow evolution to explore the space of phenotypes by choosing the most suitable set of genotypic parameters. Although such an approach is believed to be efficient on complex problems, few empirical studieshave been conducted in such domains. In this paper, three neural network representations, a direct encoding, a complexifying encoding, and an implicit encoding capable of adapting the genotype-phenotype mapping are compared on Nothello, a complex game playing domain from the AAAI General Game Playing Competition. Implicit encoding makes the search more efficient and uses several times fewer parameters. Random mutation leads to highly structured phenotypic variation that is acquired during the course of evolution rather than built into the representation itself. Thus, adaptive representations learn to become evolvable, and furthermore do so in a way that makes search efficient on difficult coevolutionary problems.|Joseph Reisinger,Risto Miikkulainen","58143|GECCO|2007|Parsimonious regularization using genetic algorithms applied to the analysis of analytical ultracentrifugation experiments|Frequently in the physical sciences experimental data are analyzed to determine model parameters using techniques known as parameter estimation. Eliminating the effects of noise from experimental data often involves Tikhonov or Maximum-Entropy regularization. These methods introduce a bias which smoothes the solution. In the problems considered here, the exact answer is sharp, containing a sparse set of parameters. Therefore, it is desirable to find the simplest set of model parameters for the data with an equivalent goodness-of-fit. This paper explains how to bias the solution towards a parsimonious model with a careful application of Genetic Algorithms. A method of representation, initialization and mutation is introduced to efficiently find this model. The results are compared with results from two other methods on simulated data with known content. Our method is shown to be the only one to achieve the desired results. Analysis of Analytical Ultracentrifugation sedimentation velocity experimental data is the primary example application.|Emre H. Brookes,Borries Demeler","66208|AAAI|2007|Approximate Solutions of Interactive Dynamic Influence Diagrams Using Model Clustering|Interactive dynamic influence diagrams (I-DIDs) offer a transparent and semantically clear representation for the sequential decision-making problem over multiple time steps in the presence of other interacting agents. Solving I-DlDs exactly involves knowing the solutions of possible models of the other agents, which increase exponentially with the number of time steps. We present a method of solving I-DlDs approximately by limiting the number of other agents' candidate models at each time step to a constant. We do this by clustering the models and selecting a representative set from the clusters. We discuss the error bound of the approximation technique and demonstrate its empirical performance.|Yifeng Zeng,Prashant Doshi,Qiongyu Chen","57914|GECCO|2007|Interactive evolution of XUL user interfaces|We attack the problem of user fatigue by using an interactive genetic algorithm to evolve user interfaces in the XUL interface definition language. The interactive genetic algorithm combines a set of computable user interface design metrics with subjective user input to guide the evolution of interfaces. Our goal is to provide user interface designers with a tool that can be used to explore innovation and creativity in the design space of user interfaces and make it easier for end-users to further customize their user interface without programming knowledge. User interface specifications are encoded as individuals in an interactive genetic algorithm's population and their fitness is computed from a weighted combination of user interface design guidelines and user input. This paper shows that we can reduce human fatigue in interactive genetic algorithms (the number of choices needing to be made by the designer), by ) only asking the user to pick two user interfaces from among ten shown on the display and ) by asking the user to make the choice once every t generations.|Juan C. Quiroz,Sushil J. Louis,Sergiu M. Dascalu","57963|GECCO|2007|Graph structured program evolution|In recent years a lot of Automatic Programming techniques have developed. A typical example of Automatic Programming is Genetic Programming (GP), and various extensions and representations for GP have been proposed so far. However, it seems that more improvements are necessary to obtain complex programs automatically. In this paper we proposed a new method called Graph Structured Program Evolution (GRAPE). The representation of GRAPE is graph structure, therefore it can represent complex programs (e.g. branches and loops) using its graph structure. Each program is constructed as an arbitrary directed graph of nodes and data set. The GRAPE program handles multiple data types using the data set for each type, and the genotype of GRAPE is the form of a linear string of integers. We apply GRAPE to four test problems, factorial, Fibonacci sequence, exponentiation and reversing a list, and demonstrate that the optimum solution in each problem is obtained by the GRAPE system.|Shinichi Shirakawa,Shintaro Ogino,Tomoharu Nagao","66083|AAAI|2007|Interactive Configuration with Regular String Constraints|In this paper we present a generalization of the problem of interactive configuration. The usual interactive configuration problem is the problem of, given some variables on small finite domains and an increasing set of assignment of values to a subnet of the variables, to compute for each of the unassigned variables which values in its domain that participate in some solution for some assignment of values to the other unassigned variables. In this paper we consider how to extend this scheme to handle infinite regular domains using string variables and constraints that involves regular-expression checks on the string variables. We first show how to do this by using one single DFA. Since this approach is vastly space consuming, we construct a data structure that simulates the large DFA and is much more space efficient. As an example a configuration problem on n string variables with only one solution in which each string variable is assigned a value oflength k the former structure will use (kn) space whereas the latter only need O(kn). We also show how this framework can be combined with the recent BDD techniques to allow both boolean, integer and string variables in the configuration problem.|Esben Rune Hansen,Henrik Reif Andersen","66002|AAAI|2007|Towards Efficient Dominant Relationship Exploration of the Product Items on the Web|In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the \"best\" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this paper can be complementary to traditional search engines.|Zhenglu Yang,Lin Li,Botao Wang,Masaru Kitsuregawa","57886|GECCO|2007|Evolutionary computation-based kernel optimal component analysis for pattern recognition|Kernel methods are mathematical tools that provide higher dimensional representation of given data set in feature space for pattern recognition and data analysis problems. Optimal Component Analysis (OCA)  poses the problem of finding an optimal linear representation. In this paper we present the results of six kernel functions and their respective performance for Evolutionary Computation-based kernel OCA on the Pima Indian Diabetes database. Empirical results show that we outperform existing techniques on this database.|Jason C. Isaacs,Simon Y. Foo,Anke Meyer-BÃ¤se","57979|GECCO|2007|Using evolution strategies for automatic extraction of parameters for stellar population synthesis of galaxy spectra from sdss|In this work we employ Evolution Strategies (ES) to automatically extract a set of physical parameters (ages, metallicities, reddening and contributions) from a sample of galaxy spectra taken from Sloan Digital Sky Survey (SDSS) for stellar populations studies. We pose this parameter extraction as an optimization problem and then solve it using ES. The idea is to reconstruct each galactic spectrum from the sample by means of a linear combination of three different theoretical models of stellar population synthesis. This combination produces a model spectrum that is compared with the original spectrum using a difference function. The goal is to find a model that minimizes this difference, using ES as the algorithm to explore the parameter space.|Juan Carlos Gomez,Olac Fuentes"],["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy LÃ©cuÃ©,Alexandre Delteil","66023|AAAI|2007|Deriving a Large-Scale Taxonomy from Wikipedia|We take the category system in Wikipedia as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexicosyntactic matching. As a result we are able to derive a large scale taxonomy containing a large amount of subsumption, i.e. isa, relations. We evaluate the quality of the created resource by comparing it with ResearchCyc, one of the largest manually annotated ontologies, as well as computing semantic similarity between words in benchmarking datasets.|Simone Paolo Ponzetto,Michael Strube","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","66210|AAAI|2007|Approximating OWL-DL Ontologies|Efficient query answering over ontologies is one of the most useful and important services to support Semantic Web applications. Approximation has been identified as a potential way to reduce the complexity of query answering over OWL DL ontologies. Existing approaches are mainly based on syntactic approximation of ontological axioms and queries. In this paper, we propose to recast the idea of knowledge compilation into approximating OWL DL ontologies with DL-Lite ontologies, against which query answering has only polynomial data complexity. We identify a useful category of queries for which our approach guarantees also completeness. Furthermore, this paper reports on the implementation of our approach in the ONTOSEARCH system and preliminary, but encouraging, benchmark results which compare ONTOSEARCH's response times on a number of queries with those of existing ontology reasoning systems.|Jeff Z. Pan,Edward Thomas","66090|AAAI|2007|Repairing Ontology Mappings|Automatically discovering semantic relations between ontologies is an important task with respect to overcoming semantic heterogeneity on the semantic web. Existing ontology matching systems, however, often produce erroneous mappings. In this paper, we address the problem of errors in mappings by proposing a completely automatic debugging method for ontology mappings. The method uses logical reasoning to discover and repair logical inconsistencies caused by erroneous mappings. We describe the debugging method and report experiments on mappings submitted to the ontology alignment evaluation challenge that show that the proposed method actually improves mappings created by different matching systems without any human intervention.|Christian Meilicke,Heiner Stuckenschmidt,Andrei Tamilin","66004|AAAI|2007|ASKNet Automated Semantic Knowledge Network|The ASKNet system is an attempt to automatically generate large scale semantic knowledge networks from natural language text. State-of-the-art language processing tools, including parsers and semantic analysers, are used to turn input sentences into fragments of semantic network. These network fragments are combined using spreading activation-based algorithms which utilise both lexical and semantic information. The emphasis of the system is on wide-coverage and speed of construction. In this paper we show how a network consisting of over . million nodes and . million edges, more than twice as large as any network currently available, can be created in less than  days. We believe that the methods proposed here will enable the construction of semantic networks on a scale never seen before, and in doing so reduce the knowledge acquisition bottleneck for AI.|Brian Harrington,Stephen Clark","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","58017|GECCO|2007|Procreating V-detectors for nonself recognition an application to anomaly detection in power systems|The artificial immune system approach for self-nonself discrimination and its application to anomaly detection problems in engineering is showing great promise. A seminal contribution in this area is the V-detectors algorithm that can very effectively cover the nonself region of the feature space with a set of detectors. The detector set can be used to detect anomalous inputs. In this paper, a multistage approach to create an effective set of V-detectors is considered. The first stage of the algorithm generates an initial set of V-detectors. In subsequent stage, new detectors are grown from existing ones, by means of a mechanism called procreation. Procreating detectors can more effectively fill hard-to-reach interstices in the nonself region, resulting in better coverage. The effectiveness of the algorithm is first illustrated by applying it to a well-known fractal, the Koch curve. The algorithm is then applied to the problem of detecting anomalous behavior in power distribution systems, and can be of much use for maintenance-related decision-making in electrical utility companies.|Min Gui,Sanjoy Das,Anil Pahwa","58183|GECCO|2007|Keyword extraction using an artificial immune system|This paper presents a model for keyword extraction which combines an artificial immune system with a mathematical background based on information theory. The proposed approach does not need any domain knowledge, neither the use of a stopword list. The output is a set of keywords for each of the categories into the corpus used.|Andres Romero,Fernando NiÃ±o"],["66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|LuÃ­s C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","58191|GECCO|2007|A destructive evolutionary process a pilot implementation|This paper describes the application of evolutionary search to the problem of Flash memory wear-out. The operating parameters of Flash memory are notoriously difficult to determine, as the optimal values vary from batch to batch. These parameters are usually established by an expensive, once off process of manual destructive testing at design time. Testing on individual batches is normally not feasible. We establish the viability of a platform that performs destructive experimentation on hard silicon, using a Genetic Algorithm to automatically discover optimal operating parameter settings. The results demonstrate a minimum average life extension of between % and % over the factory set read write and erase conditions with a maximum life extension exhibited of % for cells within the same device. It was necessary to build specialized hardware to perform the repetitive testing required by the GA, here we describe this hardware and demonstrate how the lessons learned in this pilot study will allow us to proceed with a more complex parallel evaluation platform, which will facilitate a larger problem space, larger population size and diversity of search techniques, facilitating the near no cost life extension of a split-gate Flash memory device.|Joe Sullivan,Conor Ryan","66084|AAAI|2007|Particle Filtering for Dynamic Agent Modelling in Simplified Poker|Agent modelling is a challenging problem in many modern artificial intelligence applications. The agent modelling task is especially difficult when handling stochastic choices, deliberately hidden information, dynamic agents, and the need for fast learning. State estimation techniques, such as Kalman filtering and particle filtering, have addressed many of these challenges, but have received little attention in the agent modelling literature. This paper looks at the use of particle filtering for modelling a dynamic opponent in Kuhn poker, a simplified version of Texas Hold'em poker. We demonstrate effective modelling both against static opponents as well as dynamic opponents, when the dynamics are known. We then examine an application of Rao-Blackwellized particle filtering for doing dual estimation, inferring both the opponent's state as well as a model of its dynamics. Finally, we examine the robustness of the approach to incorrect beliefs about the opponent and compare it to previous work on opponent modelling in Kuhn poker.|Nolan Bard,Michael H. Bowling","58217|GECCO|2007|Learning and anticipation in online dynamic optimization with evolutionary algorithms the stochastic case|The focus of this paper is on how to design evolutionaryalgorithms (EAs) for solving stochastic dynamicoptimization problems online, i.e.as time goes by.For a proper design, the EA must not only be capableof tracking shifting optima, it must also take intoaccount the future consequences of the evolveddecisions or actions. A previousframework describes how to build such EAs in thecase of non-stochastic problems. Most real-worldproblems however are stochastic. In this paper weshow how this framework can be extended to properlytackle stochasticity. We point out how thisnaturally leads to evolving strategiesrather than explicit decisions. We formalizeour approach in a new framework. The newframework and the various sourcesof problem-difficulty at hand are illustratedwith a running example. We also apply ourframework to inventory management problems, an importantreal-world application area in logistics. Our results show,as a proof of principle, the feasibility and benefitsof our novel approach.|Peter A. N. Bosman,Han La PoutrÃ©","58038|GECCO|2007|Towards human-human-computer interaction for biologically-inspired problem-solving in human genetics|Genetic programming (GP) shows great promise for solving complex problems in human genetics. Unfortunately, many of these methods are not accessible to biologists. This is partly due to the complexity of the algorithms that limit their ready adoption and integration into an analysis or modeling paradigm that might otherwise only use univariate statistical methods.allThis is also partly due to the lack of user-friendly, open-source, platform-independent, and freely-available software packages that are designed to be used by biologists for routine analysis. It is our objective to develop, distribute and support a comprehensive software package that puts powerful GP methods for genetic analysis in the hands of geneticists. It is our working hypothesis that the most effective use of such a software package would result from interactive analysis by both a biologist and a computer scientist (i.e. human-human-computer interaction).allWe summarize briefly here the design and implementation of an open-source software package called Symbolic Modeler (SyMod) that seeks to facilitate geneticist-bioinformaticist-computer interactions for problem solving in human genetics. More information can be found at www.epistasis.org or www.symbolicmodeler.org.|Jason H. Moore,Nate Barney,Bill C. White","58082|GECCO|2007|Parallel genetic algorithm assessment of performance in multidimensional scaling|Visualization of multidimensional data by means of Multidimensional Scaling (MDS) is a popular technique of exploratory data analysis widely usable, e.g. in analysis of bio-medical data, behavioral science, marketing research, etc. Implementations of MDS methods include a subroutine for an auxiliary global optimization problem. The latter is difficult because of high dimensionality, absence of overall smoothness, and a large number of local minima. In such a situation application of a genetic algorithm (GA) seems reasonable. A favorable assessment of application of GAs in MDS in previous publications is based on heuristic arguments without estimating quantitatively the precision of GA while applied to the solution of corresponding global optimization problems. Indeed, the estimation of precision is difficult because of complexity to find the actual global minimum not only in routine use but also in unique research experiments. Quantitatively the precision of GA was estimated, at least in the experimental problems of modest dimensionality, using global minima found by means of the developed parallel version of explicit enumeration algorithm. To cope with high complexity of the minimization problem a parallel version of GA is developed, and its efficiency for problem of higher dimensionality is investigated.|Antanas Zilinskas,Julius Zilinskas","58166|GECCO|2007|Hardware acceleration of multi-deme genetic algorithm for the application of DNA codeword searching|A large and reliable DNA codeword library is key to the success of DNA based computing. Searching for sets of reliable DNA codewords is an NP-hard problem, which can take days on state-of-art high performance cluster computers. This work presents a hybrid architecture that consists of a general purpose microprocessor and a hardware accelerator for accelerating the multi-deme genetic algorithm (GA) for the application of DNA codeword searching. The presented architecture provides more than X speed-up compared to a software only implementation. A code extender that uses exhaustive search to produce locally optimum codes in about . hours for the case of length  codes is also described. The experimental results demonstrate that the GA can find % of the words in locally optimum libraries. Finally, we investigate the performance impact of migration, mating and mutation functions in the hardware accelerator. The analysis shows that a modified GA without mating is the most effective for DNA codeword searching.|Qinru Qiu,Daniel J. Burns,Prakash Mukre,Qing Wu","58019|GECCO|2007|A particle swarm algorithm for symbols detection in wideband spatial multiplexing systems|This paper explores the application of the particle swarm algorithm for a NP-hard problem in the area of wireless communications. The specific problem is of detecting symbols in a Multi-Input Multi-Output (MIMO) communications system. This approach is particularly attractive as PSO is well suited for physically realizable, real-time applications, where low complexity and fast convergence is of absolute importance. While an optimal Maximum Likelihood (ML) detection using an exhaustive search method is prohibitively complex, we show that the Swarm Intelligence (SI) optimized MIMO detection algorithm gives near-optimal Bit Error Rate (BER) performance in fewer iterations, thereby reducing the ML computational complexity significantly. The simulation results suggest that the proposed detector gives an acceptable performance complexity trade-off in comparison with ML and VBLAST detector.|Adnan Ahmed Khan,Muhammad Naeem,Syed Ismail Shah","58101|GECCO|2007|An online implementable differential evolution tuned optimal guidance law|This paper proposes a novel application of differential evolution to solve a difficult dynamic optimisation or optimal control problem. The miss distance in a missile-target engagement is minimised using differential evolution. The difficulty of solving it by existing conventional techniques in optimal control theory is caused by the nonlinearity of the dynamic constraint equation, inequality constraint on the control input and inequality constraint on another parameter that enters problem indirectly. The optimal control problem of finding the minimum miss distance has an analytical solution subject to several simplifying assumptions. In the approach proposed in this paper, the initial population is generated around the seed value given by this analytical solution. Thereafter, the algorithm progresses to an acceptable final solution within a few generations, satisfying the constraints at every iteration. Since this solution or the control input has to be obtained in real time to be of any use in practice, the feasibility of online implementation is also illustrated.|Raghunathan Thangavelu,S. Pradeep","66003|AAAI|2007|A Distributed Constraint Optimization Solution to the PP Video Streaming Problem|The future success of application layer video multicast depends on the availability of video stream distribution methods that can scale in the number of stream senders and receivers. Previous work on the problem of application layer video streaming has not effectively addressed scalability in the number of receivers and senders. Therefore, new solutions that are amenable to analysis and can achieve scalable PP video streaming are needed. In this work we propose the use of automated negotiation algorithms to construct video streaming trees at the application layer. We show that automated negotiation can effectively solve the problem of distributing a video stream to a large number of receivers.|Theodore Elhourani,Nathan Denny,Michael M. Marefat"],["58105|GECCO|2007|Modeling selection pressure in XCS for proportionate and tournament selection|In this paper, we derive models of the selection pressure in XCS for proportionate (roulette wheel) selection and tournament selection. We show that these models can explain the empirical results that have been previously presented in the literature. We validate the models on simple problems showing that, (i) when the model assumptions hold, the theory perfectly matches the empirical evidence (ii) when the model assumptions do not hold, the theory can still provide qualitative explanations of the experimental results.|Albert Orriols-Puig,Kumara Sastry,Pier Luca Lanzi,David E. Goldberg,Ester BernadÃ³-Mansilla","57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","66267|AAAI|2007|Responding to Student Affect and Efficacy through Empathetic Companion Agents in Interactive Learning Environments|Because many students experience frustration during learning, it is important to develop affective strategies to support students' coping with frustration in interactive learning environments. First, we must devise affect recognition models to detect student affect. Second, we need to determine when to intervene these conditions are likely to be different for each student. To determine how much frustration a student can persist through, we should utilize models of student self-efficacy to predict a student's frustration threshold. Third, we should devise techniques for responding empathetically before the student reaches her threshold of frustration. We propose an approach to support students' coping with frustration in intelligent tutoring systems that utilizes induced models of affect, self-efficacy and empathetic behavior to effectively reason about precisely when and how to intervene in frustration-ridden learning situations.|Scott W. McQuiggan","58011|GECCO|2007|Introducing fault tolerance to XCS|In this paper, we introduce fault tolerance to XCS and propose a new XCS framework called XCS with Fault Tolerance (XCSFT). As an important branch of learning classifier systems, XCS has been proven capable of evolving maximally accurate, maximally general problem solutions. However, in practice, it oftentimes generates a lot of rules, which lower the readability of the evolved classification model, and thus, people may not be able to get the desired knowledge or useful information out of the model. Inspired by the fault tolerance mechanism proposed in field of data mining, we devise a new XCS framework by integrating the concept and mechanism of fault tolerance into XCS in order to reduce the number of classification rules and therefore to improve the readability of the generated prediction model. A series of $N$-multiplexer experiments, including -bit, -bit, -bit, and -bit multiplexers, are conducted to examine whether XCSFT can accomplish its goal of design. According to the experimental results, XCSFT can offer the same level of prediction accuracy on the test problems as XCS can, while the prediction model evolved by XCSFT consists of significantly fewer classification rules.|Hong-Wei Chen,Ying-Ping Chen","58201|GECCO|2007|Empirical analysis of generalization and learning in XCS with gradient descent|We analyze generalization and learning in XCS with gradient descent. At first, we show that the addition of gradient in XCS may slow down learning because it indirectly decreases the learning rate. However, in contrast to what was suggested elsewhere, gradient descent has no effect on the achieved generalization. We also show that when gradient descent is combined with roulette wheel selection, which is known to be sensitive to small values of the learning rate, the learning speed can slow down dramatically. Previous results reported no difference in the performance of XCS with gradient descent when roulette wheel selection or tournament selection were used. In contrast, we suggest that gradient descent should always be combined with tournament selection, which is not sensitive to the value of the learning rate. When gradient descent is used in combination with tournament selection, the results show that (i) the slowdown in learning is limited and (ii) the generalization capabilities of XCS are not affected.|Pier Luca Lanzi,Martin V. Butz,David E. Goldberg","57981|GECCO|2007|Modeling XCS in class imbalances population size and parameter settings|This paper analyzes the scalability of the population size required in XCS to maintain nichesthat are infrequently activated.Facetwise models have been developed to predict the effect of the imbalance ratio--ratio betweenthe number of instances of the majority class and the minority class that are sampled to XCS--on population initialization, andon the creation and deletion of classifiers of the minority class. While theoretical models show that, ideally, XCS scales linearly with the imbalance ratio, XCS with standard configuration scales exponentially. The causes that are potentially responsible for this deviation from the ideal scalability are also investigated. Specifically, the inheritance procedure of classifiers' parameters, mutation, and subsumption are analyzed, and improvements in XCS's mechanisms are proposed to effectively and efficiently handle imbalanced problems. Once the recommendations are incorporated to XCS, empirical results show that the population size in XCS indeed scales linearly with the imbalance ratio.|Albert Orriols-Puig,David E. Goldberg,Kumara Sastry,Ester BernadÃ³-Mansilla","58001|GECCO|2007|Comparing two models to generate hyper-heuristics for the d-regular bin-packing problem|The idea behind hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. This paper presents two Evolutionary-Computation-based Models to producehyper-heuristics that solve two-dimensional bin-packing problems. The first model uses an XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The second model is based on a GA that uses a variable-length representation, which evolves combinations of condition-action rules producing hyper-heuristics after going through alearning process which includes training and testing phases.Both approaches, when tested and compared using a large set ofbenchmark problems, perform better than the combinations ofsingle heuristics. The testbed is composed of problems used inother similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-MarÃ­n,ClÃ¡udia J. FarÃ­as ZÃ¡rate,Peter Ross,Manuel Valenzuela-RendÃ³n","57917|GECCO|2007|XCSF with computed continuous action|Wilson introduced XCSF as a successor to XCS. The major development of XCSF is the concept of a computed prediction. The efficiency of XCSF in dealing with numerical input and continuous payoff has been demonstrated. However, the possible actions must always be determined in advance. Yet domains such as robot control require numerical actions, so that neither XCS nor XCSF with their discrete actions can yield high performance. This paper studies computed action in XCSF, where the action is continuous with respect to the input state. In comparison with Wilson's architecture for continuous action, our XCSF version, called XCSFCA, proves to be more efficient.|Trung Hau Tran,CÃ©dric Sanza,Yves Duthen,Thuc Dinh Nguyen","57929|GECCO|2007|The second harmonic generation case-study as a gateway for es to quantum control problems|The Second Harmonic Generation (SHG), a process that turns out to be a good test case in the physics lab, can also be considered as a fairly simple theoretical test function for global optimization. Despite its symmetry properties, that will be derived here analytically, it seems to capture the complexity of the Fourier transform between the decision space to the evaluation space, and by that to challenge optimization routines. And indeed, counter-intuitively to some extent, locating its global maximum seems to be not an easy task for Evolutionary Algorithms (EAs). Although this research originates from the real-world applications domain, it aims to introduce a theoretical test case to Evolution Strategies (ES), being a possible theoretical gateway to the real-world physics regime of quantum control problems. After presenting some theoretical results, this paper introduces the study of the scalability of the decision space subject to optimization by specific variants of Derandomized Evolution Strategies. We show that the Evolution Strategy in use requires a quasi-quadratic increase of function evaluations for locating the global maximum as the dimensionality increases.|Ofer M. Shir,Thomas BÃ¤ck"],["58180|GECCO|2007|Alternative techniques to solve hard multi-objective optimization problems|In this paper, we propose the combination of different optimization techniques in order to solve \"hard\" two- and three-objective optimization problems at a relatively low computational cost. First, we use the -constraint method in order to obtain a few points over (or very near of) the true Pareto front, and then we use an approach based on rough sets to spread these solutions, so that the entire Pareto front can be covered. The constrained single-objective optimizer required by the -constraint method, is the cultured differential evolution, which is an efficient approach for approximating the global optimum of a problem with a low number of fitness function evaluations. The proposed approach is validated using several difficult multi-objective test problems, and our results are compared with respect to a multi-objective evolutionary algorithm representative of the state-of-the-art in the area the NSGA-II.|Ricardo Landa Becerra,Carlos A. Coello Coello,Alfredo GarcÃ­a HernÃ¡ndez-DÃ­az,Rafael Caballero,JuliÃ¡n Molina Luque","57987|GECCO|2007|DCMA yet another derandomization in covariance-matrix-adaptation|In a preliminary part of this paper, we analyze the necessity of randomness in evolutionstrategies. We conclude to the necessity of \"continuous\"-randomness, but with a much more limited use of randomness than whatis commonly used in evolution strategies. We then apply these results to CMA-ES, a famous evolution strategy already based on the idea of derandomization, which uses random independent Gaussian mutations. We here replace these random independent Gaussian mutations by a quasi-randomsample. The modification is very easy to do, the modified algorithm is computationally more efficient and its convergence is faster in terms of the number of iterates for a given precision.|Olivier Teytaud,Sylvain Gelly","57985|GECCO|2007|Differential evolution and non-separability using selective pressure to focus search|Recent results show that the Differential Evolution algorithm has significant difficulty on functions that are not linearly separable. On such functions, the algorithm must rely primarily on its differential mutation procedure which, unlike its recombination strategy, is rotationally invariant. We conjecture that this mutation strategy lacks sufficient selective pressure when appointing parent and donor vectors to have satisfactory exploitative power on non-separable functions. We find that imposing pressure in the form of rank-based differential mutation results in a significant improvement of exploitation on rotated benchmarks.|Andrew M. Sutton,Monte Lunacek,L. Darrell Whitley","58024|GECCO|2007|A discrete differential evolution algorithm for the permutation flowshop scheduling problem|In this paper, a novel discrete differential evolution (DDE) algorithm is presented to solve the permutation flowhop scheduling problem with the makespan criterion. The DDE algorithm is simple in nature such that it first mutates a target population to produce the mutant population. Then the target population is recombined with the mutant population in order to generate a trial population. Finally, a selection operator is applied to both target and trial populations to determine who will survive for the next generation based on fitness evaluations. As a mutation operator in the discrete differential evolution algorithm, a destruction and construction procedure is employed to generate the mutant population. We propose a referenced local search, which is embedded in the discrete differential evolution algorithm to further improve the solution quality. Computational results show that the proposed DDE algorithm with the referenced local search is very competitive to the iterated greedy algorithm which is one of the best performing algorithms for the permutation flowshop scheduling problem in the literature.|Quan-Qe Pan,Mehmet Fatih Tasgetiren,Yun-Chia Liang","66263|AAAI|2007|Graph Partitioning Based on Link Distributions|Existing graph partitioning approaches are mainly based on optimizing edge cuts and do not take the distribution of edge weights (link distribution) into consideration. In this paper, we propose a general model to partition graphs based on link distributions. This model formulates graph partitioning under a certain distribution assumption as approximating the graph affinity matrix under the corresponding distortion measure. Under this model, we derive a novel graph partitioning algorithm to approximate a graph affinity matrix under various Bregman divergences, which correspond to a large exponential family of distributions. We also establish the connections between edge cut objectives and the proposed model to provide a unified view to graph partitioning.|Bo Long,Zhongfei (Mark) Zhang,Philip S. Yu","57877|GECCO|2007|An extended mutation concept for the local selection based differential evolution algorithm|A new mutation concept is proposed to generalize local selection based Differential Evolution algorithm to work in general multi-modal problems. Three variations of the proposed method are compared with classic Differential Evolution algorithm using a set of five well known test functions and their variants. The general idea of the new mutation operation is to divide the mutation into two parts the local and global mutation. The global mutation works as a migration operator allowing the algorithm perform global search efficiently, while the local mutation improves the efficiency of local search. The results show that the concept of global mutation is able to generalize the good performance of local selection based Differential Evolution from convex uni-modal functions to general non-convex and multi-modal problems. Among the tested functions, the new method was able to outperform the classic Differential Evolution in all butone. A limited analysis of the effects of control parameters to the performance of the algorithm is also done.|Jani RÃ¶nkkÃ¶nen,Jouni Lampinen","58101|GECCO|2007|An online implementable differential evolution tuned optimal guidance law|This paper proposes a novel application of differential evolution to solve a difficult dynamic optimisation or optimal control problem. The miss distance in a missile-target engagement is minimised using differential evolution. The difficulty of solving it by existing conventional techniques in optimal control theory is caused by the nonlinearity of the dynamic constraint equation, inequality constraint on the control input and inequality constraint on another parameter that enters problem indirectly. The optimal control problem of finding the minimum miss distance has an analytical solution subject to several simplifying assumptions. In the approach proposed in this paper, the initial population is generated around the seed value given by this analytical solution. Thereafter, the algorithm progresses to an acceptable final solution within a few generations, satisfying the constraints at every iteration. Since this solution or the control input has to be obtained in real time to be of any use in practice, the feasibility of online implementation is also illustrated.|Raghunathan Thangavelu,S. Pradeep","57885|GECCO|2007|Empirical analysis of ideal recombination on random decomposable problems|This paper analyzes the behavior of a selectorecombinative genetic algorithm (GA) with an ideal crossover on a class of random additively decomposable problems (rADPs). Specifically, additively decomposable problems of order k whose subsolution fitnesses are sampled from the standard uniform distribution U, are analyzed. The scalability of the selectorecombinative GA is investigated for , rADP instances. The validity of facetwise models in bounding the population size, run duration, and the number of function evaluations required to successfully solve the problems is also verified. Finally, rADP instances that are easiest and most difficult are also investigated.|Kumara Sastry,Martin Pelikan,David E. Goldberg","58156|GECCO|2007|A genetic algorithm with exon shuffling crossover for hard bin packing problems|A novel evolutionary approach for the bin packing problem (BPP) is presented. A simple steady-state genetic algorithm is developed that produces results comparable to other approaches in the literature, without the need for any additional heuristics. The algorithm's design makes maximum use of the principle of natural selection to evolve valid solutions without the explicit need to verify constraint violations. Our algorithm is based upon a biologically inspired group encoding which allows for a modularisation of the search space in which individual sub-solutions may be assigned independent cost values. These values are subsequently utilised in a crossover event modelled on the theory of exon shuffling to produce a single offspring that inherits the most promising segments from its parents. The algorithm is tested on a set of hard benchmark problems and the results indicate that the method has a very high degree of accuracy and reliability compared to other approaches in the literature.|Philipp Rohlfshagen,John A. Bullinaria","57956|GECCO|2007|Linear selection|We investigate a form of selection, linear selection, where parents are not selected independently. One form of dependent selection, semi-linear selection, where the parents are jointly selected with a probability proportional to the average of their selection probabilities, leads the GA to behave half-way between an algorithm driven by crossover and one driven by mutation.|Mario Graff,Riccardo Poli,Alberto Moraglio"],["58096|GECCO|2007|Diverse committees vote for dependable profits|Stock selection for hedge fund portfolios is a challenging problem for Genetic Programming (GP) because the markets (the environment in which the GP solution must survive) are dynamic, unpredictable and unforgiving. How can GP be improved so that solutions are produced that are robust to non-trivial changes in the environment We explore an approach that uses a voting committee of GP individualswith differing phenotypic behaviour.|Wei Yan,Christopher D. Clack","57879|GECCO|2007|Adopting dynamic operators in a genetic algorithm|Genetic Algorithms have been used to solve difficult optimization problems in a number of fields. However, in order to solve a problem with GA, the user has to specify a number of parameters.allThis parameter tuning is a difficult task as different genetic operators are suitable in different application areas. This paper proposes a scheme for genetic algorithms where the genetic operators are changed randomly. The information of gender and age is also incorporated in this approach to maintain population diversity. The experimental result of the proposed algorithm based on a mechanical design problem shows promising result.|Khadiza Tahera,Raafat N. Ibrahim,Paul B. Lochert","58068|GECCO|2007|Solving the artificial ant on the Santa Fe trail problem in   fitness evaluations|In this paper, we provide an algorithm that systematically considers all small trees in the search space of genetic programming. These small trees are used to generate useful subroutines for genetic programming. This algorithm is tested on the Artificial Ant on the Santa Fe Trail problem, a venerable problem for genetic programming systems. When four levels of iteration are used, the algorithm presented here generates better results than any known published result by a factor of .|Steffen Christensen,Franz Oppacher","57968|GECCO|2007|An analysis of constructive crossover and selection pressure in genetic programming|A common problem in genetic programming search algorithms is destructive crossover in which the offspring of good parents generally has worse performance than the parents. Designing constructive crossover operators and integrating some local search techniques into the breeding process have been suggested as solutions. This paper reports on experiments demonstrating that premature convergence may happen more often when using these techniques in combination with standard parent selection. It shows that modifying the selection pressure in the parent selection process is necessary to obtain a significant performance improvement.|Huayang Xie,Mengjie Zhang,Peter Andreae","57997|GECCO|2007|Feature selection and classification in noisy epistatic problems using a hybrid evolutionary approach|A hybrid evolutionary approach is proposed for the combined problem of feature selection (using a genetic algorithm with IntersectionUnion recombination and a fitness function based on a counter-propagation artificial neural network) and subsequent classifier construction (using strongly-typed genetic programming), for use in nonlinear association studies with relatively large potential feature sets and noisy class data. The method was tested using synthetic data with various degrees of injected noise, based on a proposed mental health database.allResults show the algorithm has good potential for feature selection, classification and function characterization.|Drew DeHaas,Jesse Craig,Colin Rickert,Margaret J. Eppstein,Paul Haake,Kirsten Stor","58148|GECCO|2007|A hybrid GA for a supply chain production planning problem|The problem of production and delivery lot-sizing and scheduling of set of items in a two-echelon supply chain over a finite planning horizon is addressed in this paper. A single supplier produces several items on a flexible flow line (FFL) production system and delivers them directly to an assembly facility. Based on the well-known basic period (BP) policy, a new mixed zero-one nonlinear programming model has been developed to minimize average setup, inventory-holding and delivery costs per unit time in the supply chain without any stock-out. The problem is very complex and it could not be solved to optimality especially in real-sized problems. So, an efficient hybrid genetic algorithm (HGA) using the most applied BP approach (i.e. power-of-two policy) has been proposed. The solution quality of the proposed algorithm called PT-HGA has been evaluated and compared with the common cycle approach in some problem instances. Numerical experiments demonstrate the merit of the PT-HGA and indicate that it is a very promising solution method for the problem.|Masoud Jenabi,S. Ali Torabi,S. Afshin Mansouri","58188|GECCO|2007|Evolving robust GP solutions for hedge fund stock selection in emerging markets|Stock selection for hedge fund portfolios is a challenging problem for Genetic Programming (GP) because the markets (the environment in which the GP solution must survive) are dynamic, unpredictable and unforgiving. How can GP be improved so that solutions are produced that are robust to non-trivial changes in the environment We explore an approach that uses subsets of extreme environments during training.|Wei Yan,Christopher D. Clack","58087|GECCO|2007|Unwitting distributed genetic programming via asynchronous JavaScript and XML|The success of a genetic programming system in solving a problem is often a function of the available computational resources. For many problems, the larger the population size and the longer the genetic programming run the more likely the system is to find a solution. In order to increase the probability of success on difficult problems, designers and users of genetic programming systems often desire access to distributed computation, either locally or across the internet, to evaluate fitness cases more quickly. Most systems for internet-scale distributed computation require a user's explicit participation and the installation of client side software. We present a proof-of-concept system for distributed computation of genetic programming via asynchronous javascript and XML (AJAX) techniques which requires no explicit user interaction and no installation of client side software. Clients automatically and possibly even unknowingly participate in a distributed genetic programming system simply by visiting a webpage, thereby allowing for the solution of genetic programming problems without running a single local fitness evaluation. The system can be easily introduced into existing webpages to exploit unused client-side computation for the solution of genetic programming and other problems.|Jon Klein,Lee Spector","58052|GECCO|2007|Hybrid multiobjective optimization genetic algorithms for graph drawing|In this paper we introduce an application of multiobjective optimization with genetic algorithms to the problem of graph drawing and explore the potential contribution of the genetic algorithms for this particular problem.|Dana Vrajitoru","58107|GECCO|2007|Strengths and weaknesses of FSA representation|Genetic Programming and Evolutionary Programming are fields studying the application of artificial evolutionon evolving directly executable programs, in form of trees similar to Lisp expressions (GP-trees), or Finite State Automata (FSA).In this exercise, we study the performance of these methods on several example problems, and draw conclusionson the suitability of the representations with respect to the task structure and properties. We investigate the roleof incremental evolution and its bias in the context of FSA representation. The experiments are performed in simulation andor confirmed on real robots.|Pavel Petrovic"]]},"title":{"entropy":6.340623623889511,"topics":["genetic programming, genetic algorithm, algorithm for, genetic for, for the, for problem, using genetic, for, genetic and, the problem, programming and, using algorithm, the effects, genetic network, the, for network, algorithm the, probabilistic models, mutation operator, evolution strategies","support vector, method for, artificial immune, automatic generation, for classification, for planning, for data, data mining, for detection, artificial systems, reinforcement learning, evolving for, immune systems, learning with, clustering data, data using, performance heuristic, data, classification using, gene using","particle swarm, evolutionary algorithm, swarm optimization, particle optimization, optimization algorithm, and evolutionary, for optimization, search space, analysis and, evolutionary for, and search, estimation distribution, the search, algorithm with, multiobjective optimization, case study, and its, evolutionary computation, swarm for, with evolutionary","the web, neural network, for and, and learning, and logic, description logic, towards the, modal logic, for reasoning, and, logic for, reasoning about, reasoning and, web service, from the, the and, social network, and environments, temporal and, for agents","for problem, for, for using, for design, for parameter, growth for, using, using design, for application, population for, functions for, generalized, ant","for the, the problem, the, the effects, the models, evolution for, the evolution, the and, building block, evolution and, the block, the building, analyzing, artificial, form, diversity","support vector, performance and, performance for, heuristic for, performance heuristic, for machine, performance, heuristic, based, time, database, pattern, analysis, agents, search, construction, clustering, for, algorithm","method for, for detection, artificial immune, for systems, and method, systems using, artificial systems, immune systems, for application, systems, optimal, parallel, based","analysis and, and its, analysis the, for adaptive, analysis for, fitness and, adaptive, fitness, continuous, eda, sampling, convergence, scaling, variance, problem","estimation distribution, estimation algorithm, solutions the, distribution algorithm, algorithm with, for estimation, the with, with, state, stochastic","and learning, neural network, for network, network and, systems and, and tree, classifiers systems, for systems, learning for, robot, systems, multiple, mapping, cognitive, markov, control, architecture, spatial, models, towards","for and, and, for reasoning, reasoning and, reasoning about, temporal and, for representation, development and, novel and, language, information, large-scale, case-based, object, belief, graph, comparison, modeling"],"ranking":[["58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","57904|GECCO|2007|Association rule mining for continuous attributes using genetic network programming|Most association rule mining algorithms make use of discretization algorithms for handling continuous attributes. However, by means of methods of discretization, it is difficult to get highest attribute interdependency and at the same time to get lowest number of intervals. We propose a method using a new graph-based evolutionary algorithm named \"Genetic Network Programming (GNP)\" that can deal with continues values directly, that is, without using any discretization method as a preprocessing step. GNP is one of the evolutionary optimization techniques, which uses directed graph structures as solutions and is composed of three kinds of nodes start node, judgment node and processing node. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the judgment and connection from the current activated node. The features of GNP are described as follows. First, it is possible to reuse nodes because of this, the structure is compact. Second, GNP can find solutions of problems without bloat, which can be sometimes found in Genetic Programming (GP), because of the fixed number of nodes in GNP. Third, nodes that are not used at the current program executions will be used for future evolution. Fourth, GNP is able to cope with partially observable Markov processes. In this paper, we propose a method that can deal with continuous attributes, where attributes in databases correspond to judgment nodes in GNP and each continuous attribute is checked whether its value is greater than a threshold value and the association rules are represented as the connections of the judgment nodes. Threshold ai is firstly determined by calculating the mean i and standard deviation si of all attribute values of Ai. Then, initial threshold ai is selected randomly between the interval i - aisi, i + aisi where ai is a parameter to determine the range of the interval. Once the threshold ai is selected for all attributes, each value of the attribute Ai is checked if it is greater than the threshold ai in the judgment nodes of the proposed method. In addition to that, the threshold ai is also evolved by mutation between i - aisi, i + aisi in every generation in order to obtain as many association rules as possible. The features of the proposed method are as follows compared with other methods ) Extracts rules without identifying frequent itemsets used in Apriori-like mining methods. ) Stores extracted important association rules in a pool all together through generations. ) Measures the significance of associations via the chi-squared test. ) Extracts important rules sufficient enough for user's purpose in a short time. ) The pool is updated in every generation and only important association rules with higher chi-squared value are stored when the identical rules are stored. We have evaluated the proposed method by doing two simulations. Simulation  uses fixed threshold values that is, they remain fixed at initial thresholds during evolution. In simulation ,thresholds are evolved by mutation in every generation. Fig.  shows the number of rules extracted in the pool in simulation . It is found that the number of rules extracted has been increased, which means simulation  outperforms simulation .|Karla Taboada,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","57965|GECCO|2007|Effects of passengers arrival distribution to double-deck elevator group supervisory control systems using genetic network programming|The Elevator Group Supervisory Control Systems (EGSCS) are the control systems that systematically manage three or more elevators in order to efficiently transport the passengers in buildings. Double-deck elevators, where two cages are connected with each other, are expected to be the next generation elevator systems. Meanwhile, Destination Floor Guidance Systems (DFGS) are also expected in Double-Deck Elevator Systems (DDES). With these, the passengers could be served at two consecutive floors and could input their destinations at elevator halls instead of conventional systems without DFGS. Such systems become more complex than the traditional systems and require new control methods Genetic Network Programming (GNP), a graph-based evolutionary method, has been applied to EGSCS and its advantages are shown in some previous papers. GNP can obtain the strategy of a new hall call assignment to the optimal elevator because it performs crossover and mutation operations to judgment nodes and processing nodes. In studies so far, the passenger's arrival has been assumed to take Exponential distribution for many years. In this paper, we have applied Erlang distribution and Binomial distribution in order to study how the passenger's arrival distribution affects EGSCS. We have found that the passenger's arrival distribution has great influence on EGSCS. It has been also clarified that GNP makes good performances under different conditions.|Lu Yu,Jin Zhou,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu,Sandor Markon","58194|GECCO|2007|An effective genetic algorithm for improving wireless sensor network lifetime|An important scheme for extending sensor network lifetime is to divide sensor nodes into disjoint groups such that each group covers all targets and works alternatively. This scheme can be transformated to the Disjoint Set Covers (DSC) problem, which is proved to be NP-complete. Existing heuristic algorithms either get barely satisfactory solutions or take exponential time complexity. In this paper, we present a genetic algorithm to solve the DSC problem. Simulation results show that the proposed genetic algorithm can improve the most constrained-minimum constraining heuristic algorithm (MCMCC) in solution quality by $%$ with only polynomial computation time complexity.|Chih-Chung Lai,Chuan-Kang Ting,Ren-Song Ko","58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","57974|GECCO|2007|Trading rules on stock markets using genetic network programming with sarsa learning|In this paper, the Genetic Network Programming (GNP) for creating trading rules on stocks is described. GNP is an evolutionary computation, which represents its solutions using graph structures and has some useful features inherently. It has been clarified that GNP works well especially in dynamic environments since GNP can create quite compact programs and has an implicit memory function. In this paper, GNP is applied to creating a stock trading model. There are three important points The first important point is to combine GNP with Sarsa Learning which is one of the reinforcement learning algorithms. Evolution-based methods evolve their programs after task execution because they must calculate fitness values, while reinforcement learning can change programs during task execution, therefore the programs can be created efficiently. The second important point is that GNP uses candlestick chart and selects appropriate technical indices to judge the buying and selling timing of stocks. The third important point is that sub-nodes are used in each node to determine appropriate actions (buyingselling) and to select appropriate stock price information depending on the situation. In the simulations, the trading model is trained using the stock prices of  brands in ,  and . Then the generalization ability is tested using the stock prices in . From the simulation results, it is clarified that the trading rules of the proposed method obtain much higher profits than Buy&Hold method and its effectiveness has been confirmed.|Yan Chen,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58127|GECCO|2007|A doubly distributed genetic algorithm for network coding|We present a genetic algorithm which is distributed in two novel ways along genotype and temporal axes. Our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather thana subset of the population to each. This genotype distribution is shown to offer a significant gain in running time. Then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions intopipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. This temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.|Minkyu Kim,Varun Aggarwal,Una-May O'Reilly,Muriel MÃ©dard","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["58060|GECCO|2007|Volatility forecasting using time series data mining and evolutionary computation techniques|Traditional parametric methods have limited success in estimating and forecasting the volatility of financial securities. Recent advance in evolutionary computation has provided additional tools to conduct data mining effectively. The current work applies the genetic programming in a Time Series Data Mining framework to characterize the S&P high frequency data in order to forecast the one step ahead integrated volatility. Results of the experiment have shown to be superior to those derived by the traditional methods.|Irwin Ma,Tony Wong,Thiagas Sankar","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","65951|AAAI|2007|Learning Causal Models for Noisy Biological Data Mining An Application to Ovarian Cancer Detection|Undetected errors in the expression measurements from high-throughput DNA microarrays and protein spectroscopy could seriously affect the diagnostic reliability in disease detection. In addition to a high resilience against such errors, diagnostic models need to be more comprehensible so that a deeper understanding of the causal interactions among biological entities like genes and proteins may be possible. In this paper, we introduce a robust knowledge discovery approach that addresses these challenges. First, the causal interactions among the genes and proteins in the noisy expression data are discovered automatically through Bayesian network learning. Then, the diagnosis of a disease based on the network is performed using a novel error-handling procedure, which automatically identifies the noisy measurements and accounts for their uncertainties during diagnosis. An application to the problem of ovarian cancer detection shows that the approach effectively discovers causal interactions among cancer-specific proteins. With the proposed error-handling procedure, the network perfectly distinguishes between the cancer and normal patients.|Ghim-Eng Yap,Ah-Hwee Tan,HweeHwa Pang","58034|GECCO|2007|Clustering gene expression data via mining ensembles of classification rules evolved using moses|A novel approach, model-based clustering, is described foridentifying complex interactions between genes or gene-categories based on static gene expression data. The approach deals with categorical data, which consists of a set of gene expressionprofiles belonging to one category, and a set belonging to anothercategory. An evolutionary algorithm (Meta-Optimizing Semantic Evolutionary Search, or MOSES) is used to learn an ensemble of classification models distinguishing the two categories, based on inputs that are features corresponding to gene expression values. Each feature is associated with a model-based vector, which encodes quantitative information regarding the utilization of the feature across the ensembles of models. Two different ways of constructing these vectors are explored. These model-based vectors are then clustered using a variant of hierarchical clustering called Omniclust. The result is a set of model-based clusters, in which features are gathered together if they are often considered together by classification models -- which may be because they're co-expressed, or may be for subtler reasons involving multi-gene interactions. The method is illustrated by applying it to two datasets regarding human gene expression, one drawn from brain cells and pertinent to the neurogenetics of aging, and the other drawn from blood cells and relating to differentiating between types of lymphoma. We find that, compared to traditional expression-based clustering, the new method often yields clusters that have higher mathematical quality (in the sense of homogeneity and separation) and also yield novel and meaningful insights into the underlying biological processes.|Moshe Looks,Ben Goertzel,LÃºcio de Souza Coelho,Mauricio Mudado,Cassio Pennachin","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","58128|GECCO|2007|Suppression based immune mechanism to find arepresentative training set in data classification tasks|This article proposes a new classifier inspired by a biolog-ical immune systems' characteristic which also belongs tothe class of k-nearest-neighbors algorithms. Its main fea-ture is a suppression mechanism used to reduce the size of the training set maintaining the most significative sampleswithout loosing much capability of generalization.|Grazziela Patrocinio Figueredo,Nelson F. F. Ebecken,Helio J. C. Barbosa","57949|GECCO|2007|StreamGP tracking evolving GP ensembles in distributed data streams using fractal dimension|The paper presents an adaptive GP boosting ensemble method forthe classification of distributed homogeneous streaming data that comes from multiple locations. The approach is able to handle concept drift via change detection by employing a change detection strategy, based on self-similarity of the ensemble behavior, and measured by its fractal dimension. It is efficient since each nodeof the network works with its local streaming data, and communicate only the local model computed with the otherpeer-nodes. Furthermore, once the ensemble has been built, it isused to predict the class membership of new streams of data until concept drift is detected. Only in such a case the algorithm is executed to generate a new set of classifiers to update the current ensemble. Experimental results on a synthetic and reallife data set showed the validity of the approach in maintaining an accurate and up-to-date GP ensemble.|Gianluigi Folino,Clara Pizzuti,Giandomenico Spezzano","57934|GECCO|2007|Evolutionary selection of minimum number of features for classification of gene expression data using genetic algorithms|Selecting the most relevant factors from genetic profiles that can optimally characterize cellular states is of crucial importance in identifying complex disease genes and biomarkers for disease diagnosis and assessing drug efficiency. In this paper, we present an approach using a genetic algorithm for a feature subset selection problem that can be used in selecting the near optimum set of genes for classification of cancer data. In substantial improvement over existing methods, we classified cancer data with high accuracy with less features.|Alper KÃ¼Ã§Ã¼kural,Reyyan Yeniterzi,SÃ¼veyda Yeniterzi,Osman Ugur Sezerman","58183|GECCO|2007|Keyword extraction using an artificial immune system|This paper presents a model for keyword extraction which combines an artificial immune system with a mathematical background based on information theory. The proposed approach does not need any domain knowledge, neither the use of a stopword list. The output is a set of keywords for each of the categories into the corpus used.|Andres Romero,Fernando NiÃ±o","58059|GECCO|2007|Automatic mutation test input data generation via ant colony|Fault-based testing is often advocated to overcome limitations ofother testing approaches however it is also recognized as beingexpensive. On the other hand, evolutionary algorithms have beenproved suitable for reducing the cost of data generation in the contextof coverage based testing. In this paper, we propose a newevolutionary approach based on ant colony optimization for automatictest input data generation in the context of mutation testingto reduce the cost of such a test strategy. In our approach the antcolony optimization algorithm is enhanced by a probability densityestimation technique. We compare our proposal with otherevolutionary algorithms, e.g., Genetic Algorithm. Our preliminaryresults on JAVA testbeds show that our approach performed significantlybetter than other alternatives.|Kamel Ayari,Salah Bouktif,Giuliano Antoniol"],["58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,JÃ¼rgen Branke,Hartmut Schmeck","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58208|GECCO|2007|Two-level of nondominated solutions approach to multiobjective particle swarm optimization|In multiobjective particle swarm optimization (MOPSO) methods, selecting the local best and the global best for each particle of the population has a great impact on the convergence and diversity of solutions, especially when optimizing problems with high number of objectives. This paper presents a two-level of nondominated solutions approach to MOPSO. The ability of the proposed approach to detect the true Pareto optimal solutions and capture the shape of the Pareto front is evaluated through experiments on well-known non-trivial test problems. The diversity of the nondominated solutions obtained is demonstrated through different measures. The proposed approach has been assessed through a comparative study with the reported results in the literature.|M. A. Abido","58161|GECCO|2007|Improving global numerical optimization using a search-space reduction algorithm|We have developed an algorithm for reduction of search-space, called Domain Optimization Algorithm (DOA), applied to global optimization. This approach can efficiently eliminate search-space regions with low probability of containing a global optimum. DOA basically worksusing simple models for search-space regions to identify and eliminate non-promising regions. The proposed approach has shown relevant results for tests using hard benchmark functions.|Vinicius Veloso de Melo,Alexandre C. B. Delbem,Dorival Leao Pinto Junior,Fernando Marques Federson","57977|GECCO|2007|On the roles of redundancy and neutrality in evolutionary optimization an experimental study|An experimental study was performed to explore whether it is neutrality itself or simply the larger neighborhoods associated with neutral representations that influence the results achieved by evolutionary algorithms on NK fitness landscape problems.Markov chains were used to model the behaviour of a stochastic hill-climber on NK fitness landscapes, using two different types of representation a neutral network representation which exhibits neutrality and a redundant representation without neutrality which implements the same neighborhood induced by the corresponding neutral representation.|Marisol B. Correia,Carlos M. Fonseca"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","66144|AAAI|2007|Prime Implicates and Prime Implicants in Modal Logic|The purpose of this paper is to extend the notions of prime implicates and prime implicants to the basic modal logic . We consider a number of different potential definitions of clauses and terms for , which we evaluate with respect to their syntactic, semantic, and complexity-theoretic properties. We then continue our analysis by comparing the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. We provide algorithms and complexity results for the prime implicate generation and recognition tasks for the two most satisfactory definitions.|Meghyn Bienvenu","66121|AAAI|2007|A Logic of Emotions for Intelligent Agents|This paper formalizes a well-known psychological model of emotions in an agent specification language. This is done by introducing a logical language and its semantics that are used to specify an agent model in terms of mental attitudes including emotions. We show that our formalization renders a number of intuitive and plausible properties of emotions. We also show how this formalization can be used to specify the effect of emotions on an agent's decision making process. Ultimately, the emotions in this model function as heuristics as they constrain an agent's model.|Bas R. Steunebrink,Mehdi Dastani,John-Jules Ch. Meyer","66063|AAAI|2007|Reasoning about Bargaining Situations|This paper presents a logical axiomatization of bargaining solutions. A bargaining situation is described in propositional logic and the bargainers' preferences are quantified in terms of the logical structure of the bargaining situation. A solution to the n-person bargaining problems is proposed based on the maxmin rule over the degrees of bargainers' satisfaction. We show that the solution is uniquely characterized by four natural and intuitive axioms as well as three other fundamental assumptions. All the axioms and assumptions are represented in logical statements and most of them have a game-theoretic counterpart. The framework would help us to identify the logical and numerical reasoning behind bargaining processes.|Dongmo Zhang","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas WÃ¶lfl","58063|GECCO|2007|Swarming with logic|A robot swarm is a distributed entity that can sense andperform many actions simultaneously at different spatial locations. But how, with all this sensory information and capacity for action, can the individuals unite for a common purpose, coordinating their actions in space and time Inthis preliminary simulation study, we suggest that a swarmcan be designed as an engineering control system characterised by a problem specific inputoutput (IO) relationship that can be implemented through the simple (socialinsect inspired) indirect transfer of information between individuals. For such a system, the inputs represent sensory information acquired by one or more agents and the outputs are used to trigger actions that agents perform.|Robert L. Stewart,Michael Kirley"],["58239|GECCO|2007|Exploring the behavior of building blocks for multi-objective variation operator design using predator-prey dynamics|In this paper, we utilize a predator-prey model in order to identify characteristics of single-objective variation operators in the multi-objective problem domain. In detail, we analyze exemplarily Gaussian mutation and simplex recombination to find explanations for the observed behaviorswithin this model. Then, both operators are combinedto a new complex one for the multi-objective case in order to aggregate the identified properties. Finally, we show that (a) characteristic properties can still be observed in the combination and (b) the collaboration of those operators is beneficial for solving an exemplary multi-objective problem regarding convergence and diversity.|Christian Grimme,Joachim Lepping,Alexander Papaspyrou","66086|AAAI|2007|Near-optimal Observation Selection using Submodular Functions|AI problems such as autonomous robotic exploration, automatic diagnosis and activity recognition have in common the need for choosing among a set of informative but possibly expensive observations. When monitoring spatial phenomena with sensor networks or mobile robots, for example, we need to decide which locations to observe in order to most effectively decrease the uncertainty, at minimum cost. These problems usually are NP-hard. Many observation selection objectives satisfy submodularity, an intuitive diminishing returns property - adding a sensor to a small deployment helps more than adding it to a large deployment. In this paper, we survey recent advances in systematically exploiting this submodularity property to efficiently achieve near-optimal observation selections, under complex constraints. We illustrate the effectiveness of our approaches on problems of monitoring environmental phenomena and water distribution networks.|Andreas Krause,Carlos Guestrin","58210|GECCO|2007|An evolutionary multiobjective approach to design highly non-linear Boolean functions|The proliferation of all kinds of devices with different security requirements and constraints, and the arms-race nature of the security problem are increasingly demanding the development of tools to help on the automatic design of Boolean functions with security application. Nowadays, the design of strong cryptographic Boolean functions is a multiobjective problem. However, so far evolutionary multiobjective algorithms have been largely overlooked and not much is known about this problem from a multiobjective optimization perspective. In this work we focus on non-linearity related criteria and explore a multiobjective evolutionary approach aiming to find several balanced functions of similar characteristics satisfying multiple criteria. We show that the multiobjective approach is an efficient alternative to single objective optimization approaches presented so far. We also argue that it is a better framework for automatic design of cryptographic Boolean functions.|HernÃ¡n E. Aguirre,Hiroyuki Okazaki,Yasushi Fuwa","57973|GECCO|2007|Defining implicit objective functions for design problems|In many design tasks it is difficult to explicitly define an objective function. This paper uses machine learning to derive an objective in a feature space based on selected examples of previous designs, thus implicitly capturing the features that distinguish that set from others without requiring a predetermined measure of fitness. A genetic algorithm is used to generate new designs, and these are shown to recognisably display the appropriate features. It is demonstrated that the range of relevant features and optimal solutions is easily varied in proportion to the examples selected to define the objective. Methods for improving the function for GA search are discussed.|Sean Hanna","57922|GECCO|2007|Reducing the number of transistors in digital circuits using gate-level evolutionary design|This paper shows that the evolutionary design of digital circuits which is conducted at the gate level is able to produce human-competitive circuits at the transistor level. In addition to standard gates, we utilize unconventional gates (such as the NANDNOR gate and NORNAND gate) that consist of a few transistors but exhibit non-trivial -input logic functions. Novel implementations of adders and majority circuits evolved using these gates contain fewer transistors than the smallest existing implementations of these circuits. Moreover, it was shown that the use of these gates significantly improves the success rate of the search process.|Zbysek Gajda,LukÃ¡s Sekanina","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","58073|GECCO|2007|Using genetic algorithms for naval subsystem damage assessment and design improvements|Some auxiliary systems of next generation naval ships will utilize distributed automatic control. Such distributed control systems will use interconnected sensors, actuators, controllers and networking components to diagnose and reconfigure the auxiliary systems. Testing these systems will be difficult with traditional methods of fault analysis due to the interconnected and automatic nature of these subsystems. We have designed a suite of genetic algorithms to find interesting and hidden damage scenarios in a testbed of a naval subsystem. Given this knowledge, we use a genetic algorithm to improve upon the design of this subsystem.|Christopher McCubbin,David Scheidt,Oliver Bandte,Steven Marshall,Iavor Trifonov","57930|GECCO|2007|Optimal design of ad hoc injection networks by using genetic algorithms|This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.|GrÃ©goire Danoy,Enrique Alba,Pascal Bouvry,Matthias R. Brust","66156|AAAI|2007|Counting CSP Solutions Using Generalized XOR Constraints|We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.|Carla P. Gomes,Willem Jan van Hoeve,Ashish Sabharwal,Bart Selman","66232|AAAI|2007|Using AI for e-Government Automatic Assessment of Immigration Application Forms|This paper describes an e-Government AI project that provides a range of intelligent AI services to support automated assessment of various types of applications submitted to an immigration agency. The \"AI Module\" is integrated into the agency's next generation application form processing system which includes a workflow and document management system. AI services provided include rule-based assessment, workflow processing, schema-based suggestions, data mining, case-based reasoning, and machine learning. The objective is to use AI to provide faster and higher quality service to millions of citizens and visitors in processing their requests. The AI Module streamlines processes and workflows while at the same time ensuring all applications are processed fairly and accurately and that all relevant laws and regulations have been considered. It greatly shortens turnaround time and indirectly helps facilitate economic growth of the city. This is probably the first time any immigration agency in the world is using AI for automatic application assessment in such a large and broad scale.|Andy Hon Wai Chun"],["58000|GECCO|2007|Dynamic populations and length evolution key factors for analyzing fault tolerance on parallel genetic programming|This paper presents an experimental research on the size of individuals when fixed and dynamic size populationsare employed with Genetic Programming (GP). We propose an improvement to the Plague operator (PO), that we have called Random Plague (RPO). Then by further studies based on the RPO results we analyzed the Fault Tolerance onParallel Genetic Programming.|Daniel LombraÃ±a Gonzalez,Francisco FernÃ¡ndez de Vega","58024|GECCO|2007|A discrete differential evolution algorithm for the permutation flowshop scheduling problem|In this paper, a novel discrete differential evolution (DDE) algorithm is presented to solve the permutation flowhop scheduling problem with the makespan criterion. The DDE algorithm is simple in nature such that it first mutates a target population to produce the mutant population. Then the target population is recombined with the mutant population in order to generate a trial population. Finally, a selection operator is applied to both target and trial populations to determine who will survive for the next generation based on fitness evaluations. As a mutation operator in the discrete differential evolution algorithm, a destruction and construction procedure is employed to generate the mutant population. We propose a referenced local search, which is embedded in the discrete differential evolution algorithm to further improve the solution quality. Computational results show that the proposed DDE algorithm with the referenced local search is very competitive to the iterated greedy algorithm which is one of the best performing algorithms for the permutation flowshop scheduling problem in the literature.|Quan-Qe Pan,Mehmet Fatih Tasgetiren,Yun-Chia Liang","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","57963|GECCO|2007|Graph structured program evolution|In recent years a lot of Automatic Programming techniques have developed. A typical example of Automatic Programming is Genetic Programming (GP), and various extensions and representations for GP have been proposed so far. However, it seems that more improvements are necessary to obtain complex programs automatically. In this paper we proposed a new method called Graph Structured Program Evolution (GRAPE). The representation of GRAPE is graph structure, therefore it can represent complex programs (e.g. branches and loops) using its graph structure. Each program is constructed as an arbitrary directed graph of nodes and data set. The GRAPE program handles multiple data types using the data set for each type, and the genotype of GRAPE is the form of a linear string of integers. We apply GRAPE to four test problems, factorial, Fibonacci sequence, exponentiation and reversing a list, and demonstrate that the optimum solution in each problem is obtained by the GRAPE system.|Shinichi Shirakawa,Shintaro Ogino,Tomoharu Nagao","58175|GECCO|2007|Learning building block structure from crossover failure|In the classical binary genetic algorithm, although crossover within a building block (BB) does not always cause a decrease in fitness, any decrease in fitness results from the destruction of some building blocks, in problems where such structures are well defined, such as those considered here. Those crossovers that cause both offspring to be worse, or one to be worse and one unchanged, are here designated as failed crossovers. Counting the failure frequency of single-point crossovers performed at each locus reveals something of the BB structure. Guided by the failure record, GA operators could choose appropriate points for crossover, in order to work moreefficiently and effectively. Experiments on test functions RoyalRoad R and R, Holland's Royal Road Challenge function and H-IFF functions show that such a guided operator improves performance. While many methods exist to discover building blocks, this \"quick-and-dirty\" method can sketch the linkage nearly \"for free\", requiring very little extra computation.|Zhenhua Li,Erik D. Goodman","58187|GECCO|2007|Numerical-node building block analysis of genetic programming with simplification|This paper investigates the effects on building blocks of using online simplification in a GP system. Numerical nodes are tracked through individual runs to observe their behaviour. Results show that simplification disrupts building blocks early on, but also creates new building blocks.|Phillip Lee-Ming Wong,Mengjie Zhang","58110|GECCO|2007|Towards models of user preferences in interactive musical evolution|We describe the \"bottom-up\" construction of a system which aims to build models of human musicalpreferences with strong predictive power. We use Grammatical Evolution to construct models from toydatasets which mimic real-world user-generated data. These models will ultimately substitute for the subjective fitness functions that human users employ during Interactive Evolution of melodies.|Dan Costelloe,Conor Ryan","58003|GECCO|2007|Overcoming hierarchical difficulty by hill-climbing the building block structure|The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are well-suited for hierarchical problems, where efficient solving requires proper problem decomposition and assembly of solution from sub-solution with strong non-linear interdependencies. The paper proposes a hill-climber operating over the building block (BB) space that can efficiently address hierarchical problems. The new Building Block Hill-Climber (BBHC) uses hill-climb search experience to learn the problem structure. The neighborhood structure is adapted whenever new knowledge about the underlaying BB structure is incorporated into the search. This allows the method to climb the hierarchical structure by revealing and solving consecutively the hierarchical levels. It is expected that for fully non-deceptive hierarchical BB structures the BBHC can solve hierarchical problems in linearithmic time. Empirical results confirm that the proposed method scales almost linearly with the problem size thus clearly outperforms population based recombinative methods.|David Iclanzan,Dan Dumitrescu","57991|GECCO|2007|A building-block royal road where crossover is provably essential|One of the most controversial yet enduring hypotheses about what genetic algorithms (GAs) are good for concerns the idea that GAs process building-blocks. More specifically, it has been suggested that crossover in GAs can assemble short low-order schemata of above average fitness (building blocks) to create higher-order higher-fitness schemata. However, there has been considerable difficulty in demonstrating this rigorously and intuitively. Here we provide a simple building-block function that a GA with two-point crossover can solve on average in polynomial time, whereas an asexual population or mutation hill-climber cannot.|Richard A. Watson,Thomas Jansen","58109|GECCO|2007|Division blocks and the open-ended evolution of development form and behavior|We present a new framework for artificial life involving physically simulated, three-dimensional blocks called Division Blocks. Division Blocks can grow and shrink, divide and form joints, exert forces on joints, and exchange resources. They are controlled by recurrent neural networks that evolve, along with the blocks, by natural selection. Division Blocks are simulated in an environment in which energy is approximately conserved, and in which all energy derives ultimately from a simulated sun via photosynthesis. In this paper we describe our implementation of Division Blocks and some of the ways that it can support experiments on the open-ended evolution of development, form, and behavior. We also present preliminary data from simulations, demonstrating the reliable emergence of cooperative resource transactions.|Lee Spector,Jon Klein,Mark Feinstein"],["66258|AAAI|2007|Analyzing the Performance of Pattern Database Heuristics|We introduce a model for predicting the performance of IDA* using pattern database heuristics, as a function of the branching factor of the problem, the solution depth, and the size of the pattern databases. While it is known that the larger the pattern database, the more efficient the search, we provide a quantitative analysis of this relationship. In particular, we show that for a single goal state, the number of nodes expanded by IDA* is a fraction of (logb s + )s of the nodes expanded by a brute-force search, where b is the branching factor, and s is the size of the pattern database. We also show that by taking the maximum of at least two pattern databases, the number of node expansions decreases linearly with s compared to a brute-force search. We compare our theoretical predictions with empirical performance data on Rubik's Cube. Our model is conservative, and overestimates the actual number of node expansions.|Richard E. Korf","58174|GECCO|2007|On quality performance of heuristic and evolutionary algorithms for biobjective minimum spanning trees|In this paper, we consider a biobjective minimum spanning tree problem (MOST) and minimize two objectives - tree cost and diameter - in terms of Pareto-optimality. We assess the quality of obtained MOEA solutions in comparison to well-known diameter-constrained minimum spanning tree (dc-MST) algorithms and further improve MOEA solutions using problem-specific knowledge.|Rajeev Kumar,Pramod Kumar Singh","57876|GECCO|2007|A quantitative analysis of memory requirement and generalization performance for robotic tasks|In autonomous agent systems, memory is an important element to handle agent behaviors appropriately. We present the analysis of memory requirements for robotic tasks including wall following and corridor following. The robotic tasks are simulated with sensor modeling and motor actions in noisy environments. In this paper, control structures are based on finite state machines for memory-based controllers, and we use the evolutionary multiobjective optimization approach with two objectives, behavior performance and memory size. For each task, a quantitative approach to estimate internal states with a different number of sensors is applied and the best controllers are evaluated in several test environments to examine their generalization characteristics and efficiency. Finite state machines with a hierarchy of memory are also compared with feedforward neural networks for the behavior performance.|DaeEun Kim","57952|GECCO|2007|Screening the parameters affecting heuristic performance|This research screens the tuning parameters of a combinatorial optimization heuristic. Specifically, it presents a Design of Experiments (DOE) approach that uses a Fractional Factorial Design to screen the tuning parameters of Ant Colony System (ACS) for the Travelling Sales person problem. Screening is a preliminary step towards building a full Response Surface Model (RSM) . It identifies parametersthat have little influence on performance and can be omittedfrom the RSM design. This reduces the complexity andexpense of the RSM design.  algorithm parameters and  problem characteristics are considered. Open questionson the effect of  parameters on performance are answered.A further parameter, sometimes assumed important, was shown to have no effect on performance. A new problem characteristic that effects performance was identified. A full version of this paper is available .|Enda Ridge,Daniel Kudenko","57888|GECCO|2007|One-test-at-a-time heuristic search for interaction test suites|Algorithms for the construction of software interaction test suites have focussed on the special case of pairwise coverage less is known about efficiently constructing test suites for higher strength coverage. The combinatorial growth of t-tuples associated with higher strength hinders the efficacy of interaction testing. Test suites are inherently large, so testers may not run entire test suites. To address these problems, we combine a simple greedy algorithmallwith heuristic search to construct and dispense one test at a time. Our algorithm attempts to maximize the number of t-tuples covered by the earliest tests so that if a tester only runs a partial test suite, they test as many t-tuples as possible.allHeuristic search is shown to provide effective methods for achieving such coverage.|RenÃ©e C. Bryce,Charles J. Colbourn","57918|GECCO|2007|Analyzing heuristic performance with response surface models prediction optimization and robustness|This research uses a Design of Experiments (DOE) approach to build a predictive model of the performance of a combinatorial optimization heuristic over a range of heuristic tuning parameter settings and problem instance characteristics. The heuristic is Ant Colony System (ACS) for the Travelling Salesperson Problem.  heurstic tuning parameters and  problem characteristics are considered. Response Surface Models (RSM) of the solution quality and solution time predicted ACS performance on both new instances from a publicly available problem generator and new real-world instances from the TSPLIB benchmark library. A numerical optimisation of the RSMs is used to find the tuning parameter settings that yield optimal performance in terms of solution quality and solution time. This paper is the first use of desirability functions, a well-established technique in DOE, to simultaneously optimise these conflicting goals. Finally, overlay plots are used to examine the robustness of the performance of the optimised heuristic across a range of problem instance characteristics. These plots give predictions on the range of problem instances for which a given solutionquality can be expected within a given solution time.|Enda Ridge,Daniel Kudenko","58151|GECCO|2007|Performance analysis of niching algorithms based on derandomized-ES variants|A survey of niching algorithms, based on  variants of derandomized Evolution Strategies (ES), is introduced. This set of niching algorithms, ranging from the very first derandomized approach to self-adaptation of ES to the sophisticated ( +over, ) Covariance Matrix Adaptation (CMA), is applied to multimodal continuous theoretical test functions, of different levels of difficulty and various dimensions, and compared with the MPR performance analysis tool. While characterizing the performance of the different derandomized variants in the context of niching, some conclusions concerning the niching formation process of the different mechanisms are drawn, and the hypothesis of a tradeoff between learning time and niching acceleration is numerically confirmed. Niching with (+)-CMA core mechanism is shown to experimentally outperform all the other variants. Some theoretical arguments supporting the advantage of a plus-strategy for niching are discussed.|Ofer M. Shir,Thomas BÃ¤ck","58082|GECCO|2007|Parallel genetic algorithm assessment of performance in multidimensional scaling|Visualization of multidimensional data by means of Multidimensional Scaling (MDS) is a popular technique of exploratory data analysis widely usable, e.g. in analysis of bio-medical data, behavioral science, marketing research, etc. Implementations of MDS methods include a subroutine for an auxiliary global optimization problem. The latter is difficult because of high dimensionality, absence of overall smoothness, and a large number of local minima. In such a situation application of a genetic algorithm (GA) seems reasonable. A favorable assessment of application of GAs in MDS in previous publications is based on heuristic arguments without estimating quantitatively the precision of GA while applied to the solution of corresponding global optimization problems. Indeed, the estimation of precision is difficult because of complexity to find the actual global minimum not only in routine use but also in unique research experiments. Quantitatively the precision of GA was estimated, at least in the experimental problems of modest dimensionality, using global minima found by means of the developed parallel version of explicit enumeration algorithm. To cope with high complexity of the minimization problem a parallel version of GA is developed, and its efficiency for problem of higher dimensionality is investigated.|Antanas Zilinskas,Julius Zilinskas","57941|GECCO|2007|Inducing a generative expressive performance model using a sequential-covering genetic algorithm|In this paper, we describe an evolutionary approach to inducing a generative model of expressive music performance for Jazz saxophone. We begin with a collection of audio recordings of real Jazz saxophone performances from which we extract a symbolic representation of the musician's expressive performance. We then apply an evolutionary algorithm to the symbolic representation in order to obtain computational models for different aspects of expressive performance. Finally, we use these models to automatically synthesize performances with the expressiveness that characterizes the music generated by a professional saxophonist.|Rafael Ramirez,Amaury Hazan","58169|GECCO|2007|Success effort for performance comparisons|This paper looks at the production of a confidence interval for a statistic we rename success effort.|Matthew Walker,Howard Edwards,Chris H. Messom"],["58145|GECCO|2007|Induction of fuzzy rules with artificial immune systems in acgh based er status breast cancer characterization|Genomic DNA copy number aberrations are frequent in solid tumours although their underlying causes remain obscure. In this paper we show how Artificial Immune System (AIS) paradigm can be successfully employed in the elucidation of biological dynamics of cancerous processes using a novel fuzzy rule induction system for data mining (IFRAIS). Competitive results have been obtained using IFRAIS. A biological interpretation of the results, carried out using Gene Ontology, followed the statistical assessment and put in evidence interesting patterns that are currently under investigation.|Filippo Menolascina,Roberto Teixeira Alves,Stefania Tommasi,Patrizia Chiarappa,Myriam Regattieri Delgado,Giuseppe Mastronardi,Angelo Paradiso,Alex Alves Freitas,Vitoantonio Bevilacqua","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","66173|AAAI|2007|Diagnosis of Discrete-Event Systems Using Satisfiability Algorithms|The diagnosis of a discrete-event system is the problem of computing possible behaviors of the system given observations of the actual behavior, and testing whether the behaviors are normal or faulty. We show how the diagnosis problems can be translated into the propositional satisfiability problem (SAT) and solved by algorithms for SAT. Our experiments demonstrate that current SAT algorithms can solve much bigger diagnosis problems than traditional diagnosis algorithms can.|Alban Grastien,Anbulagan,Jussi Rintanen,Elena Kelareva","58019|GECCO|2007|A particle swarm algorithm for symbols detection in wideband spatial multiplexing systems|This paper explores the application of the particle swarm algorithm for a NP-hard problem in the area of wireless communications. The specific problem is of detecting symbols in a Multi-Input Multi-Output (MIMO) communications system. This approach is particularly attractive as PSO is well suited for physically realizable, real-time applications, where low complexity and fast convergence is of absolute importance. While an optimal Maximum Likelihood (ML) detection using an exhaustive search method is prohibitively complex, we show that the Swarm Intelligence (SI) optimized MIMO detection algorithm gives near-optimal Bit Error Rate (BER) performance in fewer iterations, thereby reducing the ML computational complexity significantly. The simulation results suggest that the proposed detector gives an acceptable performance complexity trade-off in comparison with ML and VBLAST detector.|Adnan Ahmed Khan,Muhammad Naeem,Syed Ismail Shah","58033|GECCO|2007|Collective specialization in multi-rover systems|Neuro-Evolution (NE) methods have been successfully applied to the rover task domain , . However, extending this domain to include the notion of using NE to facilitate emergent specialization, in order to increase task performance, has not yet been investigated. We introduce the Collective Neuro Evolution (CONE) method, and compares its efficacy for designing specialization, with a conventional NE method. CONE and conventional NE were applied to an extension of the multi-rover task ,  for the purpose of designing collective behavior. This task requires solutions for controlling groups of simulated autonomous vehicles (rovers) that seek to maximize the number of points of interest discovered in an unexplored environment (global evaluation function). Rovers operated in a discrete simulation environment, and used complementary sensors and actuators so as to maximize the global evaluation function. Specialization was defined at both the individual and group level according to the frequency with which rover sensors and actuators were activated. An individual rover was defi ned as being specialized if a given action was executed for the majority ( %) of a rovers lifetime. Likewise, a rover group was defined as being specialized if rovers with a given individual specialization constituted the majority ( %) of rovers in a group. Emergent specialization, facilitated by the NE methods was measured at both the individual and rover group level. At any simulation time step, a rover could execute one action, which equated to using one of two sensor types (detection or evaluation), or one of two actuator types (movement or communication). If a rover spent the majority of its lifetime executing a single action, it would be labeled as an evaluator, detector, communicator, or mover. Likewise, rover groups were defined as evaluator, detector, communicator, or mover groups if a majority of the group consisted of rovers with a given individual specialization. A rover domain performance benchmark was defined via specifying individual rover specialization a priori. The REVAC parameter calibration method  used to evolve a non-specialized rover group with a high task performance. One contribution was the provision of the CONE method, which facilitated emergent specialization, at both the individual agent and group level, so as to increase group fitness in collective behavior tasks. Previous research elucidated that behavioral specialization, at both the individual and group level, is beneficial for task performance . Here, the research hypothesis was that CONE is appropriate for deriving specialization at both the individual (rover controller), and at the group (composition of specialized rover controllers), where such specialization results in a high group fitness (performance). To test this hypothesis, the task performance and emergent specialization observed using the CONE method was compared to that of the conventional NE method, as well as a non-adaptive heuristic controller method. For both NE methods, each rover maintained and evolved its own population of genotypes. CONE evolved populations of neurons, from which complete neural network controllers were constructed, where as, the conventional NE method evolved populations of complete controllers. Results indicate that the CONE method was appropriate for facilitating specialization at both the individual and group levels, where as, the conventional NE method only facilitated individual specialization. An inferior performance was exhibited by rover groups controlled by the conventional NE method, and by non-specialized rover groups using heuristic controllers. In both cases, this was theorized to be a consequence of a lack of group level specialization.|Geoff Nitschke,Martijn C. Schut,A. E. Eiben","65947|AAAI|2007|Integrated Introspective Case-Based Reasoning for Intelligent Tutoring Systems|Many intelligent tutoring systems (ITSs) have been developed, deployed, assessed, and proven to facilitate learning. However, most of these systems do not generally adapt to new circumstances, do not self-evaluate and self-configure their own strategies, and do not monitor the usage history of the learning content being delivered or presented to the students. These shortcomings force ITS developers to often spend much development time in manual revision and fine-tuning of the learning and instructional contents of an ITS. In this paper, we describe an intelligent agent that delivers learning material adaptively to different students, factoring in the usage history of the learning materials and student profiles as observed by the agent. Student-tutor interaction includes the activities of going through learning material, such as a topical tutorial, a set of examples, and a set of problems. Our assumption is that our agent will be able to capture and utilize these student activities as the primer to select the appropriate examples or problems to administer to the student. Using an integrated introspective case-based reasoning approach, our agent further learns from its experience and refines its reasoning process-including the instructional strategies-to adapt to student needs. Moreover, our agent monitors the usage history of the learning materials to improve its performance. We have built an end-to-end ITS using an agent powered by this integrated introspective case-based reasoning engine. We have deployed the ITS in a CS course. Results indicate that the ITS was able to learn to deliver more appropriate examples and problems to the students.|Leen-Kiat Soh","58017|GECCO|2007|Procreating V-detectors for nonself recognition an application to anomaly detection in power systems|The artificial immune system approach for self-nonself discrimination and its application to anomaly detection problems in engineering is showing great promise. A seminal contribution in this area is the V-detectors algorithm that can very effectively cover the nonself region of the feature space with a set of detectors. The detector set can be used to detect anomalous inputs. In this paper, a multistage approach to create an effective set of V-detectors is considered. The first stage of the algorithm generates an initial set of V-detectors. In subsequent stage, new detectors are grown from existing ones, by means of a mechanism called procreation. Procreating detectors can more effectively fill hard-to-reach interstices in the nonself region, resulting in better coverage. The effectiveness of the algorithm is first illustrated by applying it to a well-known fractal, the Koch curve. The algorithm is then applied to the problem of detecting anomalous behavior in power distribution systems, and can be of much use for maintenance-related decision-making in electrical utility companies.|Min Gui,Sanjoy Das,Anil Pahwa","58183|GECCO|2007|Keyword extraction using an artificial immune system|This paper presents a model for keyword extraction which combines an artificial immune system with a mathematical background based on information theory. The proposed approach does not need any domain knowledge, neither the use of a stopword list. The output is a set of keywords for each of the categories into the corpus used.|Andres Romero,Fernando NiÃ±o","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono"],["57916|GECCO|2007|Exact analysis of the sampling distribution for the canonical particle swarm optimiser and its convergence during stagnation|Several theoretical analyses of the dynamics of particle swarms have been offered in the literature over the last decade. Virtually all rely on substantial simplifications, including the assumption that the particles are deterministic. This has prevented the exact characterisation of the sampling distribution of the PSO. In this paper we introduce a novel method, which allows one to exactly determine all the characteristics of a PSO's sampling distribution and explain how they change over any number of generations, in the presence stochasticity. The only assumption we make is stagnation, i.e., we study the sampling distribution produced by particles in search for a better personal best. We apply the analysis to the PSO with inertia weight, but the analysis is also valid for the PSO with constriction.|Riccardo Poli,David S. Broomhead","57982|GECCO|2007|Generalisation of the limiting distribution of program sizes in tree-based genetic programming and analysis of its effects on bloat|Recent research  has found that standard sub-tree crossover with uniform selection of crossover points, in the absence of fitness pressure, pushes a population of GP trees towards a Lagrange distribution of tree sizes. However, the result applied to the case of single arity function plus leaf node combinations, e.g., unary, binary, ternary, etc trees only. In this paper we extend those findings and show that the same distribution is also applicable to the more general case where the function set includes functions of mixed arities. We also provide empirical evidence that strongly corroborates this generalisation. Both predicted and observed results show a distinct bias towards the sampling of shorter programs irrespective of the mix of function arities used. Practical applications and implications of this knowledge are investigated with regard to search efficiency and program bloat. Work is also presented regarding the applicability of the theory to the traditional %-function %-terminal crossover node selection policy.|Stephen Dignum,Riccardo Poli","57909|GECCO|2007|Enhanced forma analysis of permutation problems|Forma analysis provides an approach to formally derive domain specific operators based on domain-independent operator templates by manipulating a set of equivalence relations (i.e., the basis), which is used to describe the search space. In the case of permutation problems, where the basis is highly constrained, the declarative nature of forma analysis encounters some difficulties which give rise to some additional issues, such as the interpretation of declarative constraints and the complexity of the application of operator. This paper aims to address these issues by introducing Enhanced Forma Analysis that explores a broader view of forma analysis by using ideas from constraint satisfaction.|Tao Gong,Andrew Tuson","58013|GECCO|2007|Analysis of evolutionary algorithms for the longest common subsequence problem|In the longest common subsequence problem the task is to find the longest sequence of letters that can be found as a subsequence in all members of a given finite set of sequences. The problem is one of the fundamental problems in computer science with the task of finding a given pattern in a text as an important special case. It has applications in bioinformatics problem-specific algorithms and facts about its complexity are known. Motivated by reports about good performance of evolutionary algorithms for some instances of this problem a theoretical analysis of a generic evolutionary algorithm is performed. The general algorithmic framework encompasses EAs as different as steady state GAs with uniform crossover and randomized hill-climbers. For all these algorithms it is proved that even rather simple special cases of the longest common subsequence problem can neither be solved to optimality nor approximately be solved up to an approximation factor arbitrarily close to&xa.|Thomas Jansen,Dennis Weyland","58222|GECCO|2007|Cross entropy and adaptive variance scaling in continuous EDA|This paper deals with the adaptive variance scaling issue incontinuous Estimation of Distribution Algorithms. A phenomenon is discovered that current adaptive variance scaling method in EDA suffers from imprecise structure learning. A new type of adaptation method is proposed to overcome this defect. The method tries to measure the difference between the obtained population and the prediction of the probabilistic model, then calculate the scaling factor by minimizing the cross entropy between these two distributions. This approach calculates the scaling factor immediately rather than adapts it incrementally. Experiments show that this approach extended the class of problems that can be solved, and improve the search efficiency in some cases. Moreover, the proposed approach features in that each decomposed subspace can be assigned an individual scaling factor, which helps to solve problems with special dimension property.|Yunpeng Cai,Xiaomin Sun,Hua Xu,Peifa Jia","58070|GECCO|2007|Convergence phases variance trajectories and runtime analysis of continuous EDAs|Considering the available body of literature on continuous EDAs, one must state that many important questions are still unanswered, e.g. How do continuous EDAs really work, and how can we increase their efficiency further The first question must be answered on the basis of formal models, but despite some recent results, the majority of contributions to the field is experimental. The second questionshould be answered by exploiting the insights that have been gained from formal models. We contribute to the theoretical literature on continuous EDAs by focussing on a simple, yet important, question How should the variances used tosample offspring from change over an EDA run To answer this question, the convergence process is separated into three phases and it is shown that for each phase, a preferable strategy exists for setting the variances. It is highly likely that the use of variances that have been estimated with maximum likelihood is not optimal. Thus, variance modification policies are not just a nice add-on. In the light of our findings, they become an integral component of continuous EDAs, and they should consider the specific requirements of all phases of the optimization process.|JÃ¶rn Grahl,Peter A. N. Bosman,Stefan Minner","57946|GECCO|2007|SDR a better trigger for adaptive variance scaling in normal EDAs|Recently, advances have been made in continuous, normal-distribution-based Estimation-of-DistributionAlgorithms (EDAs) by scaling the variance upfrom the maximum-likelihood estimate. When doneproperly, such scaling has been shown to preventpremature convergence on slope-like regions ofthe search space. In this paper we specificallyfocus on one way of scaling that was previouslyintroduced as Adaptive Variance Scaling (AVS). It wasfound that when using AVS, the average number offitness evaluations grows subquadratically withthe dimensionality on a wide range of unimodaltest-problems, competitively with the CMA-ES.Still, room for improvement exists because thevariance doesn't always have to be scaled. Apreviously introduced trigger based on correlationthat determines when to apply scaling was shownto fail on higher dimensional problems. Here weprovide a new solution called the Standard-DeviationRatio (SDR) trigger that is integrated with theIterated Density-Estimation Evolutionary Algorithm(IDEA). Intuitively put, scaling istriggered with SDR only if improvements are foundto be far away from the mean. SDR works even inhigh dimensions as a result of factorizing thedecision rule behind the trigger according to theestimated Bayesian factorization. We evaluateSDR-AVS-IDEA on the same set ofbenchmark problems and compare it with AVS-IDEAand CMA-ES. We find that the addition of SDR givesAVS-IDEA an important extra edgefor it to be used in future research and inapplications both in single-objective optimizationas well as in multi-objective and dynamicoptimization. In addition, we provide practical rulesof thumb for parameter settings for usingSDR-AVS-IDEA that result in anasymptotic scale-up behavior that is sublinearfor the population size (O(l.)) andsubquadratic (O(l.)) for thenumber of evaluations.|Peter A. N. Bosman,JÃ¶rn Grahl,Franz Rothlauf","58229|GECCO|2007|On the runtime analysis of the -ANT ACO algorithm|The runtime analysis of randomized search heuristics is a growing field where, in the last two decades, many rigorous results have been obtained. These results, however, apply particularly to classical search heuristics such as Evolutionary Algorithms (EAs) and Simulated Annealing. First runtime analyses of modern search heuristics have been conducted only recently w.r.t a simple Ant Colony Optimization (ACO) algorithm called -ANT. In particular, the influence of the evaporation factor in the pheromone update mechanism and the robustness of this parameter w.r.t the runtime behavior have been determined for the example function OneMax.This paper puts forward the rigorous runtime analysis of the -ANT on example functions, namely on the functions LeadingOnes and BinVal. With respect to EAs, such analyses have been essential to develop methods for the analysis on more complicated problems. The proof techniques required for the -ANT, unfortunately, differ significantly from those for EAs, which means that a new reservoir of methods has to be built up. Again, the influence of the evaporation factor is analyzed rigorously, and it is proved that its choice can be very crucial to allow efficient runtimes. Moreover, the analyses provide insight into the working principles of ACO algorithms and, in terms of their robustness, describe essential differences to other randomized search heuristics.|Benjamin Doerr,Frank Neumann,Dirk Sudholt,Carsten Witt","58218|GECCO|2007|Adaptive variance scaling in continuous multi-objective estimation-of-distribution algorithms|Recent research into single-objective continuous Estimation-of-Distribution Algorithms (EDAs)has shown that when maximum-likelihood estimationsare used for parametric distributions such as thenormal distribution, the EDA can easily suffer frompremature convergence. In this paper we argue thatthe same holds for multi-objective optimization.Our aim in this paper is to transfer a solutioncalled Adaptive Variance Scaling (AVS) from thesingle-objective case to the multi-objectivecase. To this end, we zoom in on an existing EDAfor continuous multi-objective optimization, theMIDEA, which employs mixturedistributions. We propose a means to combine AVSwith the normal mixture distribution, as opposedto the single normal distribution for which AVS wasintroduced. In addition, we improve the AVS schemeusing the Standard-Deviation Ratio(SDR) trigger. Intuitively put, variance scalingis triggered by the SDR trigger only ifimprovements are found to be far awayfrom the mean. For the multi-objective case,this addition is important to keep the variancefrom being scaled to excessively large values.From experiments performed on five well-knownbenchmark problems, the addition of SDR andAVS is found to enlarge the class of problems thatcontinuous multi-objective EDAs can solve reliably.|Peter A. N. Bosman,Dirk Thierens","66093|AAAI|2007|Evolutionary and Lifetime Learning in Varying NK Fitness Landscape Changing Environments An Analysis of Both Fitness and Diversity|This paper examines the effects of lifetime learning on populations evolving genetically in a series of changing environmets. The analysis of both fitness and diversity of the populations provides an insight into the improved performance provided by lifetime learning. The NK fitness landscape model is employed as the problem task, which has the advantage of being able to generate a variety of fitness landscapes of varying difficulty. Experiments observe the response of populations in an environment where problem difficulty increases and decreases with varying frequency. Results show that lifetime learning is capable of overall higher fitness levels and, in addition, that lifetime learning stimulates the diversity of the population. This increased diversity allows lifetime learning a greater level of recovery and stability than evolutionary learning alone.|Dara Curran,Colm O'Riordan,Humphrey Sorensen"],["57951|GECCO|2007|An estimation of distribution algorithm with guided mutation for a complex flow shop scheduling problem|An Estimation of Distribution Algorithm (EDA) is proposed toapproach the Hybrid Flow Shop with Sequence Dependent Setup Times and Uniform Machines in parallel (HFS-SDST-UM) problem. The latter motivated by the needs of a real world company. The proposed EDA implements a fairly new mechanism to improve the search of more traditional EDAs. This is the Guided Mutation (GM). EDA-GM generates new solutions by using the information from a probability model, as all EDAs, and the local information from a good known solution. The approach is tested on several instances of HFS-SDST-UM and compared with adaptations of meta-heuristics designed for very similarproblems. Encouraging results are reported.|Abdellah Salhi,JosÃ© Antonio VÃ¡zquez RodrÃ­guez,Qingfu Zhang","58157|GECCO|2007|Towards billion-bit optimization via a parallel estimation of distribution algorithm|This paper presents a highly efficient, fully parallelized implementation of the compact genetic algorithm (cGA) to solve very large scale problems with millions to billions of variables. The paper presents principled results demonstrating the scalable solution of a difficult test function on instances over a billion variables using a parallel implementation of cGA. The problem addressed is a noisy, blind problem over a vector of binary decision variables. Noise is added equaling up to a tenth of the deterministic objective function variance of the problem, thereby making it difficult for simple hillclimbers to find the optimal solution. The compact GA, on the other hand, is able to find the optimum in the presence of noise quickly, reliably, and accurately, and the solution scalability follows known convergence theories. These results on noisy problem together with other results on problems involving varying modularity, hierarchy, and overlap foreshadow routine solution of billion-variable problems across the landscape of search problems.|Kumara Sastry,David E. Goldberg,Xavier LlorÃ ","58170|GECCO|2007|Parameter cross-validation and early-stopping in univariate marginal distribution algorithm|In this paper, a cross-validation and early-stopping algorithm is devised for parameter updating in the Univariate Marginal Distribution Algorithm (UMDA) to reduce overftting. Our hypothesis is that the well-known problem of diversity loss in UMDA is a consequence of overfitting during the parameter estimation step at each generation. It is tested by experiments on two different optimization problems.|Hao Wu,Jonathan L. Shapiro","58152|GECCO|2007|Estimation of fitness landscape contours in EAs|Evolutionary algorithms applied in real domain should profit from information about the local fitness function curvature. This paper presents an initial study of an evolutionary strategy with a novel approach for learning the covariance matrix of a Gaussian distribution. The learning method is based one stimation of the fitness landscape contour line between the selected and discarded individuals. The distribution learned this way is then used to generate new population members. The algorithm presented here is the first attempt to construct the Gaussian distribution this way and should beconsidered only a proof of concept nevertheless, the empirical comparison on low-dimensional quadratic functions shows that our approach is viable and with respect to the number of evaluations needed to find a solution of certain quality, it is comparable to the state-of-the-art CMA-ES incase of sphere function and outperforms the CMA-ES in case of elliptical function.|Petr PosÃ­k,Vojtech Franc","58054|GECCO|2007|Scalable estimation-of-distribution program evolution|I present a new estimation-of-distribution approach to program evolution where distributions are not estimated over the entire space of programs. Rather, a novel representation-building procedure that exploits domain knowledge is used to dynamically select program subspaces for estimation over. This leads to a system of demes consisting of alternative rep-resentations (i.e. program subspaces) that are maintained simultaneously and managed by the overall system. Meta-optimizing semantic evolutionary search (MOSES), a program evolution system based on this approach, is described, and its representation-building subcomponent is analyzed in depth. Experimental results are also provided for the overall MOSES procedure that demonstrate good scalability.|Moshe Looks","58204|GECCO|2007|Population sizing for entropy-based model building in discrete estimation of distribution algorithms|This paper proposes a population-sizing model for entropy-based model building in discrete estimation of distribution algorithms. Specifically, the population size required for building an accurate model is investigated. The effect of selection pressure on population sizing is also preliminarily incorporated. The proposed model indicates that the population size required for building an accurate model scales as (m log m), where m is the number of substructures of the given problem and is proportional to the problem size. Experiments are conducted to verify the derivations, and the results agree with the proposed model.|Tian-Li Yu,Kumara Sastry,David E. Goldberg,Martin Pelikan","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang","66026|AAAI|2007|Improved State Estimation in Multiagent Settings with Continuous or Large Discrete State Spaces|State estimation in multiagent settings involves updating an agent's belief over the physical states and the space of other agents' models. Performance of the previous approach to state estimation, the interactive particle filter, degrades with large state spaces because it distributes the particles over both, the physical state space and the other agents' models. We present an improved method for estimating the state in a class of multiagent settings that are characterized in part by continuous or large discrete state spaces. We factor out the models of the other agents and update the agent's belief over these models, as exactly as possible. Simultaneously, we sample particles from the distribution over the large physical state space and project the particles in time. This approach is equivalent to Rao-Blackwellising the interactive particle filter. We focus our analysis on the special class of problems where the nested beliefs are represented using Gaussians, the problem dynamics using conditional linear Gaussians (CLGs) and the observation functions using softmax or CLGs. These distributions adequately represent many realistic applications.|Prashant Doshi","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith","58218|GECCO|2007|Adaptive variance scaling in continuous multi-objective estimation-of-distribution algorithms|Recent research into single-objective continuous Estimation-of-Distribution Algorithms (EDAs)has shown that when maximum-likelihood estimationsare used for parametric distributions such as thenormal distribution, the EDA can easily suffer frompremature convergence. In this paper we argue thatthe same holds for multi-objective optimization.Our aim in this paper is to transfer a solutioncalled Adaptive Variance Scaling (AVS) from thesingle-objective case to the multi-objectivecase. To this end, we zoom in on an existing EDAfor continuous multi-objective optimization, theMIDEA, which employs mixturedistributions. We propose a means to combine AVSwith the normal mixture distribution, as opposedto the single normal distribution for which AVS wasintroduced. In addition, we improve the AVS schemeusing the Standard-Deviation Ratio(SDR) trigger. Intuitively put, variance scalingis triggered by the SDR trigger only ifimprovements are found to be far awayfrom the mean. For the multi-objective case,this addition is important to keep the variancefrom being scaled to excessively large values.From experiments performed on five well-knownbenchmark problems, the addition of SDR andAVS is found to enlarge the class of problems thatcontinuous multi-objective EDAs can solve reliably.|Peter A. N. Bosman,Dirk Thierens"],["66044|AAAI|2007|A Qualitative Approach to Multiple Fault Isolation in Continuous Systems|The multiple fault diagnosis problem is important, since the single fault assumption can lead to incorrect or failed diagnoses when multiple faults occur. It is challenging for continuous systems, because faults can mask or compensate each other's effects, and the solution space grows exponentially with the number of possible faults. We present a qualitative approach to multiple fault isolation in dynamic systems based on analysis of fault transient behavior. Our approach uses the observed measurement deviations and their temporal orderings to generate multiple fault hypotheses. The approach has polynomial space requirements and prunes diagnoses, resulting in an efficient online fault isolation scheme.|Matthew J. Daigle,Xenofon D. Koutsoukos,Gautam Biswas","66194|AAAI|2007|Classifiers Fusion for EEG Signals Processing in Human-Computer Interface Systems|In this paper we study the effectiveness of using multiple classifier combination for EEG signals classification aiming to obtain more accurate results than it possible from single classifier system. The developed system employs different features vectors fused at the abstract and measurement levels for integrating information to reach a collective decision. For making decision, the majority voting scheme has been used. While at the measurement level, fuzzy integral, majority vote, decision template and some other types of combination methods have been investigated. The ensemble classification task is completed by feeding the Support Vectors Machines with Redial Basis Kernel functions classifiers with different features extracted from the EEG signal for imagination of right and left hands movements (i.e., at EEG channels C and C). The parameters of SVM classifiers were optimized by genetic algorithm. The results show that using classifier fusion methods improved the overall classification performance.|M. Esmaeili","57965|GECCO|2007|Effects of passengers arrival distribution to double-deck elevator group supervisory control systems using genetic network programming|The Elevator Group Supervisory Control Systems (EGSCS) are the control systems that systematically manage three or more elevators in order to efficiently transport the passengers in buildings. Double-deck elevators, where two cages are connected with each other, are expected to be the next generation elevator systems. Meanwhile, Destination Floor Guidance Systems (DFGS) are also expected in Double-Deck Elevator Systems (DDES). With these, the passengers could be served at two consecutive floors and could input their destinations at elevator halls instead of conventional systems without DFGS. Such systems become more complex than the traditional systems and require new control methods Genetic Network Programming (GNP), a graph-based evolutionary method, has been applied to EGSCS and its advantages are shown in some previous papers. GNP can obtain the strategy of a new hall call assignment to the optimal elevator because it performs crossover and mutation operations to judgment nodes and processing nodes. In studies so far, the passenger's arrival has been assumed to take Exponential distribution for many years. In this paper, we have applied Erlang distribution and Binomial distribution in order to study how the passenger's arrival distribution affects EGSCS. We have found that the passenger's arrival distribution has great influence on EGSCS. It has been also clarified that GNP makes good performances under different conditions.|Lu Yu,Jin Zhou,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu,Sandor Markon","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","58019|GECCO|2007|A particle swarm algorithm for symbols detection in wideband spatial multiplexing systems|This paper explores the application of the particle swarm algorithm for a NP-hard problem in the area of wireless communications. The specific problem is of detecting symbols in a Multi-Input Multi-Output (MIMO) communications system. This approach is particularly attractive as PSO is well suited for physically realizable, real-time applications, where low complexity and fast convergence is of absolute importance. While an optimal Maximum Likelihood (ML) detection using an exhaustive search method is prohibitively complex, we show that the Swarm Intelligence (SI) optimized MIMO detection algorithm gives near-optimal Bit Error Rate (BER) performance in fewer iterations, thereby reducing the ML computational complexity significantly. The simulation results suggest that the proposed detector gives an acceptable performance complexity trade-off in comparison with ML and VBLAST detector.|Adnan Ahmed Khan,Muhammad Naeem,Syed Ismail Shah","58033|GECCO|2007|Collective specialization in multi-rover systems|Neuro-Evolution (NE) methods have been successfully applied to the rover task domain , . However, extending this domain to include the notion of using NE to facilitate emergent specialization, in order to increase task performance, has not yet been investigated. We introduce the Collective Neuro Evolution (CONE) method, and compares its efficacy for designing specialization, with a conventional NE method. CONE and conventional NE were applied to an extension of the multi-rover task ,  for the purpose of designing collective behavior. This task requires solutions for controlling groups of simulated autonomous vehicles (rovers) that seek to maximize the number of points of interest discovered in an unexplored environment (global evaluation function). Rovers operated in a discrete simulation environment, and used complementary sensors and actuators so as to maximize the global evaluation function. Specialization was defined at both the individual and group level according to the frequency with which rover sensors and actuators were activated. An individual rover was defi ned as being specialized if a given action was executed for the majority ( %) of a rovers lifetime. Likewise, a rover group was defined as being specialized if rovers with a given individual specialization constituted the majority ( %) of rovers in a group. Emergent specialization, facilitated by the NE methods was measured at both the individual and rover group level. At any simulation time step, a rover could execute one action, which equated to using one of two sensor types (detection or evaluation), or one of two actuator types (movement or communication). If a rover spent the majority of its lifetime executing a single action, it would be labeled as an evaluator, detector, communicator, or mover. Likewise, rover groups were defined as evaluator, detector, communicator, or mover groups if a majority of the group consisted of rovers with a given individual specialization. A rover domain performance benchmark was defined via specifying individual rover specialization a priori. The REVAC parameter calibration method  used to evolve a non-specialized rover group with a high task performance. One contribution was the provision of the CONE method, which facilitated emergent specialization, at both the individual agent and group level, so as to increase group fitness in collective behavior tasks. Previous research elucidated that behavioral specialization, at both the individual and group level, is beneficial for task performance . Here, the research hypothesis was that CONE is appropriate for deriving specialization at both the individual (rover controller), and at the group (composition of specialized rover controllers), where such specialization results in a high group fitness (performance). To test this hypothesis, the task performance and emergent specialization observed using the CONE method was compared to that of the conventional NE method, as well as a non-adaptive heuristic controller method. For both NE methods, each rover maintained and evolved its own population of genotypes. CONE evolved populations of neurons, from which complete neural network controllers were constructed, where as, the conventional NE method evolved populations of complete controllers. Results indicate that the CONE method was appropriate for facilitating specialization at both the individual and group levels, where as, the conventional NE method only facilitated individual specialization. An inferior performance was exhibited by rover groups controlled by the conventional NE method, and by non-specialized rover groups using heuristic controllers. In both cases, this was theorized to be a consequence of a lack of group level specialization.|Geoff Nitschke,Martijn C. Schut,A. E. Eiben","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","57988|GECCO|2007|Supplementing evolutionary developmental systems with abstract models of neurogenesis|This work investigates an intermediate abstraction level, that of neural groups, for modelling the development of complex artificial neural networks. Based on Neural Darwinism citeedelman, Displacement Theory citedeacon and The Neuromeric Model citestriedter, our DEACANN system avoids the complexities of axonal and dendritic growth while maintaining key aspects of cell signalling, competition and cooperation that appear to govern the formation of neural topologies in nature. DEACANN also includes a genetic-algorithm for evolving developmental recipes, and the mature networks employ several forms of learning.|Keith L. Downing","66165|AAAI|2007|Towards an Integrated Robot with Multiple Cognitive Functions|We present integration mechanisms for combining heterogeneous components in a situated information processing system, illustrated by a cognitive robot able to collaborate with a human and display some understanding of its surroundings. These mechanisms include an architectural schema that encourages parallel and incremental information processing, and a method for binding information from distinct representations that when faced with rapid change in the world can maintain a coherent, though distributed, view of it. Provisional results are demonstrated in a robot combining vision, manipulation, language, planning and reasoning capabilities interacting with a human and manipulable objects.|Nick Hawes,Aaron Sloman,Jeremy Wyatt,Michael Zillich,Henrik Jacobsson,Geert-Jan M. Kruijff,Michael Brenner,Gregor Berginc,Danijel Skocaj","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono"],["66178|AAAI|2007|Modeling and Learning Vague Event Durations for Temporal Reasoning|This paper reports on our recent work on modeling and automatically extracting vague, implicit event durations from text (Pan et al., a, b). It is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problems. We have also proposed a method of using normal distributions to model Judgments that are intervals on a scale and measure their interannotator agreement this should extend from time to other kinds of vague but substantive information in text and commonsense reasoning.|Feng Pan,Rutu Mulkar,Jerry R. Hobbs","66006|AAAI|2007|Spatial Representation and Reasoning for Human-Robot Collaboration|How should a robot represent and reason about spatial information when it needs to collaborate effectively with a human The form of spatial representation that is useful for robot navigation may not be useful in higher-level reasoning or working with humans as a team member. To explore this question, we have extended previous work on how children and robots learn to play hide and seek to a human-robot team covertly approaching a moving target. We used the cognitive modeling system, ACT-R, with an added spatial module to support the robot's spatial reasoning. The robot interacted with a team member through voice, gestures, and movement during the team's covert approach of a moving target. This paper describes the new robotic system and its integration of metric, symbolic, and cognitive layers of spatial representation and reasoning for its individual and team behavior.|William G. Kennedy,Magdalena D. Bugajska,Matthew Marge,William Adams,Benjamin R. Fransen,Dennis Perzanowski,Alan C. Schultz,J. Gregory Trafton","66236|AAAI|2007|Optimal Regression for Reasoning about Knowledge and Actions|We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.|Hans P. van Ditmarsch,Andreas Herzig,Tiago De Lima","66063|AAAI|2007|Reasoning about Bargaining Situations|This paper presents a logical axiomatization of bargaining solutions. A bargaining situation is described in propositional logic and the bargainers' preferences are quantified in terms of the logical structure of the bargaining situation. A solution to the n-person bargaining problems is proposed based on the maxmin rule over the degrees of bargainers' satisfaction. We show that the solution is uniquely characterized by four natural and intuitive axioms as well as three other fundamental assumptions. All the axioms and assumptions are represented in logical statements and most of them have a game-theoretic counterpart. The framework would help us to identify the logical and numerical reasoning behind bargaining processes.|Dongmo Zhang","66005|AAAI|2007|Beyond Individualism Modeling Team Playing Behavior in Robot Soccer through Case-Based Reasoning|We propose a Case-Based Reasoning approach for action selection in the robot soccer domain presented in the th European Conference on Case-Based Reasoning (). Based on the current state of a game, the robots retrieve the most similar past situation and then the team reproduces the sequence of actions performed in that occasion. In this domain we have to deal with all the difficulties that a real environment involves.|Raquel Ros,Manuela M. Veloso,Ramon LÃ³pez de MÃ¡ntaras,Carles Sierra,Josep LluÃ­s Arcos","65978|AAAI|2007|Explanation Support for the Case-Based Reasoning Tool myCBR|Case-Based Reasoning, in short, is the process of solving new problems based on solutions of similar past problems, much like humans solve many problems. myCBR, an extension of the ontology editor Protg, provides such similarity-based retrieval functionality. Moreover, the user is supported in modelling appropriate similarity measures by forward and backward explanations.|Daniel Bahls,Thomas Roth-Berghofer","66073|AAAI|2007|Representing and Reasoning about Commitments in Business Processes|A variety of business relationships in open settings can be understood in terms of the creation and manipulation of commitments among the participants. These include BC and BB contracts and processes, as realized via Web services and other such technologies. Business protocols, an interaction-oriented approach for modeling business processes, are formulated in terms of the commitments. Commitments can support other forms of semantic service composition as well. This paper shows how to represent and reason about commitments in a general manner. Unlike previous formalizations, the proposed formalization accommodates complex and nested commitment conditions, and concurrent commitment operations. In this manner, a rich variety of open business scenarios are enabled.|Nirmit Desai,Amit K. Chopra,Munindar P. Singh","65947|AAAI|2007|Integrated Introspective Case-Based Reasoning for Intelligent Tutoring Systems|Many intelligent tutoring systems (ITSs) have been developed, deployed, assessed, and proven to facilitate learning. However, most of these systems do not generally adapt to new circumstances, do not self-evaluate and self-configure their own strategies, and do not monitor the usage history of the learning content being delivered or presented to the students. These shortcomings force ITS developers to often spend much development time in manual revision and fine-tuning of the learning and instructional contents of an ITS. In this paper, we describe an intelligent agent that delivers learning material adaptively to different students, factoring in the usage history of the learning materials and student profiles as observed by the agent. Student-tutor interaction includes the activities of going through learning material, such as a topical tutorial, a set of examples, and a set of problems. Our assumption is that our agent will be able to capture and utilize these student activities as the primer to select the appropriate examples or problems to administer to the student. Using an integrated introspective case-based reasoning approach, our agent further learns from its experience and refines its reasoning process-including the instructional strategies-to adapt to student needs. Moreover, our agent monitors the usage history of the learning materials to improve its performance. We have built an end-to-end ITS using an agent powered by this integrated introspective case-based reasoning engine. We have deployed the ITS in a CS course. Results indicate that the ITS was able to learn to deliver more appropriate examples and problems to the students.|Leen-Kiat Soh","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas WÃ¶lfl"]]}}