{"abstract":{"entropy":6.506558735107855,"topics":["natural language, artificial intelligence, describes system, expert system, system, knowledge representation, language, theorem proving, describes, knowledge base, knowledge system, logic programming, knowledge, programming language, present system, program, description logic, theorem prover, knowledge acquisition, natural processing","data, data base, data system, data management, management system, database, database system, data model, play role, relational database, large data, recent years, data stream, relational data, web search, xml documents, applications, query, information, received attention","constraint satisfaction, solving problem, constraint problem, machine learning, satisfaction problem, present algorithm, heuristic search, learning, algorithm, search algorithm, markov decision, neural networks, present approach, search, markov processes, problem, solution problem, present novel, reinforcement learning, search problem","mobile robot, real world, agents, temporal reasoning, qualitative reasoning, planning problem, pattern recognition, model-based diagnosis, virtual hashing, autonomous agents, spatial reasoning, planning plan, belief revision, address problem, reasoning, combinatorial auctions, planning, computational complexity, problem agents, consider problem","logic programming, description logic, knowledge representation, programming language, logic, logic program, logic knowledge, system logic, nonmonotonic logic, semantic system, modal logic, logic language, semantic, default logic, logic formalism, first-order logic, description language, description knowledge, logic reasoning, present semantic","natural language, knowledge system, knowledge base, language system, knowledge acquisition, natural processing, natural system, knowledge representation, language processing, language generation, knowledge, expert knowledge, domains knowledge, sense disambiguation, word disambiguation, word sense, equality elimination, programming language, reasoning knowledge, present language","query data, query processing, xml documents, web engine, query language, xml data, database query, queries data, query, relational xml, query system, large data, query optimization, database queries, large database, search engine, queries xml, question answering, efficient xml, xml language","play role, data mining, applications data, important data, data warehousing, applications, sensor networks, time series, applications system, important role, important applications, clustering data, data olap, play important, data time, spatial data, system important, conceptual clustering, data cube, volumes data","neural networks, bayesian networks, arc consistency, widely used, boolean satisfiability, previous work, networks learning, horn clause, constructive induction, satisfiability problem, used, satisfiability testing, problem networks, used learning, satisfiability sat, learning induction, networks, shown search, develop learning, effective search","present algorithm, markov decision, present novel, markov processes, decision tree, decision processes, decision problem, novel approach, partially observable, present approach, markov mdps, present framework, processes mdps, decision mdps, networks probabilistic, present, decision, paper algorithm, present model, observable markov","autonomous agents, problem agents, resources agents, resources allocation, agents, agents need, problem given, multiagent agents, distributed agents, knowledge agents, problem resources, task, system agents, following problem, multiagent environment, decision making, problem formal, problem designing, study agents, system task","temporal reasoning, qualitative reasoning, reasoning, belief revision, model-based diagnosis, spatial reasoning, address problem, problem reasoning, model-based system, framework reasoning, reasoning system, spatial temporal, formalism reasoning, diagnosis system, multiple agents, difficult problem, qualitative system, important reasoning, allows agents, self-interested agents"],"ranking":[["13365|IJCAI|1973|A Gobal View of Automatic Programming|This paper presents a framework for characterizing automatic programming systems In terms of how a task Is communicated to the system, the method and time at which the system acquires the knowledge to perform the task, and the characteristics of the resulting program to perform that task. It describes one approach In which both tasks and knowledge about the task domain are stated in natural language In the terms of that domain. All knowledge of computer science necessary to Implement the task Is Internalized Inside the system.|Robert Balzer","14014|IJCAI|1983|Towards Knowledge Acquisition From Natural Language Documents - Automatic Model Construction From Hardware Manual|In this paper, we explore automatic model construction by analyzing natural language documents. The extracted model will be utilized by a CAD system. A system called hmU, in the course of development, is designed to allow knowledge on very complex hardware module like LSI or VLSI to be incorporated into its knowledge base. The acquired knowledge will be utilized for helping human designer understand the component from various levels of abstraction. The focus of this paper is attentioned more to issues on knowledge representation and model inference than that on natural language analysis. Hierarchical model is employed. In particular, cause-effect representation is used to make it clear how actions of each module and events are related to each other. A brief description is given to illustrate our approach.|Toyoaki Nishida,Akira Kosaka,Shuji Doshita","14443|IJCAI|1987|Amalgamating Multiple Programming Paradigms in PROLOG|This paper discusses the issues in amalgamating multiple programming paradigms in the logic programming language, Prolog. It is shown that multiple paradigms can be incorporated without disturbing logic programming language features and efficiency. It also introduces a new programming paradigm called the relation-oriented paradigm. The research results are reflected in the implementation of the Prolog-based knowledge programming system PEACE, which is used to realize an expert system in a diagnostic domain. PEACE provides a relation-oriented programming paradigm, as well as previously discussed paradigms, such as object-oriented, data-oriented, and rule-oriented paradigms. These paradigms are nicely amalgamated in Prolog language and can be used intermixedly.|Yoshiyuki Koseki","13740|IJCAI|1981|Acquisition of Procedural Knowledge from Domain Experts|Procedural knowledge forms an important part of expertise. This paper describes a system which allows domain experts to themselves enter procedural Knowledge into a knowledge base. The system, a stylized form of scientific English embodied within the Unit System for knowledge acquisition and representation, has been used successfully within the domain of molecular biology.|Peter Friedland","13403|IJCAI|1973|D-SCRlPT A Computational Theory of Descriptions|This paper describes D-Script, a language for representing knowledge in artificial intelligence (AI) programs. D-Script contains a powerful formalism for descriptions, which permits the representation of statements that are problematical for other systems. Particular attention is paid to problems of opaque contexts, time contexts, and knowledge about knowledge. The design of a deductive system for this language is also considered.|Robert C. Moore","14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi","13436|IJCAI|1973|Understanding Without Proofs|The paper describes the analysis part of a running analysis and generation program for natural language. The system is entirely oriented to matching meaningful patterns onto fragmented paragraph length input. Its core Is a choice system based on what I call \"semantic density\". The system is contrasted with () syntax oriented linguistic approaches and () theorem proving approaches to the understanding problem. It is argued by means of examples that the present system is not only more workable, but more intuitively acceptable, at least as an understander for the purpose of translation, than deduction-based systems.|Yorick Wilks","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","15058|IJCAI|1993|Using Classification as a Programming Language|Our experience in the IDAS natural language generation project has shown us that IDAS'S KLONE-like classifier, originally built solely to hold a domain knowledge base, could also be used to perform many of the computations required by a natural-language generation system in fact it seems possible to use the classifier to encode and execute arbitrary programs. We discuss IDAS'S classification system and how it differs from other such systems (perhaps most notably in the presence of template' constructs that enable recursion to be encoded) give examples of program fragments encoded in the classification system and compare the classification approach to other AI programming paradigms (e.g., logic programming).|Chris Mellish,Ehud Reiter","14140|IJCAI|1985|SAPHIR  RESEDA A New Approach to Intelligent Data Base Access|This paper describes a transportable natural language interface to databases, augmented with a knowledge base and inference techniques. The inference mechanism, based on a classical expert system's type of approach, allows, when needed, to automatically convert an Input query into another one which is \"semantically close\". According to RESEDAS theory, \"semantically close\" means that the answer to the transformed query Implies what could have been the answer to the original question. The presented system Integrates natural language processing, expert system and knowledge representation technology to provide a cooperative database access.|Bernard Euzenat,Bernard Normier,Antoine Ogonowski,Gian Piero Zarri"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80699|VLDB|2006|GORDIAN Efficient and Scalable Discovery of Composite Keys|Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets.|Yannis Sismanis,Paul Brown,Peter J. Haas,Berthold Reinwald","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80061|VLDB|1981|DSIS - A Database System with Interrelational Semantics|DSIS is an experimental multi-user database system that supports an entity-oriented data model with relational and interrelational semantics. It maintains a sharp distinction between the user workspace and the shared database. Transactions are executed by a simulated network of stream processors.|Y. Edmund Lien,Jonathan E. Shopiro,Shalom Tsur","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80034|VLDB|1981|Derived Relations A Unified Mechanism for Views Snapshots and Distributed Data|In a relational system, a database is composed of base relations, views, and snapshots. We show that this traditional approach can be extended to different classes of derived relations, and we propose a unified data definition mechanism for centralized and distributed databases. Our mechanism, called DEREL, can be used to define base relations and to derive different classes of views, snapshots, partitioned and replicated data. DEREL is intended to be part of a general purpose distributed relational database management system.|Michel E. Adiba"],["16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir","15312|IJCAI|1995|SEM a System for Enumerating Models|Model generation can be regarded as a special case of the Constraint Satisfaction Problem (CSP). It has many applications in AI, computer science and mathematics. In this paper, we describe SEM, a System for Enumerating finite Models of first-order many-sorted theories. To the best of our knowledge, SEM outperforms any other finite model generation system on many test problems. The high performance of SEM relies on the following two techniques (a) an efficient implementation of constraint propagation which requires little dynamic allocation of storage (b) a powerful heuristic which eliminates many isomorphic partial models during the search. We will present the basic algorithm of SEM along with these two techniques. Our experimental results show that general purpose finite model generators are indeed useful in many applications.|Jian Zhang 0001,Hantao Zhang","15560|IJCAI|1999|The LPSAT Engine  Its Application to Resource Planning|Compilation to boolean satisfiability has become a powerful paradigm for solving AI problems. However, domains that require metric reasoning cannot be compiled efficiently to SAT even if they would otherwise benefit from compilation. We address this problem by introducing the LCNF representation which combines propositional logic with metric constraints. We present LPSAT, an engine which solves LCNF problems by interleaving calls to an incremental simplex algorithm with systematic satisfaction methods. We describe a compiler which converts metric resource planning problems into LCNF for processing by LPSAT. The experimental section of the paper explores several optimizations to LPSAT, including learning from constraint failure and randomized cutoffs.|Steven A. Wolfman,Daniel S. Weld","14099|IJCAI|1985|A Study of Search Methods The Effect of Constraint Satisfaction and Adventurousness|This research addresses how constraint satisfaction interacts with the search mode, and how the ratio of breadth of effort to depth of effort can be controlled. Four search paradigms, each the best of its kind for non adversary problems, are investigated. One is depth first, and the others best first. All methods except one highly informed best first search use the same knowledge, and each of these methods is tested with and without the use of a constraint satisfaction procedure on sets of progressively more difficult problems. As expected, the most informed search does better than the less informed as the problems get more difficult. Constraint satisfaction is found to have a pronouncedly greater effect when coupled with the most informed algorithm. Large performance increments over A* can be produced by the use of a coefficient associated with the h term, and this algorithm produces solutions that are only % worse than optimal. This is a known phenomenon however, the range of this coefficient is very narrow. We term this coefficient, which controls the ratio of depth of effort to breadth of effort, the adventurousness coefficient. The less tractable a problem the greater the adventurousness should be. We present evidence to support this.|Hans J. Berliner,Gordon Goetsch","15786|IJCAI|2003|Propagate the Right Thing How Preferences Can Speed-Up Constraint Solving|We present an algorithm Pref-AC that limits arc consistency (AC) to the preferred choices of a tree search procedure and that makes constraint solving more efficient without changing the pruning and shape of the search tree. Arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problems such as configuration and scheduling. Moreover, Pref-AC directly computes a preferred solution for treelike constraint satisfaction problems.|Christian Bessière,Anaïs Fabre,Ulrich Junker","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann","15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan"],["15671|IJCAI|2001|Executing Reactive Model-based Programs through Graph-based Temporal Planning|In the future, webs of unmanned air and space vehicles will act together to robustly perform elaborate missions in uncertain environments. We coordinate these systems by introducing a reactive model-based programming language (RMPL) that combines within a single unified representation the flexibility of embedded programming and reactive execution languages, and the deliberative reasoning power of temporal planners. The KIRK planning system takes as input a problem expressed as a RMPL program, and compiles it into a temporal plan network (TPN), similar to those used by temporal planners, but extended for symbolic constraints and decisions. This intermediate representation clarifies the relation between temporal planning and causal-link planning, and permits a single task model to be used for planning and execution. Such a unified model has been described as a holy grail for autonomous agents by the designers of the Remote AgentMuscettola et al., b.|Phil Kim,Brian C. Williams,Mark Abramson","16007|IJCAI|2003|Backdoors To Typical Case Complexity|There has been significant recent progress in reasoning and constraint processing methods. In areas such as planning and finite model-checking, current solution techniques can handle combinatorial problems with up to a million variables and five million constraints. The good scaling behavior of these methods appears to defy what one would expect based on a worst-case complexity analysis. In order to bridge this gap between theory and practice, we propose a new framework for studying the complexity of these techniques on practical problem instances. In particular, our approach incorporates general structural properties observed in practical problem instances into the formal complexity analysis. We introduce a notion of \"backdoors\", which are small sets of variables that capture the overall combinatorics of the problem instance. We provide empirical results showing the existence of such backdoors in real-world problems. We then present a series of complexity results that explain the good scaling behavior of current reasoning and constraint methods observed on practical problem instances.|Ryan Williams,Carla P. Gomes,Bart Selman","17033|IJCAI|2009|Planning Games|We introduce planning games, a study of interactions of self-motivated agents in automated planning settings. Planning games extend STRIPS-like models of single-agent planning to systems of multiple self-interested agents, providing a rich class of structured games that capture subtle forms of local interactions. We consider two basic models of planning games and adapt game-theoretic solution concepts to these models. In both models, agents may need to cooperate in order to achieve their goals, but are assumed to do so only in order to increase their net benefit. For each model we study the computational problem of finding a stable solution and provide efficient algorithms for systems exhibiting acyclic interaction structure.|Ronen I. Brafman,Carmel Domshlak,Yagil Engel,Moshe Tennenholtz","16246|IJCAI|2005|Multi-Agent Assumption-Based Planning|The purpose of this poster is to introduce a dialectical theory for plan synthesis based on a multiagent approach. This approach is a promising way to devise systems able to take into account partial knowledge and heterogeneous skills of agents. We propose to consider the planning problem as a defeasible reasoning where agents exchange proposals and counter-proposals and are able to conjecture i.e., formulate plan steps based on hypothetical states of the world.|Damien Pellier,Humbert Fiorino","13878|IJCAI|1983|Planning Using a Temporal World Model|Current problem solving systems are constrained in their applicability by inadequate world models. We suggest a world model based on a temporal logic. This approach allows the problem solver to gather constraints on the ordering of actions without having to commit to an ordering when a conflict is detected. As such, it generalizes the work on nonlinear planning by Sacerdoti and Tate. In addition, it allows more general descriptions of actions that may occur simultaneously or overlap, and appears promising in supporting reasoning about external events and actions caused by other agents.|James F. Allen,Johannes A. G. M. Koomen","14448|IJCAI|1987|Tractable Meta-Reasoning in Propositional Logics of Belief|Finding adequate semantic models of deductive reasoning is a difficult problem, if deductions are to be performed efficiently and in a semantically appropriate way. The model of reasoning provided by possible-worlds semantics has been found deficient both for computational and intuitive reasons. Existing semantic approaches that were proposed as alternatives to possible-worlds semantics either suffer from computational intractability or do not allow agents to have meta-beliefs. This work, based on relevance logic, proposes a model of belief where an agent can hold met a-beliefs and reason about them and other world knowledge efficiently. It is also shown how the model can be extended to include positive introspection without losing efficiency.|Gerhard Lakemeyer","16434|IJCAI|2007|Mediating between Qualitative and Quantitative Representations for Task-Orientated Human-Robot Interaction|In human-robot interaction (HRI) it is essential that the robot interprets and reacts to a human's utterances in a manner that reflects their intended meaning. In this paper we present a collection of novel techniques that allow a robot to interpret and execute spoken commands describing manipulation goals involving qualitative spatial constraints (e.g. \"put the red ball near the blue cube\"). The resulting implemented system integrates computer vision, potential field models of spatial relationships, and action planning to mediate between the continuous real world, and discrete, qualitative representations used for symbolic reasoning.|Michael Brenner,Nick Hawes,John D. Kelleher,Jeremy Wyatt","14615|IJCAI|1989|Introducing Actions into Qualitative Simulation|Many potential uses of qualitative physics, such as robot planning and intelligent computer-aided engineering, require integrating physics with actions taken by agents. Here we show how to extend qualitative simulation to include the effects of actions, resulting in action-augmented envisionments. The action-augmented envisionment incorporates both the effects of an agent's actions and what may happen in the physical world whether or not an agent does something. Consequently, it provides a richer basis for planning and for reasoning about procedures than any previous representation. This paper defines action-augmented envisionments and presents an algorithm for directly computing them. The properties of the algorithm are analyzed along with its suitability for robot planning and reasoning about engineering procedures. We describe results generated by a working implementation and discuss potential extensions, including incremental algorithms.|Kenneth D. Forbus","16061|IJCAI|2005|Efficiency and envy-freeness in fair division of indivisible goods logical representation and complexity|We consider the problem of allocating fairly a set of indivisible goods among agents from the point of view of compact representation and computational complexity. We start by assuming that agents have dichotomous preferences expressed by propositional formulae. We express efficiency and envy-freeness in a logical setting, which reveals unexpected connections to nonmonotonic reasoning. Then we identify the complexity of determining whether there exists an efficient and envy-free allocation, for several notions of efficiency, when preferences are represented in a succinct way (as well as restrictions of this problem). We first study the problem under the assumption that preferences are dichotomous, and then in the general case.|Sylvain Bouveret,Jérôme Lang","14122|IJCAI|1985|Temporal Reasoning Involving Counterfactuals and Disjunctions|This paper describes a mechanism for nonmonotonic temporal reasoning involving counterfactuals and disjunctions. The mechanism supports a method for exploring alternatives well suited to automatic planning. The application of these techniques to robot problem solving is discussed with an emphasis on reasoning about exclusive choices and monitoring the continued warrant and effectiveness of prevention tasks.|Thomas Dean"],["14904|IJCAI|1991|Nonmonotonic Databases and Epistemic Queries|The approach to database query evaluation developed by Levesque and Reiter treats databases as first order theories, and queries as formulas of the language which includes, in addition to the language of the database, an epistemic modal operator. In this epistemic query language, one can express questions not only about the external world described by the database, but also about the database itself-- about what the database knows. On the other hand, epistemic formulas are used in knowledge representation for the purpose of expressing defaults. Autoepistemic logic is the best known epistemic nonmonotonic formalism the logic of grounded knowledge, proposed recently by Lin and Shoham, is another such system. This paper brings these two directions of research together. We describe a new version of the LinShoham logic, similar in spirit to the LevesqueReiter theory of epistemic queries. Using this formalism, we can give meaning to epistemic queries in the context of nonmonotonic databases, including logic programs with negation as failure.|Vladimir Lifschitz","16235|IJCAI|2005|Possibilistic Stable Models|In this work, we define a new framework in order to improve the knowledge representation power of Answer Set Programming paradigm. Our proposal is to use notions from possibility theory to extend the stable model semantics by taking into account a certainty level, expressed in terms of necessity measure, on each rule of a normal logic program. First of all, we introduce possibilistic definite logic programs and show how to compute the conclusions of such programs both in syntactic and semantic ways. The syntactic handling is done by help of a fix-point operator, the semantic part relies on a possibility distribution on all sets of atoms and we show that the two approaches are equivalent. In a second part, we define what is a possibilistic stable model for a normal logic program, with default negation. Again, we define a possibility distribution allowing to determine the stable models.|Pascal Nicolas,Laurent Garcia,Igor Stéphan","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","15037|IJCAI|1993|Nonmonotonic Model Inference-A Formalization of Student Modeling|A student model description language and its synthesis method are presented. The language called SMDL is based on a logic programming language taking  truth values such as true, false, unknown and fail. A modeling method called HSMIS is a new nonmonotonic model inference system and has the following major characteristics () Model inference of logic program taking  truth values, () Treatment of nonmonotonicity of both student's belief and inference process itself. HSMIS incorporates de Kleer's ATMS as a vehicle for formulating the nonmonotonicity. Both SMDL interpreter and HSMIS have been implemented in Common ESP(Extended Self-contained Prolog) and incorporated into a framework for ITS, called FITS.|Mitsuru Ikeda,Yasuyuki Kono,Riichiro Mizoguchi","13879|IJCAI|1983|Integrating Logic Programs and Schemata|(orn clauseSchema Representation Language) is the result of an athor to combine the- tools of logic program-niinp. and schema based knowledge represention into a single hybrid system. knowledge, compressed in schemala can be accessed during the execution of logic programs, and the retrieval of the values of a SLOT in a schema can involve- the execution of logic programs that attempt. to declare the- values prior to presenting- to inheritence, should the slot be empty. ISRI. supports the implementation of programs that take advantage of the best controles. of the logical and objec oriented approaches to knowledge represenlation.|Bradley P. Allen,J. Mark Wright","16787|IJCAI|2007|A Faithful Integration of Description Logics with Logic Programming|Integrating description logics (DL) and logic programming (LP) would produce a very powerful and useful formalism. However, DLs and LP are based on quite different principles, so achieving a seamless integration is not trivial. In this paper, we introduce hybrid MKNF knowledge bases that faithfully integrate DLs with LP using the logic of Minimal Knowledge and Negation as Failure (MKNF) Lifschitz, . We also give reasoning algorithms and tight data complexity bounds for several interesting fragments of our logic.|Boris Motik,Riccardo Rosati","14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","14563|IJCAI|1989|The Logic of Time Structures Temporal and Nonmonotonic Features|We Imbed Into a first order logic a representation language that combines a temporal knowledge with time stamps in a hierarchical fashion. Each time structure contains its own chronology of events sufficient information for an encoding of a classical temporal logic. By quantifying over time structures, we encode a modal logic of temporal knowledge. In addition, we show how to achieve the effect of nonmonotonic inference, by simulating preferential entailment within a first order framework.|Mira Balaban,Neil V. Murray","16356|IJCAI|2007|Embedding Non-Ground Logic Programs into Autoepistemic Logic for Knowledge-Base Combination|In the context of the Semantic Web, several approaches to the combination of ontologies, given in terms of theories of classical first-order logic, and rule bases have been proposed. They either cast rules into classical logic or limit the interaction between rules and ontologies. Autoepistemic logic (AEL) is an attractive formalismwhich allows to overcome these limitations, by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it. For the latter, so far only the propositional setting has been considered. In this paper, we present several embeddings of normal and disjunctive non-ground logic programs under the stable-model semantics into first-order AEL, and compare them in combination with classical theories, with respect to stable expansions and autoepistemic consequences. Our results reveal differences and correspondences of the embeddings and provide a useful guidance in the choice of a particular embedding for knowledge combination.|Jos de Bruijn,Thomas Eiter,Axel Polleres,Hans Tompits"],["16122|IJCAI|2005|Feature Generation for Text Categorization Using World Knowledge|We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.|Evgeniy Gabrilovich,Shaul Markovitch","14014|IJCAI|1983|Towards Knowledge Acquisition From Natural Language Documents - Automatic Model Construction From Hardware Manual|In this paper, we explore automatic model construction by analyzing natural language documents. The extracted model will be utilized by a CAD system. A system called hmU, in the course of development, is designed to allow knowledge on very complex hardware module like LSI or VLSI to be incorporated into its knowledge base. The acquired knowledge will be utilized for helping human designer understand the component from various levels of abstraction. The focus of this paper is attentioned more to issues on knowledge representation and model inference than that on natural language analysis. Hierarchical model is employed. In particular, cause-effect representation is used to make it clear how actions of each module and events are related to each other. A brief description is given to illustrate our approach.|Toyoaki Nishida,Akira Kosaka,Shuji Doshita","14349|IJCAI|1987|A Microfeature-based Scheme for Modelling Semantics|One fundamental problem of natural language processing is word sense disambiguation. Solving this problem involves the integration of multiple knowledge sources syntactic, semantic, and pragmatic. Recent work has shown how this problem can be modelled as a constraint satisfaction process between competing syntactic and semantic structures. We have defined and implemented a \"locally-distributed\" microfeature based model called MIBS, that uses a distributed short-term memory (STM) composed of microfeatures to represent the underlying sentence semantics. This work represents an improvement over previous work, as it provides a natural language understanding system a means to dynamically determine the current context and adjust its relationship with the sentences that follow. Here, the meaning of a word is represented not as a symbol in some semantic net, but as a collection of smaller features. The values of the microfeatures in STM vary dynamically as the sentence is processed, reflecting the system's \"settling\" in on the sentence's meaning. In addition they represent an automatic context mechanism that helps the system to disambiguate the sentences that follow.|Lawrence A. Bookman","14552|IJCAI|1989|Bidirectional Use of Knowledge in the Multi-modal NL Access System XTRA|The acceptability and effectiveness of an expert system is critically dependent on its user interface. Natural language could be a well-suited communicative medium however, current NL interfaces to expert systems show an impeding number of shortcomings. In this paper, it is shown how the XTRA system - a German NL interface between a user and one of various expert systems - employs a framework of novel AI techniques to overcome these shortcomings it uses various highly interacting knowledge sources for domain-independent linguistic knowledge and for domain-specific world knowledge. This supports the view of NL as a universal communicative medium. In order to allow for a communicatively adequate dialog processing, knowledge sources are bidircctionaly used by both the analysis and the generation component. Both are enhanced with knowledge about the visual context of the dialog situation and about the capability to perform pointing gestures. SB-ONE - a knowledge representation formalism especially tailored for NL processing - deals with these types of knowledge. Together with some important tools of this formalism it constitutes the core of the system, whose performance is demonstrated with the help of an example dialog.|Jürgen Allgayer,Roman M. Jansen-Winkeln,Carola Reddig,Norbert Reithinger","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","13862|IJCAI|1981|A Knowledge-Based Approach to Language Processing A Progress Report|We present a model of natural language use meant to encompass the language-specific aspects of understanding and production. The model is motivated by the pervasiveness of nongenerative language, by the desirability of a language analyzer ana a language production mechanism to share their knowledge, and the advantages of knowledge engineering features such as ease of extention and modification. This model has been used as the basis for PHRAN, a language analyzer, and PHRED, a language production mechanism. We have implemented both these systems using a common knowledge base we have produced versions of PHRAN that understand Spanish and Chinese with only changing the knowledge base and not modifying the program and we have implemented PHRAN using the query language of a conventional relational data base system, and compared the performance of this system to a conventional LISP implementation.|Robert Wilensky","13790|IJCAI|1981|The Interaction with Incomplete Knowledge Bases A Formal Treatment|Some formal representation Issues underlying the Interaction between an expert system and Its knowledge base are discussed. It is argued that a language that can refer both to the application domain and to the state of the knowledge base is required to specify and to question an incomplete knowledge base. A formal logical language with this ability is presented and its semantics and proof theory are defined. It is then shown how this language must be used to interact with the knowledge base.|Hector J. Levesque","14846|IJCAI|1991|Parsing  Parsimonious Covering Abduction in Logical Form Generation|Many researchers believe that certain aspects of natural language processing, such as word sense disambiguation and plan recognition in stories, constitute abductive inferences. We have been working with a specific model of abduction, called parsimonious covering, applied in diagnostic problem solving, word sense disambiguation and logical form generation in some restricted settings. Diagnostic parsimonious covering has been extended into a dual-route model to account for syntactic and semantic aspects of natural language. The two routes of covering are integrated by defining \"open class\" linguistic concepts, aiding each other. The diagnostic model has dealt with sets, while the extended version, where syntactic considerations dictate word order, deals with sequences of linguistic concepts. Here we briefly describe the original model and the extended version, and briefly characterize the notions of covering and different criteria of parsimony. Finally we examine the question of whether parsimonious covering can serve as a general framework for parsing.|Venu Dasigi","14140|IJCAI|1985|SAPHIR  RESEDA A New Approach to Intelligent Data Base Access|This paper describes a transportable natural language interface to databases, augmented with a knowledge base and inference techniques. The inference mechanism, based on a classical expert system's type of approach, allows, when needed, to automatically convert an Input query into another one which is \"semantically close\". According to RESEDAS theory, \"semantically close\" means that the answer to the transformed query Implies what could have been the answer to the original question. The presented system Integrates natural language processing, expert system and knowledge representation technology to provide a cooperative database access.|Bernard Euzenat,Bernard Normier,Antoine Ogonowski,Gian Piero Zarri","13594|IJCAI|1977|Conceptual Analysis of Noun Groups in English|An expectation-based system, NGP, for parsing English noun groups into the Conceptual Dependency representation is described. The system is a part of ELI (English Language Interpreter) which is used as the front end to several natural language understanding systems and is capable of handling a wide range of sentences of considerable complexity. NGP processes the input from left to right, one word at a time, using linguistic and world knowledge to find the meaning of a noun group. Dictionary entries for individual words contain much of the program's knowledge. In addition, a limited ability for the handling of slightly incorrect sentences and unknown words is incorporated.|Anatole Gershman"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80274|VLDB|2003|Temporal Slicing in the Evaluation of XML Queries|As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, XQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a XQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.|Dengfeng Gao,Richard T. Snodgrass","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80236|VLDB|2002|XMark A Benchmark for XML Data Management|While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.|Albrecht Schmidt 0002,Florian Waas,Martin L. Kersten,Michael J. Carey,Ioana Manolescu,Ralph Busse","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","80293|VLDB|2003|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.|Christoph Koch"],["15961|IJCAI|2003|Gaussian Process Models of Spatial Aggregation Algorithms|Multi-level spatial aggregates are important for data mining in a variety of scientific and engineering applications, from analysis of weather data (aggregating temperature and pressure data into ridges and fronts) to performance analysis of wireless systems (aggregating simulation results into configuration space regions exhibiting particular performance characteristics). In many of these applications, data collection is expensive and time consuming, so effort must be focused on gathering samples at locations that will be most important for the analysis. This requires that we be able to functionally model a data mining algorithm in order to assess the impact of potential samples on the mining of suitable spatial aggregates. This paper describes a novel Gaussian process approach to modeling multi-layer spatial aggregation algorithms, and demonstrates the ability of the resulting models to capture the essential underlying qualitative behaviors of the algorithms. By helping cast classical spatial aggregation algorithms in a rigorous quantitative framework, the Gaussian process models support diverse uses such as directed sampling, characterizing the sensitivity of a mining algorithm to particular parameters, and understanding how variations in input data fields percolate up through a spatial aggregation hierarchy.|Naren Ramakrishnan,Christopher Bailey-Kellogg","80833|VLDB|2007|Efficient Bulk Deletes for Multi Dimensionally Clustered Tables in DB|In data warehousing applications, the ability to efficiently delete large chunks of data from a table is very important. This feature is also known as Rollout or Bulk Deletes. Rollout is generally carried out periodically and is often done on more than one dimension or attribute. The ability to efficiently handle the updates of RID indexes while doing Rollouts is a well known problem for database engines and its solution is very important for data warehousing applications. DB UDB V. introduced a new physical clustering scheme called Multi Dimensional Clustering (MDC) which allows users to cluster data in a table on multiple attributes or dimensions. This is very useful for query processing and maintenance activities including deletes. Subsequently, an enhancement was incorporated in DB UDB Viper  which allows for very efficient online rollout of data on dimensional boundaries even when there are a lot of secondary RID indexes defined on the table. This is done by the asynchronous updates of these RID indexes in the background while allowing the delete to commit and the table to be accessed. This paper details the design of MDC Rollout and the challenges that were encountered. It discusses some performance results which show order of magnitude improvements using it and the lessons learnt.|Bishwaranjan Bhattacharjee,Timothy Malkemus,Sherman Lau,Sean Mckeough,Jo-Anne Kirton,Robin Von Boeschoten,John Kennedy","79893|VLDB|1977|Psychological Issues in Data Base Management|In order to maximize total data base system effectiveness, it is becoming increasingly important to attend to psychological issues. This paper identifies issues in data base design and management where psychological considerations are likely to play a significant role. Specific studies that address some of these issues are reviewed. Next, a number of areas of basic psychological research that are likely to be of use to data base system designers are indicated. Finally, general recommendations and considerations for data base interfaces are given.|John C. Thomas","80408|VLDB|2004|GPX Interactive Mining of Gene Expression Data|Discovering co-expressed genes and coherent expression patterns in gene expression data is an important data analysis task in bioinformatics research and biomedical applications. Although various clustering methods have been proposed, two tough challenges still remain on how to integrate the users' domain knowledge and how to handle the high connectivity in the data. Recently, we have systematically studied the problem and proposed an effective approach . In this paper, we describe a demonstration of GPX (for Gene Pattern eXplorer), an integrated environment for interactive exploration of coherent expression patterns and co-expressed genes in gene expression data. GPX integrates several novel techniques, including the coherent pattern index graph, a gene annotation panel, and a graphical interface, to adopt users' domain knowledge and support explorative operations in the clustering procedure. The GPX system as well as its techniques will be showcased, and the progress of GPX will be exemplified using several real-world gene expression data sets.|Daxin Jiang,Jian Pei,Aidong Zhang","80137|VLDB|1994|Efficient and Effective Clustering Methods for Spatial Data Mining|Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also develop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms. Furthermore, experiments conducted to compare the performance of CLARANS with that of existing clustering methods show that CLARANS is the most efficient.|Raymond T. Ng,Jiawei Han","16833|IJCAI|2009|Early Prediction on Time Series A Nearest Neighbor Approach|In this paper, we formulate the problem of early classification of time series data, which is important in some time-sensitive applications such as health-informatics. We introduce a novel concept of MPL (Minimum Prediction Length) and develop ECTS (Early Classification on Time Series), an effective -nearest neighbor classification method. ECTS makes early predictions and at the same time retains the accuracy comparable to that of a NN classifier using the full-length time series. Our empirical study using benchmark time series data sets shows that ECTS works well on the real data sets where NN classification is effective.|Zhengzheng Xing,Jian Pei,Philip S. Yu","80301|VLDB|2003|Efficacious Data Cube Exploration by Semantic Summarization and Compression|Data cube is the core operator in data warehousing and OLAP. Its efficient computation, maintenance, and utilization for query answering and advanced analysis have been the subjects of numerous studies. However, for many applications, the huge size of the data cube limits its applicability as a means for semantic exploration by the user. Recently, we have developed a systematic approach to achieve efficacious data cube construction and exploration by semantic summarization and compression. Our approach is pivoted on a notion of quotient cube that groups together structurally related data cube cells with common (aggregate) measure values into equivalence classes. The equivalence relation used to partition the cube lattice preserves the roll-updrill-down semantics of the data cube, in that the same kind of explorations can be conducted in the quotient cube as in the original cube, between classes instead of between cells. We have also developed compact data structures for representing a quotient cube and efficient algorithms for answering queries using a quotient cube for its incremental maintenance against updates. We have implemented SOCQET, a prototype data warehousing system making use of our results on quotient cube. In this demo, we will demonstrate () the critical techniques of building a quotient cube () use of a quotient cube to answer various queries and to support advanced OLAP () an empirical study on the effectiveness and efficiency of quotient cube-based data warehouses and OLAP () a user interface for visual and interactive OLAP and () SOCQET, a research prototype data warehousing system integrating all the techniques. The demo reflects our latest research results and may stimulate some interesting future studies.|Laks V. S. Lakshmanan,Jian Pei,Yan Zhao","80347|VLDB|2003|Data Bubbles for Non-Vector Data Speeding-up Hierarchical Clustering in Arbitrary Metric Spaces|To speed-up clustering algorithms, data summarization methods have been proposed, which first summarize the data set by computing suitable representative objects. Then, a clustering algorithm is applied to these representatives only, and a clustering structure for the whole data set is derived, based on the result for the representatives. Most previous methods are, however, limited in their application domain. They are in general based on sufficient statistics such as the linear sum of a set of points, which assumes that the data is from a vector space. On the other hand, in many important applications, the data is from a metric non-vector space, and only distances between objects can be exploited to construct effective data summarizations. In this paper, we develop a new data summarization method based only on distance information that can be applied directly to non-vector data. An extensive performance evaluation shows that our method is very effective in finding the hierarchical clustering structure of non-vector data using only a very small number of data summarizations, thus resulting in a large reduction of runtime while trading only very little clustering quality.|Jianjun Zhou,Jörg Sander","79919|VLDB|1978|Privacy and Security of Data Communications and Data Bases|Computer security deals with the managerial procedures and technological safeguards applied to computer hardware, software, and data to assure against accidental or deliberate unauthorized access to and the dissemination of computer system data. Computer privacy, on the other hand, is concerned with the moral and legal requirements to protect data from unauthorized access and dissemination. The issues involved in computer privacy are therefore political decisions regarding who may have access to what and who may disseminate what, whereas the issues involved in computer security are procedures and safeguards for enforcing the privacy decisions. The motivations for security and privacy can be found in the desire for secrecy in military affairs, for nondisclosure in industrial applications, and for information-sharing in modern society. These motivations have become particularly acute where computers are used since computers play a major and important role in processing and storing of information. This paper discusses computer privacy and data security. Other issues such as operational security, physical security, hardware security, and operating system security are discussed in ,, and will not be included here.|David K. Hsiao,Douglas S. Kerr,Stuart E. Madnick","80423|VLDB|2004|High-Dimensional OLAP A Minimal Cubing Approach|Data cube has been playing an essential role in fast OLAP (online analytical processing) in many multi-dimensional data warehouses. However, there exist data sets in applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, e.g., over  dimensions, and moderate size, e.g., around  tuples. No feasible data cube can be constructed with such data sets. In this paper we will address the problem of developing an efficient algorithm to perform OLAP on such data sets. Experience tells us that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Based on this observation, we propose a novel method that computes a thin layer of the data cube together with associated value-list indices. This layer, while being manageable in size, will be capable of supporting flexible and fast OLAP operations in the original high dimensional space. Through experiments we will show that the method has IO costs that scale nicely with dimensionality. Furthermore, the costs are comparable to that of accessing an existing data cube when full materialization is possible.|Xiaolei Li,Jiawei Han,Hector Gonzalez"],["15079|IJCAI|1993|On the Hardness of Approximate Reasoning|Many AI problems, when formulated, reduce to evaluating the probability that a prepositional expression is true. In this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an approximation to this probability. We consider various methods used in approximate reasoning such as computing degree of belief and Bayesian belief networks, as well as reasoning techniques such as constraint satisfaction and knowledge compilation, that use approximation to avoid computational difficulties, and reduce them to model-enumeration problems over a propositional domain. We prove that counting satisfying assignments of propositional languages is intractable even for Horn and monotone formulae, and even when the size of clauses and number of occurrences of the variables are extremely limited. This should he contrasted with the case of deductive reasoning, where Horn theories and theories with binary clauses are distinguished by the existence of linear time satisfiability al gorithms. What is even more surprising is that, as we show, even approximating the number of satisfying assignments (i.e., \"approximating\" approximate reasoning), is intractable for most of these restricted theories. We also identify some restricted classes of propositional formulae for which we develop efficient algorithms for counting satisfying assignments.|Dan Roth","14993|IJCAI|1993|Satisfiability of Boolean Formulas over Linear Constraints|Testing the satisfiability of a Boolean formula over linear constraints is not a simple matter. Existing AI systems handle that kind of problems with a general proof method for their Boolean parts and a separate module for combining linear constraints. On the contrary, traditional operations research methods need the problem to be transformed, and solved with a Mixed Integer Linear Programming algorithm. Both approaches appear to be improvable if no early separation is introduced between the logical and numerical parts. In this case, combinatorial explosion can be dramatically reduced thanks to efficient looking-ahead techniques and learning methods. Indeed, propagating bounds following the initial formula gives precious information. Besides, an especially tight linear relaxation can be driven from the formula, and allows a Simplex algorithm to make a good test for satisfiability. Finally, these two looking-ahead methods can be easily coupled for more efficiency and completed by local enumeration. Moreover, discovering a good failure explanation is relatively easy in the proposed framework. By \"learning\" these explanations, it is possible to prune important redundant parts of the search tree.|Henri Beringer,Bruno De Backer","15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell","14534|IJCAI|1987|Guiding Constructive Induction for Incremental Learning from Examples|LAIR is a system that incrementally learns conjunctive concept descriptions from positive and negative examples. These concept descriptions are used to create and extend a domain theory that is applied, by means of constructive induction, to later learning tasks. Important issues for constructive induction are when to do it and how to control it LAIR demonstrates how constructive induction can be controlled by () reducing it to simpler operations, () constraining the simpler operations to preserve relative correctness, () doing deductive inference on an as-needed basis to meet specific information requirements of learning subtasks, and () constraining the search space by subtask-dependent constraints.|Larry Watanabe,Renee Elio","16684|IJCAI|2007|Constructing New and Better Evaluation Measures for Machine Learning|Evaluation measures play an important role in machine learning because they are used not only to compare different learning algorithms, but also often as goals to optimize in constructing learning models. Both formal and empirical work has been published in comparing evaluation measures. In this paper, we propose a general approach to construct new measures based on the existing ones, and we prove that the new measures are consistent with, and finer than, the existing ones. We also show that the new measure is more correlated to RMS (Root Mean Square error) with artificial datasets. Finally, we demonstrate experimentally that the greedy-search based algorithm (such as artificial neural networks) trained with the new and finer measure usually can achieve better prediction performance. This provides a general approach to improve the predictive performance of existing learning algorithms based on greedy search.|Jin Huang,Charles X. Ling","17066|IJCAI|2009|Learning Probabilistic Hierarchical Task Networks to Capture User Preferences|While much work on learning in planning focused on learning domain physics (i.e., action models), and search control knowledge, little attention has been paid towards learning user preferences on desirable plans. Hierarchical task networks (HTN) are known to provide an effective way to encode user prescriptions about what constitute good plans. However, manual construction of these methods is complex and error prone. In this paper, we propose a novel approach to learning probabilistic hierarchical task networks that capture user preferences by examining user-produced plans given no prior information about the methods (in contrast, most prior work on learning within the HTN framework focused on learning \"method preconditions\"--i.e., domain physics--assuming that the structure of the methods is given as input). We will show that this problem has close parallels to the problem of probabilistic grammar induction, and describe how grammar induction methods can be adapted to learn task networks. We will empirically demonstrate the effectiveness of our approach by showing that task networks we learn are able to generate plans with a distribution close to the distribution of the user-preferred plans.|Nan Li,Subbarao Kambhampati,Sung Wook Yoon","15762|IJCAI|2001|Genetic Algorithm based Selective Neural Network Ensemble|Neural network ensemble is a learning paradigm where several neural networks are jointly used to solve a problem. In this paper, the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural networks is analyzed, which reveals that ensembling a selective subset of individual networks is superior to ensembling all the individual networks in some cases. Therefore an approach named GASEN is proposed, which trains several individual neural networks and then employs genetic algorithm to select an optimum subset of individual networks to constitute an ensemble. Experimental results show that, comparing with a popular ensemble approach, i.e. averaging all, and a theoretically optimum selective ensemble approach, i.e. enumerating, GASEN has preferable performance in generating ensembles with strong generalization ability in relatively small computational cost.|Zhi-Hua Zhou,Jianxin Wu,Yuan Jiang,Shifu Chen","15308|IJCAI|1995|Theoretical Analysis of Davis-Putnam Procedure and Propositional Satisfiability|This paper presents a statistical analysis of the Davis-Putnam procedure and propositional satisfiability problems (SAT). SAT has been researched in AI because of its strong relationship to automated reasoning and recently it is used as a benchmark problem of constraint satisfaction algorithms. The Davis-Putnam procedure is a well-known satisfiability checking algorithm based on tree search technique. In this paper, I analyze two average case complexities for the Davis-Putnam procedure, the complexity for satisfiability checking and the complexity for finding all solutions. I also discuss the probability of satisfiability. The complexities and the probability strongly depend on the distribution of formulas to be tested and I use the fixed clause length model as the distribution model. The result of the analysis coincides with the experimental result well.|Nobuhiro Yugami","15275|IJCAI|1995|Local Learning in Probabilistic Networks with Hidden Variables|Probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks Because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process, thereby significantly improving the learning rate Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.|Stuart J. Russell,John Binder,Daphne Koller,Keiji Kanazawa","16199|IJCAI|2005|A Greedy Approach to Establish Singleton Arc Consistency|In this paper, we propose a new approach to establish Singleton Arc Consistency (SAC) on constraint networks. While the principle of existing SAC algorithms involves performing a breadth-first search up to a depth equal to , the principle of the two algorithms introduced in this paper involves performing several runs of a greedy search (where at each step, arc consistency is maintained). It is then an original illustration of applying inference (i.e. establishing singleton arc consistency) by search. Using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. Further-more, both space and time complexities are quite competitive.|Christophe Lecoutre,Stéphane Cardon"],["15126|IJCAI|1995|Exploiting Structure in Policy Construction|Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods.|Craig Boutilier,Richard Dearden,Moisés Goldszmidt","16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","15453|IJCAI|1999|Computing Near Optimal Strategies for Stochastic Investment Planning Problems|We present efficient techniques for computing near optimal strategies for a class of stochastic commodity trading problems modeled as Markov decision processes (MDPs). The process has a continuous state space and a large action space and cannot be solved efficiently by standard dynamic programming methods. We exploit structural properties of the process, and combine it with Monte-Carlo estimation techniques to obtain novel and efficient algorithms that closely approximate the optimal strategies.|Milos Hauskrecht,Gopal Pandurangan,Eli Upfal","15470|IJCAI|1999|Efficient Reinforcement Learning in Factored MDPs|We present a provably efficient and near-optimal algorithm for reinforcement learning in Markov decision processes (MDPs) whose transition model can be factored as a dynamic Bayesian network (DBN). Our algorithm generalizes the recent E algorithm of Kearns and Singh, and assumes that we are given both an algorithm for approximate planning, and the graphical structure (but not the parameters) of the DBN. Unlike the original E algorithm, our new algorithm exploits the DBN structure to achieve a running time that scales polynomially in the number of parameters of the DBN, which may be exponentially smaller than the number of global states.|Michael J. Kearns,Daphne Koller","15471|IJCAI|1999|A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes|An issue that is critical for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or even infinite state spaces, traditional planning and reinforcement learning algorithms are often inapplicable, since their running time typically scales linearly with the state space size. In this paper we present a new algorithm that, given only a generative model (simulator) for an arbitrary MDP, performs near-optimal planning with a running time that has no dependence on the number of states. Although the running time is exponential in the horizon time (which depends only on the discount factor  and the desired degree of approximation to the optimal policy), our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large, unstructured MDPs. Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs KMN.|Michael J. Kearns,Yishay Mansour,Andrew Y. Ng","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","15475|IJCAI|1999|Computing Factored Value Functions for Policies in Structured MDPs|Many large Markov decision processes (MDPs) can be represented compactly using a structured representation such as a dynamic Bayesian network. Unfortunately, the compact representation does not help standard MDP algorithms, because the value function for the MDP does not retain the structure of the process description. We argue that in many such MDPs, structure is approximately retained. That is, the value functions are nearly additive closely approximated by a linear function over factors associated with small subsets of problem features. Based on this idea, we present a convergent, approximate value determination algorithm for structured MDPs. The algorithm maintains an additive value function, alternating dynamic programming steps with steps that project the result back into the restricted space of additive functions. We show that both the dynamic programming and the projection steps can be computed efficiently, despite the fact that the number of states is exponential in the number of state variables.|Daphne Koller,Ronald Parr","15972|IJCAI|2003|Modular self-organization for a long-living autonomous agent|The aim of this paper is to provide a sound framework for addressing a difficult problem the automatic construction of an autonomous agent's modular architecture. We briefly present two apparently uncorrelated frameworks Autonomous planning through Markov Decision Processes and Kernel Clustering. Our fundamental idea is that the former addresses autonomy whereas the latter allows to tackle self-organizing issues. Relying on both frameworks, we show that modular self-organization can be formalized as a clustering problem in the space of MDPs. We derive a modular self-organizing algorithm in which an autonomous agent learns to efficiently spread n planning problems over m initially blank modules with m  n.|Bruno Scherrer","15872|IJCAI|2003|Automated Generation of Understandable Contingency Plans|Markov Decision Processes (MDPs) and contingency planning (CP) are two widely used approaches to planning under uncertainty. MDPs are attractive because the model is extremely general and because many algorithms exist for deriving optimal plans. In contrast, CP is normally performed using heuristic techniques that do not guarantee optimality, but the resulting plans are more compact and more understandable. The inability to present MDP policies in a clear, intuitive way has limited their applicability in some important domains. We introduce an anytime algorithm for deriving contingency plans that combines the advantages of the two approaches.|Max Horstmann,Shlomo Zilberstein"],["16534|IJCAI|2007|An Efficient Protocol for Negotiation over Multiple Indivisible Resources|We study the problem of autonomous agents negotiating the allocation of multiple indivisible resources. It is difficult to reach optimal outcomes in bilateral or multi-lateral negotiations over multiple resources when the agents' preferences for the resources are not common knowledge. Self-interested agents often end up negotiating inefficient agreements in such situations. We present a protocol for negotiation over multiple indivisible resources which can be used by rational agents to reach efficient outcomes. Our proposed protocol enables the negotiating agents to identify efficient solutions using systematic distributed search that visits only a subspace of the whole solution space.|Sabyasachi Saha,Sandip Sen","14282|IJCAI|1985|The Use of Multiple Problem Decompositions in Time Constrained Planning Tasks|Problems requiring the synthesis of a collection of plans accomplishing distinct (but possibly related) goals has received increasing attention within Al. Such problems are typically formulated as multi-agent planning problems, emphasizing a problem decomposition wherein individual agents assume responsibility for the generation of individual plans while taking into account the goals and beliefs of other agents in the system. One consequence of such a problem decomposition is a simplified view of resource allocation that assumes avoidance of conflicts to be the sole concern. The validity of this assumption comes into question in time constrained problem domains requiring the allocation of multiple, shared resources. In job shop scheduling, for example, where sequences of manufacturing operations must be determined and scheduled for multiple orders, it is necessary to consider much more than availability to efficiently allocate resources over time. We argue that in such domains, an ability to reason from both resource-based and agent-based perspectives is essential to appropriate consideration of all domain constraints.|Stephen F. Smith,Peng Si Ow","16893|IJCAI|2009|Dynamic Configuration of Agent Organizations|It is useful to impose organizational structure over multiagent coalitions. Hierarchies, for instance, allow for compartmentalization of tasks if organized correctly, tasks in disjoint subtrees of the hierarchy may be performed in parallel. Given a notion of the way in which a group of agents need to interact, the Dynamic Distributed Multiagent Hierarchy Generation (DynDisMHG) problem is to determine the best hierarchy that might expedite the process of coordination. This paper introduces a distributed algorithm, called Mobed, for both constructing and maintaining organizational agent hierarchies, enabling exploitation of parallelism in distributed problem solving. The algorithm is proved correct and it is shown that individual additions of agents to the hierarchy will run in an amortized linear number of rounds. The hierarchies resulting after perturbations to the agent coalition have constant-bounded edit distance, making Mobed very well suited to highly dynamic problems.|Evan Sultanik,Robert N. Lass,William C. Regli","16069|IJCAI|2005|Fast convergence to satisfying distributions|We investigate an environment where self-interested agents have to find high-quality service resources. Agents have common knowledge about resources which are able to provide these services. The performance of resources is measured by the satisfaction obtained by agents using them. The performance of a resource depends on its intrinsic capability and its current load. We use a satisfying rather than an optimizing framework, where agents are content to receive service quality above a threshold. We introduce a formal framework to characterize the convergence of agents to a state where each agent is satisfied with the performance of the service it is currently using. We analyzed the convergence behavior of such a system and identified a mechanism to speed up convergence.|Teddy Candale,Sandip Sen","16989|IJCAI|2009|Collaborative Multi Agent Physical Search with Probabilistic Knowledge|This paper considers the setting wherein a group of agents (e.g., robots) is seeking to obtain a given tangible good, potentially available at different locations in a physical environment. Traveling between locations, as well as acquiring the good at any given location consumes from the resources available to the agents (e.g., battery charge). The availability of the good at any given location, as well as the exact cost of acquiring the good at the location is not fully known in advance, and observed only upon physically arriving at the location. However, apriori probabilities on the availability and potential cost are provided. Given such as setting, the problem is to find a strategyplan that maximizes the probability of acquiring the good while minimizing resource consumption. Sample applications include agents in exploration and patrol missions, e.g., rovers on Mars seeking to mine a specific mineral. Although this model captures many real world scenarios, it has not been investigated so far. We focus on the case where locations are aligned along a path, and study several variants of the problem, analyzing the effects of communication and coordination. For the case that agents can communicate, we present a polynomial algorithm that works for any fixed number of agents. For noncommunicating agents, we present a polynomial algorithm that is suitable for any number of agents. Finally, we analyze the difference between homogeneous and heterogeneous agents, both with respect to their allotted resources and with respect to their capabilities.|Noam Hazon,Yonatan Aumann,Sarit Kraus","15699|IJCAI|2001|Identifying the Scope of Modeling for Time-Critical Multiagent Decision- Making|Decision-making in multiagent settings requires significant computational resources. Agents need to model each other to decide how to coordinate - this sometimes may require solving nested models of many other agents and may be impractical to perform in an acceptable time. In this paper, we investigate ways in which the agents can be equipped with flexible decision-making procedures to allow multiagent decision-making under time pressure. One of the techniques we implemented uses iterative deepening algorithm guided by performance profiles generated offline. When the interaction involves many agents, the algorithm iteratively enhances the quality of coordinated decision-making by incrementally adding the levels of nesting considered, but with the additional penalty of increased running time. To identify the appropriate scope of modeling online we use the concept of urgency which represents cost of delaying decisions. We validate our framework with experiments in a simulated anti-air defense domain. The contribution of our framework is that it endows our autonomous agents with flexibility to cope with time pressure in complex multiagent settings.|Sanguk Noh,Piotr J. Gmytrasiewicz","16637|IJCAI|2007|An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource Allocation Mechanism|Multiagent distributed resource allocation requires that agents act on limited, localized information with minimum communication overhead in order to optimize the distribution of available resources. When requirements and constraints are dynamic, learning agents may be needed to allow for adaptation. One way of accomplishing learning is to observe past outcomes, using such information to improve future decisions. When limits in agents' memory or observation capabilities are assumed, one must decide on how large should the observation window be. We investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems, namely dispersion games. We show by numerical experiments over a specific dispersion game (the Minority Game) that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. We then provide an information-theoretic explanation for the observed behaviors, showing that a downward causation effect takes place.|Ricardo M. Araujo,Luís C. Lamb","15967|IJCAI|2003|Minimally intrusive negotiating agents for resource sharing|We study the problem of agents negotiating periods of time during which they can have use of resources, thus allowing for the sharing of resources. We define a multi-stage negotiation framework where agents, in order to obtain resources, step through a sequence of stages, each characterised by an increased chance of a mutually agreeable deal but at the price of disclosing more and more information. In the sequence, the agents may agree to move to the next stage if the previous stage fails to produce a deal amongst them. In this paper, we concentrate on two early negotiation stages, characterised by minimal disclosure of information. Thus, the agents negotiating at these stages can be thought of as \"minimally intrusive\".|Fariba Sadri,Francesca Toni,Paolo Torroni","15735|IJCAI|2001|Reflective Negotiating Agents for Real-Time Multisensor Target Tracking|In this paper we describe a multiagent system in which agents negotiate to allocate resources and satisfy constraints in a real-time environment of multisensor target tracking. The agents attempt to optimize the use of their own consumable resources while adhering to the global goal, i.e., accurate and effective multisensor target tracking. Agents negotiate based on different strategies which are selected and instantiated using case-based reasoning (CBR). Agents are also fully reflective in that they are aware of all their resources including system-level ones such as CPU allocation, and this allows them to achieve real-time behavior. We focus our discussion on multisensor target racking, case-based negotiation, and real-time behavior, and present experimental results comparing our methodology to ones using either no negotiation or using a static negotiation protocol.|Leen-Kiat Soh,Costas Tsatsoulis","15953|IJCAI|2003|Distributed Patient Scheduling in Hospitals|Patient scheduling in hospitals is a highly complex task. Hospitals have a distributed organisational structure being divided into several autonomous wards and ancillary units. Moreover, the treatment process is dynamic (information about the patients' diseases often varies during treatments, causing changes in the treatment process). Current approaches are insufficient because they either focus only on the single ancillary units, and therefore do not consider the entire treatment process of the patients, or they do not account for the distribution and dynamics of the patient scheduling problem. Therefore, we propose an agent based approach in which the patients and hospital resources are modelled as autonomous agents with their own goals, reflecting the decentralised structures in hospitals. In this multi-agent system, the patient agents compete over the scarce hospital resources. Moreover to improve the overall solution, the agents then negotiate with one another. To this end, a market mechanism is described, in which each self interested agent tries to improve its own situation. In particular we focus on how the agents can calculate demand and supply prices based upon their current schedule. Further, an evaluation of first results of the proposed method is given.|Torsten O. Paulussen,Nicholas R. Jennings,Keith S. Decker,Armin Heinzl"],["13988|IJCAI|1983|Reasoning in Time and Space|This paper describes a new approach to representing and reasoning with temporal and spatial information. A wide variety of temporal and spatial specifications can be converted into linear inequalities relating the midpoints of events or boundary surfaces of objects respectively, Linear programming is then used to represent these constraints and perform deductions. The temporal information is modularized into semantically related clusters of events each with its own tableau and related to each other by a reference frame transformation. A similar grouping can be done for objects making the system computationally efficient. For temporal reasoning, the system is formally adequate except for linguistic fuzziness. For geometric reasoning, polyhedra can be represented by allowing paramvtiization. The uniformity of the time and space representation makes this approach particularly attractive.|Jitendra Malik,Thomas O. Binford","15143|IJCAI|1995|Device Representation and Reasoning with Affective Relations|Device representation and reasoning with affective relations occupies a middle ground between classical model-based diagnosis and heuristic expert systems. A device is modeled by specifying a set of diagnostically motivated affective relations among its components. Reasoning is then performed by a set of inference rules that reason with the model to propagate symptoms through the components. Representation and reasoning with affective relations extends several benefits of classical model-based diagnosis--the model as a unifying framework for knowledge, methodical coverage of the domain, and diagnostic reasoning based on equipment design and causality--to a class of problems where classical model-based diagnosis cannot be applied because the required models cannot be reasonably obtained or represented. Our work evolved from our redesign of a heuristic expert system for monitoring long-distance telephone switching systems, and is applicable to highly complex self-checking systems.|James M. Crawford,Daniel Dvorak,Diane J. Litman,Anil Mishra,Peter F. Patel-Schneider","15671|IJCAI|2001|Executing Reactive Model-based Programs through Graph-based Temporal Planning|In the future, webs of unmanned air and space vehicles will act together to robustly perform elaborate missions in uncertain environments. We coordinate these systems by introducing a reactive model-based programming language (RMPL) that combines within a single unified representation the flexibility of embedded programming and reactive execution languages, and the deliberative reasoning power of temporal planners. The KIRK planning system takes as input a problem expressed as a RMPL program, and compiles it into a temporal plan network (TPN), similar to those used by temporal planners, but extended for symbolic constraints and decisions. This intermediate representation clarifies the relation between temporal planning and causal-link planning, and permits a single task model to be used for planning and execution. Such a unified model has been described as a holy grail for autonomous agents by the designers of the Remote AgentMuscettola et al., b.|Phil Kim,Brian C. Williams,Mark Abramson","14392|IJCAI|1987|Qualitative Kinematics A Framework|Qualitative spatial reasoning has seen little progress This paper attempts to explain why We provide a framework for qualitative kinematics (QK), qualitative spatial reasoning about motion We propose that no general-purpose, purely qualitative kinematics exists. We propose instead the MDPV model of spatial reasoning, which combines the power of diagrams with qualitative representations Next we propose connectivity as the organizing principle for kinematic state, and describe a set of basic inferences which every QK system must make. The framework's utility is illustrated by considering two programs, one finished and one in progress We end by discussing the research questions this framework raises.|Kenneth D. Forbus,Paul Nielson,Boi Faltings","14217|IJCAI|1985|SIGMA A Framework for Image Understanding - Integration of Bottom-Up and Top-Down Analysis|The framework and control structure of an image understanding system SIGMA arc presented. SIGMA consists of three experts Gelmetric Reasoning Expert (GRE) for spatial reasoning. Model Selection Expert (MSE) for appearance model selection, and Low level Vision Expert (LIVE) for knowledge-based picture processing. Tills paper mainly describes the control mechanism for the spatial reasoning by GRE, where bottom-up and top-down analyses are integrated into a unified reasoning process.|Takashi Matsuyama,Vincent Shang-Shouq Hwang","13878|IJCAI|1983|Planning Using a Temporal World Model|Current problem solving systems are constrained in their applicability by inadequate world models. We suggest a world model based on a temporal logic. This approach allows the problem solver to gather constraints on the ordering of actions without having to commit to an ordering when a conflict is detected. As such, it generalizes the work on nonlinear planning by Sacerdoti and Tate. In addition, it allows more general descriptions of actions that may occur simultaneously or overlap, and appears promising in supporting reasoning about external events and actions caused by other agents.|James F. Allen,Johannes A. G. M. Koomen","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15637|IJCAI|2001|A Multiagent System for Helping Urban Traffic Management|This paper describes the MASHGREEN DM prototype following three goals ) The identification and implementation of the tasks related to urban traffic control that use deep reasoning mechanisms as main tools for their resolution. The chosen model for explaining urban traffic behaviour is a qualitative model, which includes among its main features their low temporal and spatial computational costs. ) The definition of a functional architecture with soft real time constraints that integrates the developed tasks. A specilized component named agent, composed by four functional modules executes every task. These modules are the following ones communication protocols, methods (agent specilization), data space and control. This architecture verifies that the execution performances of every agent are not compromised by the inclusion of new agents in the system, or by the interactions due to the active system agents. ) The implementation of a prototype in a high performance computational architecture, a Beowulf computer system.|Luis A. García,Francisco Toledo","15025|IJCAI|1993|Non-Omniscient Belief as Context-Based Resoning|This paper describes a general framework for the formalization of monotonic reasoning about belief in a multiagent environment. The agents* beliefs are modeled as logical theories. The reasoning about their beliefs is formalized in still another theory, which we call the theory of the computer. The framework is used to model non-omniscient belief and shown to have many advantages. For instance, it allows for an exhaustive classification of the \"basic\" forms of non logical omniscience and for their \"composition\" into the structure of the system modeling multiagent omniscient belief.|Fausto Giunchiglia,Luciano Serafini,Enrico Giunchiglia,Marcello Frixione","16523|IJCAI|2007|Qualitative Temporal Reasoning about Vague Events|The temporal boundaries of many real-world events are inherently vague. In this paper, we discuss the problem of qualitative temporal reasoning about such vague events. We show that several interesting reasoning tasks, such as checking satisfiability, checking entailment, and calculating the best truth value bound, can be reduced to reasoning tasks in a well-known point algebra with disjunctions. Furthermore, we identify a maximal tractable subset of qualitative relations to support efficient reasoning.|Steven Schockaert,Martine De Cock,Etienne E. Kerre"]]},"title":{"entropy":6.025601181180194,"topics":["algorithm for, problem solving, constraint satisfaction, heuristic search, search for, for planning, learning for, search, search and, reinforcement learning, with, the problem, planning with, for problem, algorithm and, local search, planning, problem, constraint problem, heuristic for","the and, natural language, and, the, reasoning about, for and, for logic, logic and, logic, reasoning, reasoning and, description logic, for reasoning, knowledge representation, logic programming, language for, reasoning with, model for, for the, logic programs","artificial intelligence, neural networks, mobile robot, theorem proving, using, and image, sense disambiguation, pattern recognition, from, for recognition, learning from, for networks, word sense, word disambiguation, networks, information extraction, first order, for robot, for image, learning examples","data base, for system, data, for data, and data, system, database, the data, expert system, database system, data management, data model, for, and database, the system, data system, for database, management system, data streams, relational database","algorithm for, search for, problem solving, search and, heuristic search, the problem, for problem, search, and problem, heuristic for, the search, algorithm and, local search, search with, problem, and heuristic, the algorithm, search algorithm, the heuristic, search engine","learning for, and learning, learning, learning with, reinforcement learning, the learning, strategies for, case study, machine learning, learning algorithm, partially observable, learning through, for plans, global local, path planning, for games, for multiagent, strategy for, learning search, search games","natural language, language and, language for, model for, the language, model and, semantic and, the model, language, model, and natural, semantic for, programming language, language system, for natural, preliminary report, and analysis, theory and, the analysis, the semantic","logic programs, knowledge and, representation and, knowledge representation, for knowledge, the use, representation for, for programs, the knowledge, the representation, knowledge system, and programs, knowledge, its application, programs, automatic programs, representation system, representation, knowledge base, and use","theorem proving, method for, sense disambiguation, for disambiguation, word sense, word disambiguation, method and, three dimensional, structure and, and text, for sense, theorem for, theorem prover, and discovery, for text, method, random fields, and application, and perception, recognition method","mobile robot, for robot, using, using and, using for, combining and, feature for, using model, and learning, learning for, learning using, robot control, learning rules, for mobile, concept learning, learning, feature selection, rules for, using the, transfer learning","query for, for xml, efficient for, query database, query processing, query optimization, for processing, for queries, query and, query, over data, for information, xml, xml data, query the, web services, scheme for, queries, and information, and processing","and database, relational database, the database, for database, database system, database, for design, design and, performance system, performance for, conceptual for, data database, design system, database design, the design, schema matching, tool for, performance base, the architecture, high performance"],"ranking":[["16753|IJCAI|2007|A Heuristic Search Approach to Planning with Temporally Extended Preferences|Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.|Jorge A. Baier,Fahiem Bacchus,Sheila A. McIlraith","16803|IJCAI|2007|Using Learned Policies in Heuristic-Search Planning|Many current state-of-the-art planners rely on forward heuristic search. The success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. Such estimates are effective in guiding search for many domains, but there remain many other domains where current heuristics are inadequate to guide forward search effectively. In some of these domains, it is possible to learn reactive policies from example plans that solve many problems. However, due to the inductive nature of these learning techniques, the policies are often faulty, and fail to achieve high success rates. In this work, we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. We propose a simple approach that uses the policy to augment the states expanded during each search step. In particular, during each search node expansion, we add not only its neighbors, but all the nodes along the trajectory followed by the policy from the node until some horizon. Empirical results show that our proposed approach benefits both of the leveraged automated techniques, learning and heuristic search, outperforming the state-of-the-art in most benchmark planning domains.|Sung Wook Yoon,Alan Fern,Robert Givan","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","16229|IJCAI|2005|Combination of Local Search Strategies for Rotating Workforce Scheduling Problem|Rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work places (e.g. industrial plants). Solving this problem is of a high practical relevance. In this paper we propose the combination of tabu search with random walk and min conflicts strategy to solve this problem. Computational results for benchmark examples in literature and other real life examples show that combination of tabu search with random walk and min conflicts strategy improves the performance of tabu search for this problem. The methods proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedules.|Nysret Musliu","14779|IJCAI|1989|Abstraction in Problem Solving and Learning|Abstraction has proven to be a powerful tool for controlling the combinatorics of a problemsolving search. It is also of critical importance for learning systems. In this article we present, and evaluate experimentally, a general abstraction method -- impasse-driven abstraction - which is able to provide necessary assistance to both problem solving and learning. It reduces the amount of time required to solve problems, and the time required to learn new rules. In addition, it results in the acquisition of rules that are more general than would have otherwise been learned.|Amy Unruh,Paul S. Rosenbloom","13935|IJCAI|1983|A - An Efficient Near Admissible Heuristic Search Algorithm|The algorithm A* (Nilsson, ) presents two significant drawbacks. First, in seeking strict optimal solution paths it necessarily has high order of complexity. Second, the algorithm does not explicitly descriminate between the cost of a solution path and the cost of finding the solution path. To confront these problems we propose the algorithm AE, a generalization of A*. Instead of seeking an optimal solution, it seeks one which is within a factor (+e) of optimum (e  ). The basic idea is to avoid doing any search at all on most near optimal partial solutions by sticking to a small number of most fruitful paths. Various strategies for searching for near optimal partial solutions are discussed. Experimental results are presented indicating that A e has average complexity of lower order than A* and compares favorably to the related algorithm Af* (Pearl and Kim, ).|Malik Ghallab,Dennis G. Allard","13695|IJCAI|1981|Tuning of Search of the Problem Space for Geometry Proofs|In planning a proof, a student searches through a space of inferences leading forward from the givens of the problem and backward from the to-be-proven statement. One dimension of growth of expertise is that students become more tuned in the search of this problem space. This can be shown to result from the application of various learning operators to production embodiments of the inference rules. Rules are evaluated after the solution of a problem according to whether they led to or led away from the solution. Rules that contributed to a solution are strengthened and an attempt is made to formulate general versions of these rules that will apply in other situations. Rules that led away from the solution are weakened and a discrimination process is evoked to try to add features to the rules that will try to restrict them to the correct circumstances of application. Composition is a learning process that collapses successful sequences of rule operations into single macro-rule productions. There is also a process that converts the backward reasoning rules formed by composition into forward reasoning rules. The effect of these learning processes is to put into production conditions tests for problem features that are heunstically predictive of the rule's success.|John R. Anderson","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","14053|IJCAI|1983|Flexible Learning of Problem Solving Heuristics Through Adaptive Search|Noting that the methods employed by existing learning systems are often bound to the intended task domain and have little applicability outside that domain, this paper considers an alternative learning system design that offers greater flexibility without sacrificing performance. An operational prototype, constructed around a powerful adaptive search technique, is presented and applied to the problem of acquiring problem solving heuristics through experience. Some performance results obtained with the system in a poker betting domain are reported and compared with those of a previously investigated learning system in the same domain. It is seen that comparable levels of performance are achieved by the two systems, despite the latter's dependence on a considerable amount of domain specific knowledge for effective operation.|Stephen F. Smith","16777|IJCAI|2007|Recent Progress in Heuristic Search A Case Study of the Four-Peg Towers of Hanoi Problem|We integrate a number of new and recent advances in heuristic search, and apply them to the fourpeg Towers of Hanoi problem. These include frontier search, disk-based search, parallel processing, multiple, compressed, disjoint, and additive pattern database heuristics, and breadth-first heuristic search. New ideas include pattern database heuristics based on multiple goal states, a method to reduce coordination among multiple parallel threads, and a method for reducing the number of heuristic calculations. We perform the first complete breadth-first searches of the  and -disc fourpeg Towers of Hanoi problems, and extend the verification of \"presumed optimal solutions\" to this problem from  to  discs. Verification of the -disc problem is in progress.|Richard E. Korf,Ariel Felner"],["15973|IJCAI|2003|Non-Standard Reasoning Services for the Debugging of Description Logic Terminologies|Current Description Logic reasoning systems provide only limited support for debugging logically erroneous knowledge bases. In this paper we propose new non-standard reasoning services which we designed and implemented to pinpoint logical contradictions when developing the medical terminology DICE. We provide complete algorithms for unfoldable ACC-TBoxes based on minimisation of axioms using Boolean methods for minimal unsatisfiability-presening sub-TBoxes, and an incomplete bottom-up method for generalised incoherence-preserving terminologies.|Stefan Schlobach,Ronald Cornet","13947|IJCAI|1983|Logic Modelling of Cognitive Reasoning|Logic modelling is presented as an approach for exploring cognitive reasoning. The notion of mental construction and execution of propositional models is introduced. A model is constructed through inclusions and exclusions of assertions and assumptions about the task. A constructed model is executed in a logical control structure. Formal rules of inference are argued to be an essential feature of this architecture. A few examples are given for purpose of illustration.|Göran Hagert,\u2026ke Hansson","15791|IJCAI|2003|A Logic For Causal Reasoning|We introduce a logical formalism of irreflexive casual production relations that possesses both a standard monotonic semantics, and a natural nonmonotonic semantics. The formalism is shown to provide a complete characterization for the casual reasoning behind casual theories from McCain and Turner, . It is shown also that any causal relation is reducible to its Horn sub-relation with respect to the nonmonotonic semantics. We describe also a general correspondence between casual relations and abductive systems, which shows, in effect, that casual relations allow to express abductive reasoning. The results of the study seem to suggest causal production relations as a viable general framework for nonmonotonic reasoning.|Alexander Bochman","16542|IJCAI|2007|Using the Probabilistic Logic Programming Language P-log for Causal and Counterfactual Reasoning and Non-Naive Conditioning|P-log is a probabilistic logic programming language, which combines both logic programming style knowledge representation and probabilistic reasoning. In earlier papers various advantages of P-log have been discussed. In this paper we further elaborate on the KR prowess of P-log by showing that (i) it can be used for causal and counterfactual reasoning and (ii) it provides an elaboration tolerant way for non-naive conditioning.|Chitta Baral,Matt Hunsaker","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16866|IJCAI|2009|A Logic for Reasoning about Counterfactual Emotions|The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. An example of this kind of emotions is regret. In order to meet this objective, we exploit the well-known STIT logic Belnap et al.,  Horty, . STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable. In this work we study a decidable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions.|Emiliano Lorini,François Schwarzentruber","15759|IJCAI|2001|EPDL A Logic for Causal Reasoning|This paper presents an extended system EPDL of propositional dynamic logic by allowing a proposition as a modality for representing and specifying direct and indirect effects of actions in a unified logical structure. A set of causal logics based on the framework are proposed to model causal propagations through logical relevancy and iterated effects of causation. It is shown that these logics capture the basic properties of causal reasoning.|Dongmo Zhang,Norman Y. Foo","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang"],["16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","16726|IJCAI|2007|Combining Learning and Word Sense Disambiguation for Intelligent User Profiling|Understanding user interests from text documents can provide support to personalized information recommendation services. Typically, these services automatically infer the user profile, a structured model of the user interests, from documents that were already deemed relevant by the user. Traditional keyword-based approaches are unable to capture the semantics of the user interests. This work proposes the integration of linguistic knowledge in the process of learning semantic user profiles that capture concepts concerning user interests. The proposed strategy consists of two steps. The first one is based on a word sense disambiguation technique that exploits the lexical database WordNet to select, among all the possible meanings (senses) of a polysemous word, the correct one. In the second step, a nave Bayes approach learns semantic sense-based user profiles as binary text classifiers (user-likes and user-dislikes) from disambiguated documents. Experiments have been conducted to compare the performance obtained by keyword-based profiles to that obtained by sense-based profiles. Both the classification accuracy and the effectiveness of the ranking imposed by the two different kinds of profile on the documents to be recommended have been considered. The main outcome is that the classification accuracy is increased with no improvement on the ranking. The conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those documents whose classification score is close to the likesdislikes threshold (the items for which the classification is highly uncertain).|Giovanni Semeraro,Marco Degemmis,Pasquale Lops,Pierpaolo Basile","15850|IJCAI|2003|Improving Word Sense Disambiguation in Lexical Chaining|Previous algorithms to compute lexical chains suffer either from a lack of accuracy in word sense disambiguation (WSD) or from computational inefficiency. In this paper, we present a new linear-time algorithm for lexical chaining that adopts the assumption of one sense per discourse. Our results show an improvement over previous algorithms when evaluated on a WSD task.|Michel Galley,Kathleen McKeown","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin","13883|IJCAI|1983|Learning Word Meanings From Examples|This paper describes work in progress on a computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple English example stories. The central idea is an extension of Winston's (Winston ) program that learned the structural descriptions of blocks world scenes. In the new research, English verbs take the place of blocks world objects like ARCH and TOWER, with frame-based descriptions of causal relationships serving as the structural descriptions. Syntactic constraints derived from the parsing of story plots are used to drive an analogical matching procedure. Analogical matching gives a way to compare descriptions of known words to unknown words. The \"meaning\" of a new verb is learned by matching pan of the causal network description of a story precis containing the unknown word to a set of such descriptions derived from similar stories that contain only known words. The best match forges an assignment between objects and relations such that the unknown verb is matched to a known verb, with the assignment being guided by syntactic constraints. The causal network surrounding the unknown item is then used as a scaffolding to construct a network representing the use of the novel word in a particular context. Words (and their associated stories) that are \"best matches\" are grouped together into a similarity network, according to the match score.|Robert C. Berwick","13328|IJCAI|1971|Pattern Recognition by an Artificial Tactile Sense|This paper proposes an artificial tactile pattern recognition system, which combines recognition by touching the object surface with an artificial tactile sense and recognition by grasping the object with an artificial hand. The inspiration for this proposition was found in the function of the tactile sense of a human hand. The fundermental principle of artificial tactile pattern recognition is to process a stress distribution that the unknown object produces in the artificial tactile sense elements. In the proposed method, the -dimensional stress distribution is partitioned into a -dimensional peripheral pattern and a threshold decrement by analogy with threshold phenomena in the living body. The object surface is recognized as a sequence of the peripheral processings at each threshold decrement. A simple experiment classifying cylinders and square pillars was performed by the artificial hand with on-off switches instead of the pressure sense elements. As the result, a high reliability of recognition is obtained.|Gen-ichiro Kinoshita,Shuhei Aida,Masahiro Mori","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","79986|VLDB|1979|Query Processing in a Relational Database Management System|In this paper the various tactics for query processing in INGRESS are empirically evaluated on a test bed of sample queries.|Karel Youssefi,Eugene Wong","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80456|VLDB|2004|BilVideo Video Database Management System|A prototype video database management system, which we call BilVideo, is presented. BilVideo provides an integrated support for queries on spatio-temporal, semantic and low-level features (color, shape, and texture) on video data. BilVideo does not target a specific application, and thus, it can be used to support any application with video data. An example application, news archives search system, is presented with some sample queries.|\u2013zgür Ulusoy,Ugur Güdükbay,Mehmet Emin Dönderler,Ediz Saykol,Cemil Alper","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","13593|IJCAI|1977|A Deductive Question Answering System on Relational Data Bases|This paper describes a new formalization of a deductive question answering system on a relational data base using a theorem proving technique. A theorem proving procedure for a finite domain is investigated and a direct proof procedure based on substitutions of equivalent formulas which employs the breadth first search is introduced. The search strategy is then expanded to set operations of the relational algebra which are in corporated into the proof procedure in order to increase the data base search efficiency. Virtual relations are realized by means of introducing several axioms and utilizing the deductive capability of the logical system. Furthermore, a conditional domain is, introduced as one of the virtual domains and is used to give a relational view to a pseudo relational data base which can represent exceptional cases using some link information. A query transformation system called DBAP (Data Base Access Planner) which embodies those features is implemented in QJJSP.|Koichi Furukawa"],["13352|IJCAI|1971|Heuristic Search vs Exhaustive Search|A theorem proving system embodying a systematic search procedure is de sen bed. Al though the search spaces are usually infinite, and not even locally finite, the asymmetric way in which thev are generated results in a speed that is es ma- ted to be one to two orders of magnitude faster than the theorem provers of Quinlan and Hunt, and Chang and the problem-solver of Kikes, to which this systern has been compared extenslvely.|Laurent Siklóssy,Vesko Marinov","13388|IJCAI|1973|The Bandwidth Heuristic Search|By placing various restrictions on the heuristic estimator it is possible to constrain the heuristic search process to fit specific needs. This paper introduces a new restriction upon the heuristic, called the \"bandwidth\" condition, that enables the ordered search to better cope with time and space difficulties. In particular, the effect of error within the heuristic is considered in detail. Beyond this the bandwidth condition quite naturally allows for the extension of the heuristic search to MINMAX trees. The resulting game playing algorithm affords many desirable practical features not found in minimax based techniques, as well as maintaining the theoretical framework of ordered searchs. The development of this algorithm provides some additional insight to the general problem of searching game trees by showing that certain, somewhat surprising changes in the cost estimates are required to properly search the tree. Furthermore, the use of an ordered search of MINMAX trees brings about a rather provocative departure from the conventional approach to computer game playing.|Larry R. Harris","14322|IJCAI|1985|A Weighted Technique in Heuristic Search|As shown in , we examine search as a statistic sampling process. Based on some statistical inference method the probability that a subtree in search tree contains the goal can be decided. Thus some weight is in tentionaly added to the evaluation function of those nodes which are unlikely in the solution path so that the search will concentrate on the most promising path. It results in a new weighted algorithm-WSA. Tn a uniform m-ary tree, we show that a goal can be found by WSA in the polynomial time, although the computational complexity of A (or A*) may be O(e) for searching the same space. Where N is the depth at which the goal is located.|Bo Zhang,Ling Zhang","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","14655|IJCAI|1989|Parallel Iterative A Search An Admissible Distributed Heuristic Search Algorithm|In this paper, a distributed heuristic search algorithm is presented. We show that the algorithm is admissible and give an informal analysis of its load balancing, scalability, and speedup. A flow-shop scheduling problem has been implemented on a BBN Butterfly Multicomputer using up to  processors to empirically test this algorithm. From our experiments, this algorithm is capable of achieving almost linear speedup on a large number of processors with a relatively small problem size.|Shie-rei Huang,Larry S. Davis","13935|IJCAI|1983|A - An Efficient Near Admissible Heuristic Search Algorithm|The algorithm A* (Nilsson, ) presents two significant drawbacks. First, in seeking strict optimal solution paths it necessarily has high order of complexity. Second, the algorithm does not explicitly descriminate between the cost of a solution path and the cost of finding the solution path. To confront these problems we propose the algorithm AE, a generalization of A*. Instead of seeking an optimal solution, it seeks one which is within a factor (+e) of optimum (e  ). The basic idea is to avoid doing any search at all on most near optimal partial solutions by sticking to a small number of most fruitful paths. Various strategies for searching for near optimal partial solutions are discussed. Experimental results are presented indicating that A e has average complexity of lower order than A* and compares favorably to the related algorithm Af* (Pearl and Kim, ).|Malik Ghallab,Dennis G. Allard","13286|IJCAI|1969|Concepts and Methods for Heuristic Search|The transformation or derivation problem treated by most \"problem-solving\" programs is expressed in a formal notation, and various methods for \"problem-solving\" are reviewed. The conventional search tree is generalized in to a search lattice which can accomodate multiple-input operators, e.g. resolution. The paper argues that descriptions of heuristic methods can be significantly compacted if a higher degree of formalization is used. This point is illustrated with two practical examples.|Erik Sandewall","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan","16777|IJCAI|2007|Recent Progress in Heuristic Search A Case Study of the Four-Peg Towers of Hanoi Problem|We integrate a number of new and recent advances in heuristic search, and apply them to the fourpeg Towers of Hanoi problem. These include frontier search, disk-based search, parallel processing, multiple, compressed, disjoint, and additive pattern database heuristics, and breadth-first heuristic search. New ideas include pattern database heuristics based on multiple goal states, a method to reduce coordination among multiple parallel threads, and a method for reducing the number of heuristic calculations. We perform the first complete breadth-first searches of the  and -disc fourpeg Towers of Hanoi problems, and extend the verification of \"presumed optimal solutions\" to this problem from  to  discs. Verification of the -disc problem is in progress.|Richard E. Korf,Ariel Felner"],["16886|IJCAI|2009|Inverse Reinforcement Learning in Partially Observable Environments|Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behaviour of an expert. Most of the existing algorithms for IRL assume that the expert's environment is modeled as a Markov decision process (MDP), although they should be able to handle partially observable settings in order to widen the applicability to more realistic scenarios. In this paper, we present an extension of the classical IRL algorithm by Ng and Russell to partially observable environments. We discuss technical issues and challenges, and present the experimental results on some of the benchmark partially observable domains.|Jaedeug Choi,Kee-Eung Kim","14517|IJCAI|1987|CYPRESS-Soar A Case Study in Search and Learning in Algorithm Design|This paper describes a partial reimplementation of Doug Smith's CYPRESS algorithm design system within the Soar problem-solving architecture. The system, CYPRESS-SOAR, reproduces most of CYPRESS' behavior in the synthesis of three divide-and-conquer sorting algorithms from formal specifications. CYPRESS-Soar is based on heuristic search of problem spaces, and uses search to compensate for missing knowledge in some instances. CYPRESS-Soar also learns as it designs algorithms, exhibiting significant transfer of learned knowledge, both within a single design run, and across designs of several different algorithms. These results were produced by reimplementing just the high-level synthesis control of CYPRESS, simulating the results of calls to CYPRESS deduction engine. Thus after only two months of effort, we had a surprisingly effective research vehicle for investigating the roles of search, knowledge, and learning in this domain.|David M. Steier","14387|IJCAI|1987|The Mixed Approach for Motion Planning Learning Global Strategies from a Local Planner|In this paper, we propose a mixed approach for motion planning that decomposes the problem into two levels. At the global level, we build a graph whose nodes represent relatively large cells of the Configuration Space of the robotic system. Adjacent cells are connected by edges weighted by the probability for the local planner to succeed in computing a trajectory from a point in one cell to a goal in the other. These probabilities are used by a minimum cost path finding algorithm to generate subgoals for the local planner. They are updated using a Bayesian rule from the results of the execution of planned trajectories at the local level. At the global level, no geometric information is stored, thus eliminating the expensive transformation of obstacles into the Configuration Space needed by usual global methods. We take advantage of the ability of our local planner to move close to obstacles so that only a crude discretization of the Configuration Space is needed. This makes it possible to apply this technique to robotic systems with a large number of degrees of freedom. In mobile robot applications, sensors being used by the local planner, this method achieves the learning of planning strategies in an unknown environment without building a complete geometric model of the world.|Bernard Faverjon,Pierre Tournassoud","15603|IJCAI|2001|Rational and Convergent Learning in Stochastic Games|This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm,WoLF policy hillclimbing, that is based on a simple principle \"learn quickly while losing, slowly while winning.\" The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.|Michael H. Bowling,Manuela M. Veloso","16034|IJCAI|2005|Learning Partially Observable Deterministic Action Models|We present the first tractable, exact solution for the problem of identifying actions' effects in partially observable STRIPS domains. Our algorithms resemble Version Spaces and Logical Filtering, and they identify all the models that are consistent with observations. They apply in other deterministic domains (e.g., with conditional effects), but are inexact (may return false positives) or inefficient (we could not bound the representation size). Our experiments verify the theoretical guarantees, and show that we learn STRIPS actions efficiently, with time that is significantly better than approaches for HMMs and Reinforcement Learning (which are inexact). Our results are especially surprising because of the inherent intractability of the general deterministic case. These results have been applied to an autonomous agent in a virtual world, facilitating decision making, diagnosis, and exploration.|Eyal Amir","16634|IJCAI|2007|Transfer Learning in Real-Time Strategy Games Using Hybrid CBRRL|The goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. In this paper, we present a multilayered architecture named CAse-Based Reinforcement Learner (CARL). It uses a novel combination of Case-Based Reasoning (CBR) and Reinforcement Learning (RL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTSTM, a commercial Real Time Strategy game. Our experiments demonstrate that CARL not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.|Manu Sharma,Michael P. Holmes,Juan Carlos Santamaría,Arya Irani,Charles Lee Isbell Jr.,Ashwin Ram","15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer","16324|IJCAI|2005|Learning Payoff Functions in Infinite Games|We consider a class of games with real-valued strategies and payoff information available only in the form of data from a given sample of strategy profiles. Solving such games with respect to the underlying strategy space requires generalizing from the data to a complete payoff-function representation. We address payoff-function learning as a standard regression problem, with provision for capturing known structure (symmetry) in the multiagent environment. To measure learning performance, we consider the relative utility of prescribed strategies, rather than the accuracy of payoff functions per se. We demonstrate our approach and evaluate its effectiveness on two examples a two-player version of the first-price sealed-bid auction (with known analytical form), and a five-player marketbased scheduling game (with no known solution).|Yevgeniy Vorobeychik,Michael P. Wellman,Satinder P. Singh","15036|IJCAI|1993|Evolutionary Learning Strategy using Bug-Based Search|We introduce a new approach to GA (Genetic Algorithms) based problem solving. Earlier GAs did not contain local search (i.e. hill climbing) mechanisms, which led to optimization difficulties, especially in higher dimensions. To overcome such difficulties, we introduce a \"bug-based\" search strategy, and implement a system called BUGS. The ideas behind this new approach are derived from biologically realistic bug behaviors. These ideas were confirmed empirically by applying them to some optimization and computer vision problems.|Hitoshi Iba,Tetsuya Higuchi,Hugo de Garis,Taisuke Sato","13977|IJCAI|1983|Learning Effective Search Heuristics|SAGE. is a production system that improves its search strategies with practice. The program incorporates four different heuristics for assigning credit and blame, and employs a discrimination process to direct its search through the space of move-proposing rules. The system has shown its generality by learning search heuristics in five different task domains. In addition to improving its search behavior on practice problems, SAGE. was able to transfer its expertise to scaled-up versions of a task, and in one case transferred its acquired search strategy to problems with different initial and goal states.|Pat Langley"],["14611|IJCAI|1989|POPEL-HOW A Distributed Parallel Model for Incremental Natural Language Production with Feedback|This paper presents a new model for the production of natural language. The novel idea is to combine incremental and bidirectional generation with parallelism. The operational basis of our model is a distributed parallel system at every level of representation. Starting point of the production are segments of the conceptual level. These segments are related to active objects which have to map themselves across the linguistic levels (i.e. functional-semantic, syntactic and morphologic level) as fast and as independently as possible. Therefore the incremental behavior caused by a successive input is propagated on all levels. Linguistic requirements detected by objects which are related to already produced segments at any level influence and restrict the decision of what to say next.|Wolfgang Finkler,Günter Neumann","13446|IJCAI|1975|Toward A Multiple Environments Model Of Natural Language|The idea of treating utterances as programs to be run in human brains or on a computer is jxirsued by advocating the use of the environment notion for natural language semantics. One structure of environments is devoted to keeping track of the real or fictive interlocutors and another to distinguishing somebody's linguistic behavior from its pretense and belief, while the third one represents the structure of topics in a discourse. The required flexibility of environment manipulation is expected to be supplied by the Bobrow-Wegbrelt primitives.|J. Bian","14050|IJCAI|1983|Japanese Language Semantic Analyzer Based on an Extended Case Frame Model|This paper describes a Japanese language semantic analyzer based on an extended case frame model, which consists of a relatively large collection of case relations, modalities and conjunctive relations. The analyzer performs four stage analysis using a frame type knowledge base. It also utilizes plausibility scores for dealing with ambiguities and local scene frames for the prediction of omitted case elements.|Akira Shimazu,Shozo Naito,Hirosato Nomura","14205|IJCAI|1985|Weighted Interaction of Syntax and Semantics in Natural Language Analysis|The present paper discusses the extensions to the parsing strategies adopted for FIDO (a Flexible Interface for Database Operations). The parser is able to deal with ill-formed inputs (syntactically ill - formed sentences, fragments, conjunctions, etc.) because of the strict cooperation among syntax and semantics. The syntactic knowledge is represented by means of packets of condition-action rules associated with syntactic categories. The non-determinism is mainly handled by means of rules which restructure the parse tree (called \"natural changes\") so that the use of backtracking is strongly limited. In order to deal with difficult cases in which no clear-cut mechanism exists for excluding an interpretation, a weighting mechanism has been added to the parser so that it is possible to explore few different hypotheses in parallel and to choose the best one on the basis of complex interaction among syntax and semantics.|Leonardo Lesmo,Pietro Torasso","15304|IJCAI|1995|Semantic Inference in Natural Language Validating a Tractable Approach|This paper is concerned with an inferential approach to information extraction reporting in particular on the results of an empirical study that was performed to validate the approach. The study brings together two lines of research () the RHO framework for tractable terminological knowledge representation and () the Alembic message understanding system. There are correspondingly two principal aspects of interest to this work From the knowledge representation perspective the present study serves to validate experimentally a normal form hypothesis that guarantees tractability of inference in the RHO framework. From the message processing perspective this study substantiates the utility of limited inference to information extraction.|Marc B. Vilain","14433|IJCAI|1987|A Knowledge Framework for Natural Language Analysis|Recent research in language analysis and language generation has highlighted the role of knowledge representation in both processes. Certain knowledge representation foundations, such as structured inheritance networks and feature-based linguistic representations, have proved useful in a variety of language processing tasks. Augmentations to this common framework, however, are required to handle particular issues, such as the ROLE RELATIONSHIP problem the task of determining how roles, or slots, of a given frame, are filled based on knowledge about other roles. Three knowledge structures are discussed that address this problem. The semantic interpreter of an analyzer called TRUMP (TRansportable Understanding Mechanism Package) uses these structures to determine the fillers of roles effectively without requiring excessive specialized information about each frame.|Paul S. Jacobs","13531|IJCAI|1975|Generating Hierarchical Semantic Networks Froma Natural Language Discourse|The following paper contains a description of a computer program that constructs hierarchical semantic networks from natural language texts in simulating completely and precisely the meaning of the input text. The program works with a formal grammar describing sentences syntactically and a formal semantic stransducing texts to networks dependent on their syntactic description. The networks' nodes have concepts or again networks, as their values. The networks' edges are many-place relations among the concepts respectively networks. They are realized as reference structures. The program in its fietual state works on domains which art to comprehend sematically rather well A network has been constructured automatically for the area of general topology from a compendi urn's definitions (N. Bourbaki. E lenient s of Mathcmatics. General topology). Another one is being constructed for the area of computer science.|Camilla Schwind","13287|IJCAI|1969|A Conceptual Parser for Natural Language|This paper describes an operable automatic parser for natural language. It is a conceptual parser, concerned with determining the underlying meaning of the input utilizing a network of concepts explicating the beliefs inherent in a piece of discourse.|Roger C. Schank,Lawrence G. Tesler","14887|IJCAI|1991|High Performance Natural Language Processing on Semantic Network Array Processor|This paper describes a natural language processing system developed for the Semantic Network Array Processor (SNAP). The goal of our work is to develop a scalable and high-performance natural language processing system which utilizes the high degree of parallelism provided by the SNAP machine. We have implemented an experimental machine translation system as a central part of a real-time speech-to-speech dialogue translation system. It is a SNAP version of the  DMDIAIOG speech-to-speech translation system. Memory-based natural language processing and syntactic constraint network model has been incorporated using parallel marker-passing which is directly supported from hardware level. Experimental results demonstrate that the parsing of a sentence is done in the order of milliseconds.|Hiroaki Kitano,Dan I. Moldovan,Seungho Cha","13573|IJCAI|1977|NLG - Natural Language Graphics|The goal of the NLG project is to enhance person-computer interaction by using more than one mode of communication. The project uses a combination of natural language and graphics for both input and output. This allows development of practical, habitable systems suitable for users who are naive about programming, and encourages productive use of computers by more people. Mixed graphical and linguistic interaction could be an important tool in a variety of application areas.|David C. Brown,Stanley C. Kwasny,H. William Buttelmann,B. Chandrasekaran,Norman K. Sondheimer"],["13912|IJCAI|1983|The Mercator Representation of Spatial Knowledge|The MERCATOR program constructs a cognitive map from a sequence of scene descriptions. A new representation of two-dimensional geography was developed for this program. Objects are represented by sets of polygons their boundaries, by sets of directed edges. The relative positions of objects are determined by connecting edges. A truth-conditional semantics for this representation is presented, its strengths and weaknesses are evaluated, and it is compared to other AI representations of shape and position.|Ernest Davis","14043|IJCAI|1983|Classification in the KL-ONE Knowledge Representation System|KL-ONE lets one define and use a class of descriptive terms called Concepts, where each Concept denotes a set of objects A subsumption relation between Concepts is defined which is related to set inclusion by way of a semantics for Concepts. This subsumption relation defines a partial order on Concepts, and KL-ONE organizes all Concepts into a taxonomy that reflects this partial order. Classification is a process that takes a new Concept and determines other Concepts that either subsume it or that it subsumes, thereby determining the location for the new Concept within a given taxonomy. We discuss these issues and demonstrate some uses of the classification algorithm.|James G. Schmolze,Thomas A. Lipkis","14408|IJCAI|1987|Constraints in a Hybrid Knowledge Representation System|In our research group, the hybrid knowledge representation system Babylon has been developed providing formalisms for rules, prolog and frames. Beyond it, we implemented Consat, a system for constraint satisfaction. Since applications of Babylon for process diagnosis, planning etc. required constraints. we integrated Consat into the Babylon environment. The paper describes the integration of Consat into Babylon, regarding two aspects. First, constraints should be available as another Babylon formalism by using the functional interface of Consat. On the other hand, it is important to have constraints implicitly controlling other Babylon formalisms, for instance, in order to keep the system's database consistent. While with respect to the first point, the paper describes work already finished, the second form of integration is work in progress.|Hans W. Guesgen,Ulrich Junker,Angi Voß","13856|IJCAI|1981|The Design Of A System For Designing Knowledge Representation Systems|The use of abstract data types as a basis for designing experimental knowledge representation systems is discussed. Abstract data types are shown to have features in common with severel diverse representation formalisms (e.g. semantic networks, frames and KLONE). For example, abstract data types have notions analogous to concept, subconcept and inheritance. The relatively small conceptual distance between abstraect data types and knowledge representation formalisms make them an ideal vehicle for implementing such formalisms.|James L. Weiner,Martha Palmer","14135|IJCAI|1985|Knowledge Representation in an Expert Storm Forecasting System|METEOR is a rule- and frame-based system for short-term (- hour) severe convective storm forecasting. This task requires a framework that supports inferences about the temporal and spatial features of meteorological changes. Initial predictions are based on interpretations of contour maps generated by statistical predictors of storm severity, Ib confirm these predictions, METEOR considers additional quantitative measurements, ongoing meteorological conditions and events, and how the expert forecaster interprets these extra factors. Meteorological events are derived from interpreting human observations of weather conditions in the forecast area. To accommodate the large amounts of different types of knowledge characterizing this problem, a number of extensions to the rule and frame representations were developed. These extensions include a view scheme to direct property inheritance through intermingled hierarchies and the automatic generation of production system rules from frame descriptions on an as-needed basis for event recognition.|Renee Elio,Johannes de Haan","13964|IJCAI|1983|Representation of Temporal Knowledge|The paper describes a system of notions (a T-model) developed for representing temporal information on the semantics-pragmatics level in a natural language understanding system. The choice of the notions to be considered has been relied upon a necessity of special facilities for describing situations with various degrees of detailing according to an inexact character of temporal information in natural language texts. Time is modelled as a straight directed line, and four main objects point, interval, quantity and chain together with four groups of notions associated with them are included into the model. A structure of the T-model and its base notions are briefly outlined in the first section of the paper. The representation of temporal information by means of T-model objects is then considered by examples.|E. Yu Kandrashina","14216|IJCAI|1985|Motor Knowledge Representation|The motor control problem is considered in the framework of knowledge representation. In the AIRobotic world, a formal model for Motor knowledge should fill a gap between task planning and low level robot languages such model should be able to \"virtualize\" the robot and the interaction with the environment so that the planner could produce (and rely on) high level abstract actions, characterized by high autonomy and skill. The paper discusses some general aspects about actions, actors, and scenes, and describes the NEM language, which is able to represent and animate humanoids in a scene and is meant to provide a software laboratory for experimenting with action schemas.|Giuseppe Marino,Pietro Morasso,Renato Zaccaria","14250|IJCAI|1985|Self-Knowledge and Self-Representation|In this paper I introduce a contrast between homomorphic and nonhomomorphic ascriptions of informational content to representations. In the former case there is a mapping from the parts of the representation onto the constituents of the content. In the latter case, there is not some of the constituents of the content are settled by background factors. I contrast this distinction with that between context dependent and context independent ascriptions of content. I note that in cases where the ascriber of content shares the background with the agent, one is inclined to ascribe homomorphic content of a sort that does not have a fixed truth-value to a representation. This leads to the notion of relative information. Some uses for relative information are noted. Finally, the distinctions developed are used to distinguish three types of self-knowledge and account for their relations.|John Perry","15862|IJCAI|2003|Corpus-Based Knowledge Representation|A corpus-based knowledge representation system consists of a large collection of disparate knowledge fragments or schemas, and a rich set of statistics computed over the corpus. We argue that by collecting such a corpus and computing the appropriate statistics, corpus-based representation offers an alternative to traditional knowledge representation for a broad class of applications. The key advantage of corpus-based representation is that we avoid the laborious process of building a (often brittle) knowledge base. We describe the basic building blocks of a corpus-based representation system and a set of applications for which such a paradigm is appropriate, including one application where the approach is already showing promising results.|Alon Y. Halevy,Jayant Madhavan","14283|IJCAI|1985|Representation and Use of Explicit Justifications for Knowledge Base Refinements|We discuss the representation and use of justification structures as an aid to knowledge base refinement We show how justifications can be used by a system to generate explanations - for its own use-of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.|Reid G. Smith,Howard A. Winston,Tom M. Mitchell,Bruce G. Buchanan"],["16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","16726|IJCAI|2007|Combining Learning and Word Sense Disambiguation for Intelligent User Profiling|Understanding user interests from text documents can provide support to personalized information recommendation services. Typically, these services automatically infer the user profile, a structured model of the user interests, from documents that were already deemed relevant by the user. Traditional keyword-based approaches are unable to capture the semantics of the user interests. This work proposes the integration of linguistic knowledge in the process of learning semantic user profiles that capture concepts concerning user interests. The proposed strategy consists of two steps. The first one is based on a word sense disambiguation technique that exploits the lexical database WordNet to select, among all the possible meanings (senses) of a polysemous word, the correct one. In the second step, a nave Bayes approach learns semantic sense-based user profiles as binary text classifiers (user-likes and user-dislikes) from disambiguated documents. Experiments have been conducted to compare the performance obtained by keyword-based profiles to that obtained by sense-based profiles. Both the classification accuracy and the effectiveness of the ranking imposed by the two different kinds of profile on the documents to be recommended have been considered. The main outcome is that the classification accuracy is increased with no improvement on the ranking. The conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those documents whose classification score is close to the likesdislikes threshold (the items for which the classification is highly uncertain).|Giovanni Semeraro,Marco Degemmis,Pasquale Lops,Pierpaolo Basile","15850|IJCAI|2003|Improving Word Sense Disambiguation in Lexical Chaining|Previous algorithms to compute lexical chains suffer either from a lack of accuracy in word sense disambiguation (WSD) or from computational inefficiency. In this paper, we present a new linear-time algorithm for lexical chaining that adopts the assumption of one sense per discourse. Our results show an improvement over previous algorithms when evaluated on a WSD task.|Michel Galley,Kathleen McKeown","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin","15818|IJCAI|2003|Hierarchical Semantic Classification Word Sense Disambiguation with World Knowledge|We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \"world knowledge\". The model compiles knowledge contained in a dictionary-ontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \"hierarchical classifier\" outperforms a state-of-the-art standard \"flat\" one.|Massimiliano Ciaramita,Thomas Hofmann,Mark Johnson","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng","16816|IJCAI|2009|Word Sense Disambiguation for All Words without Hard Labor|While the most accurate word sense disambiguation systems are built using supervised learning from sense-tagged data, scaling them up to all words of a language has proved elusive, since preparing a sense-tagged corpus for all words of a language is time-consuming and human labor intensive. In this paper, we propose and implement a completely automatic approach to scale up word sense disambiguation to all words of English. Our approach relies on English-Chinese parallel corpora, English-Chinese bilingual dictionaries, and automatic methods of finding synonyms of Chinese words. No additional human sense annotations or word translations are needed. We conducted a large-scale empirical evaluation on more than , noun tokens in English texts annotated in OntoNotes ., based on its coarsegrained sense inventory. The evaluation results show that our approach is able to achieve high accuracy, outperforming the first-sense baseline and coming close to a prior reported approach that requires manual human efforts to provide Chinese translations of English senses.|Zhi Zhong,Hwee Tou Ng"],["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","16860|IJCAI|2009|Efficient Skill Learning using Abstraction Selection|We present an algorithm for selecting an appropriate abstraction when learning a new skill. We show empirically that it can consistently select an appropriate abstraction using very little sample data, and that it significantly improves skill learning performance in a reasonably large real-valued reinforcement learning domain.|George Konidaris,Andrew G. Barto","16183|IJCAI|2005|Using Neutral Examples for Learning Polarity|Sentiment analysis is an example of polarity learning. Most research on learning to identify sentiment ignores \"neutral\" examples and instead performs training and testing using only examples of significant polarity. We show that it is crucial to use neutral examples in learning polarity for a variety of reasons and show how neutral examples help us obtain superior classification results in two sentiment analysis test-beds.|Moshe Koppel,Jonathan Schler","15566|IJCAI|1999|Dynamic Refinement of Feature Weights Using Quantitative Introspective Learning|Recently more and more researchers have been supporting the view that learning is a goaldriven process. One of the key properties of a goal-driven learner is introspectiveness-the ability to notice the gaps in its knowledge and to reason about the information required to fill in those gaps. In this paper, we introduce a quantitative introspective learning paradigm into case-based reasoning (CBR). The result is an integrated problem-solving model which will learn introspectively feature weights in a case base in order to be responsive dynamically to its users. In contrast to the existing qualitative methods for introspective learning, our model has the advantage of being able to capture accurate learning information in the interactions with its users. A CBR system equipped with quantitative introspective learning ability can allow the feature weights to be captured automatically and to track its users' changing preferences continuously. In such a system, while the reasoning part is still case-based, the learning part is shouldered by a quantitative introspective learning model. Weight learning and evolution are accomplished in the background. The effectiveness of this integration will be demonstrated through a series of empirical experiments.|Zhong Zhang,Qiang Yang","16581|IJCAI|2007|Using a Mobile Robot for Cognitive Mapping|When animals (including humans) first explore a new environment, what they remember is fragmentary knowledge about the places visited. Yet, they have to use such fragmentary knowledge to find their way home. Humans naturally use more powerful heuristics while lower animals have shown to develop a variety of methods that tend to utilize two key pieces of information, namely distance and orientation information. Their methods differ depending on how they sense their environment. Could a mobile robot be used to investigate the nature of such a process, commonly referred to in the psychological literature as cognitive mapping What might be computed in the initial explorations and how is the resulting \"cognitive map\" be used to return home In this paper, we presented a novel approach using a mobile robot to do cognitive mapping. Our robot computes a \"cognitive map\" and uses distance and orientation information to find its way home. The process developed provides interesting insights into the nature of cognitive mapping and encourages us to use a mobile robot to do cognitive mapping in the future, as opposed to its popular use in robot mapping.|Chee K. Wong,Jochen Schmidt,Wai K. Yeap","16957|IJCAI|2009|Transfer Learning Using Task-Level Features with Application to Information Retrieval|We propose a probabilistic transfer learning model that uses task-level features to control the task mixture selection in a hierarchical Bayesian model. These task-level features, although rarely used in existing approaches, can provide additional information to model complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available. To estimate the model parameters, we develop an empirical Bayes method based on variational approximation techniques. Our experiments on information retrieval show that the proposed model achieves significantly better performance compared with other transfer learning methods.|Rong Yan,Jian Zhang 0003","16634|IJCAI|2007|Transfer Learning in Real-Time Strategy Games Using Hybrid CBRRL|The goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. In this paper, we present a multilayered architecture named CAse-Based Reinforcement Learner (CARL). It uses a novel combination of Case-Based Reasoning (CBR) and Reinforcement Learning (RL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTSTM, a commercial Real Time Strategy game. Our experiments demonstrate that CARL not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.|Manu Sharma,Michael P. Holmes,Juan Carlos Santamaría,Arya Irani,Charles Lee Isbell Jr.,Ashwin Ram","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","14881|IJCAI|1991|Learning Concept Classification Rules Using Genetic Algorithms|In this paper we explore the use of an adaptive search technique (genetic algorithms) to construct a system GABEL which continually learns and refines concept classification rules from its interaction with the environment. The performance of the system is measured on a set of concept learning problems and compared with the performance of two existing systems IDR and C.. Preliminary results support that, despite minimal system bias, GABIL is an effective concept learner and is quite competitive with IDR and C. as the target concept increases in complexity.|Kenneth A. De Jong,William M. Spears","13710|IJCAI|1981|Learning of Sensory-Motor Schemas in a Mobile Robot|A learning system is described which was used to control a simple robot vehicle and to autonomously learn behaviour patterns. The system is loosely based on Becker's model of Intermediate Cognition.|Alan H. Bond,David H. Mott"],["80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80267|VLDB|2003|Query Processing for High-Volume XML Message Brokering|XML filtering solutions developed to date have focused on the matching of documents to large numbers of queries but have not addressed the customization of output needed for emerging distributed information infrastructures. Support for such customization can significantly increase the complexity of the filtering process. In this paper, we show how to leverage an efficient, shared path matching engine to extract the specific XML elements needed to generate customized output in an XML Message Broker. We compare three different approaches that differ in the degree to which they exploit the shared path matching engine. We also present techniques to optimize the post-processing of the path matching engine output, and to enable the sharing of such processing across queries. We evaluate these techniques with a detailed performance study of our implementation.|Yanlei Diao,Michael J. Franklin","80004|VLDB|1980|Knowledge-Based Query Processing|Contemporary database query processing systems base their actions principally on \"syntactic\" considerations, and seek only the most efficacious way of answering a query as originally formulated. An alternative approach seeks to use knowledge of the semantics of the database's application to transform the original query into an alternative form, possibly quite different in its expression, but which is both equivalent to the original (in terms of the set of records from the database that it qualifies) and more efficient to process, given the existing file structures and access methods. The architecture of a system supporting such knowledge-based \"semantic\" transformations has been developed. It addresses such issues as the kinds of knowledge that should be included in the knowledge base and how it should be expressed, the kinds of transformations that can exploit this knowledge to improve query processing, and the way in which the system as a whole can be organized in the presence of large and intricate knowledge bases and a multiplicity of possible transformation types. This latter structure is based on a multi-processing model, in which each possible transformation is treated as a process, whose priority is assigned by a scheduler embodying a variety of heuristics. The principal contribution of the work is the establishment of a conceptual framework for this type of query optimization and the design of an architecture that can grow with the development of additional transformation techniques.|Michael Hammer,Stanley B. Zdonik","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma \u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80714|VLDB|2006|R-SOX Runtime Semantic Query Optimization over XML Streams|Optimizing queries over XML streams has been an important and non-trivial issue with the emergence of complex XML stream applications such as monitoring sensor networks and online transaction processing. Our system, R-SOX, provides a platform for runtime query optimization based on dynamic schema knowledge embedded in the XML streams. Such information provides refined runtime schema knowledge thus dramatically enlarged the opportunity for schema-based query optimizations. In this demonstration, we focus on the following three aspects () annotation of runtime schema knowledge () incremental maintenance of run-time schema knowledge () dynamic semantic query optimization techniques. The overall framework for runtime semantic query optimization, including several classes of dynamic optimization techniques, will be shown in this demonstration.|Song Wang,Hong Su,Ming Li,Mingzhu Wei,Shoushen Yang,Drew Ditto,Elke A. Rundensteiner,Murali Mani","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80278|VLDB|2003|Mixed Mode XML Query Processing|Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.|Alan Halverson,Josef Burger,Leonidas Galanis,Ameet Kini,Rajasekar Krishnamurthy,Ajith Nagaraja Rao,Feng Tian,Stratis Viglas,Yuan Wang,Jeffrey F. Naughton,David J. DeWitt","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani","80789|VLDB|2007|Query Processing over Incomplete Autonomous Databases|Incompleteness due to missing attribute values (aka \"null values\") is very common in autonomous web databases, on which user accesses are usually supported through mediators. Traditional query processing techniques that focus on the strict soundness of answer tuples often ignore tuples with critical missing attributes, even if they wind up being relevant to a user query. Ideally we would like the mediator to retrieve such possible answers and gauge their relevance by accessing their likelihood of being pertinent answers to the query. The autonomous nature of web databases poses several challenges in realizing this objective. Such challenges include the restricted access privileges imposed on the data, the limited support for query patterns, and the bounded pool of database and network resources in the web environment. We introduce a novel query rewriting and optimization framework QPIAD that tackles these challenges. Our technique involves reformulating the user query based on mined correlations among the database attributes. The reformulated queries are aimed at retrieving the relevant possible answers in addition to the certain answers. QPIAD is able to gauge the relevance of such queries allowing tradeoffs in reducing the costs of database query processing and answer transmission. To support this framework, we develop methods for mining attribute correlations (in terms of Approximate Functional Dependencies), value distributions (in the form of Na&iumlve Bayes Classifiers), and selectivity estimates. We present empirical studies to demonstrate that our approach is able to effectively retrieve relevant possible answers with high precision, high recall, and manageable cost.|Garrett Wolf,Hemal Khatri,Bhaumik Chokshi,Jianchun Fan,Yi Chen,Subbarao Kambhampati"],["79838|VLDB|1976|High Performance Hardware for Database Systems|The work is founded on a comprehensive database model which provides various user languages and supports different \"views\" of the data. The low-level database structure is based on the n-ary relational model. A special-purpose processor is described which can directly perform the relational primitive operations. Novel algorithms for these operations are outlined.|D. R. McGregor,R. G. Thomson,W. N. Dawson","79856|VLDB|1977|The Decomposition Versus Synthetic Approach to Relational Database Design|Two of the competing approaches to the logical design of relational databases are the third normal form decomposition approach of Codd and the synthetic approach of Bernstein and others. The synthetic approach seems on the surface to be the more powerful unfortunately, to avoid serious problems, a nonintuitive constraint (the \"uniqueness\" of functional dependencies) must be assumed. We demonstrate the fourth normal form approach, which not only can deal with this difficulty, but which is also more powerful than either of the earlier approaches. The input of the new method includes attributes (potential column names), along with semantic information in the form of functional and multivalued dependencies the output is a \"good\" (fourth normal form) logical design. The new method is semi-automatic, which is especially helpful in the case of a very large database with many attributes that interrelate in complex ways.|Ronald Fagin","80122|VLDB|1985|A Tool for Modular Database Design|A database design method, based on the concept of moduLe, is first described. The method incorporates both a strategy for enforcing integrity constraints and a tactic for organizing large sets of database structures, integrity constraints and operations. A software toot that helps the development and maintenance of database schemas designed according to the method is then specified. Finally, a prototype expert system offering a partial implementation of the tool is described.|Luiz Tucherman,Antonio L. Furtado,Marco A. Casanova","79841|VLDB|1976|MERLIN - Design of a National Bibliographic Database|This paper describes MERLIN, the British Library's national bibliographic information retrieval system. It describes the functions performed by the system and the users for whom it is intended. In particular it outlines the main problems associated with bibliographic data for a national library. The logical design of the database is discussed with particular reference to the space economy necessary for a database of  million items. A highly shared structure, which allows flexibility as well as economy of storage, is proposed. Methods of access, authority handling and creation of private views are given.|Peter L. Noerr,M. C. White","79875|VLDB|1977|Design Criteria for Distributed Database Systems|This paper clarifies design criteria for adaptive heterogeneous distributed database systems. Compositional and decompositional design approaches, database sharing models, and an abstract design approach are discussed.|Tosiyasu L. Kunii,Hideko S. Kunii","80091|VLDB|1985|Database Design Tools An Expert System Approach|In this paper, we report on the implementation of SECSI, an expert system for database design written in Prolog. Starting from an application description given with either a subset of the natural language, or a formal language, or a graphical interface, the system generates a specific semantic network portraying the application. Then, using a set of design rules, it completes and simplifies the semantic network up to reach flat normalized relations. All the design is interactively done with the end-user. The system is evolutive in the sense that it also offers an interactive interface which allows the database design expert to modify or add design rules.|Mokrane Bouzeghoub,Georges Gardarin,Elisabeth Métais","80020|VLDB|1980|A Pragmatic Approach to Database Design|The purpose of the logical design phase of database development is to produce a model of the data structure that () effectivelv satisfies the organizational information storage\"requirements, () will allow timely access to the information, () is flexible and readily adaptable to future requirements, and () is simple and comprehendable. Most methodologies for logical database design have adopted but one of these objectives or attempted to address them concurrently. Recently, there have been methodoloqies proposed that address all four goals individually.|Steven H. Spewak","79847|VLDB|1977|CS A Tool for Database Design by Infological Simulation Abstract|The task of physical data base design in an DBTG enviornment is examined. Generation of an internal schema which considers questions of storage versus access costs, efficient implementation of data relationships, efficient placement of data within the data base and allocation of primary and secondary storage is shown to be a formidable task. Current data base design aids are reviewed. One aid, a sophisicated mathematical model of DBTG data bases (Gerritsen ) is shown to be a potentially valuable tool. A limitation of this model is that no optimization algorithm other than total enumeration has been found. This paper proposes an implementation of the Gerritsen model. An interactive design tool, based upon the Gerritsen model, is discussed. A Data Base Design Decision Support System (DBD-DSS) for use by the DBA is developed. The objectives and structure of the DBD-DSS are examined, A comprehensive example illustrating both the user interface and the potential benefits of the interactive tool is presented.|Stig Berild,Sam Nachmens","79861|VLDB|1977|A Conceptual Design of a Generalized Database Subsystem|A database subsystem (GDS) is proposed. The main objective is to provide a basis for constructing efficient database management systems. A functional interface (GDI), defined for the subsystem, primarily manipulates a single record. Various access functions are provided in order to manipulate a wide variety of logical data structures. Mapping from a logical data structure to a physical data structure is completely maintained in the GDS. Data definition and control functions are also included in the GDI. The subsystem is implemented on a special purpose processor (GDP) which independently operates from a host processor, responsible for the rest of the database management functions. The GDP is composed of special hardware and firmware function modules, local memories and a shared memory which is used for communicating with the host processor. Parallel processing at the host-GDP level and intra-GDP level assures high performance. Furthermore, the interprocessor communication mechanism using the shared memory significantly reduces overhead time by avoiding useless data transfer operations. The subsystem can be applied to various database management system, such as a CODASYL type, a relational type, end user facilities etc.|Katsuya Hakozaki,Takenori Makino,Masayuki Mizuma,Mamoru Umemura,Shigeki Hiyoshi","80117|VLDB|1985|The MR Diagram - A Model for Conceptual Database Design|Traditional database models are not sufficiently expressive for a variety of standard and non-standard database applications. Several models supporting greater abstraction have been proposed to fill this gap, but no one model has gained wide acceptance. This paper defines a model, based on the notion of molecules and non-first normal form relations, that provides a power- ful abstraction mechanism using aggregation. The model also provides a simple pictorial representation that allows a compact and clear specification of a database. We illustrate the model with several examples and show how it can be used in the design of databases.|Raghu Ramakrishnan,Abraham Silberschatz"]]}}