{"abstract":{"entropy":6.072288242195212,"topics":["data base, data management, data, data systems, management systems, data stream, data model, large data, base management, base systems, information systems, data important, information data, data integration, database systems, real world, applications data, widely used, conceptual schema, time series","xml data, xml documents, query, query processing, xml, relational xml, query optimization, search engine, query data, query language, queries, queries data, efficient xml, database query, query plan, database queries, xquery xml, schema mappings, stream processing, queries xml","virtual hashing, web search, data sources, nearest neighbor, web, compression techniques, web pages, data mining, large number, text classification, world wide, emerging applications, web engine, portals web, data web, sensor network, world web, type hashing, techniques data, retrieves points","database systems, relational database, relational data, relational systems, database, materialized views, relational base, distributed systems, normal form, design database, presented model, describes systems, design logical, problem distributed, consider problem, distributed database, provides users, functional dependency, general data, design objective","management systems, data management, database systems, data systems, data requirements, database management, stream systems, database research, data input, research community, data research, database tuning, data time, database community, applications systems, stream management, data need, database data, data part, stream applications","data model, data description, data different, schema data, data level, data essential, conceptual data, real world, conceptual schema, database schema, increasing data, data world, tutorial data, data changes, complex data, conceptual level, complex database, real data, different model, data become","queries data, query language, database queries, queries, top-k queries, queries query, processing queries, stream processing, programming language, data language, complex queries, processing distributed, ranking queries, similarity queries, xpath queries, problem queries, queries relational, large queries, study queries, extension language","query optimizers, search engine, execution query, querying bp-ql, bp-ql language, business bp-ql, novel query, processes bp-ql, present novel, plan optimizers, database query, query bp-ql, novel bp-ql, sql query, novel search, database sql, database optimizers, present bp-ql, present data, present engine","services data, modern applications, data cube, data warehousing, database policies, data privacy, database technology, highly data, technology, services, technologies, rfid, sensor, enables, range, due, need, devices, server, memory","data sources, compression techniques, techniques data, information systems, access data, protection data, information data, sharing data, use data, information, information retrieval, systems storage, challenge data, applications require, store data, challenge applications, information autonomous, database require, autonomous data, data storage","normal form, data objects, database objects, form data, functional dependency, functional dependencies, dependency data, query objects, relations, database relations, data relations, objects, transformation, introduce, table, join, practical, issue, future, derived","distributed systems, distributed database, problem distributed, design logical, distributed base, distributed data, algorithms data, logical database, logical data, paper data, design distributed, present algorithms, distributed, base logical, first data, distributed synchronization, data structuring, concept first, database algorithms, data presented"],"ranking":[["79921|VLDB|1978|EUFID The End User Friendly Interface to Data Management Systems|This paper describes a man-machine interface system, called EUFID, that will permit users of data management systems to communicate with those systems in natural language. At the same time, EUFID will act as a security screen to prevent unauthorized users from having access to particular fields in a data base. Our specific objective is to build a system that will be practical, efficient, and widely usable in existing, real-world applications. Our approach is to model the restricted set of linguistic structures and functions required for each application, rather than the manifold linguistic properties of natural language per se. This allows our system to be powerful enough to efficiently process English queries against specific data bases without attempting to understand forms of English that have little or no function in the contexts of those data bases.|I. Kameny,J. Weiner,M. Crilley,J. Burger,R. Gates,David Brill","80022|VLDB|1980|Status Report on ISOTCSCWG - Data Base Management Systems|This paper discussed the current status of the work of the ISOTCSCWG data base management systems working group. In existence for over five years, this working group has devoted it energies to the elaboration and extension of the notion of a \"conceptual schema\" as initially defined by the ANSIXSPARCDBMS Study Group. The focus of the ISO work has been the construction of a report on conceptual schema concepts, identification of various methodologies for their construction and the application of such methodologies to an example \"world\" in such a fashion that comparisons can be made between the different approaches. Underlying the work of this group has been the identification of fundamental philosophical differences in the various methodologies specifically that some assume the conceptual schema models the real world and others assume the conceptual schema models the data in the information base that holds information about the real world. This difference leads to considerable confusion in discussion when it is not explicitly recognized and one of the principal objectives of the ISOTCSCWG report is to expurgate that difference.|Thomas B. Steel Jr.","79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","79809|VLDB|1975|Hierarchical Performance Analysis Models for Data Base Systems|This paper presents a comprehensive set of hierarchically organized synthetic models developed for the performance evaluation of data base management systems. The first set of algebraic models for data base management system itself contains a program behavior model, a retrieval model, a logical data base model, a physical data base model, and a data processing model. These models are intended to clarify logical and physical data base structures and essential operations in the data processing on them. Another set of models for performance analysis contains a macroscopic program behavior model, a storage model, a processor model, a user behavior model, and an interactive model. In this set this paper is particularly concerned with the first  models. The macroscopic program behavior model is based on the discussion of data locality and allows us to estimate the frequency of data page loading expected on a given application program and a data base. The storage model enables us to estimate the traverse time of data page loading. Finally the processor model allows us to evaluate the data retrieval processing time of data manipulation commands of a given data base structure, a data base management system and a set of application programs under the multiprogramming environment. These models are to be applied to the optimization of the application programs or the data base structure in order to obtain a higher data retrieval performance.|Isao Miyamoto","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","79820|VLDB|1975|A Data Base Management Facility for Automatic Generation of Data Base Managers|This paper presents a facility for the implementation of data base management systems having high degrees of uhorizontalu data independence, i.e. independence from chosen logical properties of a data base as opposed to uverticalu independence from storage structures. The facility consists of a high level language for the specification of virtual data base managers, a compiler from this language to a pseudo-machine language, and an interpreter for the pseudo-machine language.It is shown how this facility can be used to produce efficient data base management systems with any degree of both horizontal and vertical data independence. Two key features of this tool are the compilation of tailored data base managers from individual schemas and multiple levels of optional binding.|David W. Stemple","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79783|VLDB|1975|Using Large Data Bases for Interactive Problem Solving|Interactive problem solving involves user-machine dialogues to solve unstructured problems, such as selecting marketing strategies and planning mass transit routes. A characteristic of most interactive problem solving systems is that they operate on data bases that are special purpose subsets derived from a large data base. The problem solving system must include code for interfacing with this data base, or the data must be converted to the formats required by the problem solving system. Effective interactive problem solving requires a data management system which provides flexibility in accessing a large data base and fast performance for the problem solving system. This paper describes a method, called data extraction, for interfacing large data bases with interactive problem solving systems. Figure  shows the basic components in the interface. A large data base management system is used to maintain and protect a set of data files. Data extraction is used to aggregate and subset from this set to provide information for problem solving. In addition to an IO interface, data extraction provides interactive data description and presentation functions. For more details on the functional requirements of the data extraction components see .|Eric D. Carlson","79840|VLDB|1976|An Approach to Data Communication between Different Generalized Data Base Management Systems|A number of data base management systems (DBMS) are now in operation in industry, government and the academic world. However, since most of these DBMSs have different conceptual and internal mo- dels, they cannot interact and exchange information in a network environment. This report discusses some of the problems and provides a methodology for communicating and exchanging information between hetrogeneous models of data or DBMSs that are commercially available.|E. Nahouraii,L. O. Brooks,Alfonso F. Cardenas"],["80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80274|VLDB|2003|Temporal Slicing in the Evaluation of XML Queries|As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, XQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a XQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.|Dengfeng Gao,Richard T. Snodgrass","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma √\u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80236|VLDB|2002|XMark A Benchmark for XML Data Management|While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.|Albrecht Schmidt 0002,Florian Waas,Martin L. Kersten,Michael J. Carey,Ioana Manolescu,Ralph Busse","80315|VLDB|2003|XML Schemas in Oracle XML DB|The WC XML Scheme language is becomimg increasingly popular for expressing the data model for XML documents. It is a powerful language that incorporates both strutural and datatype modeling features. There are many benefits to storing XML Schema compliant data in a database system, including better queryability, optimied updates and stronger validation. However, the fidelity of the XML document cannot be sacrificed. Thus, the fundamental problem facing database implementers is how can XML Schemes be mapped to relational (and object-relational) database without losing schema semantics or data-fidelity In this paper, we present the Oracle XML DB solution for a flexible mapping of XML Schemas to object-relational database. It preserves document fidelity, including ordering, namespaces, comments, processing instructions etc., and handles all the XML Schema semantics including cyclic definitions, dervations (extension and restriction), and wildcards. We also discuss various query and update optimiations that involve rewriting XPath operations to directly operate on the underlying relational data.|Ravi Murthy,Sandeepan Banerjee","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos","80262|VLDB|2003|From Tree Patterns to Generalized Tree Patterns On Efficient Evaluation of XQuery|XQuery is the de facto standard XML query language, and it is important to have efficient query evaluation techniques available for it. A core operation in the evaluation of XQuery is the finding of matches for specified tree patterns, and there has been much work towards algorithms for finding such matches efficiently. Multiple XPath expressions can be evaluated by computing one or more tree pattern matches. However, relatively little has been done on efficient evaluation of XQuery queries as a whole. In this paper, we argue that there is much more to XQuery evaluation than a tree pattern match. We propose a structure called generalized tree pattern (GTP) for concise representation of a whole XQuery expression. Evaluating the query reduces to finding matches for its GTP. Using this idea we develop efficient evaluation plans for XQuery expressions, possibly involving join, quantifiers, grouping, aggregation, and nesting. XML data often conforms to a schema. We show that using relevant constraints from the schema, one can optimize queries significantly, and give algorithms for automatically inferring GTP simplifications given a schema. Finally, we show, through a detailed set of experiments using the TIMBER XML database system, that plans via GTPs (with or without schema knowledge) significantly outperform plans based on navigation and straightforward plans obtained directly from the query.|Zhimin Chen,H. V. Jagadish,Laks V. S. Lakshmanan,Stelios Paparizos"],["80385|VLDB|2004|Simlarity Search for Web Services|Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem locating desired web services. Traditional keyword search is insufficient in this context the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over  web-service operations that shows the high recall and precision of our algorithms.|Xin Dong,Alon Y. Halevy,Jayant Madhavan,Ema Nemes,Jun Zhang","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80154|VLDB|2001|Self-similarity in the Web|Algorithmic tools for searching and mining the Web are becoming increasingly sophisticated and vital. In this context, algorithms that use and exploit structural information about the Web perform better than generic methods in both efficiency and reliability.We present an extensive characterization of the graph structure of the Web, with a view to enabling high-performance applications that make use of this structure. In particular, we show that the Web emerges as the outcome of a number of essentially independent stochastic processes that evolve at various scales. A striking consequence of this scale invariance is that the structure of the Web is \"fractal\"---cohesive subregions display the same characteristics as the Web at large. An understanding of this underlying fractal nature is therefore applicable to designing data services across multiple domains and scales.We describe potential applications of this line of research to optimized algorithm design for Web-scale data analysis.|Stephen Dill,Ravi Kumar,Kevin S. McCurley,Sridhar Rajagopalan,D. Sivakumar,Andrew Tomkins","80263|VLDB|2003|VIPAS Virtual Link Powered Authority Search in the Web|With the exponential growth of the World Wide Web, looking for pages with high quality and relevance in the Web has become an important research field. There have been many keyword-based search engines built for this purpose. However, these search engines usually suffer from the problem that a relevant Web page may not contain the keyword in its page text. Algorithms exploiting the link structure of Web documents, such as HITS, have also been proposed to overcome the problems of traditional search engines. Though these algorithms perform better than keyword-based search engines, they still have some defects. Among others, one major problem is that links in Web pages are only able to reflect the view of the page authors on the topic of those pages but not that of the page readers. In this paper, we propose a new algorithm with the idea of using virtual links which are created according to what the user behaves in browsing the output list of the query result. These virtual links are then employed to identify authoritative resources in the Web. Speci fically, the algorithm, referred to as algorithm VIPAS (standing for virtual link powered authority search), is divided into three phases. The first phase performs basic link analysis. The second phase collects statistics by observing the user behavior in browsing pages listed in the query result, and virtual links are then created according to what observed. In the third phase, these virtual links as well as real ones are taken together to produce an updated list of authoritative pages that will be presented to the user when the query with similar keywords is encountered next time. A Web warehouse is built and the algorithm is integrated into the system. By conducting experiments on the system, we have shown that VIPAS is not only very effective but also very adaptive in providing much more valuable information to users.|Chi-Chun Lin,Ming-Syan Chen","80325|VLDB|2003|Complex Queries over Web Repositories|Web repositories, such as the Stanford WebBase repository, manage large heterogeneous collections of Web pages and associated indexes. For effective analysis and mining, these repositories must provide a declarative query interface that supports complex expressive Web queries. Such queries have two key characteristics (i) They view a Web repository simultaneously as a collection of text documents, as a navigable directed graph, and as a set of relational tables storing properties of Web pages (length, URL, title, etc.). (ii) The queries employ application-specific ranking and ordering relationships over pages and links to filter out and retrieve only the \"best\" query results. In this paper, we model a Web repository in terms of \"Web relations\" and describe an algebra for expressing complex Web queries. Our algebra extends traditional relational operators as well as graph navigation operators to uniformly handle plain, ranked, and ordered Web relations. In addition, we present an overview of the cost-based optimizer and execution engine that we have developed, to efficiently execute Web queries over large repositories.|Sriram Raghavan,Hector Garcia-Molina","80437|VLDB|2004|WIC A General-Purpose Algorithm for Monitoring Web Information Sources|The Web is becoming a universal information dissemination medium, due to a number of factors including its support for content dynamicity. A growing number of Web information providers post near real-time updates in domains such as auctions, stock markets, bulletin boards, news, weather, roadway conditions, sports scores, etc. External parties often wish to capture this information for a wide variety of purposes ranging from online data mining to automated synthesis of information from multiple sources. There has been a great deal of work on the design of systems that can process streams of data from Web sources, but little attention has been paid to how to produce these data streams, given that Web pages generally require \"pull-based\" access. In this paper we introduce a new general-purpose algorithm for monitoring Web information sources, effectively converting pull-based sources into push-based ones. Our algorithm can be used in conjunction with continuous query systems that assume information is fed into the query engine in a push-based fashion. Ideally, a Web monitoring algorithm for this purpose should achieve two objectives () timeliness and () completeness of information captured. However, we demonstrate both analytically and empirically using real-world data that these objectives are fundamentally at odds. When resources available for Web monitoring are limited, and the number of sources to monitor is large, it may be necessary to sacrifice some timeliness to achieve better completeness, or vice versa. To take this fact into account, our algorithm is highly parameterized and targets an application-specified balance between timeliness and completeness. In this paper we formalize the problem of optimizing for a flexible combination of timeliness and completeness, and prove that our parameterized algorithm is a - approximation in all cases, and in certain cases is optimal.|Sandeep Pandey,Kedar Dhamdhere,Christopher Olston","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","80398|VLDB|2004|Combating Web Spam with TrustRank|Web spam pages use various techniques to achieve higher-than-deserved rankings in a search engine's results. While human experts can identify spam, it is too expensive to manually evaluate a large number of pages. Instead, we propose techniques to semi-automatically separate reputable, good pages from spam. We first select a small set of seed pages to be evaluated by an expert. Once we manually identify the reputable seed pages, we use the link structure of the web to discover other pages that are likely to be good. In this paper we discuss possible ways to implement the seed selection and the discovery of good pages. We present results of experiments run on the World Wide Web indexed by AltaVista and evaluate the performance of our techniques. Our results show that we can effectively filter out spam from a significant fraction of the web, based on a good seed set of less than  sites.|Zolt√°n Gy√∂ngyi,Hector Garcia-Molina,Jan O. Pedersen"],["79856|VLDB|1977|The Decomposition Versus Synthetic Approach to Relational Database Design|Two of the competing approaches to the logical design of relational databases are the third normal form decomposition approach of Codd and the synthetic approach of Bernstein and others. The synthetic approach seems on the surface to be the more powerful unfortunately, to avoid serious problems, a nonintuitive constraint (the \"uniqueness\" of functional dependencies) must be assumed. We demonstrate the fourth normal form approach, which not only can deal with this difficulty, but which is also more powerful than either of the earlier approaches. The input of the new method includes attributes (potential column names), along with semantic information in the form of functional and multivalued dependencies the output is a \"good\" (fourth normal form) logical design. The new method is semi-automatic, which is especially helpful in the case of a very large database with many attributes that interrelate in complex ways.|Ronald Fagin","79862|VLDB|1977|A Scan-Driven Sort Facility for a Relational Database System|In this paper the design and implementation aspects of an integrated sort package are described which was implemented by the author as a part of an experimental relational database system called System R. The sort facility developed for variable length sort keys and tuples is discussed with regard to its anticipated use in relational database systems. It is considered to be a powerful tool to implement complex relational operations like join and projection and to support the dynamic creation of access paths and the loading and reorganizing of data clustered by field values. The input loop of the sort can be driven by various scans which allow the filtering of qualified tuples according to disjunctive normal forms of simple search arguments. Various aspects of the integration with storage and transaction management, locking and logging components are described. A number of design decisions for the sort facility concerning sorting, peerage, and merging techniques are discussed. Finally, some implementation aspects for sorting variable length keys consisting of a number of distributed fields are pointed out.|Theo H√§rder","79789|VLDB|1975|The Enity-Relationship Model Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed.The entity-relationship model can be used as a basis for unification of different views of data the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.|Peter P. Chen","80013|VLDB|1980|An Intuitive Approach to Normalize Network Structured Data|Although the relational model of data has been around since , it has not been very easy for the practitioners to understand it. A part of this difficulty can be attributed to the fact that there is no easy-to-understand method for normalization of existing data which will enable users to visualize their own data relationally. Moreover, relational model serves an important function as a tool in the problems related to design or conversion of data. Practitioner's accessibility of this tool can be facilitated if an intuitive procedure to normalize existing data in a general network form is developed. This paper is an extension of the author's concept of Schema Analysis and Schema Diagrams (see ) in that direction. The problem is discussed in the context of database design.|Shamkant B. Navathe","79880|VLDB|1977|A Consideration on Normal Form of Not-Necessarily-Normalized Relation in the Relational Data Model|In this paper definitions of unnormalized relation, functional dependency on it and Normal Form are presented. The Normal Form plays a key role in our relational data model in which unnormalized relations are admitted as does the Third Normal Form of Codd in the data model of normalized relation. Properties pertaining to Normal Form are discussed emphasizing comparison with NF along with analysis of some typical examples. Similarity and dissimilarity between new functional dependency and Fagin's multivalued dependency are also discussed and presented in the form of proposition.|Akifumi Makinouchi","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","79875|VLDB|1977|Design Criteria for Distributed Database Systems|This paper clarifies design criteria for adaptive heterogeneous distributed database systems. Compositional and decompositional design approaches, database sharing models, and an abstract design approach are discussed.|Tosiyasu L. Kunii,Hideko S. Kunii","80348|VLDB|2003|Distributed Top-N Query Processing with Possibly Uncooperative Local Systems|We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to find the N tuples that satisfy the query the best but not necessarily completely in an efficient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The first step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four different techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare different methods and very promising results are obtained using the method that requires no cooperation from local databases.|Clement T. Yu,George Philip,Weiyi Meng","80034|VLDB|1981|Derived Relations A Unified Mechanism for Views Snapshots and Distributed Data|In a relational system, a database is composed of base relations, views, and snapshots. We show that this traditional approach can be extended to different classes of derived relations, and we propose a unified data definition mechanism for centralized and distributed databases. Our mechanism, called DEREL, can be used to define base relations and to derive different classes of views, snapshots, partitioned and replicated data. DEREL is intended to be part of a general purpose distributed relational database management system.|Michel E. Adiba","80285|VLDB|2003|Querying the Internet with PIER|The database research community prides itself on scalable technologies. Yet database systems traditionally do not excel on one important scalability dimension the degree of distribution. This limitation has hampered the impact of database technologies on massively distributed systems like the Internet. In this paper, we present the initial design of PIER, a massively distributed query engine based on overlay networks, which is intended to bring database query processing facilities to new, widely distributed environments. We motivate the need for massively distributed queries, and argue for a relaxation of certain traditional database research goals in the pursuit of scalability and widespread adoption. We present simulation results showing PIER gracefully running relational queries across thousands of machines, and show results from the same software base in actual deployment on a large experimental cluster.|Ryan Huebsch,Joseph M. Hellerstein,Nick Lanham,Boon Thau Loo,Scott Shenker,Ion Stoica"],["79982|VLDB|1979|Database Program Conversion A Framework for Research|As requirements change, database administrators come under pressure to change the schema which is a description of the database structure. Although writing a new schema is a relatively easy job and transforming the database to match the schema can be accomplished with a modest effort, transforming the numerous programs which operate on the database often requires enormous effort. This interim report describes previous research, defines the problem and proposes a framework for research on the automatic conversion of database programs to match the schema transformations. The approach is based on a precise description of the data structures, integrity constraints, and permissible operations. This work will help designers of manual and computer aided conversion facilities, database administrators who are considering conversions and developers of future database management systems, which will have ease of conversion as a design goal.|Robert W. Taylor,James P. Fry,Ben Shneiderman,Diane C. P. Smith,Stanley Y. W. Su","80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80336|VLDB|2003|Tuple Routing Strategies for Distributed Eddies|Many applications that consist of streams of data are inherently distributed. Since input stream rates and other system parameters such as the amount of available computing resources can fluctuate significantly, a stream query plan must be able to adapt to these changes. Routing tuples between operators of a distributed stream query plan is used in several data stream management systems as an adaptive query optimization technique. The routing policy used can have a significant impact on system performance. In this paper, we use a queuing network to model a distributed stream query plan and define performance metrics for response time and system throughput. We also propose and evaluate several practical routing policies for a distributed stream management system. The performance results of these policies are compared using a discrete event simulator. Finally, we study the impact of the routing policy on system throughput and resource allocation when computing resources can be shared between operators.|Feng Tian,David J. DeWitt","80745|VLDB|2007|The End of an Architectural Era Its Time for a Complete Rewrite|In previous papers SC, SBC+, some of us predicted the end of \"one size fits all\" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by -- orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a \"one size fits all\" solution, in fact, excel at nothing. Hence, they are  year old legacy code lines that should be retired in favor of a collection of \"from scratch\" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs.|Michael Stonebraker,Samuel Madden,Daniel J. Abadi,Stavros Harizopoulos,Nabil Hachem,Pat Helland","80070|VLDB|1981|IML-Inscribed Nets for Modeling Text Processing and Data Base Management Systems|Predicatetransition-nets with a suitable inscription allow building practice-oriented and expressive models of distributed information systems with concurrent processes as found in office information systems or decentralized database management systems. The paper presents the first version of an inscription language called IML (Information Management Language) which has been derived from the results of some long range database research activities. The abstract objects \"construct\" and \"form\" are introduced and basic operations are defined on them. It is shown how to apply these language elements to inscribe net symbols for high-fidelity modeling of text processing and data management.|Gernot Richter","80439|VLDB|2004|A Multi-Purpose Implementation of Mandatory Access Control in Relational Database Management Systems|Mandatory Access Control (MAC) implementations in Relational Database Management Systems (RDBMS) have focused solely on Multilevel Security (MLS). MLS has posed a number of challenging problems to the database research community, and there has been an abundance of research work to address those problems. Unfortunately, the use of MLS RDBMS has been restricted to a few government organizations where MLS is of paramount importance such as the intelligence community and the Department of Defense. The implication of this is that the investment of building an MLS RDBMS cannot be leveraged to serve the needs of application domains where there is a desire to control access to objects based on the label associated with that object and the label associated with the subject accessing that object, but where the label access rules and the label structure do not necessarily match the MLS two security rules and the MLS label structure. This paper introduces a flexible and generic implementation of MAC in RDBMS that can be used to address the requirements from a variety of application domains, as well as to allow an RDBMS to efficiently take part in an end-to-end MAC enterprise solution. The paper also discusses the extensions made to the SQL compiler component of an RDBMS to incorporate the label access rules in the access plan it generates for an SQL query, and to prevent unauthorized leakage of data that could occur as a result of traditional optimization techniques performed by SQL compilers.|Walid Rjaibi,Paul Bird","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["80022|VLDB|1980|Status Report on ISOTCSCWG - Data Base Management Systems|This paper discussed the current status of the work of the ISOTCSCWG data base management systems working group. In existence for over five years, this working group has devoted it energies to the elaboration and extension of the notion of a \"conceptual schema\" as initially defined by the ANSIXSPARCDBMS Study Group. The focus of the ISO work has been the construction of a report on conceptual schema concepts, identification of various methodologies for their construction and the application of such methodologies to an example \"world\" in such a fashion that comparisons can be made between the different approaches. Underlying the work of this group has been the identification of fundamental philosophical differences in the various methodologies specifically that some assume the conceptual schema models the real world and others assume the conceptual schema models the data in the information base that holds information about the real world. This difference leads to considerable confusion in discussion when it is not explicitly recognized and one of the principal objectives of the ISOTCSCWG report is to expurgate that difference.|Thomas B. Steel Jr.","79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","79835|VLDB|1976|New Criteria for the Conceptual Model|In the three-level approach to data description, the conceptual or information level is supposed to provide a data independent model of the enterprise's information. It may be useful to examine the conceptual model in this liqht primarily, rather than as an extension of traditional data processing technology.|William Kent","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","79946|VLDB|1979|On the Role of Understanding Models in Conceptual Schema Design|This paper suggests that two levels should be considered when designing a conceptual schema the 'understanding level' and the 'conceptual data base level'. The first level includes two realms of importance  the set of information requirements (IRQ) and  the conceptual information model (CIM). The next level includes two realms  the conceptual data base model (CDBM) and  the conceptual processing model (CPM). The CIM is a full time-perspective, unrestricted view of the enterprise and aims at defining relevant relationships, events and inference relations. The CDBM is, together with CPM, a data base realization of CIM which satisfies IRQ. In this paper designs at both levels are illustrated and the utility of the conceptual information model is discussed.|Janis A. Bubenko Jr.","80042|VLDB|1981|A Database Approach to Modelling and Managing Security Information|This paper describes a model defining the security conceptual schema in a data base. The proposed model is a binary type. A language is presented to define the model. Finally a method is shown, suitable to automatically control the rights propagation in the data base.|U. Bussolati,Giancarlo Martella","79857|VLDB|1977|Coexistence and Transformation of Data|A general solution to the data transformation problem is essential for the coexistence of arbitrary data models within one and the same data base application or system. The described method of data transformation is founded on the ideas of conceptualization and deep structure well-known in linguistics. A method of conceptualization as well as some notes on a conceptual or deep-structure data model are presented. The data transformation method can be applied to any data model, and in particular to the coexistence architecture of DBMS, where the deep structure of data is chosen as a basis for the central schema.|Eckhard D. Falkenberg","79914|VLDB|1978|A Model and a Method for Logical Data Base Design|This paper presents both a data model and a method for its practical use. There are two main parts  the first one deals with the data model, new concepts adequate for the design of conceptual schema are introduced. The second part presents complete method for the construction of the logical data base structure according to the concepts of the given model.|Andr√© Flory,Jacques Kouloumdjian","79844|VLDB|1977|The Role Concept in Data Models|A new data model is described which permits the representation of the different roles which a real world entity may play. This data model is an extension of the network model, as used in I-D-S and its derivative, the CODASYL database system. The concepts of item and set are retained. The record concept has been refined to clarify the old record concept and introduce the new role-segment concept. The record represents the existence of an entity of the real world while the role-segment represents the existence of one of the entity's roles. A person and a corporation are examples of an entity, while a stockholder and a customer are examples of a role that either the person or corporation can assume. A role-segment occurrence serves to group and name the properties concerning the existence of one role. This paper shows that the record and role segment concepts can be integrated into the required data description and data manipulation language. The meta entity types of the role data model are contrasted with those of older data models for the data occurrence domain. The ambigous use of meta types by the older models is thus shown. This ambiguity appears to prohibit those older data models from serving as the basis of a conceptual schema where data transformation support of richer data models is required. The meta entity types of the role data model are identified, described and related to the real world, data occurrence, conceptual schema, and data description domains.|Charles W. Bachman,Manilal Daya","79969|VLDB|1979|Statistice for the Usage of a Conceptual Data Model as a Basis for Logical Data Base Design|A data base design aid, SYDADA, is introduced. Input to SYDADA is a requirements specification, expressed in a language called SYSDOC. A SYSDOC specification consists of two main parts a conceptual data model and a transaction description. The conceptual data model is a high level, implementation independent data description. The transaction description is expressed in a very high level language which uses the conceptual data model. SYDADA analyzes the requirements specification by modelling the specified system. In the system model the conceptual data model is the data base, and the transactions specified can be executed using this data base. The most important result of the analysis is a set of statistics showing how, and how frequently, the various parts of the conceptual data model are used. We have shown how these statistics can be used as a basis for logical data base design.|Ole Oren,Frode Aschim"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","79886|VLDB|1977|IDA An Intelligent Data Access Program|IDA was developed to allow a casual user to retrieve information from a data base, knowing the fields present in the data base, but not how they are distributed to the files. IDA is part of the LADDER System that accepts queries in a restricted subset of English about a data base stored on CCA's Datacomputer. IDA's input is a very simple, formal query language which is essentially a list of restrictions on fields and queries about fields, with no mention of the structure of the data base. It produces a series of DBMS queries, which are transmitted over the ARPA network. The results of these queries are combined by IDA to provide the answer to the user's query. In this paper, we define the input language, and give examples of IDA's behavior. We also present our representation of the \"structural schema,\" which is the information needed by IDA to know how the data base is actually organized. We give the heuristics used to produce a program in the language of the DBMS. Finally, we discuss the limitations of this approach, as well as future research areas.|Daniel Sagalowicz","79843|VLDB|1977|Structure and Implementation of a Relational Query Language for the Problem Solver|AQL is a relational query language intended for use by the non computer specialist for interactive problem solving. It combines relational capabilities with the powerful computational facilities and control structures of the host programming language. A prototype version of AQL, that has been implemented, is reviewed. The Relational Memory System has been designed to minimize the data access operations necessary to process queries. The interpreter has a user friendly interface which allows the use of default options, synonyms and definitions for the domain names, inference, and a menu facility for interactive completion of the query.|Francesco Antonacci,P. Dell'Orco,V. N. Spadavecchia,Antonio Turtur","80732|VLDB|2007|Supporting Time-Constrained SQL Queries in Oracle|The growing nature of databases, and the flexibility inherent in the SQL query language that allows arbitrarily complex formulations, can result in queries that take inordinate amount of time to complete. To mitigate this problem, strategies that are optimized to return the 'first-few rows' or 'top-k rows' (in case of sorted results) are usually employed. However, both these strategies can lead to unpredictable query processing times. Thus, in this paper we propose supporting time-constrained SQL queries. Specifically, a user issues a SQL query as before but additionally provides nature of constraint (soft or hard), an upper bound for query processing time, and acceptable nature of results (partial or approximate). The DBMS takes the criteria (constraint type, time limit, quality of result) into account in generating the query execution plan, which is expected (guaranteed) to complete in the allocated time for soft (hard) time constraint. If partial results are acceptable then the technique of reducing result set cardinality (i.e. returning first few or top-k rows) is used, whereas if approximate results are acceptable then sampling is used, to compute query results within the specified time limit. For the latter case, we argue that trading off quality of results for predictable response time is quite useful. However, for this case, we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval. This paper presents the notion of time-constrained SQL queries, discusses the challenges in supporting such a construct, describes a framework for supporting such queries, and outlines its implementation in Oracle Database by exploiting Oracle's cost-based optimizer and extensibility capabilities.|Ying Hu,Seema Sundara,Jagannathan Srinivasan","80421|VLDB|2004|Query Languages and Data Models for Database Sequences and Data Streams|We study the fundamental limitations of relational algebra (RA) and SQL in supporting sequence and stream queries, and present effective query language and data model enrichments to deal with them. We begin by observing the well-known limitations of SQL in application domains which are important for data streams, such as sequence queries and data mining. Then we present a formal proof that, for continuous queries on data streams, SQL suffers from additional expressive power problems. We begin by focusing on the notion of nonblocking (NB) queries that are the only continuous queries that can be supported on data streams. We characterize the notion of nonblocking queries by showing that they are equivalent to monotonic queries. Therefore the notion of NB-completeness for RA can be formalized as its ability to express all monotonic queries expressible in RA using only the monotonic operators of RA. We show that RA is not NB-complete, and SQL is not more powerful than RA for monotonic queries. To solve these problems, we propose extensions that allow SQL to support all the monotonic queries expressible by a Turing machine using only monotonic operators. We show that these extensions are (i) user-defined aggregates (UDAs) natively coded in SQL (rather than in an external language), and (ii) a generalization of the union operator to support the merging of multiple streams according to their timestamps. These query language extensions require matching extensions to basic relational data model to support sequences explicitly ordered by times-tamps. Along with the formulation of very powerful queries, the proposed extensions entail more efficient expressions for many simple queries. In particular, we show that nonblocking queries are simple to characterize according to their syntactic structure.|Yan-Nei Law,Haixun Wang,Carlo Zaniolo","80612|VLDB|2006|SIREN A Similarity Retrieval Engine for Complex Data|This paper presents a similarity retrieval engine - SIREN-that allows posing similarity queries in a relational DBMS using an extended syntax that adds the support for such type of queries in the SQL language. It discusses the main architecture of SIREN, describes some key features and provides a description of the demo.|Maria Camila Nardini Barioni,Humberto Luiz Razente,Agma J. M. Traina,Caetano Traina Jr.","80718|VLDB|2006|Answering Top-k Queries with Multi-Dimensional Selections The Ranking Cube Approach|Observed in many real applications, a top-k query often consists of two components to reflect a user's preference a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server  show that our proposed approaches have significant improvement over the previous methods.|Dong Xin,Jiawei Han,Hong Cheng,Xiaolei Li","80412|VLDB|2004|Schema-based Scheduling of Event Processors and Buffer Minimization for Queries on Structured Data Streams|We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach.|Christoph Koch,Stefanie Scherzinger,Nicole Schweikardt,Bernhard Stegmaier","79865|VLDB|1977|Developing a Natural Language Interface to Complex Data|Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed.|Gary G. Hendrix,Earl D. Sacerdoti,Daniel Sagalowicz,Jonathan Slocum","80293|VLDB|2003|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.|Christoph Koch"],["80189|VLDB|2002|Plan Selection Based on Query Clustering|Query optimization is a computationally intensive process, especially for complex queries. We present here a tool, called PLASTIC, that can be used by query optimizers to amortize the optimization cost. Our scheme groups similar queries into clusters and uses the optimizer-generated plan for the cluster representative to execute all future queries assigned to the cluster. Query similarity is evaluated based on a comparison of query structures and the associated table schemas and statistics, and a classifier is employed for efficient cluster assignments. Experiments with a variety of queries on a commercial optimizer show that PLASTIC predicts the correct plan choice in most cases, thereby providing significantly improved query optimization times. Further, when errors are made, the additional execution cost incurred due to the sub-optimal plan choices is marginal.|Antara Ghosh,Jignashu Parikh,Vibhuti S. Sengar,Jayant R. Haritsa","80757|VLDB|2007|On the Production of Anorexic Plan Diagrams|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduction has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, identifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches. We investigate here the plan reduction issue from theoretical, statistical and empirical perspectives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPCH-based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to \"anorexic\" (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs.|Harish D.,Pooja N. Darera,Jayant R. Haritsa","80616|VLDB|2006|Querying Business Processes|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (Business Process Execution Language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers(peers).We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80806|VLDB|2007|Monitoring Business Processes with Queries|Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. These are complex, often distributed, processes. Monitoring the execution of such processes for interesting patterns is critical for enforcing business policies and meeting efficiency and reliability goals. BP-Mon (Business Processes Monitoring) is a novel query language for monitoring business processes, that allows users to visually define monitoring tasks and associated reports, using a simple intuitive interface, similar to those used for designing BPEL processes. We describe here the BP-Mon language and its underlying formal model. We also present the language implementation and describe our novel optimization techniques. An important feature of the implementation is that BP-Mon queries are translated to BPEL processes that run on the same execution engine as the monitored processes. Our experiments indicate that this approach incurs very minimal overhead, hence is a practical and efficient approach to monitoring.|Catriel Beeri,Anat Eyal,Tova Milo,Alon Pilberg","80555|VLDB|2005|Consistently Estimating the Selectivity of Conjuncts of Predicates|Cost-based query optimizers need to estimate the selectivity of conjunctive predicates when comparing alternative query execution plans. To this end, advanced optimizers use multivariate statistics (MVS) to improve information about the joint distribution of attribute values in a table. The joint distribution for all columns is almost always too large to store completely, and the resulting use of partial distribution information raises the possibility that multiple, non-equivalent selectivity estimates may be available for a given predicate. Current optimizers use ad hoc methods to ensure that selectivities are estimated in a consistent manner. These methods ignore valuable information and tend to bias the optimizer toward query plans for which the least information is available, often yielding poor results. In this paper we present a novel method for consistent selectivity estimation based on the principle of maximum entropy (ME). Our method efficiently exploits all available information and avoids the bias problem. In the absence of detailed knowledge, the ME approach reduces to standard uniformity and independence assumptions. Our implementation using a prototype version of DB UDB shows that ME improves the optimizer's cardinality estimates by orders of magnitude, resulting in better plan quality and significantly reduced query execution times.|Volker Markl,Nimrod Megiddo,Marcel Kutsch,Tam Minh Tran,Peter J. Haas,Utkarsh Srivastava","80380|VLDB|2004|Automatic SQL Tuning in Oracle g|SQL tuning is a very critical aspect of database performance tuning. It is an inherently complex activity requiring a high level of expertise in several domains query optimization, to improve the execution plan selected by the query optimizer access design, to identify missing access structures and SQL design, to restructure and simplify the text of a badly written SQL statement. Furthermore, SQL tuning is a time consuming task due to the large volume and evolving nature of the SQL workload and its underlying data. In this paper we present the new Automatic SQL Tuning feature of Oracle g. This technology is implemented as a core enhancement of the Oracle query optimizer and offers a comprehensive solution to the SQL tuning challenges mentioned above. Automatic SQL Tuning introduces the concept of SQL profiling to transparently improve execution plans. It also generates SQL tuning recommendations by performing cost-based access path and SQL structure \"what-if\" analyses. This feature is exposed to the user through both graphical and command line interfaces. The Automatic SQL Tuning is an integral part of the Oracle's framework for self-managing databases. The superiority of this new technology is demonstrated by comparing the results of Automatic SQL Tuning to manual tuning using a real customer workload.|Beno√Æt Dageville,Dinesh Das,Karl Dias,Khaled Yagoub,Mohamed Za√Øt,Mohamed Ziauddin","80721|VLDB|2006|Automatic Extraction of Dynamic Record Sections From Search Engine Result Pages|A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features () it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and () it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.|Hongkun Zhao,Weiyi Meng,Clement T. Yu","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo"],["80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80377|VLDB|2004|HiFi A Unified Architecture for High Fan-in Systems|Advances in data acquisition and sensor technologies are leading towards the development of \"High Fan-in\" architectures widely distributed systems whose edges consist of numerous receptors such as sensor networks and RFID readers and whose interior nodes consist of traditional host computers organized using the principle of successive aggregation. Such architectures pose significant new data management challenges. The HiFi system, under development at UC Berkeley, is aimed at addressing these challenges. We demonstrate an initial prototype of HiFi that uses data stream query processing to acquire, filter, and aggregate data from multiple devices including sensor motes, RFID readers, and low power gateways organized as a High Fan-in system.|Owen Cooper,Anil Edakkunni,Michael J. Franklin,Wei Hong,Shawn R. Jeffery,Sailesh Krishnamurthy,Frederick Reiss,Shariq Rizvi,Eugene Wu 0002","80681|VLDB|2006|The New Casper Query Processing for Location Services without Compromising Privacy|This paper tackles a major privacy concern in current location-based services where users have to continuously report their locations to the database server in order to obtain the service. For example, a user asking about the nearest gas station has to report her exact location. With untrusted servers, reporting the location information may lead to several privacy threats. In this paper, we present Casper a new framework in which mobile and stationary users can entertain location-based services without revealing their location information. Casper consists of two main components, the location anonymizer and the privacy-aware query processor. The location anonymizer blurs the users' exact location information into cloaked spatial regions based on user-specified privacy requirements. The privacy-aware query processor is embedded inside the location-based database server in order to deal with the cloaked spatial areas rather than the exact location information. Experimental results show that Casper achieves high quality location-based services while providing anonymity for both data and queries.|Mohamed F. Mokbel,Chi-Yin Chow,Walid G. Aref","80712|VLDB|2006|State-Slice New Paradigm of Multi-query Optimization of Window-based Stream Queries|Modern stream applications such as sensor monitoring systems and publishsubscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams. Efficient sharing of computations among multiple continuous queries, especially for the memory- and CPU-intensive window-based operations, is critical. A novel challenge in this scenario is to allow resource sharing among similar queries, even if they employ windows of different lengths. This paper first reviews the existing sharing methods in the literature, and then illustrates the significant performance shortcomings of these methods.This paper then presents a novel paradigm for the sharing of window join queries. Namely we slice window states of a join operator into fine-grained window slices and form a chain of sliced window joins. By using an elaborate pipelining methodology, the number of joins after state slicing is reduced from quadratic to linear. This novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes. Based on the state-slice sharing paradigm, two algorithms are proposed for the chain buildup. One minimizes the memory consumption while the other minimizes the CPU usage. The algorithms are proven to find the optimal chain with respect to memory or CPU usage for a given query workload. We have implemented the slice-share paradigm within the data stream management system CAPE. The experimental results show that our strategy provides the best performance over a diverse range of workload settings among all alternate solutions in the literature.|Song Wang,Elke A. Rundensteiner,Samrat Ganguly,Sudeept Bhatnagar","80815|VLDB|2007|Data Access Patterns in The Amazoncom Technology Platform|The Amazon.com technology platform provides a set of highly advanced business and infrastructure services implemented using ultra-scalable distributed systems technologies. Within this environment we can identify a number of specific data access patterns, each with their own availability, consistency, performance and operational requirements in order to serve a collection of highly diverse business processes. In this presentation we will reviews these different patterns in detail and discuss which technologies are required to support them in an always-on environment.|Werner Vogels","80444|VLDB|2004|Clotho Decoupling memory page layout from storage organization|As database application performance depends on the utilization of the memory hierarchy, smart data placement plays a central role in increasing locality and in improving memory utilization. Existing techniques, however, do not optimize accesses to all levels of the memory hierarchy and for all the different workloads, because each storage level uses different technology (cache, memory, disks) and each application accesses data using different patterns. Clotho is a new buffer pool and storage management architecture that decouples in-memory page layout from data organization on non-volatile storage devices to enable independent data layout design at each level of the storage hierarchy. Clotho can maximize cache and memory utilization by (a) transparently using appropriate data layouts in memory and non-volatile storage, and (b) dynamically synthesizing data pages to follow application access patterns at each level as needed. Clotho creates in-memory pages individually tailored for compound and dynamically changing workloads, and enables efficient use of different storage technologies (e.g., disk arrays or MEMS-based storage devices). This paper describes the Clotho design and prototype implementation and evaluates its performance under a variety of workloads using both disk arrays and simulated MEMS-based storage devices.|Minglong Shao,Jiri Schindler,Steven W. Schlosser,Anastassia Ailamaki,Gregory R. Ganger","80246|VLDB|2002|Self-tuning Database Technology and Information Services from Wishful Thinking to Viable Engineering|Automatic tuning has been an elusive goal for database technology for a long time and is becoming a pressing issue for modern E-services. This paper reviews and assesses the advances that have been made on this important subject during the last ten years. A major conclusion is that self-tuning database technology should be based on the paradigm of a feedback control loop, but is also bound to build on mathematical models and their proper engineering into system components. In addition, the composition of information services into truly self-tuning, higher-level E-services may require a radical departure towards simpler, highly componentized software architectures with narrow interfaces between RISC-style \"autonomic\" components.|Gerhard Weikum,Axel M√∂nkeberg,Christof Hasse,Peter Zabback","80368|VLDB|2004|Integrating Automatic Data Acquisition with Business Processes - Experiences with SAPs Auto-ID Infrastructure|Smart item technologies, like RFID and sensor networks, are considered to be the next big step in business process automation . Through automatic and real-time data acquisition, these technologies can benefit a great variety of industries by improving the efficiency of their operations. SAP's Auto-ID infrastructure enables the integration of RFID and sensor technologies with existing business processes. In this paper we give an overview of the existing infrastructure, discuss lessons learned from successful customer pilots, and point out some of the open research issues.|Christof Bornh√∂vd,Tao Lin,Stephan Haller,Joachim Schaper","80345|VLDB|2003|Tabular Placement of Relational Data on MEMS-based Storage Devices|Due to the advances in semiconductor manufacturing, the gap between main memory and secondary storage is constantly increasing. This becomes a significant performance bottleneck for Database Management Systems, which rely on secondary storage heavily to store large datasets. Recent advances in nanotechnology have led to the invention of alternative means for persistent storage. In particular, MicroElectroMechanical Systems (MEMS) based storage technology has emerged as the leading candidate for next generation storage systems. In order to integrate MEMS-based storage into conventional computing platform, new techniques are needed for IO scheduling and data placement. In the context of relational data, it has been observed that access to relations needs to be enabled in both row-wise as well as in columnwise fashions. In this paper, we exploit the physical characteristics of MEMS-based storage devices to develop a data placement scheme for relational data that enables retrieval in both row-wise and column-wise manner. We demonstrate that this data layout not only improves IO utilization, but results in better cache performance.|Hailing Yu,Divyakant Agrawal,Amr El Abbadi","80376|VLDB|2004|Managing RDFI Data|Radio-Frequency Identification (RFID) technology enables sensors to efficiently and inexpensively track merchandise and other objects. The vast amount of data resulting from the proliferation of RFID readers and tags poses some interesting challenges for data management. We present a brief introduction to RFID technology and highlight a few of the data management challenges.|Sudarshan S. Chawathe,Venkat Krishnamurthy,Sridhar Ramachandran,Sanjay E. Sarma"],["80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","80507|VLDB|2005|iMeMex Escapes from the Personal Information Jungle|Modern computer work stations provide thousands of applications that store data in  . files on the file system of the underlying OS. To handle these files data processing logic is reinvented inside each application. This results in a jungle of data processing solutions and a jungle of data and file formats. For a user, it is extremely hard to manage information in this jungle. Most of all it is impossible to use data distributed among different files and formats for combined queries, e.g., join and union operations. To solve the problems arising from file based data management, we present a software system called iMeMex as a unified solution to personal information management and integration. iMeMex is designed to integrate seamlessly into existing operating systems like Windows, Linux and Mac OS X. Our system enables existing applications to gradually dispose file based storage. By using iMeMex modern operating systems are enabled to make use of sophisticated DBMS, IR and data integration technologies. The seamless integration of iMeMex into existing operating systems enables new applications that provide concepts of data storage and analysis unseen before.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles,Donald Kossmann,Lukas Blunschi","79805|VLDB|1975|Recent Results on the Attribute Based Data Model - A Tutorial|Data base systems are intended to help in the storage and retrieval of information which is needed to make decisions in an organization. Senko  has stated that we should try to assure \"() the efficient use of human resources in the design, implementation, and utilization of information systems, and () the efficient utilization of the physical mechanical resources of the systems themselves.\"|Douglas S. Kerr","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80290|VLDB|2003|Grid Data Management Systems  Services|The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities. Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies. The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.|Arun Jagatheesan,Reagan Moore,Norman W. Paton,Paul Watson","80232|VLDB|2002|Champagne Data Change Propagation for Heterogeneous Information Systems|Flexible methods supporting the data interchange between autonomous information systems are important for today's increasingly heterogeneous enterprise IT infrastructures. Updates, insertions, and deletions of data objects in autonomous information systems often have to trigger data changes in other autonomous systems, even if the distributed systems are not integrated into a global schema. We suggest a solution to this problem based on the propagation and transformation of data using several XML technologies. Our prototype manages dependencies between the schemas of distributed data sources and allows to define and process arbitrary actions on changed data by manipulating all dependent data sources. The prototype comprises a propagation engine that interprets scripts based on a workflow specification language, a data dependency specification tool, a system administration tool, and a repository that stores all relevant information for these tools.|Ralf Rantzau,Carmen Constantinescu,Uwe Heinkel,Holger Meinecke","79845|VLDB|1977|Protection of Information in Relational Data Bases|This paper is concerned with protection of information in relational databases from disclosure to properly identified users. It is assumed that the only means of access to the database is through a relational query language. The objective of the paper is to formalize the notion of protection. We first describe the information content of the database by a set of propositions and their truth values. The objects to be protected are (the truth values of) certain propositions that have been declared confidential. A query violates a protected proposition if its answer modifies the knowledge of tHe user about (the truth value of) this proposition. Following this approach, we propose a model for evaluating protection systems. In this model a protection system is characterized by the type of queries it takes as its input, the type of data it can protect, the means of protection against queries (e.g. rejection or modification) and the type of protection it provides (e.g., total protection, partial protection, protection against user's inference). Some examples of the use of the model as a tool for analysis are given.|Fran√ßois Bancilhon,Nicolas Spyratos","80752|VLDB|2007|Update Exchange with Mappings and Provenance|We consider systems for data sharing among heterogeneous peers related by a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to ask queries over related data from other peers as well. To achieve this, every peer's updates propagate along the mappings to the other peers. However, this update exchange is filtered by trust conditions --- expressing what data and sources a peer judges to be authoritative --- which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. These systems target scientific data sharing applications, and their general principles and architecture have been described in . In this paper we present methods for realizing such systems. Specifically, we extend techniques from data integration, data exchange, and incremental view maintenance to propagate updates along mappings we integrate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance we discuss strategies for implementing our techniques in conjunction with an RDBMS and we experimentally demonstrate the viability of our techniques in the ORCHESTRA prototype system.|Todd J. Green,Grigoris Karvounarakis,Zachary G. Ives,Val Tannen","79858|VLDB|1977|Using New Clues to Find Data|Conventional date base management systems use symbolic clues to retrieve data--descriptions of the name, source or contents of data. A new data base management system also allows retrieval of information using spatial clues--where the data is located in the data base management system --and iconic clues--how the data looks. Information is displayed in a simulated space through which users can \"fly\" in search of data, much as people store and retrieve information from particular places in their offices e.g. , specific places on a bookshelf, specific piles of paper on a table, specific places within a filing cabinet. Exploiting the power of human spatial memory, the new system has the significant advantage of allowing the location of information when it cannot be precisely specified e.g., getting near information and browsing through information.|Craig Fields,Nicholas Negroponte","80478|VLDB|2005|Approximate Matching of Hierarchical Data Using pq-Grams|When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.|Nikolaus Augsten,Michael H. B√∂hlen,Johann Gamper"],["79926|VLDB|1978|Mutual Dependencies and Some Results on Undecomposable Relations|In this paper the new concept of mutual dependency in data base relations is introduced. Mutual dependency appears as a generalization of functional and multivalued ones. Furthermore, it provides a necessary and sufficient condition for a relation to be nonloss decomposable into three of its projections. Then, attention is paid to more general decompositions and a necessary and sufficient condition for a relation to be undecomposable is proven.|Jean-Marie Nicolas","79876|VLDB|1977|Domain-Oriented Relational Languages|A view of the semantics of a relational data base consists in considering that the data base domains represent the sets of objects of the subject matter and that the relations represent various kinds of associations among these objects. This view is supported by query languages where each variable ranges on a domain of the relational data base and predicates correspond to the associations modeled by relations. This paper defines two domain-oriented relational languages. DRC is a many-sorted calculus, where the structural role of domains is emphasized by defining types for variables and type-checking rules. ILL is an English-like language, which is wholly built upon a structure of expressions nested inside other expressions. \"Domain-languages\" are contrasted with \"tuple languages\", which manipulate as basic objects the relation n-tuples. Directly manipulating domains and domain values interacts more directly with the semantics expressed by the relations and produces simpler and more English-like languages.|Michel Lacroix,Alain Pirotte","79880|VLDB|1977|A Consideration on Normal Form of Not-Necessarily-Normalized Relation in the Relational Data Model|In this paper definitions of unnormalized relation, functional dependency on it and Normal Form are presented. The Normal Form plays a key role in our relational data model in which unnormalized relations are admitted as does the Third Normal Form of Codd in the data model of normalized relation. Properties pertaining to Normal Form are discussed emphasizing comparison with NF along with analysis of some typical examples. Similarity and dissimilarity between new functional dependency and Fagin's multivalued dependency are also discussed and presented in the form of proposition.|Akifumi Makinouchi","80108|VLDB|1985|Computing Queries from Derived Relations|Assume that a set of derived relations are available in stored form. Given a query (or subquery), can it be computed from the derived rela- tions and, if so, how Variants of this problem arise in several areas of query processing. Relation fragments stored at a site in a distributed database system, data- base snapshots, and intermediate results obtained dur- ing the processing of a query are all examples of stored, derived relations. In this paper we give neces- sary and sufficient conditions for when a query is com- putable from a single derived relation. It is assumed that both the query and the derived relation are defined by PSJ-expressions, that is, relational algebra expressions involving only projections, selections, and joins, in any combination. The solution is constructive not only does it tell whether the query is computable or not, but it also shows how to compute it.|Per-√\u2026ke Larson,H. Z. Yang","80079|VLDB|1981|Algorithms for Acyclic Database Schemes|Many real-world situations can be captured by a set of functional dependencies and a single join dependency of a particular form called acyclic B... The join dependency corresponds to a natural decomposition into meaningfull objects (an acyclic database scheme). Our purpose in this paper is to describe efficient algorithms in this setting for various problems, such as computing projections, minimizing joins, inferring dependencies, and testing for dependency satisfaction.|Mihalis Yannakakis","79905|VLDB|1978|A Sophisticates Introduction to Database Normalization Theory|Formal database semantics has concentrated on dependency constraints, such as functional and multivalued dependencies, and on normal forms for relations. Unfortunately, much of this work has been inaccessible to researchers outside this field, due to the unfamiliar formalism in which the work is couched. In addition, the lack of a single set of definitions has confused the relationships among certain results. This paper is intended to serve the two-fold purpose of introducing the main issues and theorems of formal database semantics to the uninitiated, and to clarify the terminology of the field.|Catriel Beeri,Philip A. Bernstein,Nathan Goodman","79934|VLDB|1978|A Normal Form for Abstract Syntax|McCarthy's abstract syntax is the most widely used metalanguage for specifying data structure. It is embedded in various forms in most recent programing languages and data models. A simpler, yet more powerful, abstract syntax is defined which is particularly effective in database applications. An abstract syntax specification shows how objects are composed as the union and cartesian product of other objects. If a specification is not properly constructed, it is demonstrably difficult to write application programs, maintain database integrity and provide graceful evolution. A normal form, called (,)NF, for abstract syntax specifications is introduced. Specifications in this normal form are subject to far fewer of the above utilization problems. Unlike previous normal forms which only prescribe composition with respect to cartesian product, (,)NF also prescribes composition with respect to union. Examples of normalization are given and the advantages and limitations of the approach are discussed.|John Miles Smith","80232|VLDB|2002|Champagne Data Change Propagation for Heterogeneous Information Systems|Flexible methods supporting the data interchange between autonomous information systems are important for today's increasingly heterogeneous enterprise IT infrastructures. Updates, insertions, and deletions of data objects in autonomous information systems often have to trigger data changes in other autonomous systems, even if the distributed systems are not integrated into a global schema. We suggest a solution to this problem based on the propagation and transformation of data using several XML technologies. Our prototype manages dependencies between the schemas of distributed data sources and allows to define and process arbitrary actions on changed data by manipulating all dependent data sources. The prototype comprises a propagation engine that interprets scripts based on a workflow specification language, a data dependency specification tool, a system administration tool, and a repository that stores all relevant information for these tools.|Ralf Rantzau,Carmen Constantinescu,Uwe Heinkel,Holger Meinecke","80070|VLDB|1981|IML-Inscribed Nets for Modeling Text Processing and Data Base Management Systems|Predicatetransition-nets with a suitable inscription allow building practice-oriented and expressive models of distributed information systems with concurrent processes as found in office information systems or decentralized database management systems. The paper presents the first version of an inscription language called IML (Information Management Language) which has been derived from the results of some long range database research activities. The abstract objects \"construct\" and \"form\" are introduced and basic operations are defined on them. It is shown how to apply these language elements to inscribe net symbols for high-fidelity modeling of text processing and data management.|Gernot Richter","80044|VLDB|1981|Join Graphs and Acyclic Database Schemes|The use of one acyclic join dependency and a set of functional dependencies appears powerful enough to represent data semantics in a relational database. There are several ways to characterize the acyclic join dependency, each providing its own desirable properties. In this paper we provide a sufficient condition for a join dependency to be acyclic and an algorithm that can aid a database designer. If the join dependency the designer wishes to use is cyclic, then, based on the results of the algorithm, we give two distinct methods that the designer can use to solve this problem. The methods can be implemented syntactically however, the choice of a solution in a given case depends on the intended semantics. We also show how a combination of the two methods might be best in some cases.|Karen Chase"],["79917|VLDB|1978|Application Specification for Distributed Data Base Systems|Distributed data base systems can take on various architectures that pose different requirements and have both logical and performance implications for the application. Conversely, the application specifics also influence the choice of architecture. There is thus an interplay between choice of architecture and application specification. However, it has been argued that since the application need is often defined first, even before the decision to use a distributed approach is made, it should be possible for the application to be specified independently of the architecture, provided that an architecture - independent application definition scheme for distributed data base systems is available. The circularity of causality can then be cut and the design can proceed hierarchically. The essentials of such a scheme are presented here. The scheme uses the concept of abstract data types with two extensions termed instance extension and disinstantiation and is also applicable to data base systems not distributed.|Prakash G. Hebalkar","79996|VLDB|1980|Update Synchronization in Distributed Databases|A critical problem in the implementation of distributed databases is that of update \"synchronization.\" This paper presents a distributed algorithm for update control whereby the update initiating node acts as a semi- centralized manager for that update. A timestamp is not required to be stored with each of the data items for this algorithm to work. An analysis of the performance of the algorithm, along with a comparison with the majority consensus algorithm, is also included.|Wing Kai Cheng,Geneva G. Belford","79968|VLDB|1979|Application of Sub-Predicate Tests in Database Systems|The work described in this paper outgrew the design process of the distributed database system VDN. Sub-predicate tests can be used for distributing data, authorization, access path selection, subtype selection, synchronization and integrity constraints. An algorithm for sub-predicate testing for a suitable class of predicates is presented and its time-behaviour is investigated.|Rudolf Munz,H.-J. Schneider,Frank Steyer","79985|VLDB|1979|Decentralized Authorization in a Database System|Decentralization of the security functions of the database administrator is needed in large or complex databases for efficiency and simplicity, independently of whether the database itself is centralized or is distributed throughout a network. In this paper, a model of authorization for decentralized administration is developed. A key concept of the model is the use of data classes, sets of database objects, as the unit of delegation of authorization functions. Mechanisms for the delegation of administration and its revocation are presented. A possible scheme for the distribution of authorization-related data throughout a distributed processing network is described.|Christopher Wood,Eduardo B. Fern√°ndez","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","79885|VLDB|1977|Architectural Issues in Distributed Data Base Systems|The design of a distributed data base is a complex and difficult task requiring careful consideration of data organization, data distribution, user interface, updatingretrieval schemes, programdata placement, security policies and reliability issues. In this paper, we have discussed a design methodology which can be used to design a distributed data base. This design methodology is a systematic way to guide the designer in making decisions during the requirement process phase, the design process phase and the implementation phase. We conclude this paper by examining the architectural issues in the design of distributed data bases.|C. V. Ramamoorthy,Gary S. Ho,T. Kirshnarao,Benjamin W. Wah","79945|VLDB|1979|Transaction and Catalog Management of the Distributed File Management System DISCO|This paper presents the structure and concepts of two main functional components of the distributed file management system DISCO The component 'access management for logical files' is responsible for the access to the distributed data. In order to achieve parallel transaction processing a page buffer system is maintained. It is shown how this page buffer system fits into the recovery concepts of DISCO. The main tasks of the 'management of the catalog for logical files' are to provide a network wide unique naming space and to provide file description and mapping. The structure of the catalog system and the mechanism to preserve consistency of redundant file descriptions are described.|H. Breitwieser,U. Kersten","79935|VLDB|1978|MICRONET A Microcomputer Network System for Managing Distributed Relational Databases|The hardware and software design and the analysis and evaluation of a microcomputer network system for managing distributed relational databases are presented. The system contains a set of commercially available microcomputers interconnected in a simple architectural structure with one serving as a controller processor and the rest as data processors which process distributed relation segments in a parallel fashion. A special purpose communication controller is designed to support data communication, command control, and synchronization among the set of processors. Parallel algorithms are formulated to carry out the macrocommands designed for processing relational algebraic operators. Data organization and processing technique are used to simulate associative processing of data by content and context rather than by their addresses. A failing processor can be replaced by any available processor. Thus, the system is a simple, low cost, highly reliable, flexible and efficient network.|Stanley Y. W. Su,Stefan Lupkiewicz,Chang-jung Lee,Der Her Lo,Keith L. Doty"]]},"title":{"entropy":5.63303710279607,"topics":["and the, and data, the data, model and, the, schema matching, time series, the model, data model, schema mapping, the for, schema xml, conceptual schema, and, for and, the conceptual, structures and, data manipulation, data quality, and programming","query for, query optimization, and database, optimization for, for database, query processing, the query, and web, the web, query and, query language, query database, web services, query, the database, and optimization, for web, database system, for and, search web","data base, for data, for system, data, database system, data management, system, data system, management system, and data, for base, base system, relational database, large base, large data, distributed system, the system, and base, concurrency control, for and","data streams, for xml, efficient for, algorithms for, efficient, queries, efficient and, query xml, xml, for streams, for search, for queries, search engine, materialized views, over data, xml documents, load shedding, business processes, keyword search, answering queries","the data, the, the for, the database, from data, data quality, the presence, the platform, from the, the analysis, the user, the without, information the, from, new, preserving, exploration, towards","the schema, schema matching, and time, and privacy, schema xml, schema mapping, and schema, for schema, the conceptual, conceptual schema, data privacy, and its, schema, mapping and, schema data, and conceptual, support, tables, database","with, with system, with the, for with, file, processor, text, information, multiple, towards, extraction, supporting, memory, challenges, tracking","large database, performance database, for large, techniques for, and techniques, evaluation database, and large, large, evaluation and, performance for, for and, techniques, using, evaluation, retrieval, integration, sequences, sources, improving, selection","relational data, relational base, relational database, approach data, relational system, data warehouse, for relational, and relational, the relational, for analysis, approach and, data database, implementation base, the implementation, approach database, data storage, program conversion, analysis data, approach system, implementation data","for design, data design, base design, tools for, database design, design system, for base, design and, for data, tools data, architecture for, method for, different data, the design, design, method and, file design, for logical, logical data, for and","efficient for, over data, efficient and, for processing, processing queries, processing xml, efficient data, queries over, query processing, efficient queries, efficient, over streams, over xml, for over, efficient query, efficient processing, processing data, efficient over, the queries, search over","data streams, for streams, framework for, load shedding, queries streams, clustering data, load streams, for data, and streams, shedding streams, uncertain data, scheduling for, mining streams, for clustering, for mining, for graph, framework streams, for join, join streams, continuous"],"ranking":[["79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","79808|VLDB|1975|Automatic Data Base Schema Design and Optimization|The production of an appropriate CODASYL Data Base Task Group (DBTG) Data Description Language (DDL) schema for a given data management application is a significant design problem. This research is devoted to the development of a methodology to automate and optimize the design of DBTG schema structures, using analytic modelling and optimization techniques. Given an implementation independent description of the data management requirements, it is possible to produce a schema configuration which is optimized with respect to logical record access rate, subject to storage and feasibility constraints, within a selected class of schemas. The storageaccess rate trade off is expressable as an integer program, which can be mapped into a network traversal problem with a known dynamic programming solution.|Michael F. Mitoma,Keki B. Irani","80071|VLDB|1981|A Method for Defining Information Structures and Transactions in Conceptual Schema Design|A method for defining the conceptual schema of databases is discussed within the framework of the ER(Entity-Relationship) model. Analyzing the semantic aspects of the ER schema based on the notions such the normalization, the gneralization, the instantiation, and the typification, the initial schema can be refined to have more natural and clearer meanings. We further analyze the processing requirements for the database and formalize them into the transaction descriptions which reflect the behavioral properties on the database. From the set of the transaction descriptions, the distribution of transaction workloads on the database is easily predicted.|Hirotaka Sakai","79822|VLDB|1975|Features of a Conceptual Schema|Data Base Management Systems (DBMS's) can be divided in three main categories according to the basic data organization offered to their users. The three approaches are hierarchical, network and relational. In future DBMS's all views may have to be offered to please a diverse population of users. Acknowledging this requirement a common facility has often been proposed under the name of a conceptual schema. The conceptual schema comprises a common denominator for all three DBMS approaches.|Dennis Tsichritzis","79908|VLDB|1978|Data Model Equivalence|The current proliferation of proposals for database system data models and the desire for database systems which support several different data models raise many questions concerning \"equivalence properties\" of different data models. To answer these questions, one first needs clear definitions of the concepts under discussion. This paper presents formal definitions of the terms database, operation, operation type, application model and data model. Using this formal framework, dabase state equivalence, operation equuivalence, application model equivalence and data model equivalence are distinguished. Three types of application and data model equivalence are defined isomorphic, composed operation and state dependent. Possibilities for partial equivalences are mentioned. Implementation implications of these different equivalences are discussed.|Sheldon A. Borkin","80747|VLDB|2007|Model Management and Schema Mappings Theory and Practice|We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.|Philip A. Bernstein,Howard Ho","79969|VLDB|1979|Statistice for the Usage of a Conceptual Data Model as a Basis for Logical Data Base Design|A data base design aid, SYDADA, is introduced. Input to SYDADA is a requirements specification, expressed in a language called SYSDOC. A SYSDOC specification consists of two main parts a conceptual data model and a transaction description. The conceptual data model is a high level, implementation independent data description. The transaction description is expressed in a very high level language which uses the conceptual data model. SYDADA analyzes the requirements specification by modelling the specified system. In the system model the conceptual data model is the data base, and the transactions specified can be executed using this data base. The most important result of the analysis is a set of statistics showing how, and how frequently, the various parts of the conceptual data model are used. We have shown how these statistics can be used as a basis for logical data base design.|Ole Oren,Frode Aschim","80007|VLDB|1980|Splitting the Conceptual Schema|The conceptual schema at the heart of the ANSI DBSG three-schema framework has been the subject of several polarizing arguments. Splitting the concep-tual schema yields a four-schema framework which resolves some of those arguments. The proposed approach also provides a natural growth path for the evolution of current data base systems, with gradual integration of dictionary and database design facilities.|William Kent"],["80058|VLDB|1981|QUIST A System for Semantic Query Optimization in Relational Databases|Semanfic query ophizotion is an approach to query optimization that uses knowledge of the semantics of the data to transform a query into another query that has the same answer but can be processed more efficiently. However, the indiscriminate application of semantic transformations can itself be excessively costly when there are many semantic rules upon which transformations can be based. This paper describes a semantic query optimization system called QUIST (Query lmprovement through Semantic Transformation). QUIST demonstrates significant cost reductions for a class of relational database queries. At the same time, QUIST uses knowledge about relational database structures and processing methods to insure that semantic transformations are applied selectively. This knowledge reflects cost models and optimization techniques developed in recent query optimization research. To integrate semantic, structure, and processing knowledge, QUIST analyzes a query at several levels of detail, along the lines of the plan-generate-test paradigm of artificial intelligence systems.|Jonathan J. King","80064|VLDB|1981|The Optimization Strategy for Query Evaluation in RDBV|RDBV is a fully relational database managenent system that has been developed for end users. In implementing the system, we focused our efforts upon making the system easy to use and efficient in query evaluation. This paper deals with how to gain efficiency in query evaluation that is, optimization of query evaluation. Our approach for optimization is a heuristic one. Some of the characteristics of our approach are () access cost evaluation based on statistical data, () dynamic gathering of data for use in speeding up join operation, () integration of facility for optimization into the view mechanism, and () global optimization after local optimization.|Akifumi Makinouchi,Masayoshi Tezuka,Hajime Kitakami,S. Adachi","80358|VLDB|2004|Multi-objective Query Processing for Database Systems|Query processing in database systems has developed beyond mere exact matching of attribute values. Scoring database objects and retrieving only the top k matches or Pareto-optimal result sets (skyline queries) are already common for a variety of applications. Specialized algorithms using either paradigm can avoid nave linear database scans and thus improve scalability. However, these paradigms are only two extreme cases of exploring viable compromises for each user's objectives. To find the correct result set for arbitrary cases of multi-objective query processing in databases we will present a novel algorithm for computing sets of objects that are nondominated with respect to a set of monotonic objective functions. Naturally containing top k and skyline retrieval paradigms as special cases, this algorithm maintains scalability also for all cases in between. Moreover, we will show the algorithm's correctness and instance-optimality in terms of necessary object accesses and how the response behavior can be improved by progressively producing result objects as quickly as possible, while the algorithm is still running.|Wolf-Tilo Balke,Ulrich G√ºntzer","80805|VLDB|2007|Depth Estimation for Ranking Query Optimization|A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just \"enough\" of the input in order to generate just \"enough\" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.|Karl Schnaitter,Joshua Spiegel,Neoklis Polyzotis","80458|VLDB|2004|Instance-based Schema Matching for Web Databases by Domain-specific Query Probing|In a Web database that dynamically provides information in response to user queries, two distinct schemas, interface schema (the schema users can query) and result schema (the schema users can browse), are presented to users. Each partially reflects the actual schema of the Web database. Most previous work only studied the problem of schema matching across query interfaces of Web databases. In this paper, we propose a novel schema model that distinguishes the interface and the result schema of a Web database in a specific domain. In this model, we address two significant Web database schema-matching problems intra-site and inter-site. The first problem is crucial in automatically extracting data from Web databases, while the second problem plays a significant role in meta-retrieving and integrating data from different Web databases. We also investigate a unified solution to the two problems based on query probing and instance-based schema matching techniques. Using the model, a cross validation technique is also proposed to improve the accuracy of the schema matching. Our experiments on real Web databases demonstrate that the two problems can be solved simultaneously with high precision and recall.|Jiying Wang,Ji-Rong Wen,Frederick H. Lochovsky,Wei-Ying Ma","80375|VLDB|2004|Probabilistic Ranking of Database Query Results|We investigate the problem of ranking answers to a database query when many tuples are returned. We adapt and apply principles of probabilistic models from Information Retrieval for structured data. Our proposed solution is domain independent. It leverages data and workload statistics and correlations. Our ranking functions can be further customized for different applications. We present results of preliminary experiments which demonstrate the efficiency as well as the quality of our ranking system.|Surajit Chaudhuri,Gautam Das,Vagelis Hristidis,Gerhard Weikum","80134|VLDB|1992|Parametric Query Optimization|In most database systems, the values of many important run-time parameters of the system, the data, or the query are unknown at query optimization time. Parametric query optimization attempts to identify at compile time several execution plans, each one of which is optimal for a subset of all possible values of the run-time parameters. The goal is that at run time, when the actual parameter values are known, the appropriate plan should be identifiable with essentially no overhead. We present a general formulation of this problem and study it primarily for the buffer size parameter. We adopt randomized algorithms as the main approach to this style of optimization and enhance them with a sideways information passing feature that increases their effectiveness in the new task. Experimental results of these enhanced algorithms show that they optimize queries for large numbers of buffer sizes in the same time needed by their conventional versions for a single buffer size, without much sacrifice in the output quality and with essentially zero run-time overhead.|Yannis E. Ioannidis,Raymond T. Ng,Kyuseok Shim,Timos K. Sellis","80442|VLDB|2004|Green Query Optimization Taming Query Optimization Overheads through Plan Recycling|PLASTIC  is a recently-proposed tool to help query optimizers significantly amortize optimization overheads through a technique of plan recycling. The tool groups similar queries into clusters and uses the optimizer-generated plan for the cluster representative to execute all future queries assigned to the cluster. An earlier demo  had presented a basic prototype implementation of PLASTIC. We have now significantly extended the scope, useability, and efficiency of PLASTIC, by incorporating a variety of new features, including an enhanced query feature vector, variable-sized clustering and a decision-tree-based query classifier. The demo of the upgraded PLASTIC tool is shown on commercial database platforms (IBM DB and Oracle i).|Parag Sarda,Jayant R. Haritsa","79986|VLDB|1979|Query Processing in a Relational Database Management System|In this paper the various tactics for query processing in INGRESS are empirically evaluated on a test bed of sample queries.|Karel Youssefi,Eugene Wong","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani"],["79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","79860|VLDB|1977|A Data Base Design Decision Support System|The task of physical data base design in an DBTG enviornment is examined. Generation of an internal schema which considers questions of storage versus access costs, efficient implementation of data relationships, efficient placement of data within the data base and allocation of primary and secondary storage is shown to be a formidable task. Current data base design aids are reviewed. One aid, a sophisicated mathematical model of DBTG data bases (Gerritsen ) is shown to be a potentially valuable tool. A limitation of this model is that no optimization algorithm other than total enumeration has been found. This paper proposes an implementation of the Gerritsen model. An interactive design tool, based upon the Gerritsen model, is discussed. A Data Base Design Decision Support System (DBD-DSS) for use by the DBA is developed. The objectives and structure of the DBD-DSS are examined. A comprehensive example illustrating both the user interface and the potential benefits of the interactive tool is presented.|Thomas J. Gambino,Rob Gerritsen","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","80033|VLDB|1981|A Commercial Back-End Data Base System|Back-end Data base machines receive considerable attention owing to their facility for improving performance of installations with heavy data base activity. Many prototype systems have been described in the litterature but few systems are commercially available. This paper presents a commercial back-end data base management machine called MIX (design code), which is developped by our company. The global architecture of this data base machine is described, and the principal issue, discussed here in, applies to its software.|J. P. Armisen,J. Y. Caleca","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","79831|VLDB|1976|An Enduser Guidance Component for an Intelligent Data Base System|A general application guidance system -GAGS- was developed to convenient enduser component for data base systems. It has the following four main features . Dialogue supported methods dictionary guides the user from a given application problem to the adequate solution method or program. . Dialogue supported data dictionary gives syntactic and semantic information about the data available in data administration systems. . Dialogue supported execution facility offers a system triggered execution of programs found with the help of the methods dictionary in order to process data identified by means of the data dictionary. . Dialogue supported system guidance enables the unexperienced user to work with software systems without being a DP-professional. The basic information structure for the interactive guidance which is the common basic concept of the four facilities listed above, is the information network. It is generated by breaking down the knowledge about the application area in question into information elements (network nodes) which are interconnected (network arcs).|R. Erbe,Georg Walch"],["80842|VLDB|2007|XSeek A Semantic XML Search Engine Using Keywords|We present XSeek, a keyword search engine that enables users to easily access XML data without the need of learning XPath or XQuery and studying possibly complex data schemas. XSeek addresses a challenge in XML keyword search that has been neglected in the literature how to determine the desired return information, analogous to inferring a \"return\" clause in XQuery. To infer the search semantics, XSeek recognizes possible entities and attributes in the data, differentiates search predicates and return specifications in the keywords, and generates meaningful search results based on the analysis.|Ziyang Liu,Jeffrey Walker,Yi Chen","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80706|VLDB|2006|Window-Aware Load Shedding for Aggregation Queries over Data Streams|Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a \"Window Drop\". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.|Nesime Tatbul,Stanley B. Zdonik","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80585|VLDB|2005|An Efficient and Versatile Query Engine for TopX Search|This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in ) the need to consider scores for XML elements while aggregating them at the document level, ) the combination of vague content conditions with XML path conditions, ) the need to relax query conditions if too few results satisfy all conditions, and ) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.|Martin Theobald,Ralf Schenkel,Gerhard Weikum","80260|VLDB|2003|XSEarch A Semantic Search Engine for XML|XSEarch, a semantic search engine for XML, is presented. XSEarch has a simple query language, suitable for a naive user. It returns semantically related document fragments that satisfy the user's query. Query answers are ranked using extended information-retrieval techniques and are generated in an order similar to the ranking. Advanced indexing techniques were developed to facilitate efficient implementation of XSEarch. The performance of the different techniques as well as the recall and the precision were measured experimentally. These experiments indicate that XSEarch is efficient, scalable and ranks quality results highly.|Sara Cohen,Jonathan Mamou,Yaron Kanza,Yehoshua Sagiv","80645|VLDB|2006|Efficient Discovery of XML Data Redundancies|As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often \"casually designed\" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.|Cong Yu,H. V. Jagadish","80611|VLDB|2006|On the Path to Efficient XML Queries|XQuery and SQLXML are powerful new languages for querying XML data. However, they contain a number of stumbling blocks that users need to be aware of to get the expected results and performance. For example, certain language features make it hard if not impossible to exploit XML indexes.The major database vendors provide XQuery and SQLXML support in their current or upcoming product releases. In this paper, we identify common pitfalls gleaned from the experiences of early adopters of this functionality. We illustrate these pitfalls through concrete examples, explain the unexpected query behavior, and show alternative formulations of the queries that behave and perform as anticipated. As results we provide guidelines for XQuery and SQLXML users, feedback on the language standards, and food for thought for emerging languages and APIs.|Andrey Balmin,Kevin S. Beyer,Fatma √\u2013zcan,Matthias Nicola","80769|VLDB|2007|Structured Materialized Views for XML Queries|The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad  prototype and we present a performance analysis.|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Yannis Papakonstantinou","80293|VLDB|2003|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.|Christoph Koch"],["80830|VLDB|2007|GhostDB Hiding Data from Prying Eyes|Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed, say, to the prying eyes of some insurance fraudster. GhostDB is an architecture and system to achieve this. You carry private data in a smart USB device (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose and the public data you access. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB device) in which data flows in only one direction from public to private. This demonstration shows GhostDB's query processing in action.|Christophe Salperwyck,Nicolas Anciaux,Mehdi Benzine,Luc Bouganim,Philippe Pucheral,Dennis Shasha","80820|VLDB|2007|From Data Privacy to Location Privacy Models and Algorithms|This tutorial presents the definition, the models and the techniques of location privacy from the data privacy perspective. By reviewing and revising the state of art research in data privacy area, the presenter describes the essential concepts, the alternative models, and the suite of techniques for providing location privacy in mobile and ubiquitous data management systems. The tutorial consists of two main components. First, we will introduce location privacy threats and give an overview of the state of art research in data privacy and analyze the applicability of the existing data privacy techniques to location privacy problems. Second, we will present the various location privacy models and techniques effective in either the privacy policy based framework or the location anonymization based framework. The discussion will address a number of important issues in both data privacy and location privacy research, including the location utility and location privacy trade-offs, the need for a careful combination of policy-based location privacy mechanisms and location anonymization based privacy schemes, as well as the set of safeguards for secure transmission, use and storage of location information, reducing the risks of unauthorized disclosure of location information. The tutorial is designed to be self-contained, and gives the essential background for anyone interested in learning about the concept and models of location privacy, and the principles and techniques for design and development of a secure and customizable architecture for privacy-preserving mobile data management in mobile and pervasive information systems. This tutorial is accessible to data management administrators, mobile location based service developers, and graduate students and researchers who are interested in data management in mobile information syhhhstems, pervasive computing, and data privacy.|Ling Liu","80028|VLDB|1980|On Retrieval from a Small Version of a Large Data Base|A person in the higher levels of a hierarchical organization may wish not to use a fully detailed data base, but rather an abstracted data base, perhaps dropping into detail in only a few areas. We compare answers from queries put to an abstracted data base with answers obtained by querying a full data base and then abstracting the result of the query. We show that, for some common relational retrievals, querying an abstracted data base always yields the correct information, plus, in some cases, some incorrect information, and we give simple conditions on the abstraction which ensure that only the correct information is fetched. For the cases in which the conditions may not hold, we suggest an ordering, called succinctness, for comparing the quality of different abstractions.|Adrian Walker","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80124|VLDB|1985|Hit Data Model Data Bases from the Functional Point of View|Basic notions of the HIT data model are presented. The model is based on the notion of function and uses the apparatus Of the typed lambda-calculus which enables clean and transparent formulation of the data base concepts at various levels of description.|Jir√≠ Zlatuska","80365|VLDB|2004|Managing Data from High-Throughput Genomic Processing A Case Study|Genomic data has become the canonical example of very large, very complex data sets. As such, there has been significant interest in ways to provide targeted database support to address issues that arise in genomic processing. Whether genomic data is truly a special case, or just another application area exhibiting problems common to other domains, is an as yet unanswered question. In this abstract, we explore the structure and processing requirements of a large-scale genome sequencing center, as a case study of the issues that arise in genomic data managements, and as a means to compare those issues with those that arise in other domains.|Toby Bloom,Ted Sharpe","80615|VLDB|2006|Inference of Concise DTDs from XML Data|We consider the problem to infer a concise Document Type Definition (DTD) for a given set of XML-documents, a problem which basically reduces to learning of concise regular expressions from positive example strings. We identify two such classes single occurrence regular expressions (SOREs) and chain regular expressions (CHAREs). Both classes capture the far majority of the regular expressions occurring in practical DTDs and are succinct by definition. We present the algorithm iDTD (infer DTD) that learns SOREs from strings by first inferring an automaton by known techniques and then translating that automaton to a corresponding SORE, possibly by repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. We show that iDTD outperforms existing systems in accuracy, conciseness and speed. In a scenario where only a very small amount of XML data is available, for instance when generated by Web service requests or by answers to queries, iDTD produces regular expressions which are too specific. Therefore, we introduce a novel learning algorithm CRX that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that CRX performs very well within its target class on very small data sets. Finally, we discuss incremental computation, noise, numerical predicates, and the generation of XML Schemas.|Geert Jan Bex,Frank Neven,Thomas Schwentick,Karl Tuyls","80510|VLDB|2005|Query Translation from XPath to SQL in the Presence of Recursive DTDs|The interaction between recursion in XPATH and recursion in DTDS makes it challenging to answer XPATH queries on XML data that is stored in an RDBMS via schema-based shredding. We present a new approach to translating XPATH queries into SQL queries with a simple least fixpoint (LFP) operator, which is already supported by most commercial RDBMS. The approach is based on our algorithm for rewriting XPATH queries into regular XPATH expressions, which are capable of capturing both DTD recursion and XPATH queries in a uniform framework. Furthermore, we provide an algorithm for translating regular XPATH queries to SQL queries with LFP, and optimization techniques for minimizing the use of the LFP operator. The novelty of our approach consists in its capability to answer a large class of XPATH queries by means of only low-end RDBMS features already available in most RDBMS. Our experimental results verify the effectiveness of our techniques.|Wenfei Fan,Jeffrey Xu Yu,Hongjun Lu,Jianhua Lu,Rajeev Rastogi","80463|VLDB|2004|Secure XML Publishing without Information Leakage in the Presence of Data Inference|Recent applications are seeing an increasing need that publishing XML documents should meet precise security requirements. In this paper, we consider data-publishing applications where the publisher specifies what information is sensitive and should be protected. We show that if a partial document is published carelessly, users can use common knowledge (e.g., \"all patients in the same ward have the same disease\") to infer more data, which can cause leakage of sensitive information. The goal is to protect such information in the presence of data inference with common knowledge. We consider common knowledge represented as semantic XML constraints. We formulate the process how users can infer data using three types of common XML constraints. Interestingly, no matter what sequences users follow to infer data, there is a unique, maximal document that contains all possible inferred documents. We develop algorithms for finding a partial document of a given XML document without causing information leakage, while allowing publishing as much data as possible. Our experiments on real data sets show that effect of inference on data security, and how the proposed techniques can prevent such leakage from happening.|Xiaochun Yang,Chen Li","80677|VLDB|2006|Quality Views Capturing and Exploiting the User Perspective on Data Quality|There is a growing awareness among life scientists of the variability in quality of the data in public repositories, and of the threat that poor data quality poses to the validity of experimental results. No standards are available, however, for computing quality levels in this data domain. We argue that data processing environments used by life scientists should feature facilities for expressing and applying quality-based, personal data acceptability criteria.We propose a framework for the specification of users' quality processing requirements, called quality views. These views are compiled and semi-automatically embedded within the data processing environment. The result is a quality management toolkit that promotes rapid prototyping and reuse of quality components. We illustrate the utility of the framework by showing how it can be deployed within Taverna, a scientific workflow management tool, and applied to actual workflows for data analysis in proteomics.|Paolo Missier,Suzanne M. Embury,R. Mark Greenwood,Alun D. Preece,Binling Jin"],["79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","80608|VLDB|2006|SPIDER a Schema mapPIng DEbuggeR|A schema mapping is a high-level declarative specification of how data structured under one schema, called the source schema, is to be transformed into data structured under a possibly different schema, called the target schema. We demonstrate SPIDER, a prototype tool for debugging schema mappings, where the language for specifying schema mappings is based on a widely adopted formalism. We have built SPIDER on top of a data exchange system, Clio, from IBM Almaden Research Center. At the heart of SPIDER is a data-driven facility for understanding a schema mapping through the display of routes. A route essentially describes the relationship between source and target data with the schema mapping. In this demonstration, we showcase our route engine, where we can display one or all routes starting from either source or target data, as well as the intermediary data and schema elements involved. In addition, we demonstrate \"standard\" debugging features for schema mappings that we have also built, such as computing and exploring routes step-by-step, stopping or pausing the computation with breakpoints, performing \"guided\" computation of routes by taking human input into account, as well as tracking the state of the target instance during the process of computing routes.|Bogdan Alexe,Laura Chiticariu,Wang Chiew Tan","79915|VLDB|1978|Concepts for Design of an Information System Conceptual Schema and its Utilization in the REMORA Project|The current Information System (IS) design methods restrict the IS design to the one based upon data bases. They totally omit to consider processing aspects. Nevertheless complete representation of real phenomena needs the simultaneous design and realisation of the IS three components  data, processing and their operational dynamics. We take in account all of these aspects and try to develop a piloting system for the design and realisation of IS (REMORA project). In this paper we define a global model for the design of a complete IS including data programs and time dynamics and we present the IS conceptual schema consistent with this orientation. This model is one of relational type. Its definition begins by a representation of the perceived reality analysed in terms of three categories  objects, operations, events. The integrity constrainsts associated to the model concepts allow the definition of the normalized conceptual schema that in particular ensures the non-redudancy quality of the IS data and processings.|Odile Foucaut,Colette Rolland","80647|VLDB|2006|Nested Mappings Schema Mapping Reloaded|Many problems in information integration rely on specifications, called schema mappings, that model the relationships between schemas. Schema mappings for both relational and nested data are well-known. In this work, we present a new formalism for schema mapping that extends these existing formalisms in two significant ways. First, our nested mappings allow for nesting and correlation of mappings. This results in a natural programming paradigm that often yields more accurate specifications. In particular, we show that nested mappings can naturally preserve correlations among data that existing mapping formalisms cannot. We also show that using nested mappings for purposes of exchanging data from a source to a target will result in less redundancy in the target data. The second extension to the mapping formalism is the ability to express, in a declarative way, grouping and data merging semantics. This semantics can be easily changed and customized to the integration task at hand. We present a new algorithm for the automatic generation of nested mappings from schema matchings (that is, simple element-to-element correspondences between schemas). We have implemented this algorithm, along with algorithms for the generation of transformation queries (e.g., XQuery) based on the nested mapping specification. We show that the generation algorithms scale well to large, highly nested schemas. We also show that using nested mappings in data exchange can drastically reduce the execution cost of producing a target instance, particularly over large data sources, and can also dramatically improve the quality of the generated data.|Ariel Fuxman,Mauricio A. Hern√°ndez,C. T. Howard Ho,Ren√©e J. Miller,Paolo Papotti,Lucian Popa","80071|VLDB|1981|A Method for Defining Information Structures and Transactions in Conceptual Schema Design|A method for defining the conceptual schema of databases is discussed within the framework of the ER(Entity-Relationship) model. Analyzing the semantic aspects of the ER schema based on the notions such the normalization, the gneralization, the instantiation, and the typification, the initial schema can be refined to have more natural and clearer meanings. We further analyze the processing requirements for the database and formalize them into the transaction descriptions which reflect the behavioral properties on the database. From the set of the transaction descriptions, the distribution of transaction workloads on the database is easily predicted.|Hirotaka Sakai","79822|VLDB|1975|Features of a Conceptual Schema|Data Base Management Systems (DBMS's) can be divided in three main categories according to the basic data organization offered to their users. The three approaches are hierarchical, network and relational. In future DBMS's all views may have to be offered to please a diverse population of users. Acknowledging this requirement a common facility has often been proposed under the name of a conceptual schema. The conceptual schema comprises a common denominator for all three DBMS approaches.|Dennis Tsichritzis","79946|VLDB|1979|On the Role of Understanding Models in Conceptual Schema Design|This paper suggests that two levels should be considered when designing a conceptual schema the 'understanding level' and the 'conceptual data base level'. The first level includes two realms of importance  the set of information requirements (IRQ) and  the conceptual information model (CIM). The next level includes two realms  the conceptual data base model (CDBM) and  the conceptual processing model (CPM). The CIM is a full time-perspective, unrestricted view of the enterprise and aims at defining relevant relationships, events and inference relations. The CDBM is, together with CPM, a data base realization of CIM which satisfies IRQ. In this paper designs at both levels are illustrated and the utility of the conceptual information model is discussed.|Janis A. Bubenko Jr.","80609|VLDB|2006|Incremental Schema Matching|The goal of schema matching is to identify correspondences between the elements of two schemas. Most schema matching systems calculate and display the entire set of correspondences in a single shot. Invariably, the result presented to the engineer includes many false positives, especially for large schemas. The user is often overwhelmed by all of the edges, annoyed by the false positives, and frustrated at the inability to see second- and third-best choices. We demonstrate a tool that circumvents these problems by doing the matching interactively. The tool suggests candidate matches for a selected schema element and allows convenient navigation between the candidates. The ranking of match candidates is based on lexical similarity, schema structure, element types, and the history of prior matching actions. The technical challenges are to make the match algorithm fast enough for incremental matching in large schemas and to devise a user interface that avoids overwhelming the user. The tool has been integrated with a prototype version of Microsoft BizTalk Mapper, a visual programming tool for generating XML-to-XML mappings.|Philip A. Bernstein,Sergey Melnik,John E. Churchill","80620|VLDB|2006|Putting Context into Schema Matching|Attribute-level schema matching has proven to be an important first step in developing mappings for data exchange, integration, restructuring and schema evolution. In this paper we investigate contextual schema matching, in which selection conditions are associated with matches by the schema matching process in order to improve overall match quality. We define a general space of matching techniques, and within this framework we identify a variety of novel, concrete algorithms for contextual schema matching. Furthermore, we show how common schema mapping techniques can be generalized to take more effective advantage of contextual matches, enabling automatic construction of mappings across certain forms of schema heterogeneity. An experimental study examines a wide variety of quality and performance issues. In addition, it demonstrates that contextual schema matching is an effective and practical technique to further automate the definition of complex data transformations.|Philip Bohannon,Eiman Elnahrawy,Wenfei Fan,Michael Flaster","80007|VLDB|1980|Splitting the Conceptual Schema|The conceptual schema at the heart of the ANSI DBSG three-schema framework has been the subject of several polarizing arguments. Splitting the concep-tual schema yields a four-schema framework which resolves some of those arguments. The proposed approach also provides a natural growth path for the evolution of current data base systems, with gradual integration of dictionary and database design facilities.|William Kent"],["80480|VLDB|2005|Benefits of Path Summaries in an XML Query Optimizer Supporting Multiple Access Methods|We compare several optimization strategies implemented in an XML query evaluation system. The strategies incorporate the use of path summaries into the query optimizer, and rely on heuristics that exploit data statistics.We present experimental results that demonstrate a wide range of performance improvements for the different strategies supported. In addition, we compare the speedups obtained using path summaries with those reported for index-based methods. The comparison shows that low-cost path summaries combined with optimization strategies achieve essentially the same benefits as more expensive index structures.|Attila Barta,Mariano P. Consens,Alberto O. Mendelzon","80241|VLDB|2002|Information Management Challenges from the Aerospace Industry|The aerospace industry poses significant challenges to information management unlike any other industry. Data management challenges arising from different segments of the aerospace business are identified through illustrative scenarios. These examples and challenges could provide focus and stimulus to further research in information management.|Suryanarayana M. Sripada","80799|VLDB|2007|Declarative Information Extraction Using Datalog with Embedded Extraction Predicates|In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework.|Warren Shen,AnHai Doan,Jeffrey F. Naughton,Raghu Ramakrishnan","79918|VLDB|1978|Current Research Into Specialized Processors For Text Information Retrieval|While there have been a number of projects involved with the design and construction of specialized processors to aid in the efficient operation of large structured database systems, such as RAP or CASSM, very little work has been done on comparable hardware for text information retrieval. This paper summarizes development efforts being carried out to produce backend systems for the efficient searching and retrieval of full text databases. The characteristics of text retrieval, and its special problems when compared to other database systems, are presented. Two representative applications are discussed, one the retrieval of relevant items from a database being updated online from messages originating from a large number of sources, and the second a legal reference system consisting of all court decisions. Processors to scan large amounts of data at speeds comparable to the transfer rate of the disks on which it is stored are presented, along with a network of simple processors to allow rapid merging of directory information for inverted file systems.|Lee A. Hollaar,David C. Roberts","80628|VLDB|2006|Efficiently Linking Text Documents with Relevant Structured Information|Faced with growing knowledge management needs, enterprises are increasingly realizing the importance of interlinking critical business information distributed across structured and unstructured data sources. We present a novel system, called EROCS, for linking a given text document with relevant structured data. EROCS views the structured data as a predefined set of \"entities\" and identifies the entities that best match the given document. EROCS also embeds the identified entities in the document, effectively creating links between the structured data and segments within the document. Unlike prior approaches, EROCS identifies such links even when the relevant entity is not explicitly mentioned in the document. EROCS uses an efficient algorithm that performs this task keeping the amount of information retrieved from the database at a minimum. Our evaluation shows that EROCS achieves high accuracy with reasonable overheads.|Venkatesan T. Chakaravarthy,Himanshu Gupta,Prasan Roy,Mukesh K. Mohania","79896|VLDB|1977|Design of a Balanced Multiple Valued File Organization Schema with the Least Redundancy|A new balanced file-organization scheme of order two for multiple-valued records is presented. This scheme is called HUBMFS (Hiroshima University Balanced Multiple-valued File-organization Scheme of order two). It is assumed that records are characterized by m attributes having n possible values each, and the query set consists of queries which specify values of two attributes. It is shown that the redundancy of the bucket (the probability of storing a record in the bucket) is minimized if and only if the structure of the bucket is a partite-claw. A necessary and sufficient condition for the existence of an HUBMFS, which is composed exclusively of partite-claw buckets, is given. A construction algorithm is also given. The proposed HUBMFS is superior to existing BMFS (Balanced Multiple-valued File-organization Schemes of order two) in that it has the least redundancy among all possible BMFS's having the same parameters and that it can be constructed for a less restrictive set of parameters.|Sumiyasu Yamamoto,Shinsei Tazawa,Kazuhiko Ushio,Hideto Ikeda","80649|VLDB|2006|Creating Probabilistic Databases from Information Extraction Models|Many real-life applications depend on databases automatically curated from unstructured sources through imperfect structure extraction tools. Such databases are best treated as imprecise representations of multiple extraction possibli-ties. State-of-the-art statistical models of extraction provide a sound probability distribution over extractions but are not easy to represent and query in a relational framework. In this paper we address the challenge of approximating such distributions as imprecise data models. In particular, we investigate a model that captures both row-level and column-level uncertainty and show that this representation provides significantly better approximation compared to models that use only row or only column level uncertainty. We present efficient algorithms for finding the best approximating parameters for such a model our algorithm exploits the structure of the model to avoid enumerating the exponential number of extraction possibilities.|Rahul Gupta,Sunita Sarawagi","80427|VLDB|2004|Enhancing PP File-Sharing with an Internet-Scale Query Processor|In this paper, we address the problem of designing a scalable, accurate query processor for peer-to-peer filesharing and similar distributed keyword search systems. Using a globally-distributed monitoring infrastructure, we perform an extensive study of the Gnutella filesharing network, characterizing its topology, data and query workloads. We observe that Gnutella's query processing approach performs well for popular content, but quite poorly for rare items with few replicas. We then consider an alternate approach based on Distributed Hash Tables (DHTs). We describe our implementation of PIERSearch, a DHT-based system, and propose a hybrid system where Gnutella is used to locate popular items, and PIERSearch for handling rare items. We develop an analytical model of the two approaches, and use it in concert with our Gnutella traces to study the trade-off between query recall and system overhead of the hybrid system. We evaluate a variety of localized schemes for identifying items that are rare and worth handling via the DHT. Lastly, we show in a live deployment on fifty nodes on two continents that it nicely complements Gnutella in its ability to handle rare items.|Boon Thau Loo,Joseph M. Hellerstein,Ryan Huebsch,Scott Shenker,Ion Stoica","80172|VLDB|2002|Fast and Accurate Text Classification via Multiple Linear Discriminant Projections|Abstract.Support vector machines (SVMs) have shown superb performance for text classification tasks. They are accurate, robust, and quick to apply to test instances. Their only potential drawback is their training time and memory requirement. For n training instances held in memory, the best-known SVM implementations take time proportional to n a, where a is typically between . and .. SVMs have been trained on data sets with several thousand instances, but Web directories today contain millions of instances that are valuable for mapping billions of Web pages into Yahoo-like directories. We present SIMPL, a nearly linear-time classification algorithm that mimics the strengths of SVMs while avoiding the training bottleneck. It uses Fisher's linear discriminant, a classical tool from statistical pattern recognition, to project training instances to a carefully selected low-dimensional subspace before inducing a decision tree on the projected instances. SIMPL uses efficient sequential scans and sorts and is comparable in speed and memory scalability to widely used naive Bayes (NB) classifiers, but it beats NB accuracy decisively. It not only approaches and sometimes exceeds SVM accuracy, but also beats the running time of a popular SVM implementation by orders of magnitude. While describing SIMPL, we make a detailed experimental comparison of SVM-generated discriminants with Fisher's discriminants, and we also report on an analysis of the cache performance of a popular SVM implementation. Our analysis shows that SIMPL has the potential to be the method of choice for practitioners who want the accuracy of SVMs and the simplicity and speed of naive Bayes classifiers.|Soumen Chakrabarti,Shourya Roy,Mahesh V. Soundalgekar","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram"],["80014|VLDB|1980|EEV Evaluation and Implementation of Database Systems|This paper presents the main results of an important study, founded by the Commission of the European Communities, on the evaluation and implementation of database systems in Europe. This study was executed by four national research centers in informatics  CNR, GMD, INRIA and NCC. The reports concern the experience of DBMS usage in Europe, database software descriptions, the selection and evaluation of DBMS, the maintenance of applications, the database administrations, the portability of DBMS, the implementation and standardisation.|Emile Peeters","80409|VLDB|2004|Compressing Large Boolean Matrices using Reordering Techniques|Large boolean matrices are a basic representational unit in a variety of applications, with some notable examples being interactive visualization systems, mining large graph structures, and association rule mining. Designing space and time efficient scalable storage and query mechanisms for such large matrices is a challenging problem. We present a lossless compression strategy to store and access such large matrices efficiently on disk. Our approach is based on viewing the columns of the matrix as points in a very high dimensional Hamming space, and then formulating an appropriate optimization problem that reduces to solving an instance of the Traveling Salesman Problem on this space. Finding good solutions to large TSP's in high dimensional Hamming spaces is itself a challenging and little-explored problem -- we cannot readily exploit geometry to avoid the need to examine all N inter-city distances and instances can be too large for standard TSP codes to run in main memory. Our multi-faceted approach adapts classical TSP heuristics by means of instance-partitioning and sampling, and may be of independent interest. For instances derived from interactive visualization and telephone call data we obtain significant improvement in access time over standard techniques, and for the visualization application we also make significant improvements in compression.|David S. Johnson,Shankar Krishnan,Jatin Chhugani,Subodh Kumar,Suresh Venkatasubramanian","79931|VLDB|1978|A Survey of Physical Database Design Methodology and Techniques|The problem of physical database design has received considerable attention by researchers over the past several years. This paper provides a survey of a large number of design techniques that have resulted from this activity. More attention has been given to some of the latest work. This survey is by no means exhaustive and is intended to motivate the interested reader into a more detailed study of this area.|Mario Schkolnick","80792|VLDB|2007|Performance Evaluation and Experimental Assessment - Conscience or Curse of Database Research|Performance, performance and performance used to be the three things that really mattered in database research. Most of our published works indeed include an experimental evaluation of the proposed techniques. However, such evaluations are sometimes seen as a \"must-have\" eating up the valuable space where one could describe new ideas. The experimental evaluations end up being short, lacking important information to interpret andor reproduce the results, and often end without clear conclusion.|Ioana Manolescu,Stefan Manegold","80240|VLDB|2002|Database Tuning Principles Experiments and Troubleshooting Techniques|Tuning your database for optimal performance means more than following a few short steps in a vendor-specific guide. For maximum improvement, you need a broad and deep knowledge of basic tuning principles, the ability to gather data in a systematic way, and the skill to make your system run faster. This is an art as well as a science, and Database Tuning Principles, Experiments, and Troubleshooting Techniques will help you develop portable skills that will allow you to tune a wide variety of database systems on a multitude of hardware and operating systems. Further, these skills, combined with the scripts provided for validating results, are exactly what you need to evaluate competing database products and to choose the right one.|Dennis Shasha,Philippe Bonnet","79901|VLDB|1978|Application of Sparse Matrix Techniques to Search Retrieval Classification and Relationship Analysis in Large Data Base Systems - SPARCOM|In order to meet the requirements of Large Data Base (LDB) Systems for short response time and high throughput a new approach has been investigated. In this paper only certain aspects of a powerful system, called SPARCOM, are described. The unique approach of this system is the conceptual conversion of data into large sparse binary matrices that enables one to apply sophisticated sparse matrix techniques to perform data base operations. Only for the sake of clarity the matrices are presented in their binary form. The high performance of the system is due to the direct conversion and manipulation of the matrices in their compact form as obtained from the application of the sparse matrix algorithms.|Ron Ashany","79800|VLDB|1975|Hardware and System Architecture for a Very Large Database|This paper describes practical experience in structuring a large amount of data (about  reels of -track  binary code decimal magnetic tapes of  census data). The hardware and software systems were given. The goal was to structure the database for online devices and to make the retrieval as efficient as possible. This paper explains only the database architecture finally chosen.|Robert Healey,Bradford Heckman","80095|VLDB|1985|An Evaluation of Buffer Management Strategies for Relational Database Systems|In this paper we present a new algorithm, DBMIN, for managing the buffer pool of a relational database manaegememt system. DBMIN is based on a new model of relational query behavior, the query locality set model (QLSM). Like the hot set model, the QLSM has an advantage over the stochastic models due to its ability to predict future reference hehavior. However, the QLSM avoids the potential problems of the hot set model by separating the modeling of referr-ence bahavior from any particular buffer management algorithm. After introducing the QLSM and describing the DBMIN algorithm, we present a performance evaluation methodology for evaluating buffer manage-ment algorithms in a multiuser environment. This methodology employed a hybrid model that comhines features of both trace driven and distribution driven simulation models. Using this model the performance of the DBMIN algorithm in a multiuser environment is compared with that of the hot set algorithm and four more traditional buffer replacement algorithms.|Hong-Tai Chou,David J. DeWitt","79869|VLDB|1977|A Simulation Model for Performance Analysis of Large Shared Data Bases|This paper describes a data base simulator, or rather, a family of simulators, that is being developed at the University of Stockholm. The purpose of this discrete event simulation model is to provide a tool for  investigating the performance of data base oriented information systems (existing and planned)  increasing the knowledge of i) design of data base oriented information systems, ii) design of DBMS (or setting parameters when generating a DBMS). A typical target system would be a heavily loaded multiprogrammed on-line system with a very large data base. One of the guidelines for design of the simulator has been user orientation. The preparation and execution of the model is done interactively and on a problem-oriented leve , e.g. the analyst describes the data base with DDLDML instructions in a DBTG-like language. It is possible to model hierarchical as well as network-based data bases.|Christer Hulten,Lars S√∂derlund","80592|VLDB|2005|Improving Database Performance on Simultaneous Multithreading Processors|Simultaneous multithreading (SMT) allows multiple threads to supply instructions to the instruction pipeline of a superscalar processor. Because threads share processor resources, an SMT system is inherently different from a multiprocessor system and, therefore, utilizing multiple threads on an SMT processor creates new challenges for database implementers.We investigate three thread-based techniques to exploit SMT architectures on memory-resident data. First, we consider running independent operations in separate threads, a technique applied to conventional multi-processor systems. Second, we describe a novel implementation strategy in which individual operators are implemented in a multi-threaded fashion. Finally, we introduce a new data-structure called a work-ahead set that allows us to use one of the threads to aggressively preload data into the cache.We evaluate each method with respect to its performance, implementation complexity, and other measures. We also provide guidance regarding when and how to best utilize the various threading techniques. Our experimental results show that by taking advantage of SMT technology we achieve a % to % improvement in throughput over single threaded implementations on in-memory database operations.|Jingren Zhou,John Cieslewicz,Kenneth A. Ross,Mihir Shah"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","79856|VLDB|1977|The Decomposition Versus Synthetic Approach to Relational Database Design|Two of the competing approaches to the logical design of relational databases are the third normal form decomposition approach of Codd and the synthetic approach of Bernstein and others. The synthetic approach seems on the surface to be the more powerful unfortunately, to avoid serious problems, a nonintuitive constraint (the \"uniqueness\" of functional dependencies) must be assumed. We demonstrate the fourth normal form approach, which not only can deal with this difficulty, but which is also more powerful than either of the earlier approaches. The input of the new method includes attributes (potential column names), along with semantic information in the form of functional and multivalued dependencies the output is a \"good\" (fourth normal form) logical design. The new method is semi-automatic, which is especially helpful in the case of a very large database with many attributes that interrelate in complex ways.|Ronald Fagin","79910|VLDB|1978|On Bubble Memories and Relational Data Base|Slightly modified majorminor loop bubble chips can accommodate storage and access for relational data base quite well. With the addition of dynamic indexing loops, very efficient data retrieval and manipulation can be accomplished. This paper specifies a set of relationally complete instructions. A variety of queries are executed to demonstrate the versatility of the instructions as well as understand the nature of our hardware and software operations. Since the bubble hardware is intrinsically similar to the data model, and adapted to the access requirements. we believe the overall system is simpler both in operation and in programming. The present work is also examined in the persepctive of data base trends, bubble technology development, and other hardware approaches to data base (viz. logic per track associative processing).|H. Chang","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","80100|VLDB|1985|An Efficient Implementation of a Relational Data Base|In the paper an approach to implement efficiently a relational data base is presented. The appro- ach combines a new method of physical repre- sentation with a novel database machine (DBM) architecture. The dispersed representation assumes a relation consisting of three parts the identification, value, and link (optionally). The parts are separated and the identification and link parts are converted to be expressed with pointers, and then the parts are dispersed among three memories the IDENTIFICATION, VALUE, and LINK. The DBM architecture attempts to implement the dispersed representa- tion efficiently and uses modified cellular asso- ciative memories (CAM) to store and process the value and link parts. The modification consists in providing a CAM with a respond register called a global mark register (GMR) which can be readily loaded and unloaded externally.|Marian S. Furman","79821|VLDB|1975|PRTV An Efficient Implementation for Large Relational Data Bases|PRTV  is a research prototype interactive data base. It is intended for use on its own or as part of a specialist system. The principle objectives are to provide a) high level flexible data support, b) functional extensibility and c) reasonable efficiency. The prototype is also designed as a method of generating and testing ideas in a wider data base research programme.|Stephen Todd","79866|VLDB|1977|Self-Descriptive Relational Data Base|An architectural model of a self-descriptive relational data base is presented and the implementation problems are investigated. It is argued that self-descriptivity brings excellent properties of the data base management system. For example, users can easily extend the capability of the DBMS or fewer basic access commands can cover all the accesses to the data base. Thus, the self-descriptive data base architecture is an excellent candidate for a hardware data base management system where simple and yet universal access mechanism is badly needed. It is shown that a close relation exists between a self-descriptive data base and the DDD. In fact, it is theoretically necessary that a DBMS be self-descriptive. The notions 'type' and 'occurrence' are also applied to relations, thus a relation type and corresponding relation occurrences are introduced and shown to be effective in representing the real world. An example of a self-descriptive data base relation is exhibited and the general characteristics of it is discussed. To make the system motions secure, a monitoring program is introduced and explained. A typical command process is described and its internal logic is shown in an ALGOL-like program codes. Four basic atomic commands, executed by a data base machine, are introduced and explained. The method to extend the basic data base management facility by using selfdescriptivity and exit routines is introduced and an example to enhance the integrity checking capability is demonstrated. The hierarchical locking method is applied and the problems peculiar to the self-descriptive architecture are investigated. Finally the implementation of temporary relations useful in various conditions is considered.|Ryosuke Hotaka,Masaaki Tsubaki","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","79837|VLDB|1976|Relational Transformation and a Redundancy in Relational Data Base|This paper considers some transformation and replacement of relations in a relational data base, called association and dissociation, which are intended to decrease the time of jobs processing and files updating, although it is accompanied by an increase of information redundancy. Some examples are examined. It is shown that in a considerable number of practical cases association and dissociation are effective means of processing time reduction and data base adaptation.|Eliezer L. Lozinskii","80832|VLDB|2007|A Relational Approach to Incrementally Extracting and Querying Structure in Unstructured Data|There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench.|Eric Chu,Akanksha Baid,Ting Chen,AnHai Doan,Jeffrey F. Naughton"],["79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","79802|VLDB|1975|The Use of Cluster Analysis in Physical Data Base Design|The physical structure and relative placement of information elements within a data base is critical for the efficient design of a computerized information system which is shared by a community of users. Traditionally the selection among alternative structural designs has been handled largely via heuristics. Recent research has shown that a number of significant design problems can be stated mathematically as nonlinear, integer, zero-one programming problems. In concept, therefore, mathematical programming algorithms can be used to determine \"optimal\" data base designs. In practice, one finds that realistic problems of even modest size are computationally infeasible. This paper presents a means for overcoming this difficulty in the design of data base records. A metric with which to measure the similarity of usage among data items is developed and used by a clustering algorithm to reduce the space of alternative designs to a point where solution is economically feasible.|Jeffrey A. Hoffer,Dennis G. Severance","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79860|VLDB|1977|A Data Base Design Decision Support System|The task of physical data base design in an DBTG enviornment is examined. Generation of an internal schema which considers questions of storage versus access costs, efficient implementation of data relationships, efficient placement of data within the data base and allocation of primary and secondary storage is shown to be a formidable task. Current data base design aids are reviewed. One aid, a sophisicated mathematical model of DBTG data bases (Gerritsen ) is shown to be a potentially valuable tool. A limitation of this model is that no optimization algorithm other than total enumeration has been found. This paper proposes an implementation of the Gerritsen model. An interactive design tool, based upon the Gerritsen model, is discussed. A Data Base Design Decision Support System (DBD-DSS) for use by the DBA is developed. The objectives and structure of the DBD-DSS are examined. A comprehensive example illustrating both the user interface and the potential benefits of the interactive tool is presented.|Thomas J. Gambino,Rob Gerritsen","80054|VLDB|1981|Theoretical and Practical Tools for Data Base Design|The paper presents two families of reversible transformations for a binary information model. The binary model is considered as a special case of a generalized n-ary relational model as are CODD'S and ER models. This parenthood increases the power of the binary model and makes the transformation valid for other models. Two applications of the transformations are presented within the framework of an Information System Design Approach they are namely a conceptual model conversion and a valid TOTAL schema production.|Jean-Luc Hainaut","79936|VLDB|1978|Data Base Design in Theory and Practice|The paper gives an overview of the data base design process. It claims that a practically useful, well integrated theory of data base design is clearly within reach. The following problem areas are touched upon in the paper the purpose and role of a data base, user participation, reality modeling, information needs and observation possibilities, data base modeling, design of file structures and access paths, data base integrity, concurrency problems, confidentiality, evaluation and maintenance, data base administration, meta-data bases, and strategies for \"going data base\" in an organization.|Bo Sundgren","79895|VLDB|1977|Data Base Design by Applied Data Synthesis|This paper describes an algorithm allowing systemization of data base design operations. The objectives of the algorithm are three-fold it allows (st) the synthesis of a physical data structure from an appropriate description of reality (i.e. from a conceptual view specification) such that (nd) the operations required to map the physical data structure onto the most important logical structures are minimized and (rd) anomalies in storage operations may not occur (i.e. the normalization criteria as proposed respectively by E.F. Codd (), by W. Kent  and (less formally) by M.E. Senko  are respected). The algorithm may be used both for traditional data base systems (working with hierarchical andor network data structures) as well as for relational data base systems.|M. Vetter","79803|VLDB|1975|Automating Logical File Design|Data base design is currently a costly and time consuming activity. Part of this overall design is concerned with the logic of the underlying network structure, and this part is commonly called logical design. Logical design involves a tedium of calculations which can be automated on a program and used as a design tool. The basic approach is applicable to a wide variety of data base handlers, such as IMS, the DBTC proposal, CIS, and others. The approach has been prototyped and a version suitable for IMS is now being used (DBDA) as a program product. This paper describes the basic concepts and how they can be applied to IMS, DBTG or relational implementations. The data structure needed to support a particular application program is called the local view, and input to the design tool is the collection of all local views. Local views are constructedusing certain primitives which the integration of the local views. The diagnostics of the design tool program will partially depend on the data base handler. Each handler, (IMS, DBTG., etc.) has different network restrictions which limit the local views which can be generated from the network. Different network restrictions result in different diagnostics. There is no relational data base handler to evaluate for network restrictions.|George U. Hubbard,Norman Raver","79914|VLDB|1978|A Model and a Method for Logical Data Base Design|This paper presents both a data model and a method for its practical use. There are two main parts  the first one deals with the data model, new concepts adequate for the design of conceptual schema are introduced. The second part presents complete method for the construction of the logical data base structure according to the concepts of the given model.|Andr√© Flory,Jacques Kouloumdjian","79969|VLDB|1979|Statistice for the Usage of a Conceptual Data Model as a Basis for Logical Data Base Design|A data base design aid, SYDADA, is introduced. Input to SYDADA is a requirements specification, expressed in a language called SYSDOC. A SYSDOC specification consists of two main parts a conceptual data model and a transaction description. The conceptual data model is a high level, implementation independent data description. The transaction description is expressed in a very high level language which uses the conceptual data model. SYDADA analyzes the requirements specification by modelling the specified system. In the system model the conceptual data model is the data base, and the transactions specified can be executed using this data base. The most important result of the analysis is a set of statistics showing how, and how frequently, the various parts of the conceptual data model are used. We have shown how these statistics can be used as a basis for logical data base design.|Ole Oren,Frode Aschim"],["80625|VLDB|2006|Efficient Allocation Algorithms for OLAP Over Imprecise Data|Recent work proposed extending the OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform a given imprecise fact table into a form, called the Extended Database, that can be readily used to answer OLAP aggregation queries.In this work, we present scalable, efficient algorithms for creating the Extended Database (i.e., performing allocation) for a given imprecise fact table. Many allocation policies require multiple iterations over the imprecise fact table, and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, and has not been considered previously to the best of our knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straight-foward approaches.|Douglas Burdick,Prasad M. Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80374|VLDB|2004|Efficient Indexing Methods for Probabilistic Threshold Queries over Uncertain Data|It is infeasible for a sensor database to contain the exact value of each sensor at all points in time. This uncertainty is inherent in these systems due to measurement and sampling errors, and resource limitations. In order to avoid drawing erroneous conclusions based upon stale data, the use of uncertainty intervals that model each data item as a range and associated probability density function (pdf) rather than a single value has recently been proposed. Querying these uncertain data introduces imprecision into answers, in the form of probability values that specify the likeliness the answer satisfies the query. These queries are more expensive to evaluate than their traditional counterparts but are guaranteed to be correct and more informative due to the probabilities accompanying the answers. Although the answer probabilities are useful, for many applications, it is only necessary to know whether the probability exceeds a given threshold - we term these Probabilistic Threshold Queries (PTQ). In this paper we address the efficient computation of these types of queries. In particular, we develop two index structures and associated algorithms to efficiently answer PTQs. The first index scheme is based on the idea of augmenting uncertainty information to an R-tree. We establish the difficulty of this problem by mapping one-dimensional intervals to a two-dimensional space, and show that the problem of interval indexing with probabilities is significantly harder than interval indexing which is considered a well-studied problem. To overcome the limitations of this R-tree based structure, we apply a technique we call variance-based clustering, where data points with similar degrees of uncertainty are clustered together. Our extensive index structure can answer the queries for various kinds of uncertainty pdfs, in an almost optimal sense. We conduct experiments to validate the superior performance of both indexing schemes.|Reynold Cheng,Yuni Xia,Sunil Prabhakar,Rahul Shah,Jeffrey Scott Vitter","80106|VLDB|1985|On Semantic Reefs and Efficient Processing of Correlation Queries with Aggregates|Recent transformation algorithms for speeding up pro- cessing of nested SQL-like queries with aggregates are reviewed with respect to the correctness of aggregates over empty sets. It turns out that for a particular subset of such queries these algorithms fail to compute consistent answers. Unfortunately there seems to be no uniform way to do these transformations efficiently and correctly under all cir- cumstances. Also the algorithms for QUEL are reexamined regarding their correctness. It is shown that for a specific subset of QUEL-queries with aggregates a clearer semantics can be associated. Finally, benchmark results for lngres show that considerable performance advantages may be gained for such query types by using dynamic filters. The consequence of all these observations is that more research is required to integrate correlation queries with aggregates into a unified operator tree model.|Werner Kie√üling","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80077|VLDB|1981|Efficient Processing of Interactive Relational Data Base Queries expressed in Logic|Relational database retrieval is viewed as a special case of deduction in logic. It is argued that expressing a query in logic clarifies the problems involved in processing it efficiently (\"query optimisationn). The paper describes a simple but effective strategy for planning a query so that it can be efficiently executed by the elementary deductive mechanism provided in the programming language Prolog. This planning algorithm has been implemented as part of a natural language question answering system, called Chat-. The Chat- method of query planning and execution is compared with the strategies used in other relational database systems, particularly Ingres and System R.|David H. D. Warren","80192|VLDB|2002|Efficient Algorithms for Processing XPath Queries|Our experimental analysis of several popular XPath processors reveals a striking fact Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we show how the main ideas of our algorithm can be profitably integrated into existing XPath processors. Finally, we present two fragments of XPath for which linear-time query processing algorithms exist and another fragment with linear-spacequadratic-time query processing.|Georg Gottlob,Christoph Koch,Reinhard Pichler","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis","80293|VLDB|2003|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.|Christoph Koch"],["80355|VLDB|2004|A Framework for Projected Clustering of High Dimensional Data Streams|The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is high-dimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams. In this paper, we propose a new, high-dimensional, projected data stream clustering method, called HPStream. The method incorporates a fading cluster structure, and the projection based clustering methodology. It is incrementally updatable and is highly scalable on both the number of dimensions and the size of the data streams, and it achieves better clustering quality in comparison with the previous stream clustering methods. Our performance study with both real and synthetic data sets demonstrates the efficiency and effectiveness of our proposed framework and implementation methods.|Charu C. Aggarwal,Jiawei Han,Jianyong Wang,Philip S. Yu","80251|VLDB|2003|A Framework for Clustering Evolving Data Streams|The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a few one-pass clustering algorithms have been developed for the data stream problem. Although such methods address the scalability issues of the clustering problem, they are generally blind to the evolution of the data and do not address the following issues () The quality of the clusters is poor when the data evolves considerably over time. () A data stream clustering algorithm requires much greater functionality in discovering and exploring clusters over different portions of the stream. The widely used practice of viewing data stream clustering algorithms as a class of one-pass clustering algorithms is not very useful from an application point of view. For example, a simple one-pass clustering algorithm over an entire data stream of a few years is dominated by the outdated history of the stream. The exploration of the stream over different time windows can provide the users with a much deeper understanding of the evolving behavior of the clusters. At the same time, it is not possible to simultaneously perform dynamic clustering over all possible time horizons for a data stream of even moderately large volume. This paper discusses a fundamentally different philosophy for data stream clustering which is guided by application-centered requirements. The idea is divide the clustering process into an online component which periodically stores detailed summary statistics and an offine component which uses only this summary statistics. The offine component is utilized by the analyst who can use a wide variety of inputs (such as time horizon or number of clusters) in order to provide a quick understanding of the broad clusters in the data stream. The problems of efficient choice, storage, and use of this statistical data for a fast data stream turns out to be quite tricky. For this purpose, we use the concepts of a pyramidal time frame in conjunction with a microclustering approach. Our performance experiments over a number of real and synthetic data sets illustrate the effectiveness, efficiency, and insights provided by our approach.|Charu C. Aggarwal,Jiawei Han,Jianyong Wang,Philip S. Yu","80674|VLDB|2006|Safety Guarantee of Continuous Join Queries over Punctuated Data Streams|Continuous join queries (CJQ) are needed for correlating data from multiple streams. One fundamental problem for processing such queries is that since the data streams are infinite, this would require the join operator to store infinite states and eventually run out of space. Punctuation semantics has been proposed to specifically address this problem. In particular, punctuations explicitly mark the end of a subset of data and, hence, enable purging of the stored data which will not contribute to any new query results. Given a set of available punctuation schemes, if one can identify that a CJQ still requires unbounded storage, then this query can be flagged as unsafe and can be prevented from running. Unfortunately, while punctuation semantics is clearly useful, the mechanisms to identify if and how a particular CJQ could benefit from a given set of punctuation schemes are not yet known. In this paper, we provide sufficient and necessary conditions for checking whether a CJQ can be safely executed under a given set of punctuation schemes or not. In particular, we introduce a novel punctuation graph to aid the analysis of the safety for a given query. We show that the safety checking problem can be done in polynomial time based on this punctuation graph construct. In addition, various issues and challenges related to the safety checking of CJQs are highlighted.|Hua-Gang Li,Songting Chen,Jun'ichi Tatemura,Divyakant Agrawal,K. Sel√ßuk Candan,Wang-Pin Hsiung","80788|VLDB|2007|CADS Continuous Authentication on Data Streams|We study processing and authentication of long-running queries on outsourced data streams. In this scenario, a data owner (DO) constantly transmits its data to a service provider (SP), together with additional authentication information. Clients register continuous range queries to the SP. Whenever the data change, the SP must update the results of all affected queries and inform the clients accordingly. The clients can verify the correctness of the results using the authentication information provided by the DO. Compared to conventional databases, stream environments pose new challenges such as the need for fast structure updating, support for continuous query processing and authentication, and provision for temporal completeness. Specifically, in addition to the correctness of individual results, the client must be able to verify that there are no missing results in between updates. We face these challenges through several contributions. Since there is no previous work, we first present a technique, called REF, that achieves correctness and temporal completeness but incurs false transmissions, i.e., the SP has to inform clients whenever there is a data update, even if their results are not affected. Then, we propose CADS, which minimizes the processing and transmission overhead through an elaborate indexing scheme and a virtual caching mechanism. Finally, we extend CADS to the case where multiple owners outsource their data to the same SP. The SP integrates all data in a single authentication process, independently of the number of DOs.|Stavros Papadopoulos,Yin Yang,Dimitris Papadias","80706|VLDB|2006|Window-Aware Load Shedding for Aggregation Queries over Data Streams|Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a \"Window Drop\". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.|Nesime Tatbul,Stanley B. Zdonik","80443|VLDB|2004|QStream Deterministic Querying of Data Streams|Current developments in processing data streams are based on the best-effort principle and therefore not adequate for many application areas. When sensor data is gathered by interface hardware and is used for triggering data-dependent actions, the data has to be queried and processed not only in an efficient but also in a deterministic way. Our streaming system prototype embodies novel data processing techniques. It is based on an operator component model and runs on top of a real-time capable environment. This enables us to provide real Quality-of-Service for data stream queries.|Sven Schmidt,Henrike Berthold,Wolfgang Lehner","80279|VLDB|2003|Scheduling for shared window joins over data streams|Continuous Query (CQ) systems typically exploit commonality among query expressions to achieve improved efficiency through shared processing. Recently proposed CQ systems have introduced window specifications in order to support unbounded data streams. There has been, however, little investigation of sharing for windowed query operators. In this paper, we address the shared execution of windowed joins, a core operator for CQ systems. We show that the strategy used in systems to date has a previously unreported performance flaw that can negatively impact queries with relatively small windows. We then propose two new execution strategies for shared joins. We evaluate the alternatives using both analytical models and implementation in a DBMS. The results show that one strategy, called MQT, provides the best performance over a range of workload settings.|Moustafa A. Hammad,Michael J. Franklin,Walid G. Aref,Ahmed K. Elmagarmid","80335|VLDB|2003|A Regression-Based Temporal Pattern Mining Scheme for Data Streams|We devise in this paper a regression-based algorithm, called algorithm FTP-DS (Frequent Temporal Patterns of Data Streams), to mine frequent temporal patterns for data streams. While providing a general framework of pattern frequency counting, algorithm FTP-DS has two major features, namely one data scan for online statistics collection and regression-based compact pattern representation.To attain the feature of one data scan, the data segmentation and the pattern growth scenarios are explored for the frequency counting purpose. Algorithm FTP-DS scans online transaction flows and generates candidate frequent patterns in real time. The second important feature of algorithm FTP-DS is on the regression-based compact pattern representation. Specifically, to meet the space constraint, we devise for pattern representation a compact ATF (standing for Accumulated Time and Frequency) form to aggregately comprise all the information required for regression analysis. In addition, we develop the techniques of the segmentation tuning and segment relaxation to enhance the functions of FTP-DS. With these features, algorithm FTP-DS is able to not only conduct mining with variable time intervals but also perform trend detection effectively. Synthetic data and a real dataset which contains net-Permission work alarm logs from a major telecommunication company are utilized to verify the feasibility of algorithm FTP-DS.|Wei-Guang Teng,Ming-Syan Chen,Philip S. Yu","80411|VLDB|2004|Detecting Change in Data Streams|Detecting changes in a data stream is an important area of research with many applications. In this paper, we present a novel method for the detection and estimation of change. In addition to providing statistical guarantees on the reliability of detected changes, our method also provides meaningful descriptions and quantification of these changes. Our approach assumes that the points in the stream are independently generated, but otherwise makes no assumptions on the nature of the generating distribution. Thus our techniques work for both continuous and discrete data. In an experimental study we demonstrate the power of our techniques.|Daniel Kifer,Shai Ben-David,Johannes Gehrke","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum"]]}}