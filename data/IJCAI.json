{"abstract":{"entropy":6.675294598640679,"topics":["artificial intelligence, constraint satisfaction, problem solving, constraint problem, machine learning, satisfaction problem, search, heuristic search, search algorithm, play role, present algorithm, search problem, reinforcement learning, recent years, arc consistency, solution problem, dimensionality reduction, problem learning, values variables, research area","theorem proving, logic, knowledge representation, knowledge base, logic programming, reasoning, theorem prover, description logic, qualitative reasoning, temporal reasoning, logic program, present framework, situation calculus, spatial reasoning, system logic, belief revision, nonmonotonic logic, general framework, combinatorial auctions, computational complexity","markov decision, neural networks, planning problem, markov processes, mobile robot, bayesian networks, planning, decision processes, partially observable, real world, autonomous agents, planning plan, model-based diagnosis, address problem, consider problem, agents, planning domain, markov mdps, resources allocation, decision mdps","natural language, system, describes system, expert system, describes, knowledge system, knowledge acquisition, natural processing, language processing, present system, language system, present approach, computer program, describes approach, information extraction, learning system, language generation, artificial intelligence, present novel, describes language","artificial intelligence, web search, research area, recent research, last years, research intelligence, previous work, research artificial, research, work algorithm, becoming increasingly, previous algorithm, ontology reuse, research search, work problem, research problem, problem area, past years, work learning, goal learning","machine learning, problem learning, play role, learning algorithm, learning, reinforcement learning, present learning, play important, learning methods, learning concept, widely used, learning data, learning training, problem machine, decision tree, learning examples, important role, learning domain, machine translation, inductive learning","knowledge representation, horn clause, formal knowledge, concept knowledge, representation language, concept description, description examples, formal representation, concept, answer question, language extension, problem formal, reasoning mechanism, present extension, semantic representation, provide compact, present simple, formal semantic, program given, probability theory","theorem proving, theorem prover, present framework, logic program, system logic, nonmonotonic logic, modal logic, default logic, logic, first order, combines logic, present logic, provide framework, theorem, logic model, logical framework, first-order logic, theorem system, logic programming, stable model","objects, objects recognition, pattern recognition, line drawing, image objects, web pages, objects motion, image, moving objects, universal minimal, data, large data, motion image, model data, data types, well known, deals problem, time series, present objects, large problem","neural networks, mobile robot, bayesian networks, networks, autonomous robot, fundamental problem, addresses problem, simultaneous localization, simultaneous mapping, localization mapping, problem networks, learning environment, mobile localization, statistical model, approach problem, study problem, networks model, robot environment, networks task, simultaneous slam","describes system, expert system, knowledge-based system, development system, understanding system, recognition system, describes, speech understanding, system designed, system implemented, describes called, automatic system, pattern recognition, system support, system provide, present overview, system research, describes learning, speech system, plan recognition","knowledge system, knowledge acquisition, artificial intelligence, control system, system architecture, expert knowledge, tool system, artificial system, domain knowledge, current system, knowledge, production system, describes implementation, learning knowledge, expert system, system domain, human system, knowledge representation, system implementation, control robot"],"ranking":[["15032|IJCAI|1993|Exploiting Interchangeabilities in Constraint-Satisfaction Problems|Constraint satisfaction - a method for representing and solving many AI problems in a very elegant manner - is a well-studied research area of recent years. Freuder observed that some constraint satisfaction problems are fashioned so that certain domain values of constraint variables are interchangeable. The use of such knowledge can increase search efficiency drastically by reducing the problem. In this paper we carry on these considerations and give a formal foundation of interchangeabilities by the notion of domain partitions induced by equivalence relations. We show how these domain partitions can be used in a very accurate manner by the majority of existing constraint propagation algorithms and introduce a novel backtrack procedure exploiting such interchangeabilities of domain values. Both theoretical analysis and experiments indicate that our proposed approach is an improvement of Freuder's use of neighborhood interchangeability and has very good behavior for certain problem types.|Alois Haselböck","15786|IJCAI|2003|Propagate the Right Thing How Preferences Can Speed-Up Constraint Solving|We present an algorithm Pref-AC that limits arc consistency (AC) to the preferred choices of a tree search procedure and that makes constraint solving more efficient without changing the pruning and shape of the search tree. Arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problems such as configuration and scheduling. Moreover, Pref-AC directly computes a preferred solution for treelike constraint satisfaction problems.|Christian Bessière,Anaïs Fabre,Ulrich Junker","13831|IJCAI|1981|A New Method for Solving Constraint Satisfaction Problems|This paper deals with the combinatorial search problem of finding values for a set of variables subject to a set of constraints. This problem is referred to as a constraint satisfaction problem. We present an algorithm for finding all the solutions of a constraint satisfaction problem with worst case time bound (m*kf+) and space bound (n*kf+), where n is the number of variables in the problem, m the number of constraints, k the cardinality of the domain of the variables, and fn an integer depending only on a graph which is associated with the problem. It will be shown that for planar graphs and graphs of fixed genus this f is (n).|Raimund Seidel","15021|IJCAI|1993|Using Inferred Disjunctive Constraints To Decompose Constraint Satisfaction Problems|Constraint satisfaction problems involve finding values for problem variables that satisfy constraints on what combinations of values are permitted. They have applications in many areas of artificial intelligence, from planning to natural language understanding. A new method is proposed for decomposing constraint satisfaction problems using inferred disjunctive constraints. The decomposition reduces the size of the problem. Some solutions may be lost in the process, but not all. The decomposition supports an algorithm that exhibits superior performance. Analytical and experimental evidence suggests that the algorithm can take advantage of local weak spots in globally hard problems.|Eugene C. Freuder,Paul D. Hubbe","14618|IJCAI|1989|Partial Constraint Satisfaction|A constraint satisfaction problem involves finding values for variables subject to constraints on which combinations of values are allowed. In some cases it may be impossible or impractical to solve these problems completely. We may seek to partially solve the problem in an \"optimal\" or \"sufficient\" sense. A formal model is presented for defining and studying such partial constraint satisfaction problems. The basic components of this model are a constraint satisfaction problem, a problem space, and a metric on that space. Algorithms for solving partial constraint satisfaction problems are discussed. A specific branch and bound algorithm is described. Some initial experimental experience with this algorithm is presented.|Eugene C. Freuder","14616|IJCAI|1989|Constrained Heuristic Search|We propose a model of problem solving that provides both structure and focus to search. The model achieves this by combining constraint satisfaction with heuristic search. We introduce the concepts of topology and texture to characterize problem structure and areas to focus attention respectively. The resulting model reduces search complexity and provides a more principled explanation of the nature and power of heuristics in problem solving. We demonstrate the model of Constrained Heuristic Search in two domains spatial planning and factory scheduling. In the former we demonstrate significant reductions in search.|Mark S. Fox,Norman M. Sadeh,Can A. Baykan","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder","15168|IJCAI|1995|Generalizing Inconsistency Learning for Constraint Satisfaction|Constraint satisfaction problems, where values are sought for problem variables subject to restrictions on which combinations of values are acceptable, have many applications in artificial intelligence. Conventional learning methods acquire individual tuples of inconsistent values. These learning experiences can be generalized. We propose a model of generalized learning, based on inconsistency preserving mappings, which is sufficiently focused so as to be computationally cost effective. Rather than recording an individual inconsistency that led to a failure, and looking for that specific inconsistency to recur, we observe the context of a failure, and then look for a related context in which to apply our experience opportunistically. As a result we leverage our learning power. This model is implemented, extended and evaluated using two simple but important classes of constraint problems.|Eugene C. Freuder,Richard J. Wallace","14150|IJCAI|1985|Taking Advantage of Stable Sets of Variables in Constraint Satisfaction Problems|Binary constraint satisfaction problems involve finding values for variables subject to constraints between pairs of variables. Algorithms that take advantage of the structure of constraint connections can be more efficient than simple backtrack search. Some pairs of variables may have no direct constraint between them, even if they are linked indirectly through a chain of constraints involving other variables. A set of variables with no direct constraint between any pair of them forms a stable set in a constraint graph representation of a problem. We describe an algorithm designed to take advantage of stable sets of variables, and give experimental evidence that it can outperform not only simple backtracking, but also forward checking, one of the best variants of backtrack search. Potential applications to parallel processing are noted. Some light is shed on the question of how and when a constraint satisfaction problem can be advantageously divided into subproblems.|Eugene C. Freuder,Michael J. Quinn","16286|IJCAI|2005|Structural Symmetry Breaking|Symmetry breaking has been shown to be an important method to speed up the search in constraint satisfaction problems that contain symmetry. When breaking symmetry by dominance detection, a computationally efficient symmetry breaking scheme can be achieved if we can solve the dominance detection problem in polynomial time. We study the complexity of dominance detection when value and variable symmetry appear simultaneously in constraint satisfaction problems (CSPs) with single-valued variables and set-CSPs. We devise an efficient dominance detection algorithm for CSPs with single-valued variables that yields symmetry-free search trees and that is based on the abstraction to the actual, intuitive structure of a symmetric CSP.|Meinolf Sellmann,Pascal Van Hentenryck"],["16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","15866|IJCAI|2003|A Resolution Theorem for Algebraic Domains|W. C. Rounds and G.-Q. Zhang have recently proposed to study a form of resolution on algebraic domains Rounds and Zhang, . This framework allows reasoning with knowledge which is hierarchically structured and forms a (suitable) domain, more precisely, a coherent algebraic cpo as studied in domain theory. In this paper, we give conditions under which a resolution theorem -- in a form underlying resolution-based logic programming systems -- can be obtained. The investigations bear potential for engineering new knowledge representation and reasoning systems on a firm domain-theoretic background.|Pascal Hitzler","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","15485|IJCAI|1999|Reasoning with Concrete Domains|Description logics are formalisms for the representation of and reasoning about conceptual knowledge on an abstract level. Concrete domains allow the integration of description logic reasoning with reasoning about concrete objects such as numbers, time intervals, or spatial regions. The importance of this combined approach, especially for building real-world applications, is widely accepted. However, the complexity of reasoning with concrete domains has never been formally analyzed and efficient algorithms have not been developed. This paper closes the gap by providing a tight bound for the complexity of reasoning with concrete domains and presenting optimal algorithms.|Carsten Lutz","16618|IJCAI|2007|The Mathematical Morpho-Logical View on Reasoning about Space|Qualitative reasoning about mereotopological relations has been extensively investigated, while more recently geometrical and spatio-temporal reasoning are gaining increasing attention. We propose to consider mathematical morphology operators as the inspiration for a new language and inference mechanism to reason about space. Interestingly, the proposed morpho-logic captures not only traditional mereotopological relations, but also notions of relative size and morphology. The proposed representational framework is a hybrid arrow logic theory for which we define a resolution calculus which is, to the best of our knowledge, the first such calculus for arrow logics.|Marco Aiello,Brammert Ottens","15425|IJCAI|1999|Preferred Arguments are Harder to Compute than Stable Extension|Based on an abstract framework for nonmonotonic reasoning, Bondarenko et at. have extended the logic programming semantics of admissible and preferred arguments to other nonmonotonic formalisms such as circumscription, autoepisternic logic and default logic. Although the new semantics have been tacitly assumed to mitigate the computational problems of nonmonotonic reasoning under the standard semantics of stable extensions, it seems questionable whether they improve the worst-case behaviour. As a matter of fact, we show that credulous reasoning under the new semantics in propositional logic programming and prepositional default logic has the same computational complexity as under the standard semantics. Furthermore, sceptical reasoning under the admissibility semantics is easier - since it is trivialised to monotonic reasoning. Finally, sceptical reasoning under the preferability semantics is harder than under the standard semantics.|Yannis Dimopoulos,Bernhard Nebel,Francesca Toni","15662|IJCAI|2001|A-System Problem Solving through Abduction|This paper presents a new system, called the A- System, performing abductive reasoning within the framework of Abductive Logic Programming. It is based on a hybrid computational model that implements the abductive search in terms of two tightly coupled processes a reduction process of the highlevel logical representation to a lower-level constraint store and a lower-level constraint solving process. A set of initial \"proof of principle\" experiments demonstrate the versatility of the approach stemming from its declarative representation of problems and the good underlying computational behaviour of the system. The approach offers a general methodology of declarative problem solving in AI where an incremental and modular refinement of the high-level representation with extra domain knowledge can improve and scale the computational performance of the framework.|Antonis C. Kakas,Bert Van Nuffelen,Marc Denecker","16160|IJCAI|2005|Equivalence in Abductive Logic|We consider the problem of identifying equivalence of two knowledge bases which are capable of abductive reasoning. Here, a knowledge base is written in either first-order logic or nonmonotonic logic programming. In this work, we will give two definitions of abductive equivalence. The first one, explainable equivalence, requires that two abductive programs have the same explainability for any observation. Another one, explanatory equivalence, guarantees that any observation has exactly the same explanations in each abductive framework. Explanatory equivalence is a stronger notion than explainable equivalence. In first-order abduction, explainable equivalence can be verified by the notion of extensional equivalence in default theories. In nonmonotonic logic programs, explanatory equivalence can be checked by means of the notion of relative strong equivalence. We also show the complexity results for abductive equivalence.|Katsumi Inoue,Chiaki Sakama","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada","14622|IJCAI|1989|Relating the TMS to Autoepistemic Logic|Truth maintenance systems have been studied by many authors and have become powerful tools in AI reasoning systems. From the viewpoint of commonsense reasoning, Doyle's TMS seems most interesting, for it allows nonmonotonic justifications. Its semantics, however, has remained unclear. In this paper, we shall give its declarative description in terms of autoepistemic logic, a kind of nonmonotonic logic. That is, we shall exhibit a one-to-one correspondence between states acceptable to the TMS and stable expansions of autoepistemic formulas attached to justifications. Thus, the TMS turns out to be a theorem prover of autoepistemic logic. For the practical interest, our result also suggests the possibility of implementing better TMS algorithms by using the theorem proving method of autoepistemic logic.|Yasushi Fujiwara,Shinichi Honiden"],["15126|IJCAI|1995|Exploiting Structure in Policy Construction|Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods.|Craig Boutilier,Richard Dearden,Moisés Goldszmidt","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","15471|IJCAI|1999|A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes|An issue that is critical for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or even infinite state spaces, traditional planning and reinforcement learning algorithms are often inapplicable, since their running time typically scales linearly with the state space size. In this paper we present a new algorithm that, given only a generative model (simulator) for an arbitrary MDP, performs near-optimal planning with a running time that has no dependence on the number of states. Although the running time is exponential in the horizon time (which depends only on the discount factor  and the desired degree of approximation to the optimal policy), our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large, unstructured MDPs. Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs KMN.|Michael J. Kearns,Yishay Mansour,Andrew Y. Ng","16446|IJCAI|2007|The Value of Observation for Monitoring Dynamic Systems|We consider the fundamental problem of monitoring (i.e. tracking) the belief state in a dynamic system, when the model is only approximately correct and when the initial belief state might be unknown. In this general setting where the model is (perhaps only slightly) mis-specified, monitoring (and consequently planning) may be impossible as errors might accumulate over time. We provide a new characterization, the value of observation, which allows us to bound the error accumulation. The value of observation is a parameter that governs how much information the observation provides. For instance, in Partially Observable MDPs when it is  the POMDP is an MDP while for an unobservable Markov Decision Process the parameter is . Thus, the new parameter characterizes a spectrum from MDPs to unobservable MDPs depending on the amount of information conveyed in the observations.|Eyal Even-Dar,Sham M. Kakade,Yishay Mansour","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","15648|IJCAI|2001|Max-norm Projections for Factored MDPs|Markov Decision Processes (MDPs) provide a coherent mathematical framework for planning under uncertainty. However, exact MDP solution algorithms require the manipulation of a value function, which specifies a value for each state in the system. Most real-world MDPs are too large for such a representation to be feasible, preventing the use of exact MDP algorithms. Various approximate solution algorithms have been proposed, many of which use a linear combination of basis functions as a compact approximation to the value function. Almost all of these algorithms use an approximation based on the (weighted) L-norm (Euclidean distance) this approach prevents the application of standard convergence results for MDP algorithms, all of which are based on max-norm. This paper makes two contributions. First, it presents the first approximate MDP solution algorithms - both value and policy iteration - that use max-norm projection, thereby directly optimizing the quantity required to obtain the best error bounds. Second, it shows how these algorithms can be applied efficiently in the context of factored MDPs, where the transition model is specified using a dynamic Bayesian network.|Carlos Guestrin,Daphne Koller,Ronald Parr","15972|IJCAI|2003|Modular self-organization for a long-living autonomous agent|The aim of this paper is to provide a sound framework for addressing a difficult problem the automatic construction of an autonomous agent's modular architecture. We briefly present two apparently uncorrelated frameworks Autonomous planning through Markov Decision Processes and Kernel Clustering. Our fundamental idea is that the former addresses autonomy whereas the latter allows to tackle self-organizing issues. Relying on both frameworks, we show that modular self-organization can be formalized as a clustering problem in the space of MDPs. We derive a modular self-organizing algorithm in which an autonomous agent learns to efficiently spread n planning problems over m initially blank modules with m  n.|Bruno Scherrer","16367|IJCAI|2007|A Fast Analytical Algorithm for Solving Markov Decision Processes with Real-Valued Resources|Agents often have to construct plans that obey deadlines or, more generally, resource limits for real-valued resources whose consumption can only be characterized by probability distributions, such as execution time or battery power. These planning problems can be modeled with continuous state Markov decision processes (MDPs) but existing solution methods are either inefficient or provide no guarantee on the quality of the resulting policy. We therefore present CPH, a novel solution method that solves the planning problems by first approximating with any desired accuracy the probability distributions over the resource consumptions with phasetype distributions, which use exponential distributions as building blocks. It then uses value iteration to solve the resulting MDPs by exploiting properties of exponential distributions to calculate the necessary convolutions accurately and efficiently while providing strong guarantees on the quality of the resulting policy. Our experimental feasibility study in a Mars rover domain demonstrates a substantial speedup over Lazy Approximation, which is currently the leading algorithm for solving continuous state MDPs with quality guarantees.|Janusz Marecki,Sven Koenig,Milind Tambe","15323|IJCAI|1997|Prioritized Goal Decomposition of Markov Decision Processes Toward a Synthesis of Classical and Decision Theoretic Planning|We describe an approach to goal decomposition for a certain class of Markov decision processes (MDPs). An abstraction mechanism is used to generate abstract MDPs associated with different objectives, and several methods for merging the policies for these different objectives are considered. In one technique, causal (least-commitment) structures are generated for abstract policies and plan merging techniques, exploiting the relaxation of policy commitments reflected in this structure, are used to piece the results into a single policy. Abstract value functions provide guidance if plan repair is needed. This work makes some first steps toward the synthesis of classical and decision theoretic planning methods.|Craig Boutilier,Ronen I. Brafman,Christopher W. Geib","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell"],["13365|IJCAI|1973|A Gobal View of Automatic Programming|This paper presents a framework for characterizing automatic programming systems In terms of how a task Is communicated to the system, the method and time at which the system acquires the knowledge to perform the task, and the characteristics of the resulting program to perform that task. It describes one approach In which both tasks and knowledge about the task domain are stated in natural language In the terms of that domain. All knowledge of computer science necessary to Implement the task Is Internalized Inside the system.|Robert Balzer","14747|IJCAI|1989|Outline of a Naive Semantics for Reasoning with Qualitative Linguistic Information|This paper describes the mathematical basis for a computer language which can be used for representing natural human reasoning with imprecise linguistic information. The approach to doing this employs a collection of abstraction mechanisms which are based on the concept of a linguistic variable first introduced by Zadeh . The present semantics differs from that of Zadeh, however, in that (i) it does not require the use of fuzzy sets for the interpretation of linguistic terms, and (ii) the meanings of logical inferences are given as algorithms which act directly on linguistic terms themselves, rather than on their underlying interpretations. Two distinct types of deduction algorithm are proposed. The overall objective is to devise a reasoning system having sufficient generality that it can conveniently employ these plus others in a unified frame.|Daniel G. Schwartz","13991|IJCAI|1983|Transportability and Generality in a Natural-Language Interface System|This paper describes the design of a transportable natural language (NL) interface to databases and the constraints that transportability places on each component of such a system. By a transportable NL system, we mean an NL processing system that is constructed so that a domain expert (rather than an AI or linguistics expert) can move the system to a new application domain. After discussing the general problems presented by transportability, this paper describes TEAM (an acronym for Transportable English database Access Medium), a demonstratable prototype of such a system. The discussion of TEAM shows how domain-independent and domain-dependent information can be separated in the different components of a NL interface system, and presents one method of obtaining domain-specific information from a domain expert.|Paul A. Martin,Douglas E. Appelt,Fernando C. N. Pereira","13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico","13436|IJCAI|1973|Understanding Without Proofs|The paper describes the analysis part of a running analysis and generation program for natural language. The system is entirely oriented to matching meaningful patterns onto fragmented paragraph length input. Its core Is a choice system based on what I call \"semantic density\". The system is contrasted with () syntax oriented linguistic approaches and () theorem proving approaches to the understanding problem. It is argued by means of examples that the present system is not only more workable, but more intuitively acceptable, at least as an understander for the purpose of translation, than deduction-based systems.|Yorick Wilks","13761|IJCAI|1981|Metaphor Interpretation as Selective Inferencing|Metaphor pervades natural language discourse. This paper describes a computational approach to the interpretation of metaphors. It is based on a natural language processing system that uses the discourse problems posed by a text to select the relevant inferences. The problem of interpreting metaphors can then be translated into the problem of selecting the relevant inferences to draw from the metaphorical expression. Thus, a metaphor is frequently given a correct interpretation as a by-product of the other things a natural language system has to do. Two examples of metaphors are given -- a spatial metaphor schema from computer science, and a novel metaphor -- and it is shown how the interpretation problem for each can be translated into a selective Inferencing problem and solved by the ordinary operations of the system. This framework sheds light on the analogical processes that underlie metaphor and begins to explain the power of metaphor.|Jerry R. Hobbs","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","14490|IJCAI|1987|A Representation for Natural Category Systems|Most AI systems model and represent natural concepts and categories using uniform taxonomies, in which no level in the taxonomy is distinguished. We present a representation of natural taxonomies based on the theory that human category systems are non-uniform. There is a basic level which forms the core of a taxonomy both higher and lower levels of abstraction are less important and less useful. Empirical evidence for this theory is discussed, as are the linguistic and processing implications of this theory for an artificial intelligencenatural language processing system. Among these implications are () when there is no context effect, basic level names should be used () systems should identify objects as members of their basic level categories more rapidly than as members of their superordinate or subordinate categories. We present our implementation of this theory in SNePS, a semantic network processing system which includes an ATN parser-generator, demonstrating how this design allows our system to model human performance in the natural language generation of the most appropriate category name for an object. The ability of our system to acquire classificational information from natural language sentences is also demonstrated.|Sandra L. Peters,Stuart C. Shapiro","14887|IJCAI|1991|High Performance Natural Language Processing on Semantic Network Array Processor|This paper describes a natural language processing system developed for the Semantic Network Array Processor (SNAP). The goal of our work is to develop a scalable and high-performance natural language processing system which utilizes the high degree of parallelism provided by the SNAP machine. We have implemented an experimental machine translation system as a central part of a real-time speech-to-speech dialogue translation system. It is a SNAP version of the  DMDIAIOG speech-to-speech translation system. Memory-based natural language processing and syntactic constraint network model has been incorporated using parallel marker-passing which is directly supported from hardware level. Experimental results demonstrate that the parsing of a sentence is done in the order of milliseconds.|Hiroaki Kitano,Dan I. Moldovan,Seungho Cha","14140|IJCAI|1985|SAPHIR  RESEDA A New Approach to Intelligent Data Base Access|This paper describes a transportable natural language interface to databases, augmented with a knowledge base and inference techniques. The inference mechanism, based on a classical expert system's type of approach, allows, when needed, to automatically convert an Input query into another one which is \"semantically close\". According to RESEDAS theory, \"semantically close\" means that the answer to the transformed query Implies what could have been the answer to the original question. The presented system Integrates natural language processing, expert system and knowledge representation technology to provide a cooperative database access.|Bernard Euzenat,Bernard Normier,Antoine Ogonowski,Gian Piero Zarri"],["16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","13663|IJCAI|1977|Artificial Intelligence Systems That Understand|From its beginnings, artificial intelligence has borrowed freely from the vocabulary of psychology. The use of the word \"intelligence\" to label our area of research is a case in point. Other terms referring originally to human mental processes that have consider able currency in AI are \"thinking,\" \"comprehending,\" and, with increasing frequency in the past five years, \"understanding.\" Infact, these terms are probably used more freely in AI than in experimental psychology, where a deep suspicion of \"mentalistic\" terminology still lingers as a heritage of behaviorism.|Herbert A. Simon","15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore","15907|IJCAI|2003|Web Intelligence WI What Makes Wisdom Web|Web Intelligence (WI) sheds new light on directions for scientific research and development which explores the fundamental roles as well as practical impacts of Artificial Intelligence (AI) and advanced Information Technology (IT) on the next generation of Web-empowered products, systems, services, and activities. This paper gives new perspectives on the future WI research and highlights some of the research challenges and initiatives.|Jiming Liu","14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz","14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue","15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney","13635|IJCAI|1977|Version Spaces A Candidate Elimination Approach to Rule Learning|An important research problem in artificial intelligence is the study of methods for learning general concepts or rules from a set of training instances. An approach to this problem is presented which is guaranteed to find, without backtracing, all rule versions consistent with a set of positive and negative training instances. The algorithm put forth uses a representation of the space of those rules consistent with the observed training data. This \"rule version space\" is modified in response to new training instances by eliminating candidate rule versions found to conflict with each new instance. The use of version spaces is discussed in the context of Meta-DENDRAL, a program which learns rules in the domain of chemical spectroscopy.|Tom M. Mitchell","14826|IJCAI|1991|Intelligence Without Reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.|Rodney A. Brooks"],["14786|IJCAI|1989|An Empirical Comparison of Pattern Recognition Neural Nets and Machine Learning Classification Methods|Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.|Sholom M. Weiss,Ioannis Kapouleas","14548|IJCAI|1987|Learning Idioms - With and Without Explanation|Modeling learning in any domain can be pursued in two alternative directions either by finding domain-specific heuristics, or by applying general machine-learning methods. In the linguistic domain, programs such as FOULUP  and CHILD  have used specific heuristics, regardless of the general learning methodology other programs , have focussed on the general learning methodology. In our particular task-learning idioms from examples, the evaluation of general learning models is very appealing, since such methods have been studied extensively, and may offer ready solutions. Language is a special domain in regard to learning. Consider for example behavior of idioms. By definition, idiosyncratic properties are not systematic and are not predictable. Thus, how do people learn such properties from examples What is the general learning model, therefore, which accounts for idiom acquisition We examine here the processes involved in acquisition of idioms, and relate them to existing machine-learning paradigms.|Uri Zernik","14366|IJCAI|1987|The Use of Explanations for Similarity-based Learning|Due to the difficult nature of Machine Learning, it has often been looked at in the context of \"toy\" domains or in more realistic domains with simplifying assumptions. We propose an integrated learning approach that combines Explanation-Based and Similarity-Based Learning methods to make learning in an inherently complex domain feasible. We discuss the use of explanations for Similarity-Based Learning and present an example from a program which applies thee ideas to the domain of terrorist events.|Andrea Pohoreckyj Danyluk","14518|IJCAI|1987|Concepts in Conceptual Clustering|Although it has a relatively short history, conceptual clustering is an especially active area of research in machine learning. There are a variety of ways in which conceptual patterns (the AI contribution to clustering) play a role in the clustering process. Two distinct conceptual clustering paradigms (conceptual sorting of exemplars and concept discovery) are described briefly. Then six types of conceptual clustering algorithms are characterized, attempting to cover the present spectrum of mechanisms used to conceptualize the clustering process.|Robert E. Stepp","14934|IJCAI|1991|The Utility of Feature Construcuon for Back-Propagation|The ease of learning concepts from examples in empirical machine learning depends on the attributes used for describing the training data. We show that decision-tree based feature construction can be used to improve the performance of back-propagation (BP), an artificial neural network algorithm, both in terms of the convergence speed and the number of epochs taken by the BP algorithm to converge. We use disjunctive concepts to illustrate feature construction, and describe a measure of feature quality and concept difficulty. We show that a reduction in the difficulty of the concepts to be learned by constructing better representations increases the performance of BP considerably.|Harish Ragavan,Selwyn Piramuthu","15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell","16684|IJCAI|2007|Constructing New and Better Evaluation Measures for Machine Learning|Evaluation measures play an important role in machine learning because they are used not only to compare different learning algorithms, but also often as goals to optimize in constructing learning models. Both formal and empirical work has been published in comparing evaluation measures. In this paper, we propose a general approach to construct new measures based on the existing ones, and we prove that the new measures are consistent with, and finer than, the existing ones. We also show that the new measure is more correlated to RMS (Root Mean Square error) with artificial datasets. Finally, we demonstrate experimentally that the greedy-search based algorithm (such as artificial neural networks) trained with the new and finer measure usually can achieve better prediction performance. This provides a general approach to improve the predictive performance of existing learning algorithms based on greedy search.|Jin Huang,Charles X. Ling","16958|IJCAI|2009|Semi-Supervised Metric Learning Using Pairwise Constraints|Distance metric has an important role in many machine learning algorithms. Recently, metric learning for semi-supervised algorithms has received much attention. For semi-supervised clustering, usually a set of pairwise similarity and dissimilarity constraints is provided as supervisory information. Until now, various metric learning methods utilizing pairwise constraints have been proposed. The existing methods that can consider both positive (must-link) and negative (cannot-link) constraints find linear transformations or equivalently global Mahalanobis metrics. Additionally, they find metrics only according to the data points appearing in constraints (without considering other data points). In this paper, we consider the topological structure of data along with both positive and negative constraints. We propose a kernel-based metric learning method that provides a non-linear transformation. Experimental results on synthetic and real-world data sets show the effectiveness of our metric learning method.|Mahdieh Soleymani Baghshah,Saeed Bagheri Shouraki","14213|IJCAI|1985|Verification-based Learning A Generalized Strategy for Inferring Problem-Reduction Methods|A major impediment to the development of high-performance knowledge-based systems arises from the prohibitive effort involved in equipping these systems with a sufficient set of problem-solving methods. Thus, one important research problem in Machine Learning has been the study of techniques for inferring problem-solving methods from examples. Although a number of techniques for learning problem-solving methods have been described in the literature, all of them assume a state-space model of problem-solving. In this paper we describe a new technique for learning problem-reduction methods, Verification-Based Learning (VBL), which extends the earlier techniques to the problem-reduction formulation of problem-solving. We illustrate the VBL technique with examples drawn from circuit design and symbolic integration.|Sridhar Mahadevan","15204|IJCAI|1995|A Model for Hormonal Modulation of Learning|Recent studies in neurobiology have discovered that many hormones exist in the brain, and play key roles in learning and memorization. In this paper, we discuss the possible role of hormones in learning, and propose a new learning model which incorporates hormonal effects on learning. The model is a variant of reinforcement learning with modulation on learning rate and the frequency of mental rehearsal. The modulation enables the system to focus its learning on data which are evaluated as important for the system's overall performance. The experiment demonstrates that the incorporation of hormonal modulation improves behavior learning performance, and such an evaluation network can be acquired through the evolutionary mechanism.|Hiroaki Kitano"],["14183|IJCAI|1985|EGS A Transformational Approach to Automatic Example Generation|THIS paper describes a constraint transformation approach to automatic example generation and its implementation. In this approach examples are generated as the result of successive transformations of the constraint formulas Such transformations are carried out based on various forms of knowledge Systematic global simplification, largely based on declarative knowledge, mitigates the impact of applying problem-specific and efficient procedural knowledge, with a uniform problem representation scheme. The approach suggests a general framework for example generation in which a language for describing examples can be defined. It also combines a general formal reasoning capacity and problem-specific procedural knowledge, to achieve both generality and efficiency. The implemented system has proven to be expressively powerful and efficient for a variety of applications.|Myung W. Kim","14171|IJCAI|1985|Validating Concepts from Automated Acquisition Systems|Relevant domain features are used for the representation of knowledge in production rules and for the description of examples for rule induction programs. A concept acquisition CCAS, induces domain concepts for the description of middle-game positions in chess, and uses quantitative measures of information content for validating the acquired concepts.|Albrecht Heeffer","14404|IJCAI|1987|Understanding System Specifications Written in Natural Language|This paper describes research in understanding system specifications written in natural language. This research involves the implementation of a natural language interface, PHRANSPAN, for specifying the abstract behavior of digital systems in restricted English text. A neutral formal representation for the behavior is described using the USC Design Data Structure. A small set of concepts that characterize digital system behavior are presented using this representation. An intermediate representation loosely based on Conceptual Dependency is presented. Its use with a semantic-based parser to translate from English to the formal representation is illustrated by examples.|John J. Granacki Jr.,Alice C. Parker,Yigal Arens","15739|IJCAI|2001|FCA-MERGE Bottom-Up Merging of Ontologies|Ontologies have been established for knowledge sharing and are widely used as a means for conceptually structuring domains of interest. With the growing usage of ontologies, the problem of overlapping knowledge in a common domain becomes critical. We propose the new method FCA-MERGE for merging ontologies following a bottom-up approach which offers a structural description of the merging process. The method is guided by application-specific instances of the given source ontologies, that are to be merged. We apply techniques from natural language processing and formal concept analysis to derive a lattice of concepts as a structural result of FCA-MERGE. The generated result is then explored and transformed into the merged ontology with human interaction.|Gerd Stumme,Alexander Maedche","14275|IJCAI|1985|Evidential Reasoning in Semantic Networks A Formal Theory|This paper presents an evidential approach to knowledge representation and inference wherein the principle of maximum entropy is applied to deal with uncertainty and incompleteness. It focuses on a restricted representation language - similar in expressive power to semantic network formalisms, and develops a formal theory of evidential inheritance within this language. The theory applies to a limited, but we think interesting, class of inheritance problems including those that involve exceptions and multiple inheritance hierarchies. The language and the accompanying evidential inference structure provide a natural treatment of defaults and conflicting information. The evidence combination rule proposed in this paper is incremental, commutative and associative and hence, shares most of the attractive features of the Dempster-Shafer evidence combination rule. Furthermore, it is demonstrably better than the Dempster-Shafer rule in the context of the problems addressed in this paper. The resulting theory can be implemented as a highly parallel (connectionist) network made up of active elements that can solve inheritance problems in time proportional to the depth of the conceptual hierarchy.|Lokendra Shastri,Jerome A. Feldman","14809|IJCAI|1991|A Scheme for Integrating Concrete Domains into Concept Languages|A drawback which concept languages based on KL-ONE have is that all the terminological knowledge has to be defined on an abstract logical level. In many applications, one would like to be able to refer to concrete domains and predicates on these domains when defining concepts. Examples for such concrete domains are the integers, the real numbers, or also non-arithmetic domains, and predicates could be equality, inequality, or more complex predicates. In the present paper we shall propose a scheme for integrating such concrete domains into concept languages rather than describing a particular extension by some specific concrete domain. We shall define a terminological and an assertional language, and consider the important inference problems such as subsumption, instantiation, and consistency. The formal semantics as well as the reasoning algorithms can be given on the scheme level. In contrast to existing KL-ONE based systems, these algorithms are not only sound but also complete. They generate subtasks which have to be solved by a special purpose reasoner of the concrete domain.|Franz Baader,Philipp Hanschke","13577|IJCAI|1977|Ms Maloprop A Language Comprehension Program|This paper describes Ms. Malaprop, a program (currently being designed) which will answer questions about simple stories dealing with painting, where stories, questions and answers will be expressed in semantic representation rather than English in order to allow concentration on the inferential problems involved in language comprehension. The common sense knowledge needed to accomplish the task is provided by the frame representation of \"mundane\" painting found in Charniak (b). The present paper, after reviewing this representation, goes on to describe how it is used by Ms. Malaprop. Some specific questions of matching, correcting false conclusions, and search, will be discussed.|Eugene Charniak","14029|IJCAI|1983|A Formal Approach to the Semantics of a Frame Data Model|Standard knowledge representation languages are seriously lacking an explicit formal semantic specification. This may cause considerable trouble when applied to large amounts of rapidly changing data. Based on an abstract data type view of knowledge representation languages a formal definition of a frame data model is presented in terms of a denotational semantics approach using a subset of META-IV. After introducing some basic concepts of the model several semantic integrity constraints are outlined which ultimately lead to the formulation of a set of operations in the frame data model.|Ulrich Reimer,Udo Hahn","13790|IJCAI|1981|The Interaction with Incomplete Knowledge Bases A Formal Treatment|Some formal representation Issues underlying the Interaction between an expert system and Its knowledge base are discussed. It is argued that a language that can refer both to the application domain and to the state of the knowledge base is required to specify and to question an incomplete knowledge base. A formal logical language with this ability is presented and its semantics and proof theory are defined. It is then shown how this language must be used to interact with the knowledge base.|Hector J. Levesque","14867|IJCAI|1991|Measuring and Improving the Effectiveness of Representations|This report discusses what it means to claim that a representation is an effective encoding of knowledge. We first present dimensions of merit for evaluating representations, based on the view that usefulness is a behavioral property, and is necessarily relative to a specified task. We then provide methods (based on results from mathematical statistics) for reliably measuring effectiveness empirically, and hence for comparing different representations. We also discuss weak but guaranteed methods of improving inadequate representations. Our results are an application of the ideas of formal learning theory to concrete knowledge representation formalisms.|Russell Greiner,Charles Elkan"],["14550|IJCAI|1989|Modal Theorem Proving An Equational Viewpoint|We propose a new method for automated theorem proving in first order modal logic. Essentially, the method consists in a translation of modal logic into a specially designed typed first order logic cal led Path Logic, such that classical modal systems (first order Q, T, , S, S) can be characterized by sets of equations. The question of modal theorem proving then amounts to classical theorem proving in some equational theories. Different methods can be investigated and in this paper we cons ider Resolution. We may use Resolution with Paramodulation, or a combination of Resolution and Rewriting techniques. In both cases, known results provide \"free of charge\" a framework immediately applicable to Path Logic, with completeness theorems. Considering efficiency, the Rewriting method seems better and we present here in details its application to Path Logic. In particular we show how it is possible to define a special kind of skolemisation and design a unification algorithm which insures that two clauses will always have a finite set of resolvents.|Yves Auffray,Patrice Enjalbert","16235|IJCAI|2005|Possibilistic Stable Models|In this work, we define a new framework in order to improve the knowledge representation power of Answer Set Programming paradigm. Our proposal is to use notions from possibility theory to extend the stable model semantics by taking into account a certainty level, expressed in terms of necessity measure, on each rule of a normal logic program. First of all, we introduce possibilistic definite logic programs and show how to compute the conclusions of such programs both in syntactic and semantic ways. The syntactic handling is done by help of a fix-point operator, the semantic part relies on a possibility distribution on all sets of atoms and we show that the two approaches are equivalent. In a second part, we define what is a possibilistic stable model for a normal logic program, with default negation. Again, we define a possibility distribution allowing to determine the stable models.|Pascal Nicolas,Laurent Garcia,Igor Stéphan","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","16779|IJCAI|2007|From Answer Set Logic Programming to Circumscription via Logic of GK|We first provide a mapping from Pearce's equilibrium logic and Ferraris's general logic programs to Lin and Shoham's logic of knowledge and justified assumptions, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this mapping, we obtain a mapping from general logic programs to circumscription, both in the propositional and first-order case. Furthermore, we show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic.|Fangzhen Lin,Yi Zhou","14866|IJCAI|1991|Reflective Reasoning with and between a Declarative Metatheory and the Implementation Code|The goal of this paper is to present a theorem prover where the underlying code has been written to behave as the procedural metalevel of the object logic. We have then defined a logical declarative metatheory MT which can be put in a one-to-one relation with the code and automatically generated from it. MT is proved correct and complete in the sense that, for any object level deduction, the wff representing it is a theorem of MT, and viceversa. Such theorems can be translated back in the underlying code. This opens up the possibility of deriving control strategies automatically by metatheoretic theorem proving, of mapping them into the code and thus of extending and modifying the system itself. This seems a first step towards \"really\" self-reflective systems, it. systems able to reason deductively about and modify their underlying computation mechanisms. We show that the usual logical reflection rules (so called reflection up and down) are derived inference rules of the system.|Fausto Giunchiglia,Paolo Traverso","14889|IJCAI|1991|How to Prove Higher Order Theorems in First Order Logic|In this paper we are interested in using a first order theorem prover to prove theorems that are formulated in some higher order logic. To this end we present translations of higher order logics into first order logic with flat sorts and equality and give a sufficient criterion for the soundness of these translations. In addition translations are introduced that are sound and complete with respect to L. Henkin's general model semantics. Our higher order logics are based on a restricted type structure in the sense of A. Church, they have typed function symbols and predicate symbols, but no sorts.|Manfred Kerber","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","14563|IJCAI|1989|The Logic of Time Structures Temporal and Nonmonotonic Features|We Imbed Into a first order logic a representation language that combines a temporal knowledge with time stamps in a hierarchical fashion. Each time structure contains its own chronology of events sufficient information for an encoding of a classical temporal logic. By quantifying over time structures, we encode a modal logic of temporal knowledge. In addition, we show how to achieve the effect of nonmonotonic inference, by simulating preferential entailment within a first order framework.|Mira Balaban,Neil V. Murray","15064|IJCAI|1993|First-Order Modal Logic Theorem Proving and Functional Simulation|We propose a translation approach from modal logics to first-order predicate logic which combines advantages from both, the (standard) relational translation and the (rather compact) functional translation method and avoids many of their respective disadvantages (exponential growth versus equality handling). In particular in the application to serial modal logics it allows considerable simplifications such that often even a simple unit clause suffices in order to express the accessibility relation properties. Although we restrict the approach here to first-order modal logic theorem proving it has been shown to be of wider interest, as e.g. sorted logic or terminological logic.|Andreas Nonnengart","15004|IJCAI|1993|Cooperation between Direct Method and Translation Method in Non Classical Logics Some Results in Propositional S|The aim of this work is to combine advantageously the two existing approaches for theorem proving in non classical logics proving in the considered non classical logic (called here the direct approach) and proving in classical logic by way of translation -called here the translation approach. Some results in propositional S show evidence of the relevance of this approach. We assume a translation from S into first-order logic and then we define a partial inverse formula translation from first-order classical logic into S. Semantic relations are proved to hold between the backward translated formulas. We answer positively (for S) to one conjecture stated in a previous work by the authors. An Interpolation Theorem stating a property stronger than refutational completeness is also proved. A plausible conjecture stronger than the Interpolation Theorem is proposed. These results are interpreted in the framework of a slight variant of an existing resolution calculus for S. We illustrate our method on a simple example. Future work includes applications of the approach to other modal logics.|Ricardo Caferra,Stéphane Demri"],["14090|IJCAI|1985|Utilization of a Stripe Pattern for Dynamic Scene Analysis|This paper describes a new idea to project a stripe pattern onto a time-varying scene to find moving objects and acquire scene features in the consecutive frames for estimating -D motion parameters. At first, a simple temporal difference method detects objects moving against a complex background. A ()D representation of moving objects at each frame is then obtained by estimating surface normals from the slopes and intervals of stripes in the image. The ()D image is further divided into planar or singly curved surfaces by examining the distribution of the surface normals in the gradient space. Then, the rotational motion parameters of the objects are estimated from changes in the geometry of these surfaces between frames. Determining translational ones is also discussed.|Minoru Asada,Saburo Tsuji","13855|IJCAI|1981|Structure from Motion of Rigid and Jointed Objects|A method for structure from motion is presented. The method makes a motion assumption about the objects being viewed. The motion assumption is that all motion consists of translations and rotations about a fixed axis. Parallel projection is also assumed. This makes it possible to interpret the motion of as few as two rigidly connected points. The method works for both rigid and jointed objects. Results of a test of this method on Johansson's data are presented.|Jon A. Webb,Jake K. Aggarwal","16301|IJCAI|2005|SVM-based Obstacles Recognition for Road Vehicle Applications|This paper describes an obstacle Recognition System based on SVM and vision. The basic components of the detected objects are first located in the image and then combined with a SVM-based classifier. A distributed learning approach is proposed in order to better deal with objects variability, illumination conditions, partial occlusions and rotations. A large database containing thousands of object examples extracted from real road images has been created for learning purposes. We present and discuss the results achieved up to date.|Miguel ?ngel Sotelo,Jesús Nuevo,David Fernández,I. Parra,Luis Miguel Bergasa,Manuel Ocaña,Ramón Flores","13346|IJCAI|1971|One System for Simulation of Pattern Recognition Algorithms|A general purpose system for a computer simulation of pattern recognition algorithms is proposed that consists of a universal scanner for optical images input to the computer, a display for visual output, and software. A characteristic feature of the system is a blok structure of the hard-ware and software. The system allows carrying out most of the data processing necessary for simulating the algorithms of recognizing the optical images of objects of various physical natures.|V. I. Rybak,Georgy L. Gimel'farb,E. F. Kushner","14355|IJCAI|1987|Qualitative Motion Understanding|The vision system of an Autonomous Land Vehicle is required to handle complex dynamic scenes. Vehicle motion and individually moving objects in the field of view contribute to a continuously changing camera image. It is the purpose of motion understanding to find consistent three-dimensional interpretations for these changes in the image sequence. We present a new approach to this problem, which departs from previous work by emphasizing a qualitative nature of reasoning and modeling and maintaining multiple interpretations of the scene at the same time. This approach offers advantages such as robustness and flexibility over \"hard\" numerical techniques which have been proposed in the motion understanding literature.|Wilhelm Burger,Bir Bhanu","16576|IJCAI|2007|Peripheral-Foveal Vision for Real-time Object Recognition and Tracking in Video|Human object recognition in a physical -d environment is still far superior to that of any robotic vision system. We believe that one reason (out of many) for this--one that has not heretofore been significantly exploited in the artificial vision literature--is that humans use a fovea to fixate on, or near an object, thus obtaining a very high resolution image of the object and rendering it easy to recognize. In this paper, we present a novel method for identifying and tracking objects in multiresolution digital video of partially cluttered environments. Our method is motivated by biological vision systems and uses a learned \"attentive\" interest map on a low resolution data stream to direct a high resolution \"fovea.\" Objects that are recognized in the fovea can then be tracked using peripheral vision. Because object recognition is run only on a small foveal image, our system achieves performance in real-time object recognition and tracking that is well beyond simpler systems.|Stephen Gould,Joakim Arfvidsson,Adrian Kaehler,Benjamin Sapp,Marius Messner,Gary R. Bradski,Paul Baumstarck,Sukwon Chung,Andrew Y. Ng","14060|IJCAI|1983|Correspondence in Line Drawings of Multiple Views of Objects|As an object moves relative to a viewpoint, its appearance changes. In this paper we analyze the topological constraints on the changing appearance of line drawings of objects as the objects or the camera move. We start with a Huffman-Clowes junction dictionary. We show a way of deriving vertex types from junction types by inference rather than by table look-up. We then derive three constraints on the change in appearance of an object conservation of vertices, conservation of vertex type, and conservation of adjacencies. Using these constraints, we develop a matching algorithm that traces vertices from one image to the next. Examples are given showing correct matching tor simple objects, including partially visible objects and multiple objects in the same scene.|Charles E. Thorpe,Steven A. Shafer","15506|IJCAI|1999|Tracking Many Objects with Many Sensors|Keeping track of multiple objects over time is a problem that arises in many real-world domains. The problem is often complicated by noisy sensors and unpredictable dynamics. Previous work by Huang and Russell, drawing on the data association literature, provided a probabilistic analysis and a threshold-based approximation algorithm for the case of multiple objects detected by two spatially separated sensors. This paper analyses the case in which large numbers of sensors are involved. We show that the approach taken by Huang and Russell, who used pairwise sensor-based appearance probabilities as the elementary probabilistic model, does not scale. When more than two observations are made, the objects' intrinsic properties must be estimated. These provide the necessary conditional independencies to allow a spatial decomposition of the global probability model. We also replace Huang and Russell's threshold algorithm for object identification with a polynomial-time approximation scheme based on Markov chain Monte Carlo simulation. Using sensor data from a freeway traffic simulation, we show that this allows accurate estimation of long-range origindestination information even when the individual links in the sensor chain are highly unreliable.|Hanna Pasula,Stuart J. Russell,Michael Ostland,Yaacov Ritov","14356|IJCAI|1987|Recognition in D Images of D Objects from Large Model Bases Using Prediction Hierarchies|An object recognition system is presented that it designed to handle the computational complexity posed by a large model base, an unconstrained viewpoint and the structural complexity and detail inherent in a single view. The design is based on two ideas. The first is to compute descriptions of what the objects should look like in the image, called predictions, before the recognition task begins. This reduces actual recognition to a D matching process, substantially speeding up recognition time for D objects (with manageable storage overhead). The second is to represent all the predictions by a single, combined ISA and PART-OF hierarchy called a prediction hierarchy. The nodes in this hierarchy are partial descriptions that are common to views and hence constitute shared processing subgoals during matching. Many of the problems encountered with large model bases and complex models are reduced by subgoal sharing projections with similarities explicitly share the representation and recognition of their common aspects. The original contribution of this paper is the automatic compilation, from a D model base, of a prediction hierarchy that can be used to recognise objects. A prototype system based on these ideas is demonstrated using a set of polyhedral objects and projections from an unconstrained range of viewpoints.|J. Brian Burns,Leslie J. Kitchen","13535|IJCAI|1975|Edge Finding Segmentation Of Edges And Recognition Of Complex Objects|This paper describes an approach to the recognition of real-world objects such as books or a telephone on a desk. The system consists of () edge finding process which extracts edges of curved objects from light intensity data, () segmentation of the edges into straight lines or elliptic curves, () recognition of objects by matching the liner, to the models, and () simple supervisor. The module (), () and () interact with each other through a simple supervisor so that the system can locate given objects quickly-First, most reliable edges a refound and segmented, and then recognition ir. attempted using segmented lines. If recognition is not successful, less reliable edges are searched for, arid recognition is retried. An example of locating a lamp, a book stand and a telephone is shown.|Y. Shirai"],["15836|IJCAI|2003|DP-SLAM Fast Robust Simultaneous Localization and Mapping Without Predetermined Landmarks|We present a novel, laser range finder based algorithm for simultaneous localization and mapping (SLAM) for mobile robots. SLAM addresses the problem of constructing an accurate map in real time despite imperfect information about the robot's trajectory through the environment. Unlike other approaches that assume predetermined landmarks (and must deal with a resulting data-association problem) our algorithm is purely laser based. Our algorithm uses a particle filter to represent both robot poses and possible map configurations. By using a new map representation, which we call distributed particle (DP) mapping, we are able to maintain and update hundreds of candidate maps and robot poses efficiently. The worst-case complexity of our algorithm per laser sweep is log-quadratic in the number of particles we maintain and linear in the area swept out by the laser. However, in practice our run time is usually much less than that. Our technique contains essentially no assumptions about the environment yet it is accurate enough to close loops of m in length with crisp, perpendicular edges on corridors and minimal or no misalignment errors.|Austin I. Eliazar,Ronald Parr","15936|IJCAI|2003|FastSLAM  An Improved Particle Filtering Algorithm for Simultaneous Localization and Mapping that Provably Converges|In , Montemerlo et al. proposed an algorithm called FastSLAM as an efficient and robust solution to the simultaneous localization and mapping problem. This paper describes a modified version of FastSLAM that overcomes important deficiencies of the original algorithm. We prove convergence of this new algorithm for linear SLAM problems and provide real-world experimental results that illustrate an order of magnitude improvement in accuracy over the original FastSLAM algorithm.|Michael Montemerlo,Sebastian Thrun,Daphne Koller,Ben Wegbreit","15952|IJCAI|2003|Thin Junction Tree Filters for Simultaneous Localization and Mapping|Simultaneous Localization and Mapping (SLAM) is a fundamental problem in mobile robotics while a robot navigates in an unknown environment, it must incrementally build a map of its surroundings and, at the same time, localize itself within that map. One popular solution is to treat SLAM as an estimation problem and apply the Kalman filter this approach is elegant, but it does not scale well the size of the belief state and the time complexity of the filter update both grow quadratically in the number of landmarks in the map. This paper presents a filtering technique that maintains a tractable approximation of the belief state as a thin junction tree. The junction tree grows under filter updates and is periodically \"thinned\" via efficient maximum likelihood projections so inference remains tractable. When applied to the SLAM problem, these thin junction tree filters have a linear-space belief state and a linear-time filtering operation. Further approximation yields a filtering operation that is often constant-time. Experiments on a suite of SLAM problems validate the approach.|Mark A. Paskin","15205|IJCAI|1995|Planning Executing Sensing and Replanning for Information Gathering|Current specialized planners for query processing are designed to work in local, reliable, and predictable environments. However, a number of problems arise in gathering information from large networks of distributed information. In this environment, the same information may reside in multiple places, actions can be executed in parallel to exploit distributed resources, new goals come into the system during execution, actions may fail due to problems with remote databases or networks, and sensing may need to be interleaved with planning in order to formulate efficient queries. We have developed a planner called Sage that, addresses the issues that arise in this environment. This system integrates previous work on planning, execution, replanning, and sensing and extends this work to support simultaneous and interleaved planning and execution. Sage has been applied to the problem of information gathering to provide a flexible and efficient system for integrating heterogeneous and distributed data.|Craig A. Knoblock","15324|IJCAI|1997|Active Mobile Robot Localization|Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effectors during localization. This paper proposes an active localization approach. The approach provides rational criteria for () setting the robot's motion direction (exploration), and () determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment.|Wolfram Burgard,Dieter Fox,Sebastian Thrun","15386|IJCAI|1997|Combining Probabilistic Population Codes|We study the problem of statistically correct inference in networks whose basic representations are population codes. Population codes are ubiquitous in the brain, and involve the simultaneous activity of many units coding for some low dimensional quantity. A classic example are place cells in the rat hippocampus these fire when the animal is at a particular place in an environment, so the underlying quantity has two dimensions of spatial location. We show how to interpret the activity as encoding whole probability distributions over the underlying variable rather then just single values, and propose a method of inductively learning mappings between population codes that are computationally tractable and yet offer good approximations to statistically optimal inference. We simulate the method on some simple examples to prove its competence.|Richard S. Zemel,Peter Dayan","16554|IJCAI|2007|WiFi-SLAM Using Gaussian Process Latent Variable Models|WiFi localization, the task of determining the physical location of a mobile device from wireless signal strengths, has been shown to be an accurate method of indoor and outdoor localization and a powerful building block for location-aware applications. However, most localization techniques require a training set of signal strength readings labeled against a ground truth location map, which is prohibitive to collect and maintain as maps grow large. In this paper we propose a novel technique for solving the WiFi SLAM problem using the Gaussian Process Latent Variable Model (GPLVM) to determine the latent-space locations of unlabeled signal strength data. We show how GPLVM, in combination with an appropriate motion dynamics model, can be used to reconstruct a topological connectivity graph from a signal strength sequence which, in combination with the learned Gaussian Process signal strength model, can be used to perform efficient localization.|Brian Ferris,Dieter Fox,Neil D. Lawrence","15905|IJCAI|2003|Consistent Convergent and Constant-Time SLAM|This paper presents a new efficient algorithm for simultaneous localization and mapping (SLAM), using multiple overlapping submaps, each built with respect to a local frame of reference defined by one of the features in the submap. The global position of each submap is estimated using information from other submaps in an efficient, provably consistent manner. For situations where the mobile robot is able to make repeated visits to all regions of the environment, the method achieves convergence to a near-optimal result with O(I) time complexity while maintaining consistent error bounds. Simulation results demonstrate the ability of the technique to converge to errors that are only slightly greater than the full solution, while maintaining consistency.|John J. Leonard,Paul M. Newman","16599|IJCAI|2007|Loopy SAM|Smoothing approaches to the Simultaneous Localization and Mapping (SLAM) problem in robotics are superior to the more common filtering approaches in being exact, better equipped to deal with non-linearities, and computing the entire robot trajectory. However, while filtering algorithms that perform map updates in constant time exist, no analogous smoothing method is available. We aim to rectify this situation by presenting a smoothingbased solution to SLAM using Loopy Belief Propagation (LBP) that can perform the trajectory and map updates in constant time except when a loop is closed in the environment. The SLAM problem is represented as a Gaussian Markov Random Field (GMRF) over which LBP is performed. We prove that LBP, in this case, is equivalent to Gauss-Seidel relaxation of a linear system. The inability to compute marginal covariances efficiently in a smoothing algorithm has previously been a stumbling block to their widespread use. LBP enables the efficient recovery of the marginal covariances, albeit approximately, of landmarks and poses. While the final covariances are overconfident, the ones obtained from a spanning tree of the GMRF are conservative, making them useful for data association. Experiments in simulation and using real data are presented.|Ananth Ranganathan,Michael Kaess,Frank Dellaert","16210|IJCAI|2005|Relational Object Maps for Mobile Robots|Mobile robot map building is the task of generating a model of an environment from sensor data. Most existing approaches to mobile robot mapping either build topological representations or generate accurate, metric maps of an environment. In this paper we introduce relational object maps, a novel approach to building metric maps that represent individual objects such as doors or walls. We show how to extend relational Markov networks in order to reason about a hierarchy of objects and the spatial relationships between them. Markov chain Monte Carlo is used for efficient inference and to learn the parameters of the model. We show that the spatial constraints modeled by our mapping technique yield drastic improvements for labeling line segments extracted from laser range-finders.|Benson Limketkai,Lin Liao,Dieter Fox"],["13444|IJCAI|1975|A Speech Understanding System Based Upon A Co-Routine Parser|This paper gives a brief description of the speech understanding effort under Development at the Univ. of Toronto. The main purpose so far has been to produce a base from which further research into the \"higher levels\" of speech understanding (semantics, pragmatics, user models, syntax) may build on. Some features of interest in this system are the syllable based pattern recognition, the dynamic reclassification of the input signal according to expectations, interactive pattern formation, dictionary retrieval by sound characteristics and finally, the use of an Augmented Transition Network grammar with a co-routine parsing scheme which can be guided by prosodic and semantic information. Most of the emphasis in the paper is placed on the co-routine parsing scheme which is illustrated with a detailed example.|J. Allen","13460|IJCAI|1975|Microphonemes As Fundamental Segments Of Speech Wave Primary Segmentation - Automatic Searching For Microphonemes|This paper concerns the stage of acoustic analysis in speech recognition of Speech Understanding System SUSY subsystem AKORD, Acoustic analysis in this system is based on dynamic spectral analysis of the speech wave. It corresponds with the function of the hearing organ expressed by an analog nodel of human ear. Acoustic analysis in the AKORD system uses an FFT algorithm but is not strictly based on accepting constant intervals of - ms analysis does not cause inaccuracy at the level of parametrization because of a considerable dispersion of parameters of particular segments. A conclusion was made in the AKORD system that a properly conducted process of segmentation will help in solving some problems of speech wave analysis, particularly recognition and time compression. This paper attempts to choose the primary segment (microphonem) in the most optimum way and to Indicate the algorithm of automatic segmentation based on microphonemes as the dynamic segments.|A. Dziurnikowski","13384|IJCAI|1973|System Organizations for Speech Understanding Implications of Network and Multiprocessor Computer Architecture for AI|This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented control of pro|Lee D. Erman,R. D. Fenneli,Victor R. Lesser,D. R. Reddy","14474|IJCAI|1987|Use of Procedural Knowledge for Automatic Speech Recognition|A paradigm for automatic speech recognition using networks of actions performing variable depth analysis is presented. The paradigm produces descriptions of speech properties that are related to speech units through Markov models representing system performance. Preliminary results in the recognition of isolated letters and digits are presented.|Renato de Mori,Ettore Merlo,Mathew J. Palakal,Jean Rouat","13413|IJCAI|1973|The Hearsay Speech Understanding System An Example of the Recognition Process|This paper describes the structure and operation of the Hearsay speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of Knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its Operation in a particular task situation Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.|D. R. Reddy,Lee D. Erman,R. D. Fenneli,Richard B. Neely","14016|IJCAI|1983|The FOPHO Speech Recognition Project|The FOPHO (Foreign Phonetician) speech recognition project concerns the development of a system to produce a reasonably high quality phonetic transcription output from continuous speech input. The system is developed to perform in a way which approximates the actions of a phonetician trying to transcribe a foreign tongue, (in the case of FOPHO, Australian English). Because of this central philosophy, FOPHO is a very interactive system and has facilities for automatic learning and analysis of its own performance. Good quality recognition is achieved through algorithms which are very context-dependent and which are sensitive to a variety of possible productions of similar sounds even though the system itself is speaker independent.|Mary O'Kane","13686|IJCAI|1977|Procedures for Integrating Knowledge in a Speech Understanding System|This paper describes the procedures for integrating knowledge from different sources in the SRI speech understanding system. A language definition system coordinates - at the phrase level --information from syntax, semantics and discourse in the course of the interpretation of an utterance. The system executive uses these contextual constraints in assigning priorities to alternative interpretations, combining top-down, bottom-up, and bidirectional strategies as required Experimental results that demonstrate the effectiveness of context checking are discussed.|Donald E. Walker,William H. Paxton,Barbara J. Grosz,Gary G. Hendrix,Ann E. Robinson,Jane J. Robinson,Jonathan Slocum","13409|IJCAI|1973|A Parser for a Speech Understanding System|This paper describes a parsing system specifically designed for spoken rather than written input. The parser is part of a project in progress at Stanford Research Institute to develop a computer system for understanding speech. The approach described uses as much heuristic knowledge as possible in order to minimize the demands on acoustic analysis.|William H. Paxton,Ann E. Robinson","14446|IJCAI|1987|Temporal Event Conceptualization|This paper describes a system that performs Temporal Event Recognition, ie., the task of forming causal and conceptual descriptions of stimulii given to a system over a period of time. A temporal logic that integrates state based and interval based representations is is used to represent and maintain knowledge about the temporal relationships between events. A knowledge based system is then constructed that uses this temporal knowledge to generate higher level conceptual abstractions that describe the events occurring at the input. An implementation of the system called MUSE is described and examples of the system working in a Blocks world are presented.|Krishna Kumar,Amitabha Mukerjee","13747|IJCAI|1981|GLP A General Linguistic Processor|GLP is a general linguistic processor for the analysis and generation of natural language. It is part of a speech understanding system currently under development at the Computer Science Department of our university .|G. Goerx"],["13931|IJCAI|1983|Techniques for Sensor-Based Diagnosis|This paper describes a system called PDS, a forward chaining, rule-based architecture designed for the online, realtime diagnosis of machine processes. Two issues arise in the application of expert systems to the analysis of sensor-based data spurious readings and sensor degradation. PDS implements techniques called retrospective analysis and meta-diagnosis as solutions to these problems. These techniques and our experiences in knowledge acquisition in a large organization, and the implementation of PDS as a portable diagnostic tool are described.|Mark S. Fox,Simon Lowenfeld,Pamela Kleinosky","13754|IJCAI|1981|How Expert Should an Expert System Be|A computer system which aids computer engineers in fault diagnosis is described. The system, called CRIB (Computer Retrieval Incidence Bank) is shown to fit into the class of pattern-directed inference systems. Emphasis is placed on the \"before\" and \"after\" phases of system generation and it is shown why, to be called an expert system, these phases are important. The forms of knowledge used in CRIB are shown to be adequate for diagnosis and yet possess little of the structural or functional knowledge of more advanced expert systems. Summaries are given of the three phases of implementation elicitation, implementation of knowledge structures, validation and improvement. The idea of an expert system as a \"model of competence\" is mentioned and the transferrance of the system architecture to software diagnosis, using the same model, is described. There are short discussions of system performance and the nature of expert systems.|Roger T. Hartley","14167|IJCAI|1985|A Case Study in Structured Knowledge Acquisition|Building an expert system usually comprises an entangled mixture of knowledge acquisition and Implementation efforts. An emerging methodology based on cognitive psychology and software development guides and supports knowledge acquisition while Implementation is deferred. This allows for a more deliberate choice of architecture. This paper presents a case studto test the methodology.|Paul de Greef,Joost Breuker","13740|IJCAI|1981|Acquisition of Procedural Knowledge from Domain Experts|Procedural knowledge forms an important part of expertise. This paper describes a system which allows domain experts to themselves enter procedural Knowledge into a knowledge base. The system, a stylized form of scientific English embodied within the Unit System for knowledge acquisition and representation, has been used successfully within the domain of molecular biology.|Peter Friedland","13822|IJCAI|1981|Application Design Issues in Expert System Architecture|We describe an expert system that has been applied to the task of application design. Users supply the system with problem specifications, such as the required output data, and the system produces a graphic representation of the completed application in the form of a flow diagram. The application design task has forced us to consider two important issues in expert system architecture constraint processing and the explicit representation of control flow. The resulting knowledge representation and control logic are discussed.|Harry C. Reinstein,Janice S. Aikins","13628|IJCAI|1977|The Reformulation Approach to Building Expert Systems|Many of the tasks that go into the building of an expert system - collecting the expert knowledge, setting it up for efficient problem-solving, providing mechanisms for acquisition and explanation - are structured by the choice of organization for the system's knowledge base. This paper discusses one such organization and the implementation approach it entails. This approach has been used to produce a business consultant program.|William S. Mark","14154|IJCAI|1985|Representing Procedural Knowledge in Expert Systems An Application to Process Control|The paper presents a novel expert system architecture which supports explicit representation and effective use of both declarative and procedural knowledge. These two types of expert knowledge are represented by means of production rules and event-graphs respectively, and they are processed by a unified inference engine. Communication between the rule level and the event-graph level is based on a full visibility of each level on the internal state of the other, and it is structured in such a way as to allow each level to expert control on the other. This structure offers several advantages over more traditional architectures. Knowledge representation is more natural and transparent knowledge acquisition turns out to be easier as pieces of knowledge can be immediately represented without the need of complex transformation and restructuring inference is more effective due to reduced non-determinism resulting from explicit representation of fragments of procedural knowledge in event-graphs finally, explanations are more natural and understandable. The proposed architecture has been adopted for the design of PROP, an expert system for on-line monitoring of the cycle water pollution in a thermal power plant. PROP is running on a SUN- workstation and has been tested on a sample of real cases.|Massimo Gallanti,Giovanni Guida,Luca Spampinato,Alberto Stefanini","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","14720|IJCAI|1989|On the Road to Automatic Knowledge Engineering|The paper presents a scheme for categorizing knowledge engineering tools. The classification of knowledge acquisition systems has revealed some interesting facts about these systems. It seems that systems which are able to work on multiple tasks produce very shallow (i.e., not of expert-level) knowledge bases. On the other hand, systems which produce expert-level knowledge bases function on a single task. These insights have led to the design of ASKE, a knowledge acquisition system which can be used to build expert-level knowledge bases in several domains and for different task-types. The knowledge acquisition process is based on the notion of templates, the knowledge-bearing units of ASKE. Templates provide a convenient way of representing domain knowledge.|Jitu Patel","13869|IJCAI|1981|An Information Presentation System|AIPS is a system for graphically presenting information. It promotes a high degree of interactivity between a user and a knowledge base or knowlege-based system, and is designed to be utmost domain independent and extensible. This paper describes the concept of an Information Presentation System (IPS), the intimate relationship between IPS goals and knowledge representation issues, and some of the architecture of AIPS.|Frank Zdybel,Norton Greenfeld,Martin D. Yonke,Jeff Gibbons"]]},"title":{"entropy":6.275462611947025,"topics":["model for, learning, neural networks, learning for, mobile robot, reinforcement learning, using, algorithm for, learning and, model, framework for, for networks, networks, for robot, learning from, learning with, efficient for, fault diagnosis, model-based diagnosis, web page","problems solving, heuristic search, constraint satisfaction, search for, the problems, for planning, description logic, logic programming, for logic, algorithm for, planning and, search, logic programs, with, planning, problems, planning with, and problems, local search, for problems","natural language, for system, the system, system, expert system, for language, artificial intelligence, for knowledge, and system, knowledge system, speech understanding, knowledge base, language, expert for, data base, sense disambiguation, the language, pattern recognition, for and, for natural","the and, and, reasoning about, reasoning and, theorem proving, for and, and action, for reasoning, reasoning, first order, the calculus, and its, representation and, between and, situation calculus, logic and, and image, combining and, preliminary report, belief revision","learning and, learning, learning for, learning with, reinforcement learning, learning using, learning the, learning from, for feature, learning model, feature selection, concept learning, hidden markov, machine learning, learning data, strategy for, markov processes, explanation-based learning, learning examples, semi-supervised learning","mobile robot, for robot, based for, for probabilistic, relational learning, for mobile, robot, networks based, robot and, with robot, robot control, probabilistic model, rules for, control for, based, for maps, mobile using, learning robot, fast for, based model","decision trees, the heuristic, heuristic for, programming for, decision processes, for decision, and heuristic, dynamic for, for csp, and programming, dynamic and, the programming, the dynamic, dynamic programming, dynamic with, and csp, with trees, ordering for, extending with, decomposition csp","for logic, constraint satisfaction, logic programming, logic and, the logic, description logic, logic programs, logic, logic with, constraint and, default logic, for constraint, description and, resource allocation, description with, with constraint, constraint, the description, nonmonotonic logic, for description","for system, the system, and system, expert system, system, architecture for, for agents, the architecture, integrating and, for and, system based, vision system, expert for, and task, hybrid system, design and, representation system, for the, agents, system with","for recognition, speech understanding, the use, recognition and, sense disambiguation, pattern recognition, from examples, the speech, for speech, word sense, word disambiguation, for understanding, for plans, for analysis, speech system, and plans, for disambiguation, and analysis, case study, the recognition","and its, semantic and, and application, from and, information extraction, and retrieval, its application, from image, state representation, and image, from, and memory, and searching, the its, information retrieval, motion from, motion and, shape and, semantic for, shape from","the and, representation and, and inference, structure and, relations and, representation for, the, knowledge representation, for the, the role, the representation, relations between, the relations, the structure, graph and, representation, for matching, structure for, computational model, matching image"],"ranking":[["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","15381|IJCAI|1997|An Effective Learning Method for Max-Min Neural Networks|Max and min operations have interesting properties that facilitate the exchange of information between the symbolic and real-valued domains. As such, neural networks that employ max-min activation functions have been a subject of interest in recent years. Since max-min functions are not strictly differentiate, we propose a mathematically sound learning method based on using Fourier convergence analysis of side-derivatives to derive a gradient descent technique for max-min error functions. This method is applied to a \"typical\" fuzzy-neural network model employing max-rnin activation functions. We show how this network can be trained to perform function approximation its performance was found to be better than that of a conventional feedforward neural network.|Loo-Nin Teow,Kia-Fock Loe","17031|IJCAI|2009|Learning Conditional Preference Networks with Queries|We investigate the problem of eliciting CP-nets in the well-known model of exact learning with equivalence and membership queries. The goal is to identify a preference ordering with a binary-valued CP-net by guiding the user through a sequence of queries. Each example is a dominance test on some pair of outcomes. In this setting, we show that acyclic CP-nets are not learnable with equivalence queries alone, while they are learnable with the help of membership queries if the supplied examples are restricted to swaps. A similar property holds for tree CP-nets with arbitrary examples. In fact, membership queries allow us to provide attribute-efficient algorithms for which the query complexity is only logarithmic in the number of attributes. Such results highlight the utility of this model for eliciting CP-nets in large multi-attribute domains.|Frédéric Koriche,Bruno Zanuttini","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik","15144|IJCAI|1995|Model-Based Diagnosis using Causal Networks|This paper rests on several contributions. First, we introduce the notion of a consequence, which is a boolean expression that characterizes consistency-based diagnoses. Second, we introduce a basic algorithm for computing consequences when the system description is structured using a causal network. We show that if the causal network has no undirected cycles, then a consequence has a linear size and can be computed in linear time. Finally, we show that diagnoses characterized by a consequence and meeting some preference criterion can be extracted from the consequence in time linear in its size. A dual set of results is provided for abductive diagnosis.|Adnan Darwiche","15830|IJCAI|2003|A Learning Algorithm for Web Page Scoring Systems|Hyperlink analysis is a successful approach to define algorithms which compute the relevance of a document on the basis of the citation graph. In this paper we propose a technique to learn the parameters of the page ranking model using a set of pages labeled as relevant or not relevant by a supervisor. In particular we describe a learning algorithm applied to a scheme similar to PageRank. The ranking algorithm is based on a probabilistic Web surfer model and its parameters are optimized in order to increase the probability of the surfer to visit a page labeled as relevant and to reduce it for the pages labeled as not relevant. The experimental results show the effectiveness of the proposed technique in reorganizing the page ordering in the ranking list accordingly to the examples provided in the learning set.|Michelangelo Diligenti,Marco Gori,Marco Maggini","14535|IJCAI|1987|Learning Phonetic Features Using Connectionist Networks|A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectraltemporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. A simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated % of  other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items.|Raymond L. Watrous,Lokendra Shastri","15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller"],["15997|IJCAI|2003|A lookahead strategy for solving large planning problems|Relaxed plans are used in the heuristic search planner FF for computing a numerical heuristic and extracting helpful actions. We present a novel way for extracting information from the relaxed plan and for dealing with helpful actions, by considering the high quality of the relaxed plans. In numerous domains, the performance of heuristic search planning and the size of the problems that can be handled have been drastically improved.|Vincent Vidal","15334|IJCAI|1997|Automatic SAT-Compilation of Planning Problems|Recent work by Kautz et al., provides tantalizing evidence that large, classical planning problems may be efficiently solved by translating them into propositional satisfiability problems, using stochastic search techniques, and translating the resulting truth assignments back into plans for the original problems. We explore the space of such transformations, providing a simple framework that generates eight major encodings (generated by selecting one of four action representations and one of two frame axioms) and a number of subsidiary ones. We describe a fully-implemented compiler that can generate each of these encodings, and we test the compiler on a suite of STRIPS planning problems in order to determine which encodings have the best properties.|Michael D. Ernst,Todd D. Millstein,Daniel S. Weld","13881|IJCAI|1983|A Wrinkle on Satisficing Search Problems|The problem of optimally ordering the execution of independent disjuncts is explored. Only a single answer is sought, not necessarily the best one. By definition, this is called satisfying search. Since the disjuncts are independent, the total combined probability that a solution is found does not depend on the execution order. However, the ordering does affect the total expected execution time because execution ceases as soon as any solution is discovered. Therefore, the optimal ordering is the one that minimizes the total expected work. The new result is an algorithm to find this optimal ordering when the effects of executing a disjunct must be undone before another one can be tried. The algorithm is shown to have time complexity O(n log n), where n is the number of disjuncts. This is the same complexity as for the original problem where undo times are ignored.|Jeffrey A. Barnett,Don Cohen","16700|IJCAI|2007|Constraint Partitioning for Solving Planning Problems with Trajectory Constraints and Goal Preferences|The PDDL specifications include soft goals and trajectory constraints for distinguishing highquality plans among the many feasible plans in a solution space. To reduce the complexity of solving a large PDDL planning problem, constraint partitioning can be used to decompose its constraints into subproblems of much lower complexity. However, constraint locality due to soft goals and trajectory constraints cannot be effectively exploited by existing subgoal-partitioning techniques developed for solving PDDL. problems. In this paper, we present an improved partition-andresolve strategy for supporting the new features in PDDL. We evaluate techniques for resolving violated global constraints, optimizing goal preferences, and achieving subgoals in a multivalued representation. Empirical results on the th International Planning Competition (IPC) benchmarks show that our approach is effective and significantly outperforms other competing planners.|Chih-Wei Hsu,Benjamin W. Wah,Ruoyun Huang,Yixin Chen","16938|IJCAI|2009|Declarative Programming of Search Problems with Built-in Arithmetic|We address the problem of providing a logical formalization of arithmetic in declarative modelling languages for NP search problems. The challenge is to simultaneously allow quantification over an infinite domain such as the natural numbers, provide natural modelling facilities, and control expressive power of the language. To address the problem, we introduce an extension of the model expansion (MX) based framework to finite structures embedded in an infinite secondary structure, together with \"double-guarded\" logics for representing MX specifications for these structures. The logics also contain multi-set functions (aggregate operations). Our main result is that these logics capture the complexity class NP on \"small-cost\" arithmetical structures.|Eugenia Ternovska,David G. Mitchell","16226|IJCAI|2005|Applying Local Search to Disjunctive Temporal Problems|We present a method for applying local search to overconstrained instances of the Disjunctive Temporal Problem (DTP). Our objective is to generate high quality solutions (i.e., solutions that violate few constraints) in as little time as possible. The technique presented here differs markedly from previous work on DTPs, as it operates within the total assignment space of the underlying CSP rather than the partial assignment space of the related meta-CSP. We provide experimental results demonstrating that the use of local search leads to substantially improved performance over systematic methods.|Michael D. Moffitt,Martha E. Pollack","13323|IJCAI|1971|Solving Problems by Formula Manipulation in Logic and Linear Inequalities|Using formal logic, many problems from the general area of linear inequalities can be expressed in the elementary theory of addition on the real numbers (EAR). We describe a method for eliminating quantifiers in EAR which has been programmed and demonstrate its usefulness in solving some problems related to linear programming. In the area of mechanical mathematics this kind of approach has been neglected in favor of more generalized methods based on Herbrand expansion. However, in a restricted area, such as linear inequalities, the use of these specialized methods can increase efficiency by several orders of magnitude over an axiomatic Herbrand approach, and make practical problems accessible.|Louis Hodes","15377|IJCAI|1997|Combining Local Search and Look-Ahead for Scheduling and Constraint Satisfaction Problems|We propose a solution technique for scheduling and constraint satisfaction problems that combines backtracking-free constructive methods and local search techniques. Our technique incrementally constructs the solution, performing a local search on partial solutions each time the construction reaches a dead-end. Local search on the space of partial solutions is guided by a cost function based on three components the distance to feasibility of the partial solution, a look-ahead factor, and (for optimization problems) a lower bound of the objective function. In order to improve search effectiveness, we make use of an adaptive relaxation of constraints and an interleaving of different lookahead factors. The new technique has been successfully experimented on two real-life problems university course scheduling and sport tournament scheduling.|Andrea Schaerf","14569|IJCAI|1989|Solving Time-Dependent Planning Problems|A planning problem is time-dependent, if the time spent planning affects the utility of the system's performance. In Dean and Boddy, , we define a framework for constructing solutions to time-dependent planning problems, called expectation-driven iterative refinement. In this paper, we analyze and solve a moderately complex time-dependent planning problem involving path planning for a mobile robot, as a way of exploring a methodology for applying expectation-driven iterative refinement. The fact that we construct a solution to the proposed problem without appealing to luck or extraordinary inspiration provides evidence that expectation-driven iterative refinement is an appropriate framework for solving time-dependent planning problems.|Mark S. Boddy,Thomas Dean","15636|IJCAI|2001|Solving Non-Boolean Satisfiability Problems with Stochastic Local Search|Much excitement has been generated by the recent success of stochastic local search procedures at finding satisfying assignments to large formulas. Many of the problems on which these methods have been effective are non-Boolean in that they are most naturally formulated in terms of variables with domain sizes greater than two. To tackle such a problem with a Boolean procedure the problem is first reformulated as an equivalent Boolean problem. This paper introduces and studies the alternative of extending a Boolean stochastic local search procedure to operate directly on non-Boolean problems. It then compares the non-Boolean representation to three Boolean representations and presents experimental evidence that the non-Boolean method is often superior for problems with large domain sizes.|Alan M. Frisch,Timothy J. Peugniez"],["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","13554|IJCAI|1975|On The System Of Concepts Relations And Outline Of The Natural Language System|Systems that simulates thinking processes of a man through natural language are called natural language systems. We present here outline of a natural language system in connection with the system of concepts relations which plays important role in our system. The system of concepts relations is a set of words or short sentences connected through various basic view-points. Semantic analyses procedure using the system of concepts relations is shown by an example. We regard desire as basic cause of thinking in our system. Outline of the relation between desire system and thinking processes using natural language as a base Is given in later chapters.|S. Yoshida","17048|IJCAI|2009|Argumentation System with Changes of an Agents Knowledge Base|This paper discusses a process of argumentation. We propose an algorithm for dynamic treatment of argumentation in which all lines of argumentation are executed in succession, and the agent's knowledge base can change during argumentation. We show that there exists a case in which an agent dynamically loses argumentation that would be considered won by a static analysis. We also show that the algorithm terminates, and describe acceptable arguments that are obtained after the argumentation.|Kenichi Okuno,Kazuko Takahashi","13317|IJCAI|1971|Experiments with a Natural Language Problem-Solving System|Development work on a computer program called HAPPINESS which solves basic probability problems phrased in English is presented. Emphasis is placed on () the application of heuristics to the examination of input language structure for the purpose of determining those phases richest in semantic content () the piecewise construction of combinatorial formulas for problem solution The language analysis is accomplished in several discrete stages, involving simple sentence transformation, keyword and semantic scanning, and syntactic analysis based on a simplified context-free grammar. The descriptor list result of this analysis is used as the basis for a four-stage solution procedure. A description of the implementation, and a discussion of its limitations, extensions, and applications, is also given.|Jack P. Gelb","14404|IJCAI|1987|Understanding System Specifications Written in Natural Language|This paper describes research in understanding system specifications written in natural language. This research involves the implementation of a natural language interface, PHRANSPAN, for specifying the abstract behavior of digital systems in restricted English text. A neutral formal representation for the behavior is described using the USC Design Data Structure. A small set of concepts that characterize digital system behavior are presented using this representation. An intermediate representation loosely based on Conceptual Dependency is presented. Its use with a semantic-based parser to translate from English to the formal representation is illustrated by examples.|John J. Granacki Jr.,Alice C. Parker,Yigal Arens","13991|IJCAI|1983|Transportability and Generality in a Natural-Language Interface System|This paper describes the design of a transportable natural language (NL) interface to databases and the constraints that transportability places on each component of such a system. By a transportable NL system, we mean an NL processing system that is constructed so that a domain expert (rather than an AI or linguistics expert) can move the system to a new application domain. After discussing the general problems presented by transportability, this paper describes TEAM (an acronym for Transportable English database Access Medium), a demonstratable prototype of such a system. The discussion of TEAM shows how domain-independent and domain-dependent information can be separated in the different components of a NL interface system, and presents one method of obtaining domain-specific information from a domain expert.|Paul A. Martin,Douglas E. Appelt,Fernando C. N. Pereira","13906|IJCAI|1983|Understanding Natural Language Through Parallel Processing of Syntactic and Semantic Knowledge An Application to Data Base Query|This paper describes the main features of the PARNAX system for natural language access (in Italian) to an ADABAS data base. The core of the system is constituted by the analyzer that includes parallel processing of syntactic and semantic knowledge. It is argued that this feature (together with the new macro and micro-analysis technique which is only shortly mentioned in this paper) allowed the system to reach a good linguistic coverage, still ensuring an acceptable degree of efficiency. After the basic architecture and operation of PARNAX have been described, attention is focused on the parallel syntacticsemantic analyzer which is illustrated in detail. The advantages obtained through parallelism are also shortly discussed. Examples of PARNAX operation are presented. References to related works are mentioned, and directions for future research are outlined.|R. Comino,Roberto Gemello,Giovanni Guida,Claudio Rullent,L. Sisto,Marco Somalvico","13509|IJCAI|1975|An Adaptive Natural Language System That Listens Asks And Learns|Whan a user interacts with a natural language system, ha may wall uaa words and expressions which ware not anticipated by tha systea designers. This paper describes a systea which can play TIC-TAC-TOE, and discuss tha game while it Is in progress. If the system encounters new words, new expressions, or inadvertent ungrammatlcalltles, It attempts to understand what was meant, through contextual inference, and by asking intelligent clarifying questions of the user. The system than records the meaning of any new words or expressions, thus augmenting its linguistic knowledge in the course of user interaction.|Perry Lowell Miller","13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris","15059|IJCAI|1993|Expert System Validation through Knowledge Base Refinement|Knowledge base (KB) refinement is a suitable technique to support expert system (ES) validation. When used for validation, KB refinement should be guided not only by the number of errors to solve but also by the importance of those errors. Most serious errors should be solved first, even causing other errors of lower importance but assuring a neat validity gain. These are the bases for IMPROVER, a KB refinement tool designed to support ES validation. IMPROVER refines ES for medical diagnosis with this classification of error importance false negative  false positive  ordering mismatch. IMPROVER is being used to validate a real ES and some empirical results are given.|Pedro Meseguer"],["15112|IJCAI|1995|Reasoning about Noisy Sensors in the Situation Calculus|Agents interacting with an incompletely known dynamic world need to be able to reason about the effects of their actions, and to gain further information about that world using sensors of some sort. Unfortunately, sensor information is inherently noisy, and in general serves only to increase the agent's degree of confidence in various propositions. Building on a general logical theory of action formalized in the situation calculus developed by Reiter and others, we propose a simple axiomatization of the effect on an agent's state of belief of taking a reading from a noisy sensor. By exploiting Reiter's solution to the frame problem, we automatically obtain that these sensor actions leave the rest of the world unaffected, and further, that non-sensor actions change the state of belief of the agent in appropriate ways.|Fahiem Bacchus,Joseph Y. Halpern,Hector J. Levesque","15971|IJCAI|2003|Reasoning about the Interaction of Knowledge Time and Concurrent Actions in the Situation Calculus|A formal framework for specifying and developing agentsrobots must handle not only knowledge and sensing actions, but also time and concurrency. Researchers have extended the situation calculus to handle knowledge and sensing actions. Other researchers have addressed the issue of adding time and concurrent actions. Here both of these features are combined into a united logical theory of knowledge, sensing, time, and concurrency. The result preserves the solution to the frame problem of previous work, maintains the distinction between indexical and objective knowledge of time, and is capable of representing the various ways in which concurrency interacts with time and knowledge. Furthermore, a method based on regression is developed for solving the projection problem for theories specified in this version of the situation calculus.|Richard B. Scherl","15348|IJCAI|1997|Reasoning with Incomplete Initial Information and Nondeterminism in Situation Calculus|Situation Calculus is arguably the most widely studied and used formalism for reasoning about action and change. The main reason for its popularity is the ability to reason about different action sequences as explicit objects. In particular, planning can be formulated as an existence problem. This paper shows how these properties break down when incomplete information about the initial state and nondeterministic action effects are introduced, basically due to the fact that this incompleteness is not adequately manifested on the object level. A version of Situation Calculus is presented which adequately models the alternative ways the world can develop relative to a choice of actions.|Lars Karlsson","14681|IJCAI|1989|Visual Reasoning in Geometry Theorem Proving|We study the role of visual reasoning as a computationally feasible heuristic tool in geometry problem solving. We use an algebraic notation to represent geometric objects and to manipulate them. We show that this representation captures powerful heuristics for proving geometry theorems, and that it allows a systematic manipulation of geometric features in a manner similar to what may occur in human visual reasoning.|Michelle Y. Kim","13637|IJCAI|1977|Reasoning About Knowledge and Action|This paper discusses the problems of representing and reasoning with information about knowledge and action. The first section discusses the importance of having systems that understand the concept of knowledge, and how knowledge is related to action. Section  points out some of the special problems that are involved in reasoning about knowledge, and section S presents a logic of knowledge based on the idea of possible worlds. Section  integrates this with a logic of actions and gives an example of reasoning in the combined system. Section  makes some concluding comments.|Robert C. Moore","16348|IJCAI|2007|Decidable Reasoning in a Modified Situation Calculus|We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter's version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.|Yilan Gu,Mikhail Soutchanski","15337|IJCAI|1997|Reasoning about Concurrent Execution Prioritized Interrupts and Exogenous Actions in the Situation Calculus|As an alternative to planning, an approach to highlevel agent control based on concurrent program execution is considered. A formal definition in the situation calculus of such a programming language is presented and illustrated with a detailed example. The language includes facilities for prioritizing the concurrent execution, interrupting the execution when certain conditions become true, and dealing with exogenous actions. The language differs from other procedural formalisms for concurrency in that the initial state can be incompletely specified and the primitive actions can be user-defined by axioms in the situation calculus.|Giuseppe De Giacomo,Yves Lespérance,Hector J. Levesque","15232|IJCAI|1995|Reasoning about Action and Change Using Dijkstras Semantics for Programming Languages Preliminary Report|We apply Dijkstra's semantics for programming languages to formalization of reasoning about action and change. The basic idea is to view actions as formula transformers, i.e. functions from formulae into formulae. The major advantage of our proposal is that it is very simple and more effective than most of other approaches. Yet, it deals with a broad class of actions, including those with random and indirect effects. Also, both temporal prediction and postdiction reasoning tasks can be solved without restricting initial nor final states to completely specified.|Witold Lukaszewicz,Ewa Madalinska-Bugaj","14187|IJCAI|1985|An Equational Approach to Theorem Proving in First-Order Predicate Calculus|A new approach for proving theorems in first-order predicate calculus is developed based on term rewriting and polynomial simplification methods. A formula is translated into an equivalent set of formulae expressed in terms of 'true', 'false', 'exclusive-or', and 'and' by analyzing the semantics of its top-level operator. In this representation, formulae are polynomials over atomic formulae with 'and' as multiplication and 'exclusive-or' as addition, and they can be manipulated just like polynomials using familiar rules of multiplication and addition. Polynomials representing a formula are converted into rewrite rules which are used to simplify polynomials. New rules are generated by overlapping polynomials using a critical-pair completion procedure closely related to the Knuth- Bendix procedure. This process is repeated until a contradiction is reached or it is no longer possible to generate new rules. It is shown that resolution is subsumed by this method.|Deepak Kapur,Paliath Narendran","14141|IJCAI|1985|Belief Awareness and Limited Reasoning Preliminary Report|Several new logics for belief and knowledge are introduced and studied, all of which have the property that agents are not logically omniscient. In particular, in these logics, the set of beliefs of an agent does not necessarily contain all valid formulas. Thus, these logics are more suitable than traditional logics for modelling beliefs of humans (or machines) with limited reasoning capabilities. Our first logic is essentially an extension of Levesque's logic of implicit and explicit belief, where we extend to allow multiple agents and higher-level belief (i.e., beliefs about beliefs). Our second logic deals explicitly with \"awareness\", where, roughly speaking, it is necessary to be aware of a concept before one can have beliefs about it. Our third logic gives a model of \"local reasoning\", where an agent is viewed as a \"society of minds\", each with its own cluster of beliefs, which may contradict each other.|Ronald Fagin,Joseph Y. Halpern"],["14370|IJCAI|1987|A Formal Approach to Learning From Examples|This paper presents a formal, foundational approach to learning from examples in machine learning. It is assumed that a learning system is presented with a stream of facts describing a domain of application. The task of the system is to form and modify hypotheses characterising the relations in the domain, based on this information. Presumably the set of hypotheses that may be so formed will require continual revision as further information is received. The emphasis in this paper is to characterise those hypotheses that may potentially be formed, rather than to specify the subset of the hypotheses that, for whatever reason, should be held. To this end. formal systems are derived from which the set of potential hypotheses that may be formed is precisely specified. A procedure is also derived for restoring the consistency of a set of hypotheses after conflicting evidence is encountered. In addition, this work is extended to where a learning system may be \"told\" arbitrary sentences concerning a domain The approach is intended to provide a basic framework lor the development of systems that learn from examples, as well as a neutral point from which such systems may be viewed and compared.|James P. Delgrande","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","14240|IJCAI|1985|Learning Procedures from Examples and by Doing|This paper describes a program that learns procedures by examining worked-out examples in a textbook and bv working problems two kinds of production (If-then) rules are created working forward rules that produce an action when a proceduie is executed and difference rules that suggest operators from observed transformations. Dining example learning, the program examines two states in an example, fig tires out the operator that produced the second state and creates a production with some part ot the first line in the condition with the operator-tor in the action. During learning by working problems, the program generates its own example trace by problem solving and uses the same example learning techniques.|David M. Neves","15943|IJCAI|2003|Imitation Learning of Team-play in Multiagent System based on Hidden Markov Modeling|This paper addresses agents' intentions as building blocks of imitation learning that abstract local situations of the agent, and proposes a hierarchical hidden Markov model (HMM) to represent cooperative behaviors of teamworks. The key of the proposed model is introduction of gate probabilities that restrict transition among agents' intentions according to others' intentions. Using these probabilities, the framework can control transitions flexibly among basic behaviors in a cooperative behavior.|Itsuki Noda","14272|IJCAI|1985|Learning Concept Descriptions from Examples with Errors|This paper presents a scheme for learning complex descriptions, such as logic formulas, from examples with errors. The basis for learning is provided by a selection criterion which minimizes a combined measure of discrepancy of a description with training data, and complexity of a description. Learning rules for two types of descriptors are derived one for finding descriptors with good average discrimination over a set of concepts, second for selecting the best descriptor for a specific concept. Once these descriptors are found, an unknown instance can be identified by a search using the descriptors of the first type for a fast screening of candidate concepts, and the second for the final selection of the closest concept.|Jakub Segen","13883|IJCAI|1983|Learning Word Meanings From Examples|This paper describes work in progress on a computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple English example stories. The central idea is an extension of Winston's (Winston ) program that learned the structural descriptions of blocks world scenes. In the new research, English verbs take the place of blocks world objects like ARCH and TOWER, with frame-based descriptions of causal relationships serving as the structural descriptions. Syntactic constraints derived from the parsing of story plots are used to drive an analogical matching procedure. Analogical matching gives a way to compare descriptions of known words to unknown words. The \"meaning\" of a new verb is learned by matching pan of the causal network description of a story precis containing the unknown word to a set of such descriptions derived from similar stories that contain only known words. The best match forges an assignment between objects and relations such that the unknown verb is matched to a known verb, with the assignment being guided by syntactic constraints. The causal network surrounding the unknown item is then used as a scaffolding to construct a network representing the use of the novel word in a particular context. Words (and their associated stories) that are \"best matches\" are grouped together into a similarity network, according to the match score.|Robert C. Berwick","16376|IJCAI|2007|Semi-Supervised Learning for Multi-Component Data Classification|This paper presents a method for designing a semisupervised classifier for multi-component data such as web pages consisting of text and link information. The proposed method is based on a hybrid of generative and discriminative approaches to take advantage of both approaches. With our hybrid approach, for each component, we consider an individual generative model trained on labeled samples and a model introduced to reduce the effect of the bias that results when there are few labeled samples. Then, we construct a hybrid classifier by combining all the models based on the maximum entropy principle. In our experimental results using three test collections such as web pages and technical papers, we confirmed that our hybrid approach was effective in improving the generalization performance of multi-component data classification.|Akinori Fujino,Naonori Ueda,Kazumi Saito","15810|IJCAI|2003|Learning Minesweeper with Multirelational Learning|Minesweeper is a one-person game which looks deceptively easy to play, but where average human performance is far from optimal. Playing the game requires logical, arithmetic and probabilistic reasoning based on spatial relationships on the board. Simply checking a board state for consistency is an NP-complete problem. Given the difficulty of hand-crafting strategies to play this and other games, AI researchers have always been interested in automatically learning such strategies from experience. In this paper, we show that when integrating certain techniques into a general purpose learning system (Mio), the resulting system is capable of inducing a Minesweeper playing strategy that beats the winning rate of average human players. In addition, we discuss the necessary background knowledge, present experimental results demonstrating the gain obtained with our techniques and show the strategy learned for the game.|Lourdes Peña Castillo,Stefan Wrobel","14248|IJCAI|1985|Learning Hidden Causes from Empirical Data|Models of complex phenomena often consist of hypothetical entities called \"hidden causes\", which cannot be observed directly and yet play a major role in understanding, communicating, and predicting the dynamics of those phenomena. This paper examines the cognitive and computational roles of these constructs, and addresses the question of whether they can be discovered from empirical observations. Causal models are treated as trees of binary random variables where the leaves are accessible to direct observation, and the internal nodes-representing hidden causes-account for inter-leaf dependencies. In probabilistic terms, every two leaves are conditionally independent given the value of some internal node between them. We show that if the mechanism which drives the visible variables is indeed tree-structured, then it is possible to uncover the topology of the tree uniquely by observing pair-wise dependencies among the leaves. The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to nlogn, where n is the number of leaves.|Judea Pearl","15272|IJCAI|1995|Forgetting and Compacting data in Concept Learning|Incremental concept learning algorithms using backtracking have to store previous data. These data can be ordered by the \"is more specific than\" relation. Using this order only the most informative data have to be stored, and the less informative data can be discarded. Moreover, under certain conditions some data can be replaced by automatically generated, more informative data. We investigate some conditions for data to be discarded, independently of the chosen concept learning algorithm or concept representation language. Then an algorithm for discarding data is presented in the framework of Iterative Versionspaces, which is a depth-first algorithm computing versionspaces as introduced by Mitchell. We update the datastructures used in the Iterative Versionspaces algorithm, while preserving its most important properties.|Gunther Sablon,Luc De Raedt"],["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","15933|IJCAI|2003|Non-Invasive Brain-Actuated Control of a Mobile Robot|Recent experiments have indicated the possibility to use the brain electrical activity to directly control the movement of robotics or prosthetic devices. In this paper we report results with a portable non-invasive brain-computer interface that makes possible the continuous control of a mobile robot in a house-like environment. The interface uses  surface electrodes to measure electroencephalogram (EEG) signals from which a statistical classifier recognizes  different mental states. Until now, brain-actuated control of robots has relied on invasive approaches-requiring surgical implantation of electrodes-since EEG-based systems have been considered too slow for controlling rapid and complex sequences of movements. Here we show that, after a few days of training, two human subjects successfully moved a robot between several rooms by mental control only. Furthermore, mental control was only marginally worse than manual control on the same task.|José del R. Millán,Frédéric Renkens,Josep Mouriño,Wulfram Gerstner","16581|IJCAI|2007|Using a Mobile Robot for Cognitive Mapping|When animals (including humans) first explore a new environment, what they remember is fragmentary knowledge about the places visited. Yet, they have to use such fragmentary knowledge to find their way home. Humans naturally use more powerful heuristics while lower animals have shown to develop a variety of methods that tend to utilize two key pieces of information, namely distance and orientation information. Their methods differ depending on how they sense their environment. Could a mobile robot be used to investigate the nature of such a process, commonly referred to in the psychological literature as cognitive mapping What might be computed in the initial explorations and how is the resulting \"cognitive map\" be used to return home In this paper, we presented a novel approach using a mobile robot to do cognitive mapping. Our robot computes a \"cognitive map\" and uses distance and orientation information to find its way home. The process developed provides interesting insights into the nature of cognitive mapping and encourages us to use a mobile robot to do cognitive mapping in the future, as opposed to its popular use in robot mapping.|Chee K. Wong,Jochen Schmidt,Wai K. Yeap","15324|IJCAI|1997|Active Mobile Robot Localization|Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effectors during localization. This paper proposes an active localization approach. The approach provides rational criteria for () setting the robot's motion direction (exploration), and () determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment.|Wolfram Burgard,Dieter Fox,Sebastian Thrun","16402|IJCAI|2007|Color Learning on a Mobile Robot Towards Full Autonomy under Changing Illumination|A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a \"curriculum,\" an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map.|Mohan Sridharan,Peter Stone","13393|IJCAI|1973|Robot Decisions Based on Maximizing Utility|The decision-making component of a robot that operates in a poorly known environment is considered. The usual problem-solving approach to handling a task is not suitable when each decision may turn out in several ways, and many decisions are needed to complete the task. An alternative approach, which is based on maximizing the estimated utility resulting from each decision, is illustrated. The example describes the plans, utility functions and decision procedures of a simulated insectlike robot called PERCY. In spite of its limited ability to perceive and store information about the environment, PERCY can achieve satisfactory performance on its task.|Walter Jacobs,Maxine Kiefer","14951|IJCAI|1991|Mobile Robot Navigation by an Active Control of the Vision System|In this paper, we argue that a mobile robot's environment can be determined by computing local maps surrounding feature points, called fixation points. These fixation points are obtained by searching the scene for points which present some interesting cue for robot navigation. This -D computation is based on a monocular active vision system composed of a camera, mounted on a rotating table accurately controlled by a computer, which gazes the fixation point as the robot moves. The system then computes the local map and updates it with each new observation in order to increase its accuracy and robustness. Real experimentation in a complex indoor scene illustrates that the -D scene coordinates can be obtained with a good accuracy by integrating several observations.|Patrick Stelmaszyk,Hiroshi Ishiguro,Saburo Tsuji","14529|IJCAI|1987|Visual Path Planning by a Mobile Robot|Stereo-vision-based mobile robots observe their environments and acquire - data with considerable errors in range. Rather than to use the conventional - maps, described with these Inaccurate data in the absolute coordinate system, a flexible relational model of the global world can be built with local maps suited for each function of robots such as path planning or manipulation. For the path planning, a perspective representation of the local world, the image on which the scene Interpretation is mapped, is proposed. Using world constraints and sensory Information of camera orientation, a stereo-image analyzer determines vertical projections of edge points on the floor in the image. A method for planning of promising paths to the specified goal using this representation is presented.|Saburo Tsuji,Jiang Yu Zheng","14556|IJCAI|1989|Building a World Model for a Mobile Robot Using Dynamic Semantic Constraints|We are developing a new paradigm for a world model construction system which interprets a scene and builds a world model for a mobile robot using dynamic semantic constraints. The system represents a world model in hierarchical form sensor-based maps to a global map with both numerical and symbolic descriptions. At the beginning of interpretation, sensory data (video and range images) are analyzed in bottom-up fashion. A range image is transformed into a height map, and analyzed for the purpose of generating a geometrical property list for both obstacle and traversable regions that is used as the initial input to the interpretation process. At each step of the scene interpretation process, the most reliable feature of an object is selected in the region property list to propagate semantic constraints on other objects close to it. Geometrical modeling for individual objects in the scene is performed, and parameters of each model are dynamically refined by the scene interpretation process. These model parameters and their interrelationships make spatial reasoning robust. Preliminary results with video and range images are shown.|Minoru Asada,Yoshiaki Shirai","13710|IJCAI|1981|Learning of Sensory-Motor Schemas in a Mobile Robot|A learning system is described which was used to control a simple robot vehicle and to autonomously learn behaviour patterns. The system is loosely based on Becker's model of Intermediate Cognition.|Alan H. Bond,David H. Mott"],["14257|IJCAI|1985|Dynamic Student Modelling in an Intelligent Tutor for LISP Programming|We describe an intelligent tutor for LISP programming. This tutor achieves a set of pedagogical objectives derived from Anderson's () learning theory provide instruction in the context of problem-solving, have the student generate as much of each solution as possible, provide immediate feedback on errors, and represent the goal structure of the problem-solving. The tutorial interface facilitates communication and prevents distracting low-level errors Field tests of the tutor in college classes demonstrate that it is more effective than conventional classroom instruction.|Brian J. Reiser,John R. Anderson,Robert G. Farrell","15869|IJCAI|2003|Comparing Best-First Search and Dynamic Programming for Optimal Multiple Sequence Alignment|Sequence alignment is an important problem in computational biology. We compare two different approaches to the problem of optimally aligning two or more character strings bounded dynamic programming (BDP), and divide-and-conquer frontier search (DCFS). The approaches are compared in terms of time and space requirements in  through  dimensions with sequences of varying similarity and length. While BDP performs better in two and three dimensions, it consumes more time and memory than DCFS for higher-dimensional problems.|Heath Hohwald,Ignacio Thayer,Richard E. Korf","13273|IJCAI|1969|An Introduction to the Heuristic Programming System|The Heuristic Programming System is a language, designed but not yet implemented, for research in artificial intelligence It will provide facilities for creating, modifying, and destroying complex hierarchically structured objects and descriptions of objects A search operation will be provided to retrieve objects which are specified by arbitrarily complex descriptions Another search operation will construct the desired objects according to the specifications of previously created doscriptlve objects, this will be rather like a syntax-directed compiler for a continuously changing ambiguous language A program for playing Go-Moku has been written using the System, the program features a highly efficient data structure, evaluation of feasible moves by alpha-beta minimax, and improvement of the move generation mechanism whenever the opponent makes a valuable but unexpected move.|David K. Jefferson","14245|IJCAI|1985|Stereo by Two-Level Dynamic Programming|This paper presents a stereo algorithm using dynamic programming technique. The stereo matching problem, that is, obtaining a correspondence between right and left images, can be cast as a search problem. When a pair of stereo images is rectified, pairs of corresponding points can be searched for within the same scanlines. We call this search intra-scanline search. This intra-scanline search can be treated as the problem of finding a matching path on a two dimensional (D) search plane whose axes are the right and left scanlines. Vertically connected edges in the images provide consistency constraints across the D search planes. Inter-scanline search in a three-dimensional (D) search space, which is a stack of the D search planes, is needed to utilize this constraint. Our stereo matching algorithm uses edge-delimited intervals as elements to be matched, and employs the above mentioned two searches one is inter-scanline search for possible correspondences of connected edges in right and left images and the other is intra-scanline search for correspondences of edge-delimited intervals on each scanline pair. Dynamic programming is used for both searches which proceed simultaneously in two levels the former supplies the consistency constraints to the latter while the latter supplies the matching score to the former. An interval-based similarity metric is used to compute the score.|Yuichi Ohta,Takeo Kanade","15874|IJCAI|2003|Use of Off-line Dynamic Programming for Efficient Image Interpretation|An interpretation system finds the likely mappings from portions of an image to real-world objects. An interpretation policy specifies when to apply which imaging operator, to which portion of the image, during every stage of interpretation. Earlier results compared a number of policies, and demonstrated that policies that select operators which maximize the information gain per cost, worked most effectively. However, those policies are myopic -- they rank the operators based only on their immediate rewards. This can lead to inferior overall results it may be better to use a relatively expensive operator first, if that operator provides information that will significantly reduce the cost of the subsequent operators. This suggests using some lookahead process to compute the quality for operators non-myopically. Unfortunately, this is prohibitively expensive for most domains, especially for domains that have a large number of complex states. We therefore use ideas from reinforcement learning to compute the utility of each operator sequence. In particular, our system first uses dynamic programming, over abstract simplifications of interpretation states, to precompute the utility of each relevant sequence. It does this off-line, over a training sample of images. At run time, our interpretation system uses these estimates to decide when to use which imaging operator. Our empirical results, in the challenging real-world domain of face recognition, demonstrate that this approach works more effectively than myopic approaches.|Ramana Isukapalli,Russell Greiner","16930|IJCAI|2009|Bayesian Real-Time Dynamic Programming|Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when the initial state is restricted, by focusing dynamic programming on the envelope of states reachable from an initial state set. RTDP often provides performance guarantees without visiting the entire state space. Building on RTDP, recent work has sought to improve its efficiency through various optimizations, including maintaining upper and lower bounds to both govern trial termination and prioritize state exploration. In this work, we take a Bayesian perspective on these upper and lower bounds and use a value of perfect information (VPI) analysis to govern trial termination and exploration in a novel algorithm we call VPI-RTDP. VPI-RTDP leads to an improvement over state-of-the-art RTDP methods, empirically yielding up to a three-fold reduction in the amount of time and number of visited states required to achieve comparable policy performance.|Scott Sanner,Robby Goetschalckx,Kurt Driessens,Guy Shani","15602|IJCAI|2001|Symbolic Dynamic Programming for First-Order MDPs|We present a dynamic programming approach for the solution of first-order Markov decisions processes. This technique uses an MDP whose dynamics is represented in a variant of the situation calculus allowing for stochastic actions. It produces a logical description of the optimal value function and policy by constructing a set of first-order formulae that minimally partition state space according to distinctions made by the value function and policy. This is achieved through the use of an operation known as decision-theoretic regression. In effect, our algorithm performs value iteration without explicit enumeration of either the state or action spaces of the MDP. This allows problems involving relational fluents and quantification to be solved without requiring explicit state space enumeration or conversion to propositional form.|Craig Boutilier,Raymond Reiter,Bob Price","13505|IJCAI|1975|Form Dynamic Programming To Search Algorithms With Functional Costs|In this paper we approach, using artificial intelligence methods, the problem of finding a minimal-cost path in a functionally weighted graph, i.e a graph with monotone cost functions associated with the arcs This problem is important since solving any system of functional equations in a general dynamic programming formulation can be shown equivalent to it. A general heuristic search algorithm with estimate is given, which is a nontrivial extension of algorithm A. by Hart, Nilsson and Raphael. Putting some constraints on cost functions and on the estimate, this algorithm can be simplified until the classical version, with additive cost functions, is reached.|Alberto Martelli,Ugo Montanari","16384|IJCAI|2007|Efficiently Exploiting Symmetries in Real Time Dynamic Programming|Current approaches to solving Markov Decision Processes (MDPs) are sensitive to the size of the MDP. When applied to real world problems though, MDPs exhibit considerable implicit redundancy, especially in the form of symmetries. Existing model minimization methods do not exploit this redundancy due to symmetries well. In this work, given such symmetries, we present a time-efficient algorithm to construct a functionally equivalent reduced model of the MDP. Further, we present a Real Time Dynamic Programming (RTDP) algorithm which obviates an explicit construction of the reduced model by integrating the given symmetries into it. The RTDP algorithm solves the reduced model, while working with parameters of the original model and the given symmetries. As RTDP uses its experience to determine which states to backup, it focuses on parts of the reduced state set that are most relevant. This results in significantly faster learning and a reduced overall execution time. The algorithms proposed are particularly effective in the case of structured automorphisms even when the reduced model does not have fewer features. We demonstrate the results empirically on several domains.|Shravan Matthur Narayanamurthy,Balaraman Ravindran","16552|IJCAI|2007|Memory-Bounded Dynamic Programming for DEC-POMDPs|Decentralized decision making under uncertainty has been shown to be intractable when each agent has different partial information about the domain. Thus, improving the applicability and scalability of planning algorithms is an important challenge. We present the first memory-bounded dynamic programming algorithm for finite-horizon decentralized POMDPs. A set of heuristics is used to identify relevant points of the infinitely large belief space. Using these belief points, the algorithm successively selects the best joint policies for each horizon. The algorithm is extremely efficient, having linear time and space complexity with respect to the horizon length. Experimental results show that it can handle horizons that are multiple orders of magnitude larger than what was previously possible, while achieving the same or better solution quality. These results significantly increase the applicability of decentralized decision-making techniques.|Sven Seuken,Shlomo Zilberstein"],["16682|IJCAI|2007|Conjunctive Query Answering for the Description Logic SHIQ|Conjunctive queries play an important role as an expressive query language for Description Logics (DLs). Although modern DLs usually provide for transitive roles, it was an open problem whether conjunctive query answering over DL knowledge bases is decidable if transitive roles are admitted in the query. In this paper, we consider conjunctive queries over knowledge bases formulated in the popular DL SHIQ and allow transitive roles in both the query and the knowledge base. We show that query answering is decidable and establish the following complexity bounds regarding combined complexity, we devise a deterministic algorithm for query answering that needs time single exponential in the size of the KB and double exponential in the size of the query. Regarding data complexity, we prove co-NP-completeness.|Birte Glimm,Ian Horrocks,Carsten Lutz,Ulrike Sattler","14634|IJCAI|1989|Simulation of Hybrid Circuits in Constraint Logic Programming|This paper presents LOGISIM, a CAD tool to simulate the temporal behaviour of hybrid circuits containing electro-mechanical, electro-hydraulic, hydro-mechanic, and digital control devices. LOGISIM combines the advantages of both qualitative and quantitative reasoning by producing a high-level description (discrete states) of the circuit behaviour while reasoning at the quantitative level (physical values). In addition, device models in LOGISIM follow a particular description methodology proposed to avoid introducing an artificial computational complexity in the simulation. LOGISIM is fully implemented in the constraint logic programming language CHIP. The constraint-solving techniques of CHIP used in LOGISIM, i.e. an incremental decision procedure for linear constraints over rational numbers, consistency techniques on domain-variables and conditional propagation, are all necessary to solve the problem efficiently. LOGISIM has been applied successfully to real-life industrial circuits from aerospace industry in the ELSA project and clearly demonstrates the potential of this kind of tool to support the design process for these circuits.|Thomas Graf,Pascal Van Hentenryck,Claudine Pradelles,Laurent Zimmer","15973|IJCAI|2003|Non-Standard Reasoning Services for the Debugging of Description Logic Terminologies|Current Description Logic reasoning systems provide only limited support for debugging logically erroneous knowledge bases. In this paper we propose new non-standard reasoning services which we designed and implemented to pinpoint logical contradictions when developing the medical terminology DICE. We provide complete algorithms for unfoldable ACC-TBoxes based on minimisation of axioms using Boolean methods for minimal unsatisfiability-presening sub-TBoxes, and an incomplete bottom-up method for generalised incoherence-preserving terminologies.|Stefan Schlobach,Ronald Cornet","16492|IJCAI|2007|Completing Description Logic Knowledge Bases Using Formal Concept Analysis|We propose an approach for extending both the terminological and the assertional part of a Description Logic knowledge base by using information provided by the knowledge base and by a domain expert. The use of techniques from Formal Concept Analysis ensures that, on the one hand, the interaction with the expert is kept to a minimum, and, on the other hand, we can show that the extended knowledge base is complete in a certain, well-defined sense.|Franz Baader,Bernhard Ganter,Baris Sertkaya,Ulrike Sattler","16787|IJCAI|2007|A Faithful Integration of Description Logics with Logic Programming|Integrating description logics (DL) and logic programming (LP) would produce a very powerful and useful formalism. However, DLs and LP are based on quite different principles, so achieving a seamless integration is not trivial. In this paper, we introduce hybrid MKNF knowledge bases that faithfully integrate DLs with LP using the logic of Minimal Knowledge and Negation as Failure (MKNF) Lifschitz, . We also give reasoning algorithms and tight data complexity bounds for several interesting fragments of our logic.|Boris Motik,Riccardo Rosati","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15777|IJCAI|2003|Terminological Cycles in a Description Logic with Existential Restrictions|Cyclic definitions in description logics have until now been investigated only for description logics allowing for value restrictions. Even for the most basic language FL, which allows for conjunction and value restrictions only, deciding subsumption in the presence of terminological cycles is a PSPACE-complete problem. This paper investigates subsumption in the presence of terminological cycles for the language EL, Which allows for conjunction, existential restrictions, and the topconcept. In contrast to the results for FL., subsumption in EL remains polynomial, independent of whether we use least fixpoint semantics, greatest fixpoint semantics, or descriptive semantics.|Franz Baader","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman"],["13999|IJCAI|1983|Integrating Multiple Knowledge Representations and Learning Capabilities in an Expert System The ADVISE System|The ADVISE system is an integrated set of tools for the development of and experimentation with expert systems in various specific application domains. It functions as a multi-purpose inference system that employs three knowledge representations a rule-base, a conceptual network and a relational data base. In addition, it includes learning capabilities by incorporating the inductive learning programs GEM (for learning from examples) and CLUSTER (for constructing classifications). Three expert systems have been developed using ADVISE PLANTds (version )--for diagnosing soybean diseases, PLANTcd--for predicting cutworm damage to corn, and BABY--a consultant for the neo-natal intensive care unit.|Ryszard S. Michalski,Arthur B. Baskin","14488|IJCAI|1987|Repair Strategies in a Diagnostic Expert System|Successful machine diagnosis consists not only of sound diagnostic reasoning, but also the selection of appropriate repairs, sequencing the repairs correctly, interactively validating the success of each repair, and performing follow-on diagnosis in case of repair failure. We discuss some of the issues involved in formulating a repair strategy for an expert system, review some aspects of expert human behavior with respect to repair strategy in a complex domain, and finally describe an implementation of a repair strategy in the TEST diagnostic shell.|Jeff Pepper,Gary S. Kahn","14408|IJCAI|1987|Constraints in a Hybrid Knowledge Representation System|In our research group, the hybrid knowledge representation system Babylon has been developed providing formalisms for rules, prolog and frames. Beyond it, we implemented Consat, a system for constraint satisfaction. Since applications of Babylon for process diagnosis, planning etc. required constraints. we integrated Consat into the Babylon environment. The paper describes the integration of Consat into Babylon, regarding two aspects. First, constraints should be available as another Babylon formalism by using the functional interface of Consat. On the other hand, it is important to have constraints implicitly controlling other Babylon formalisms, for instance, in order to keep the system's database consistent. While with respect to the first point, the paper describes work already finished, the second form of integration is work in progress.|Hans W. Guesgen,Ulrich Junker,Angi Voß","13754|IJCAI|1981|How Expert Should an Expert System Be|A computer system which aids computer engineers in fault diagnosis is described. The system, called CRIB (Computer Retrieval Incidence Bank) is shown to fit into the class of pattern-directed inference systems. Emphasis is placed on the \"before\" and \"after\" phases of system generation and it is shown why, to be called an expert system, these phases are important. The forms of knowledge used in CRIB are shown to be adequate for diagnosis and yet possess little of the structural or functional knowledge of more advanced expert systems. Summaries are given of the three phases of implementation elicitation, implementation of knowledge structures, validation and improvement. The idea of an expert system as a \"model of competence\" is mentioned and the transferrance of the system architecture to software diagnosis, using the same model, is described. There are short discussions of system performance and the nature of expert systems.|Roger T. Hartley","13822|IJCAI|1981|Application Design Issues in Expert System Architecture|We describe an expert system that has been applied to the task of application design. Users supply the system with problem specifications, such as the required output data, and the system produces a graphic representation of the completed application in the form of a flow diagram. The application design task has forced us to consider two important issues in expert system architecture constraint processing and the explicit representation of control flow. The resulting knowledge representation and control logic are discussed.|Harry C. Reinstein,Janice S. Aikins","14303|IJCAI|1985|The Restricted Language Architecture of a Hybrid Representation System|Hybrid architectures have been used in several recent knowledge representation systems. This paper explores some distinctions between various hybrid representation architectures, focusing in particular on systems built around restricted representation languages. This restricted language architecture is illustrated by describing KL-TWO, a hybrid reasoner based on the restricted representation facility RUP. The bulk of this paper discusses KL-TWO, its subcomponents, and the techniques used to interface them.|Marc B. Vilain","14182|IJCAI|1985|Model Expert System MES|The comprehensive oil log interpretation is a procedure of numerical calculation and logical (or plausible) inference. In other words, the results of preliminary inference will provide a basis for the choice of parameters of calculation formula, and then the results of numerical calculation will be used as evidences for further inference, and so on and so forth. In view of this situation, we have designed a model expert system (tool system) MES and established a comprehensive oil log interpretation expert system WELIES based on the system MES. In MES, we introduced the concept of \"TASK\" to make numerical calculation intergrated with logical (or plausible) inference during the task solution. The execution of MES is a procedure of task solution. Ways of solving a task are defined by Task Transformation Rules. The Knowledge Representation is divided into two parts ) Task Transformation Rules, including Task Decomposition Rules and Task Derivation Rules ) Inference Rules, including Production Rules and Procedure Rules. We adopt DempsterShafer's Evidence Theory  to express uncertainty.|Guan Jiwen,Xu Ying,Chang Minche,Zhao Jizhi","14135|IJCAI|1985|Knowledge Representation in an Expert Storm Forecasting System|METEOR is a rule- and frame-based system for short-term (- hour) severe convective storm forecasting. This task requires a framework that supports inferences about the temporal and spatial features of meteorological changes. Initial predictions are based on interpretations of contour maps generated by statistical predictors of storm severity, Ib confirm these predictions, METEOR considers additional quantitative measurements, ongoing meteorological conditions and events, and how the expert forecaster interprets these extra factors. Meteorological events are derived from interpreting human observations of weather conditions in the forecast area. To accommodate the large amounts of different types of knowledge characterizing this problem, a number of extensions to the rule and frame representations were developed. These extensions include a view scheme to direct property inheritance through intermingled hierarchies and the automatic generation of production system rules from frame descriptions on an as-needed basis for event recognition.|Renee Elio,Johannes de Haan","15183|IJCAI|1995|A Hybrid Fuzzy-Neural Expert System for Diagnosis|Fuzzy Logic, a neural network and an expert system are combined to build a hybrid diagnosis system. With this system we introduce a new approach to the acquisition of knowledge bases. Our system consists of a fuzzy expert system with a dual source knowledge base. Two sets of rules are acquired, inductively from given examples and deductively formulated by a physician. A fuzzy neural network serves to learn from sample data and allows to extract fuzzy rules for the knowledge base. The diagnosis of electroencephalograms by interpretation of graphoelements serves as visualization for our approach. Preliminary results demonstrate the promising possibilities offered by our method.|Christoph S. Herrmann","14121|IJCAI|1985|The Architecture of the FAIM- Symbolic Multiprocessing System|The FAIM -  is an ultra-concurrent symbolic multiprocessor which attempts to significantly improve the performance of AI systems. The system includes a language in which concurrent AI application programs can be written, a machine which provides direct hardware support for the language, and a resource allocation mechanism which maps programs onto the machine in order to exploit the program's concurrency in an efficient manner at run-time. The paper provides a brief synopsis of the nature of the language and resource allocation mechanism, but is primarily concerned with the description of the physical architecture of the machine. The architecture is consistent with high performance VLSI implementation and packaging technology, and is easily extended to include arbitrary numbers of processors.|Alan L. Davis,Shane V. Robison"],["15503|IJCAI|1999|Learning Rules for Large Vocabulary Word Sense Disambiguation|Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C. is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.|Georgios Paliouras,Vangelis Karkaletsis,Constantine D. Spyropoulos","14474|IJCAI|1987|Use of Procedural Knowledge for Automatic Speech Recognition|A paradigm for automatic speech recognition using networks of actions performing variable depth analysis is presented. The paradigm produces descriptions of speech properties that are related to speech units through Markov models representing system performance. Preliminary results in the recognition of isolated letters and digits are presented.|Renato de Mori,Ettore Merlo,Mathew J. Palakal,Jean Rouat","15850|IJCAI|2003|Improving Word Sense Disambiguation in Lexical Chaining|Previous algorithms to compute lexical chains suffer either from a lack of accuracy in word sense disambiguation (WSD) or from computational inefficiency. In this paper, we present a new linear-time algorithm for lexical chaining that adopts the assumption of one sense per discourse. Our results show an improvement over previous algorithms when evaluated on a WSD task.|Michel Galley,Kathleen McKeown","13413|IJCAI|1973|The Hearsay Speech Understanding System An Example of the Recognition Process|This paper describes the structure and operation of the Hearsay speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of Knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its Operation in a particular task situation Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.|D. R. Reddy,Lee D. Erman,R. D. Fenneli,Richard B. Neely","14004|IJCAI|1983|A System for Improving the Recognition of Fluently Spoken German Speech|A research project for improving the recognition of fluently spoken German speech is presented. The work is in progress at present. It should be investigated, how far aspects of semantics and inferences could improve the automatic speech recognition. The work is part of a speech recognition system that receives speech signals, converts them into forms suitable for further actions and finally puts out the spoken text in characters. The system itself operates at three stages. Within the first one the signal analysis is performed using a well-known method. This analysis segments the signal into certain subword units and, for each segment, produces a set of weighted candidates. At the second stage these candidates are used to generate weighted word hypotheses with the aid of an extensive lexicon. The hypotheses have to be verified or falsified within the following processing steps at the third stage. Thereby the algorithm uses a best - first strategy (hypotheses with highest weight first). Besides syntacticgrammatical aspects, semantic analysis and inferences mentioned above are the methods, that should lead to a certain text-comprehension.|Joachim Mudler","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","14016|IJCAI|1983|The FOPHO Speech Recognition Project|The FOPHO (Foreign Phonetician) speech recognition project concerns the development of a system to produce a reasonably high quality phonetic transcription output from continuous speech input. The system is developed to perform in a way which approximates the actions of a phonetician trying to transcribe a foreign tongue, (in the case of FOPHO, Australian English). Because of this central philosophy, FOPHO is a very interactive system and has facilities for automatic learning and analysis of its own performance. Good quality recognition is achieved through algorithms which are very context-dependent and which are sensitive to a variety of possible productions of similar sounds even though the system itself is speaker independent.|Mary O'Kane","15226|IJCAI|1995|A WordNet-based Algorithm for Word Sense Disambiguation|We present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in WordNet and on the results of surface-syntactic analysis The algorithm is part of a system that analyzes texts in order to acquire knowledge in the presence of as little pre-coded semantic knowledge as possible On the other hand, we want to make the besl use of public-domain information sources such as WordNet Rather than depend on large amounts of hand-crafted knowledge or statistical data from large corpora, we use syntactic information and information in WordNet and minimize the need for other knowledge sources in the word sense disambiguation process We propose to guide disambiguation by semantic similarity between words and heuristic rules based on this similarity The algorithm has been applied to the Canadian Income Tax Guide Test results indicate that even on a relatively small text the proposed method produces correct noun meaning more than % of the time.|Xiaobin Li,Stan Szpakowicz,Stan Matwin","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["13911|IJCAI|1983|Some Issues in Generation From a Semantic Representation|This paper investigates certain problems in the production of text from a languaqe-free representation and proposes a model of generation to treat these problems. We deal with generation of connected text. We show that the generation of a connected text cannot be reduced to a simple combination of phrases expressing sub-parts of the representation, but must be based on patterns of discourse structure reflecting the whole representation.|Laurence Danlos","16737|IJCAI|2007|Open Information Extraction from the Web|Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a ,, Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of % on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER's ,, highest probability tuples, and show that they contain over ,, concrete facts and over ,, more abstract assertions.|Michele Banko,Michael J. Cafarella,Stephen Soderland,Matthew Broadhead,Oren Etzioni","14460|IJCAI|1987|Recovering Three Dimensional Shape from a Single Image of Curved Objects|We propose an algorithm to recover three-dimensional shape i.e. surface orientation and relative depth from a single segmented image of scenes containing opaque curved objects bounded by piecewise smooth surfaces. It is assumed that the surfaces have no markings or texture and that the reflectance map E  R (n) is known. Solutions for simpler versions of this problem have been presented by Sugihara for polyhedra, and by Horn et al for smooth surface patches. We first analyze the constraints from the line drawing and the image brightness values on the faces, edges and vertices in the scene. For a face this is done by Horn's image irradiance equation. To set up the other constraints we need to know line labels-which can be found using the algorithm developed in Malik '. At a limb, the direction of the surface normal is known directly. We develop a variational formulation of the constraints at an edgeboth from the known direction of the image curve corresponding to the edge and the shading. The associated Euler-Lagrange equation completely captures the local information. At a vertex, the constraints are modelled by a set of non-linear equations. An algorithm has been developed to solve this system of constraints.|Jitendra Malik","16862|IJCAI|2009|Fast Active Tabu Search and its Application to Image Retrieval|This paper proposes a novel framework for image retrieval. The retrieval is treated as searching for an ordered cycle in an image database. The optimal cycle can be found by minimizing the geometric manifold entropy of images. The minimization is solved by the proposed method, fast active tabu search. Experimental results demonstrate the framework for image retrieval is feasible and quite promising.|Chao Zhang,Hongyu Li,Qiyong Guo,Jinyuan Jia,I-Fan Shen","14107|IJCAI|1985|Shape and Source from Shading|Well-known met hods for solving the shape-from-shading problem require knowledge of the reflectance map. Here we show how the shape-from-shading problem can be solved when the reflectance map is not available, but is known to have a given form with some unknown parameters. This happens, for example, when the surface is known to be Lambertian, but the direction to the light source is not known. We give an iterative algorithm which alternately estimate the surface shape and the light source direction. Use of the unit normal in parameterizing the reflectance map, rather than the gradient or stereographic coordinates, simplifies the analysis. Our approach also leads to an iterative scheme for computing shape from shading that adjusts the current estimates of the local normals toward or away from the direction of the light source. The amount of adjustment is proportional to the current difference between the predicted and the observed brightness. We also develop generalizations to less constrained forms of reflectance maps.|Michael J. Brooks,Berthold K. P. Horn","13891|IJCAI|1983|An Extremum Principle for Shape From Contour|An extremum principle is developed that determines three-dimensional surface orientation from a two-dimensional contour. The principle maximizes (he ratio of the area to the square of the perimeter, a measure of the compactness or symmetry of the three-dimensional surface. The principle interprets regular figures correctly, it interprets skew symmetries as oriented real symmetries, and it is approximated by he maximum likelihood method on irregular figures.|Michael Brady,Alan L. Yuille","14542|IJCAI|1987|Recovering Surface Shape from Boundary|Humans can recover the shape of a surface from its image boundary. The authors have presented a new approach to shape from boundary . It includes three points  the line of curvature (LOC) regularity, an algorithm for constructing LOC net from boundary, and the computation of surface orientation from LOC net. Briefing these ideas, this paper examines the underlying problems of the approach and proposes improvements.|Gang Xu,Saburo Tsuji","14239|IJCAI|1985|Determining -D Motion of Planar Objects from Image Brightness Patterns|The brightness patterns in two successive image frames are used to recover the motion of a planar object without computing the optical flow as an intermediate step. Based on a least-squares formulation, a set of nine nonlinear equations are derived. A simple iterative scheme for solving these equations is presented. Using a selected example, it is shown that in general, the scheme may converge to cither of two possible solutions depending on the initial condition. Only in the special case where the translational motion vector is perpendicular to the surface does our algorithm converge to a unique solution.|Shahriar Negahdaripour,Berthold K. P. Horn","13877|IJCAI|1983|Inferring Motion of Cylindrical Object From Shape Information|This paper explores methods for Inferring motion of a cylindrical object from an image sequence. A local shading analysis method segments each Image Into spherical, cylindrical, and planar 'surfaces. The cylindrical surface Is characterized with a direction of generating lines determined from spatial derivatives In the image. An extended reflectance map method Is applicable to estimate gradients of base In consecutive frames If the lighting conditions are known. Another approach to Inferring of motion Is to use shape Information which needs an additional constraint. If two planes of the object are hot parallel, then the finding of correspondence between frames Is easy. For the case of parallel planes, we introduce an additional assumption that the planes are orthogonal to the generating lines. This strong constraint Is not sufficient for determining the surface normals from single view. The orthogonality also gives another constraint between a change In area of the base plane and that In length of generatling fine from the first to the second frames. Thus Iwe can estimate orientations of the bcise plane from two Views, Finally, motion parameters are estimated by correlating two shapes of the base boundary mapped from the two views.|Minoru Asada,Saburo Tsuji","14087|IJCAI|1985|Shape from Texture|Measurements on image texture interpreted under an approximate perspective image model can be used with an iterative constraint propagation algorithm to determine surface orientation. An extension of the ideas allows their robust application to natural images of textured planes. The techniques are demonstrated on synthetic and natural images.|John Aliomonos,Michael J. Swain"],["13912|IJCAI|1983|The Mercator Representation of Spatial Knowledge|The MERCATOR program constructs a cognitive map from a sequence of scene descriptions. A new representation of two-dimensional geography was developed for this program. Objects are represented by sets of polygons their boundaries, by sets of directed edges. The relative positions of objects are determined by connecting edges. A truth-conditional semantics for this representation is presented, its strengths and weaknesses are evaluated, and it is compared to other AI representations of shape and position.|Ernest Davis","15309|IJCAI|1995|On the Representation of Nonmontonic Relations in the Theory of Evidence|A Dempster-Shafer belief structure provides a mechanism for representing uncertain knowledge about a variable. A compatibility relation provides a structure for obtaining information about one variable based upon our knowledge about a second variable. An inference scheme in the theory of evidence involves the use of a belief structure on one variable, called the antecedent, and a compatibility relationship to infer a belief structure on the second variable, called the consequent. The concept of monotonicity in this situation is related to change in the specificity of the consequent belief structure as the antecedent belief structure becomes more specific. We show that the usual compatibility relations, type , are always monotonic. We introduce type II compatibility relations and show that a special class of these, which we call irregular, are needed to represent nonmonotonic relations between variables. We discuss a special class of nonmonotonic relations called default relations.|Ronald R. Yager","13796|IJCAI|1981|Representation and Inference in the Consul System|Users of interactive systems need a single cooperative interface for all of the services in their environment. The interface must behave in a consistent manner in understanding natural user requests and in providing explanation and help as required. The Consul system is designed to provide such an interface. Its natural interaction capability is achieved by mapping between detailed descriptions of users and systems in order to translate requests and provide explanations An interactive system of this Kind would be infeasible if the ones of constructing the knowledge base and inference techniques were placed on the individual service builders in Consul, service-dependent information is incorporated into the Knowledge base by semi-automatic acquisition, resulting in incorporation of the new Knowledge into the system's built-in abstract framework. This incorporation allows the service-dependent data to appropriately influence Consul's knowledge based mapping processes. The current Consul prototype demonstates natural request handling and explanation for a mail service.|William Mark","15143|IJCAI|1995|Device Representation and Reasoning with Affective Relations|Device representation and reasoning with affective relations occupies a middle ground between classical model-based diagnosis and heuristic expert systems. A device is modeled by specifying a set of diagnostically motivated affective relations among its components. Reasoning is then performed by a set of inference rules that reason with the model to propagate symptoms through the components. Representation and reasoning with affective relations extends several benefits of classical model-based diagnosis--the model as a unifying framework for knowledge, methodical coverage of the domain, and diagnostic reasoning based on equipment design and causality--to a class of problems where classical model-based diagnosis cannot be applied because the required models cannot be reasonably obtained or represented. Our work evolved from our redesign of a heuristic expert system for monitoring long-distance telephone switching systems, and is applicable to highly complex self-checking systems.|James M. Crawford,Daniel Dvorak,Diane J. Litman,Anil Mishra,Peter F. Patel-Schneider","14019|IJCAI|1983|Predicate Logic Involving Data Structure as a Knowledge Representation Language|A modification of predicate logic, called multilayered logic, is discussed. It is designed as the knowledge representation language for describing the systems that uses engineering knowledge to support the system analysis and design. A multi level data structure is used to represent the complex systems in which an abstract entity at a level of abstraction is formed as the collection of the entities at the next lower level. Multi layered logic adapts to this structure of the system and meets the conditions for supporting analysis and design of complex systems.|Setsuo Ohsuga","13964|IJCAI|1983|Representation of Temporal Knowledge|The paper describes a system of notions (a T-model) developed for representing temporal information on the semantics-pragmatics level in a natural language understanding system. The choice of the notions to be considered has been relied upon a necessity of special facilities for describing situations with various degrees of detailing according to an inexact character of temporal information in natural language texts. Time is modelled as a straight directed line, and four main objects point, interval, quantity and chain together with four groups of notions associated with them are included into the model. A structure of the T-model and its base notions are briefly outlined in the first section of the paper. The representation of temporal information by means of T-model objects is then considered by examples.|E. Yu Kandrashina","14216|IJCAI|1985|Motor Knowledge Representation|The motor control problem is considered in the framework of knowledge representation. In the AIRobotic world, a formal model for Motor knowledge should fill a gap between task planning and low level robot languages such model should be able to \"virtualize\" the robot and the interaction with the environment so that the planner could produce (and rely on) high level abstract actions, characterized by high autonomy and skill. The paper discusses some general aspects about actions, actors, and scenes, and describes the NEM language, which is able to represent and animate humanoids in a scene and is meant to provide a software laboratory for experimenting with action schemas.|Giuseppe Marino,Pietro Morasso,Renato Zaccaria","14250|IJCAI|1985|Self-Knowledge and Self-Representation|In this paper I introduce a contrast between homomorphic and nonhomomorphic ascriptions of informational content to representations. In the former case there is a mapping from the parts of the representation onto the constituents of the content. In the latter case, there is not some of the constituents of the content are settled by background factors. I contrast this distinction with that between context dependent and context independent ascriptions of content. I note that in cases where the ascriber of content shares the background with the agent, one is inclined to ascribe homomorphic content of a sort that does not have a fixed truth-value to a representation. This leads to the notion of relative information. Some uses for relative information are noted. Finally, the distinctions developed are used to distinguish three types of self-knowledge and account for their relations.|John Perry","15173|IJCAI|1995|Representation Dependence in Probabilistic Inference|Non-deductive reasoning systems are often representation dependent representing the same situation in two different ways may cause such a system to return two different answers. Some have viewed this as a significant problem. For example, the principle of maximum entropy has been subjected to much criticism due to its representation dependence. There has, however, been almost no work investigating representation dependence. In this paper, we formalize this notion and show that it is not a problem specific to maximum entropy. In fact, we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment, in a precise sense. Moreover, we show that representation independence is incompatible with even a weak default assumption of independence. We then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata, and provide a construction of a family of inference procedures that provides such restricted representation independence, using relative entropy.|Joseph Y. Halpern,Daphne Koller","13865|IJCAI|1981|D Object Representation and Matching with B-Splines and Surface Patches|An approach to D object representation and matching, which employs cubic B-spline curves and Coons surface patches is described. D objects are represented as structured collections of surface patches whose boundary curves are approximated by cubic II-splines. Both curved and polyhedral objects may be represented. A variety of D and D shape fetures are defined which are easily computed for this representation. Many of the D leatures have D analogues, thus providing access paths from D to D. It is anticipated that this approach will prove useful for matching D object models against D object descriptions. The representations our approaches to the problems of indexing and matching. the state of the implementation. and our experience with the implementation are briefly discussed.|Bryant W. York,Allen R. Hanson,Edward M. Riseman"]]}}