{"abstract":{"entropy":6.658944368352982,"topics":["data, data base, data system, data management, recent years, database system, database, relational database, data stream, information, large data, management system, data mining, data model, web search, relational data, web, problem data, combinatorial auctions, large number","genetic algorithm, algorithm, evolutionary algorithm, genetic programming, problem, machine learning, solving problem, present algorithm, particle swarm, optimization problem, optimization algorithm, search algorithm, algorithm problem, evolutionary computation, optimization, heuristic search, present novel, reinforcement learning, consider problem, search","constraint satisfaction, logic, neural networks, knowledge base, description logic, knowledge representation, logic programming, theorem proving, reasoning, present framework, knowledge, temporal reasoning, theorem prover, programming language, belief revision, model based, qualitative reasoning, bayesian networks, situation calculus, approach based","artificial intelligence, natural language, system, describes system, markov decision, expert system, learning system, markov processes, play role, partially observable, decision processes, agents, mobile robot, present system, robot, autonomous agents, markov mdps, decision making, control system, decision mdps","data mining, information system, time series, clustering data, information extraction, information, sources information, information retrieval, information data, classification data, time data, question answering, task data, problem information, labeled data, series data, data sets, web information, task classification, given data","web search, query processing, search engine, xml documents, web pages, support vector, semantic web, wide range, database query, query, query language, xml data, language processing, query optimization, data web, web engine, web, database queries, query data, relational xml","machine learning, problem learning, learning, reinforcement learning, learning algorithm, present learning, learning classifier, widely used, ant colony, approach learning, well known, learning methods, classification problem, neural networks, bayesian networks, classifier xcs, learning model, learning training, problem machine, explanation-based learning","genetic algorithm, genetic programming, estimation distribution, algorithm use, distribution algorithm, estimation algorithm, genetic gas, selection algorithm, estimation edas, previous work, distribution edas, genetic used, algorithm based, present genetic, use genetic, algorithm edas, based genetic, dimensionality reduction, crossover operators, work algorithm","knowledge base, theorem proving, belief revision, theorem prover, domains knowledge, knowledge, problem knowledge, knowledge system, computational complexity, present knowledge, effects actions, knowledge acquisition, knowledge representation, probabilistic model, arc consistency, expert knowledge, situation calculus, approach knowledge, inference rules, association rules","constraint satisfaction, constraint problem, present framework, present approach, approach based, present based, general framework, constraint, constraint csp, satisfaction problem, consistency constraint, constraint variables, preferences voting, satisfaction csp, present general, general problem, framework, model based, problem domains, voting rules","mobile robot, multi-agent system, distributed system, software system, distributed agents, design system, sensor networks, multiagent agents, multiagent system, case-based cbr, recognition system, production system, design, plan recognition, agents coordination, games playing, activity recognition, system environment, software, addresses problem","markov decision, markov processes, play role, partially observable, decision processes, planning plan, autonomous agents, agents, markov mdps, decision making, real world, decision mdps, observable markov, processes mdps, partially markov, problem agents, observable decision, partially decision, system agents, actions agents"],"ranking":[["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80061|VLDB|1981|DSIS - A Database System with Interrelational Semantics|DSIS is an experimental multi-user database system that supports an entity-oriented data model with relational and interrelational semantics. It maintains a sharp distinction between the user workspace and the shared database. Transactions are executed by a simulated network of stream processors.|Y. Edmund Lien,Jonathan E. Shopiro,Shalom Tsur","15177|IJCAI|1995|Advances of the DBLearn System for Knowledge Discovery in Large Databases|A prototyped data mining system, DBLearn, was developed in Simon Fraser Univ., which integrates machine learning methodologies with database technologies and efficiently and effectively extracts characteristic and discriminant rules from relational databases. Further developments, of DBLearn lead to a new generation data mining system DBMiner, with the following features () mining new kinds of rules from large databases, including multiple-level association rules, classification rules, cluster description rules, etc., () automatic generation and refinement of concept hierarchies, () high level SQL-like and graphical data mining interfaces, and () clientserver architecture and performance improvements for large applications. The major features of the system are demonstrated with experiments in a research grant information database.|Jiawei Han,Yongjian Fu,Simon Tang","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80034|VLDB|1981|Derived Relations A Unified Mechanism for Views Snapshots and Distributed Data|In a relational system, a database is composed of base relations, views, and snapshots. We show that this traditional approach can be extended to different classes of derived relations, and we propose a unified data definition mechanism for centralized and distributed databases. Our mechanism, called DEREL, can be used to define base relations and to derive different classes of views, snapshots, partitioned and replicated data. DEREL is intended to be part of a general purpose distributed relational database management system.|Michel E. Adiba","66839|AAAI|2010|Structure Learning for Markov Logic Networks with Many Descriptive Attributes|The multiterabyte Sloan Digital Sky Survey's (SDSS's) catalog data is stored in a commercial relational database management system with SQL query access and a built-in query optimizer. The SDSS catalog archive server adds advanced data mining features to the DBMS to provide fast online access to the data.|Hassan Khosravi,Oliver Schulte,Tong Man,Xiaoyuan Xu,Bahareh Bina"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Suárez,Manuel Valenzuela-Rendón,Hugo Terashima-Marín,Eduardo Uresti-Charre","58515|GECCO|2008|Parameter-less evolutionary search|The paper presents the parameter-less implementation of an evolutionary-based search. It does not need any predefined control parameters values, which are usually used for genetic algorithms and similar techniques. Efficiency of the proposed algorithm was evaluated by CEC benchmark functions and a real-world product optimization problem.|Gregor Papa","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58401|GECCO|2008|Improving the efficiency of the extended compact genetic algorithm|Evolutionary Algorithms are largely used search and optimization procedures that, when properly designed, can solve intractable problems in tractable polynomial time. Efficiency enhancements are used to turn them from tractable to practical. In this paper we show preliminary results of two efficiency enhancements proposed for the Extended Compact Genetic Algorithm. First, a model building enhancement was used to reduce the complexity of the process from O($n$) to O($n$), speeding up the algorithm by  times on a  bits problem. Then, local-search hybridization was used to reduce the population size by at least  times, reducing the memory and running time required by the algorithm. These results draw the first steps toward a competent and efficient Genetic Algorithm.|Thyago S. P. C. Duque,David E. Goldberg,Kumara Sastry","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","58554|GECCO|2008|An evolutionary approach for competency-based curriculum sequencing|The process of creating e-learning contents using reusable learning objects (LOs) can be broken down in two sub-processes LOs finding and LO sequencing. Sequencing is usually performed by instructors, who create courses targeting generic profiles rather than personalized materials. This paper proposes an evolutionary approach to automate this latter problem while, simultaneously, encourages reusability and interoperability by promoting standards employment. A model that enables automated curriculum sequencing is proposed. By means of interoperable competency records and LO metadata, the sequencing problem is turned into a constraint satisfaction problem. Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) agents are designed, built and tested in real and simulated scenarios. Results show both approaches succeed in all test cases, and that they handle reasonably computational complexity inherent to this problem, but PSO approach outperforms GA.|Luis de Marcos,José-Javier Martínez,José Antonio Gutiérrez,Roberto Barchino,José María Gutiérrez","58338|GECCO|2008|Overfitting in the selection of classifier ensembles a comparative study between PSO and GA|Classifier ensemble selection may be formulated as a learning task since the search algorithm operates by minimizingmaximizing the objective function. As a consequence, the selection process may be prone to overfitting. The objectives of this paper are () to show how overfitting can be detected when the selection is performed by two classical search algorithms Genetic Algorithm and Particle Swarm Optimization and () to verify which algorithm is more prone to overfitting. The experimental results demonstrate that GA appears to be more affected by overfitting.|Eulanda Miranda dos Santos,Luiz S. Oliveira,Robert Sabourin,Patrick Maupin"],["15866|IJCAI|2003|A Resolution Theorem for Algebraic Domains|W. C. Rounds and G.-Q. Zhang have recently proposed to study a form of resolution on algebraic domains Rounds and Zhang, . This framework allows reasoning with knowledge which is hierarchically structured and forms a (suitable) domain, more precisely, a coherent algebraic cpo as studied in domain theory. In this paper, we give conditions under which a resolution theorem -- in a form underlying resolution-based logic programming systems -- can be obtained. The investigations bear potential for engineering new knowledge representation and reasoning systems on a firm domain-theoretic background.|Pascal Hitzler","66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|Luís C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","14269|IJCAI|1985|Prolog Extensions Based on Tableau Calculus|The intention of this paper is to help bridging the gap between logic programming and theorem proving. It presents the design of a Gentzen type proof search procedure, based on classical tableau calculus, for knowledge bases consisting of arbitrary first order formulas. At each proof search step, when a new formula is to be chosen from the knowledge base, the procedure chooses in such a way that the search space is small. When applied to a Horn clause knowledge base and an atomic goal, it performs the same proof search steps as any PROLOG interpreter would do. Hence, PROLOG can be viewed as a special Gentzen type procedure just as it is a special (namely, linear input) resolution procedure.|Wolfgang Schönfeld","14929|IJCAI|1991|Representing Diagnostic Knowledge for Probabilistic Horn Abduction|This paper presents a simple logical framework for abduction, with probabilities associated with hypotheses. The language is an extension to pure Prolog, and it has straight-forward implementations using branch and bound search with either logic-programming technology or ATMS technology. The main focus of this paper is arguing for a form of representational adequacy of this very simple system for diagnostic reasoning. It is shown how it can represent model-based knowledge, with and without faults, and with and without nonintermittency assumptions. It is also shown how this representation can represent any probabilistic knowledge representable in a Bayesian belief network.|David Poole","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","65074|AAAI|1987|Inferring Formal Software Specifications from Episodic Descriptions|The WATSON automatic programming system computes formal behavior specifications for process-control software from informal \"scenarios\" traces of typical system operation. It first generalizes scenarios into stimulus-response rules, then modifies and augments these rules to repair inconsistency and incompleteness. It finally produces a formal specification for the class of computations which implement that scenario and which are also compatible with a set of \"domain axioms\". A particular automaton from that class is constructed as an executable prototype for the specification. WATSON's inference engine combines theorem proving in a very weak temporal logic with faster and stronger, but approximate, model-based reasoning. The use of models and of closed-world reasoning over \"snapshots\" of an evolving knowledge base leads to an interesting special case of non-monotonic reasoning.|Van E. Kelly,Uwe Nonnenmann","65121|AAAI|1987|Default Reasoning through Belief Revision Strategy|The thesis of this paper is that default reasoning can be accomplished rather naturally if an appropriate strategy of belief revision is employed. The idea is based on the premise that new beliefs introduced into a situation change the structure of current beliefs to accomodate the new beliefs as exceptions. It is easy to characterise these exceptions in beliefs if we extend the belief language to include some modal operator and prefix the exceptions with the operator. This serves to make the exceptions syntactically explicit, which can then be processed in a routine way by a default reasoning theorem prover.|Chern H. Seet","15662|IJCAI|2001|A-System Problem Solving through Abduction|This paper presents a new system, called the A- System, performing abductive reasoning within the framework of Abductive Logic Programming. It is based on a hybrid computational model that implements the abductive search in terms of two tightly coupled processes a reduction process of the highlevel logical representation to a lower-level constraint store and a lower-level constraint solving process. A set of initial \"proof of principle\" experiments demonstrate the versatility of the approach stemming from its declarative representation of problems and the good underlying computational behaviour of the system. The approach offers a general methodology of declarative problem solving in AI where an incremental and modular refinement of the high-level representation with extra domain knowledge can improve and scale the computational performance of the framework.|Antonis C. Kakas,Bert Van Nuffelen,Marc Denecker","15375|IJCAI|1997|PRISM A Language for Symbolic-Statistical Modeling|We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs.|Taisuke Sato,Yoshitaka Kameya","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"],["66878|AAAI|2010|Using Bisimulation for Policy Transfer in MDPs|Much of the work on using Markov Decision Processes (MDPs) in artificial intelligence (AI) focuses on solving a single problem. However, AI agents often exist over a long period of time, during which they may be required to solve several related tasks. This type of scenario has motivated a significant amount of recent research in knowledge transfer methods for MDPs. The idea is to allow an agent to continue to re-use the expertise accumulated while solving past tasks over its lifetime (see Taylor & Stone, , for a comprehensive survey).|Pablo Samuel Castro,Doina Precup","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","66461|AAAI|2008|Incorporating Mental Simulation for a More Effective Robotic Teammate|How can we facilitate human-robot teamwork The teamwork literature has identified the need to know the capabilities of teammates. How can we integrate the knowledge of another agent's capabilities for a justifiably intelligent teammate This paper describes extensions to the cognitive architecture, ACT-R, and the use of artificial intelligence (AI) and cognitive science approaches to produce a more cognitively-plausible, autonomous robotic system that \"mentally\" simulates the decision-making of its teammate. The extensions to ACT-R added capabilities to interact with the real world through the robot's sensors and effectors and simulate the decision-making of its teammate. The AI applications provided visual sensor capabilities by methods clearly different than those used by humans. The integration of these approaches into intelligent team-based behavior is demonstrated on a mobile robot. Our \"TeamBot\" matches the descriptive work and theories on human teamwork. We illustrate our approach in a spatial, team-oriented task of a guard force responding appropriately to an alarm condition that requires the human and robot team to \"man\" two guard stations as soon as possible after the alarm.|William G. Kennedy,Magdalena D. Bugajska,William Adams,Alan C. Schultz,J. Gregory Trafton","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","16446|IJCAI|2007|The Value of Observation for Monitoring Dynamic Systems|We consider the fundamental problem of monitoring (i.e. tracking) the belief state in a dynamic system, when the model is only approximately correct and when the initial belief state might be unknown. In this general setting where the model is (perhaps only slightly) mis-specified, monitoring (and consequently planning) may be impossible as errors might accumulate over time. We provide a new characterization, the value of observation, which allows us to bound the error accumulation. The value of observation is a parameter that governs how much information the observation provides. For instance, in Partially Observable MDPs when it is  the POMDP is an MDP while for an unobservable Markov Decision Process the parameter is . Thus, the new parameter characterizes a spectrum from MDPs to unobservable MDPs depending on the amount of information conveyed in the observations.|Eyal Even-Dar,Sham M. Kakade,Yishay Mansour","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","15648|IJCAI|2001|Max-norm Projections for Factored MDPs|Markov Decision Processes (MDPs) provide a coherent mathematical framework for planning under uncertainty. However, exact MDP solution algorithms require the manipulation of a value function, which specifies a value for each state in the system. Most real-world MDPs are too large for such a representation to be feasible, preventing the use of exact MDP algorithms. Various approximate solution algorithms have been proposed, many of which use a linear combination of basis functions as a compact approximation to the value function. Almost all of these algorithms use an approximation based on the (weighted) L-norm (Euclidean distance) this approach prevents the application of standard convergence results for MDP algorithms, all of which are based on max-norm. This paper makes two contributions. First, it presents the first approximate MDP solution algorithms - both value and policy iteration - that use max-norm projection, thereby directly optimizing the quantity required to obtain the best error bounds. Second, it shows how these algorithms can be applied efficiently in the context of factored MDPs, where the transition model is specified using a dynamic Bayesian network.|Carlos Guestrin,Daphne Koller,Ronald Parr","13398|IJCAI|1973|Planning Considerations for a Roving Robot with Arm|The Jet Propulsion Laboratory is engaged in a robot research program. The program is aimed at the development and demonstration of technology required to integrate a variety of robntic functions (locomotion, manipulation, sensing and perception, decision making, and man-robot interaction) into a working robot unit operating in a real world environment and dealing with both man-made and natural objects. This paper briefly describes the hardware and software system architecture of the robot breadboard and summarizes the developments to date. The content of the paper is focused on the unique planning considerations involved in incorporating a manipulator as part of an autonomous robot system. In particular, the effects of system architecture, arm trajectory calculations, and arm dynamics and control are discussed in the context of planning arm motion in complex and changing sensory and workspace environments.|Richard A. Lewis,Antal K. Bejczy","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|Håkan L. S. Younes"],["13451|IJCAI|1975|QUESTION-ANSWER - A Multipurpose Information System|We describe a \"QUESTION-ANSWER\" information system implemented on computer BESM- in the time-shared system. The \"QUESTION-ANSWER\" system is capable of deducing facts that have not been explicitly given to it by using a large data base and interpreting some of data as rules of inference. The system employs a special procedure which allows not to use contradictory information even if it is contained in the data base.|Y. Buchstab,S. Kamynin","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","66814|AAAI|2010|Exploiting Monotonicity in Interval Constraint Propagation|Model-based clustering is one of the most important ways for time series data mining. However, the process of clustering may encounter several problems. In this paper, a novel clustering algorithm of time-series which incorporates recursive hidden Markov model(HMM) training is proposed. Our contributions are as follows ) We recursively train models and use these model information in the process agglomerative hierarchical clustering. ) We built HMM of time-series clusters to describe clusters. To evaluate the effectiveness of the algorithm, several experiments are conducted on both synthetic data and real world data. The result shows that the proposed approach can achieve better performance in correctness rate than the traditional HMM-based clustering algorithm|Ignacio Araya,Gilles Trombettoni,Bertrand Neveu","16349|IJCAI|2007|A Subspace Kernel for Nonlinear Feature Extraction|Kernel based nonlinear Feature Extraction (KFE) or dimensionality reduction is a widely used preprocessing step in pattern classification and data mining tasks. Given a positive definite kernel function, it is well known that the input data are implicitly mapped to a feature space with usually very high dimensionality. The goal of KFE is to find a low dimensional subspace of this feature space, which retains most of the information needed for classification or data analysis. In this paper, we propose a subspace kernel based on which the feature extraction problem is transformed to a kernel parameter learning problem. The key observation is that when projecting data into a low dimensional subspace of the feature space, the parameters that are used for describing this subspace can be regarded as the parameters of the kernel function between the projected data. Therefore current kernel parameter learning methods can be adapted to optimize this parameterized kernel function. Experimental results are provided to validate the effectiveness of the proposed approach.|Mingrui Wu,Jason D. R. Farquhar","17005|IJCAI|2009|Relation Regularized Matrix Factorization|In many applications, the data, such as web pages and research papers, contain relation (link) structure among entities in addition to textual content information. Matrix factorization (MF) methods, such as latent semantic indexing (LSI), have been successfully used to map either content information or relation information into a lower-dimensional latent space for subsequent processing. However, how to simultaneously model both the relation information and the content information effectively with an MF framework is still an open research problem. In this paper, we propose a novel MF method called relation regularized matrix factorization (RRMF) for relational data analysis. By using relation information to regularize the content MF procedure, RRMF seamlessly integrates both the relation information and the content information into a principled framework. We propose a linear-time learning algorithm with convergence guarantee to learn the parameters of RRMF. Extensive experiments on real data sets show that RRMF can achieve state-of-the-art performance.|Wu-Jun Li,Dit-Yan Yeung","13614|IJCAI|1977|Ghosts in the Machine An AI Treatment of Medieval History|This paper gives a generalized overview of RESEDA, an interactive question answering system designed primarily for use by historians. Its data base consists of historical information, which attemps to describe the attitudes, political, religious and interpersonal, of the chief characters of the period. Question answering is done by search of the data base and by inference on the information therein. The difficulties of representing this type of data and of formulating inference rules dealing with human motivations and attitudes is also discussed.|Margaret King,Monique Ornato,Gian Piero Zarri,L. Zarri-Baldi,A. Zwiebel","80665|VLDB|2006|A Decade of Progress in Indexing and Mining Large Time Series Databases|Time series data is ubiquitous large volumes of time series data are routinely created in scientific, industrial, entertainment, medical and biological domains. Examples include gene expression data, electrocardiograms, electroencephalograms, gait analysis, stock market quotes, space telemetry etc. Although statisticians have worked with time series for more than a century, many of their techniques hold little utility for researchers working with massive time series databases.A decade ago, a seminal paper by Faloutsos, Ranganathan, Manolopoulos appeared in SIGMOD. The paper, Fast Subsequence Matching in Time-Series Databases, has spawned at least a thousand references and extensions in the database data mining and information retrieval communities. This tutorial will summarize the decade of progress since this influential paper appeared.|Eamonn J. Keogh","66532|AAAI|2008|Text Categorization with Knowledge Transfer from Heterogeneous Data Sources|Multi-category classification of short dialogues is a common task performed by humans. When assigning a question to an expert, a customer service operator tries to classify the customer query into one of N different classes for which experts are available. Similarly, questions on the web (for example questions at Yahoo Answers) can be automatically forwarded to a restricted group of people with a specific expertise. Typical questions are short and assume background world knowledge for correct classification. With exponentially increasing amount of knowledge available, with distinct properties (labeled vs unlabeled, structured vs unstructured), no single knowledge-transfer algorithm such as transfer learning, multi-task learning or selftaught learning can be applied universally. In this work we show that bag-of-words classifiers performs poorly on noisy short conversational text snippets. We present an algorithm for leveraging heterogeneous data sources and algorithms with significant improvements over any single algorithm, rivaling human performance. Using different algorithms for each knowledge source we use mutual information to aggressively prune features. With heterogeneous data sources including Wikipedia, Open Directory Project (ODP), and Yahoo Answers, we show .% and .% correct classification on Google Answers corpus and Switchboard corpus using only  featuresclass. This reflects a huge improvement over bag of words approaches and -% error reduction over previously published state of art (Gabrilovich et. al. ).|Rakesh Gupta,Lev-Arie Ratinov","16796|IJCAI|2007|Parametric Kernels for Sequence Data Analysis|A key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. Many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. However, finding an effective transformation scheme for sequence (e.g. time series) data is a difficult task. In this paper, we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwritten character recognition and object recognition from sensor readings. Ordering information is represented by values of a parameter associated with each input data element. A similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a Mercer kernel suitable for use in methods such as support vector machine (SVM). This scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. We apply our method to object and handwritten character recognition tasks and compare against current approaches. The results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. Our contribution is the introduction of a general technique for designing SVM kernels tailored for the classification of sequence data.|Young-In Shin,Donald S. Fussell","80153|VLDB|2001|Lineage Tracing for General Data Warehouse Transformations|Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex &ldquodata cleansing&rdquo procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.|Yingwei Cui,Jennifer Widom"],["80307|VLDB|2003|Optimized Query Execution in Large Search Engines with Global Page Ordering|Large web search engines have to answer thousands of queries per second with interactive response times. A major factor in the cost of executing a query is given by the lengths of the inverted lists for the query terms, which increase with the size of the document collection and are often in the range of many megabytes. To address this issue, IR and database researchers have proposed pruning techniques that compute or approximate term-based ranking functions without scanning over the full inverted lists. Over the last few years, search engines have incorporated new types of ranking techniques that exploit aspects such as the hyperlink structure of the web or the popularity of a page to obtain improved results. We focus on the question of how such techniques can be efficiently integrated into query processing. In particular, we study pruning techniques for query execution in large engines in the case where we have a global ranking of pages, as provided by Pagerank or any other method, in addition to the standard term-based approach. We describe pruning schemes for this case and evaluate their efficiency on an experimental cluster-based search engine with million web pages. Our results show that there is significant potential benefit in such techniques.|Xiaohui Long,Torsten Suel","80274|VLDB|2003|Temporal Slicing in the Evaluation of XML Queries|As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, XQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a XQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.|Dengfeng Gao,Richard T. Snodgrass","80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80217|VLDB|2002|XPathLearner An On-line Self-Tuning Markov Histogram for XML Path Selectivity Estimation|The extensible mark-up language (XML) is gaining widespread use as a format for data exchange and storage on the World Wide Web. Queries over XML data require accurate selectivity estimation of path expressions to optimize query execution plans. Selectivity estimation of XML path expression is usually done based on summary statistics about the structure of the underlying XML repository. All previous methods require an off-line scan of the XML repository to collect the statistics. In this paper, we propose XPathLearner, a method for estimating selectivity of the most commonly used types of path expressions without looking at the XML data. XPathLearner gathers and refines the statistics using query feedback in an on-line manner and is especially suited to queries in Internet scale applications since the underlying XML repository is either inaccessible or too large to be scanned in its entirety. Besides the on-line property, our method also has two other novel features (a) XPathLearner is workload-aware in collecting the statistics and thus can be more accurate than the more costly off-line method under tight memory constraints, and (b) XPathLearner automatically adjusts the statistics using query feedback when the underlying XML data change. We show empirically the estimation accuracy of our method using several real data sets.|Lipyeow Lim,Min Wang,Sriram Padmanabhan,Jeffrey Scott Vitter,Ronald Parr","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80236|VLDB|2002|XMark A Benchmark for XML Data Management|While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.|Albrecht Schmidt 0002,Florian Waas,Martin L. Kersten,Michael J. Carey,Ioana Manolescu,Ralph Busse","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos"],["57228|GECCO|2003|Towards Building Block Propagation in XCS A Negative Result and Its Implications|The accuracy-based classifier system XCS is currently the most successful learning classifier system. Several recent studies showed that XCS can produce machine-learning competitive results. Nonetheless, until now the evolutionary mechanisms in XCS remained somewhat ill-understood. This study investigates the selectorecombinative capabilities of the current XCS system. We reveal the accuracy dependence of XCS's evolutionary algorithm and identify a fundamental limitation of the accuracy-based fitness approach in certain problems. Implications and future research directions conclude the paper.|Kurian K. Tharakunnel,Martin Butz,David E. Goldberg","66058|AAAI|2007|Efficient Structure Learning in Factored-State MDPs|We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.|Alexander L. Strehl,Carlos Diuk,Michael L. Littman","15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell","58562|GECCO|2009|Discrete dynamical genetic programming in XCS|A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of well-known test problems.|Richard Preen,Larry Bull","57246|GECCO|2003|Evolving Keepaway Soccer Players through Task Decomposition|In some complex control tasks, learning a direct mapping from an agent's sensors to its actuators is very difficult. For such tasks, decomposing the problem into more manageable components can make learning feasible. In this paper, we provide a task decomposition, in the form of a decision tree, for one such task. We investigate two different methods of learning the resulting subtasks. The first approach, layered learning, trains each component sequentially in its own training environment, aggressively constraining the search. The second approach, coevolution, learns all the subtasks simultaneously from the same experiences and puts few restrictions on the learning algorithm. We empirically compare these two training methodologies using neuro-evolution, a machine learning algorithm that evolves neural networks. Our experiments, conducted in the domain of simulated robotic soccer keepaway, indicate that neuro-evolution can learn effective behaviors and that the less constrained coevolutionary approach outperforms the sequential approach. These results provide new evidence of coevolution's utility and suggest that solution spaces should not be over-constrained when supplementing the learning of complex tasks with human knowledge.|Shimon Whiteson,Nate Kohl,Risto Miikkulainen,Peter Stone","58139|GECCO|2007|Ensemble learning for free with evolutionary algorithms|Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.|Christian Gagné,Michèle Sebag,Marc Schoenauer,Marco Tomassini","59027|GECCO|2010|Adaption of XCS to multi-learner predatorprey scenarios|Learning classifier systems (LCSs) are rule-based evolutionary reinforcement learning systems. Today, especially variants of Wilson's extended classifier system (XCS) are widely applied for machine learning. Despite their widespread application, LCSs have drawbacks, e. g., in multi-learner scennarios, since the Markov property is not fulfilled. In this paper, LCSs are investigated in an instance of the generic homogeneous and non-communicating predatorprey scenario. A group of predators collaboratively observe a (randomly) moving prey as long as possible, where each predator is equipped with a single, independent XCS. Results show that improvements in learning are achieved by cleverly adapting a multi-step approach to the characteristics of the investigated scenario. Firstly, the environmental reward function is expanded to include sensory information. Secondly, the learners are equipped with a memory to store and analyze the history of local actions and given payoffs.|Clemens Lode,Urban Richter,Hartmut Schmeck","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","66231|AAAI|2007|Learning by Combining Observations and User Edits|We introduce a new collaborative machine learning paradigm in which the user directs a learning algorithm by manually editing the automatically induced model. We identify a generic architecture that supports seamless interweaving of automated learning from training samples and manual edits of the model, and we discuss the main difficulties that the framework addresses. We describe Augmentation-Based Learning (ABL), the first learning algorithm that supports interweaving of edits and learning from training samples. We use examples based on ABL to outline selected advantages of the approach--dealing with bad data by manually removing their effects from the model, and learning a model with fewer training samples.|Vittorio Castelli,Lawrence D. Bergman,Daniel Oblinger","16723|IJCAI|2007|Generalized Additive Bayesian Network Classifiers|Bayesian network classifiers (BNC) have received considerable attention in machine learning field. Some special structure BNCs have been proposed and demonstrate promise performance. However, recent researches show that structure learning in BNs may lead to a non-negligible posterior problem, i.e, there might be many structures have similar posterior scores. In this paper, we propose a generalized additive Bayesian network classifiers, which transfers the structure learning problem to a generalized additive models (GAM) learning problem. We first generate a series of very simple BNs, and put them in the framework of GAM, then adopt a gradient-based algorithm to learn the combining parameters, and thus construct a more powerful classifier. On a large suite of benchmark data sets, the proposed approach outperforms many traditional BNCs, such as naive Bayes, TAN, etc, and achieves comparable or better performance in comparison to boosted Bayesian network classifiers.|Jianguo Li,Changshui Zhang,Tao Wang,Yimin Zhang"],["57339|GECCO|2005|Not all linear functions are equally difficult for the compact genetic algorithm|Estimation of distribution algorithms (EDAs) try to solve an optimization problem by finding a probability distribution focussed around its optima. For this purpose they conduct a sampling-evaluation-adjustment cycle, where search points are sampled with respect to a probability distribution, which is adjusted according to the evaluation of the sampled points. Although there are many successful experiments suggesting the usefulness of EDAs, there are only few rigorous theoretical results apart from convergence results without time bounds. Here we present first rigorous runtime analyses of a simple EDA, the compact genetic algorithm, for linear pseudo-boolean functions on n variables. We prove a number of results showing that not all linear functions have the same asymptotical runtime.|Stefan Droste","57501|GECCO|2005|Real-coded crossover as a role of kernel density estimation|This paper presents a kernel density estimation method by means of real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine the sampling strategy by means of a parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and the sampling strategy is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation.|Jun Sakuma,Shigenobu Kobayashi","57887|GECCO|2007|An application of EDA and GA to dynamic pricing|E-commerce has transformed the way firms develop their pricing strategies, producing shift away from fixed pricing to dynamic pricing. In this paper, we use two different Estimation of distribution algorithms (EDAs), a Genetic Algorithm (GA) and a Simulated Annealing (SA) algorithm for solving two different dynamic pricing models. Promising results were obtained for an EDA confirming its suitability for resource management in the proposed model. Our analysis gives interesting insights into the application of population based optimization techniques for dynamic pricing.|Siddhartha Shakya,Fernando Oliveira,Gilbert Owusu","57456|GECCO|2005|A comparison study between genetic algorithms and bayesian optimize algorithms by novel indices|Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA's defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization Algorithm (BOA), a well-known Estimation of Distribution Algorithms (EDAs).|Naoki Mori,Masayuki Takeda,Keinosuke Matsumoto","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","58744|GECCO|2009|Approximating the search distribution to the selection distribution in EDAs|In an Estimation of Distribution Algorithm (EDA) with an infinite sized population the selection distribution equals the search distribution. For a finite sized population these distributions are different. In practical EDAs the goal of the search distribution learning algorithm is to approximate the selection distribution. The source data is the selected set, which is derived from the population by applying a selection operator. The new approach described here eliminates the explicit use of the selection operator and the selected set. We rewrite for a finite population the selection distribution equations of four selection operators. The new equation is called the empirical selection distribution. Then we show how to build the search distribution that gives the best approximation to the empirical selection distribution. Our approach gives place to practical EDAs which can be easily and directly implemented from well established theoretical results. This paper also shows how common EDAs with discrete and real variables are adapted to take advantage of the empirical selection distribution. A comparison and discussion of performance is presented.|Sergio Ivvan Valdez Peña,Arturo Hernández Aguirre,Salvador Botello Rionda","58484|GECCO|2008|On the effectiveness of distributions estimated by probabilistic model building|Estimation of distribution algorithms (EDAs) are a class of evolutionary algorithms that capture the likely structure of promising solutions by explicitly building a probabilistic model and utilize the built model to guide the further search. It is presumed that EDAs can detect the structure of the problem by recognizing the regularities of the promising solutions. However, in certain situations, EDAs are unable to discover the entire structure of the problem because the set of promising solutions on which the model is built contains insufficient information regrading some parts of the problem and renders EDAs incapable of processing those parts accurately. In this work, we firstly propose a general concept that the estimated probabilistic models should be inspected to reveal the effective search directions. Based on that concept, we design a practical approach which utilizes a reserved set of solutions to examine the built model for the fragments that may be inconsistent with the actual problem structure. Furthermore, we provide an implementation of the designed approach on the extended compact genetic algorithm (ECGA) and conduct numerical experiments. The experimental results indicate that the proposed method can significantly assist ECGA to handle problems comprising building blocks of disparate scalings.|Chung-Yao Chuang,Ying-ping Chen","57103|GECCO|2003|Hybridization of Estimation of Distribution Algorithms with a Repair Method for Solving Constraint Satisfaction Problems|Estimation of Distribution Algorithms (EDAs) are new promising methods in the field of genetic and evolutionary algorithms. In the case of conventional Genetic and Evolutionary Algorithm studies to apply Constraint Satisfaction Problems (CSPs), it is well-known that the incorporation of the domain knowledge in the CSPs is quite effective. In this paper, we propose a hybridization method (memetic algorithm) of Estimation of Distribution Algorithms with a repair method. Experimental results on general CSPs tell us the effectiveness of the proposed method.|Hisashi Handa","57674|GECCO|2006|The correlation-triggered adaptive variance scaling IDEA|It has previously been shown analytically and experimentally that continuous Estimation of Distribution Algorithms (EDAs) based on the normal pdf can easily suffer from premature convergence. This paper takes a principled first step towards solving this problem. First, prerequisites for the successful use of search distributions in EDAs are presented. Then, an adaptive variance scaling theme is introduced that aims at reducing the risk of premature convergence. Integrating the scheme into the iterated density--estimation evolutionary algorithm (IDEA) yields the correlation-triggered adaptive variance scaling IDEA (CT-AVS-IDEA). The CT-AVS-IDEA is compared to the original IDEA and the Evolution Strategy with Covariance Matrix Adaptation (CMA-ES) on a wide range of unimodal test-problems by means of a scalability analysis. It is found that the average number of fitness evaluations grows subquadratically with the dimensionality, competitively with the CMA-ES. In addition, CT-AVS-IDEA is indeed found to enlarge the class of problems that continuous EDAs can solve reliably.|Jörn Grahl,Peter A. N. Bosman,Franz Rothlauf","57523|GECCO|2005|Using a Markov network model in a univariate EDA an empirical cost-benefit analysis|This paper presents an empirical cost-benefit analysis of an algorithm called Distribution Estimation Using MRF with direct sampling (DEUMd). DEUMd belongs to the family of Estimation of Distribution Algorithm (EDA). Particularly it is a univariate EDA. DEUMd uses a computationally more expensive model to estimate the probability distribution than other univariate EDAs. We investigate the performance of DEUMd in a range of optimization problem. Our experiments shows a better performance (in terms of the number of fitness evaluation needed by the algorithm to find a solution and the quality of the solution) of DEUMd on most of the problems analysed in this paper in comparison to that of other univariate EDAs. We conclude that use of a Markov Network in a univariate EDA can be of net benefit in defined set of circumstances.|Siddhartha Shakya,John A. W. McCall,Deryck F. Brown"],["13861|IJCAI|1981|Knowledge Acquisition in the Consul System|Many knowledge-based systems feature general machinery that operates on externally supplied information, These systems must solve the acquisition problem how to represent the external knowledge, determine if it is adequate, and incorporate it into the knowledge base. As a mediator between users and interactive services, the Consul system must understand the intent and behavior of programs that perform interactive functions To Consul, understanding a function means classifying a description of it in a highly structured, prebuilt knowledge base. A special formalism has been designed in which a service builder both programs functions and describes their actions. The resulting functional descriptions are then translated and interactively classified into Consul's knowledge base by Consuls acquisition component The acquisition dialogue with the service builder will be shown to be robust with respect to the information provided by the service builder. Inference rules are automatically generated to account for discrepancies between a program's specifications and expectations derived from Consuls knowledge base.|David Wilczynski","13627|IJCAI|1977|Computer Understanding of Mathematical Proofs|Mathematical proofs constitute a mixture of formulas with a subset of natural language. They can be represented as a sequence of lines expressible in the symbolism of predicate calculus. The transition from step to step may depend on a series of logical manipulations andor on intricate mathematical knowledge associated with the domain of the proof. The organization of the proof may depend on different conventions adopted by mathematicians in communication with each other. This paper deals with problems involved in following the mathematical argument along those lines. Some of the ideas were implemented as a part of a system for teaching axiomatic set theory to college students. The most powerful and frequently used rules of inference utilize a resolution theorem prover. To the best of our knowledge this is the only resolution theorem prover, perhaps the only general purpose theorem prover used in actual production.|Vesko Marinov","14326|IJCAI|1987|Compiling Design Plans from Descriptions of Artifacts and Problem Solving Heuristics|An analysis of the design plans in the Pride expert system shows that they integrate knowledge about structure and functionality of artifacts as well as problem-solving heuristics A method is presented by which such plans can be automatically generated by compiling knowledge about artifacts, problem solving heuristics, and characteristics of specific problems. Knowledge compilation allows the creation of plans tailored to particular problems and offers potential benefits in maintaining a knowledge base, in reusing the same knowledge for different purposes, and in providing a framework for more systematic knowledge acquisition.|Agustin A. Araya,Sanjay Mittal","14306|IJCAI|1985|Designing Examples for Semantically Guided Hierarchical Deduction|Semantically guided hierarchical deduction prover is a resolution-based theorem-proving procedure which is capable of using the domain dependent knowledge presented in well designed examples. This paper gives an overview of the basic deduction components of the prover, investigates some rules for human design of examples, and demonstrates their usage in proving several non-trivial theorems.|Tie-Cheng Wang","14720|IJCAI|1989|On the Road to Automatic Knowledge Engineering|The paper presents a scheme for categorizing knowledge engineering tools. The classification of knowledge acquisition systems has revealed some interesting facts about these systems. It seems that systems which are able to work on multiple tasks produce very shallow (i.e., not of expert-level) knowledge bases. On the other hand, systems which produce expert-level knowledge bases function on a single task. These insights have led to the design of ASKE, a knowledge acquisition system which can be used to build expert-level knowledge bases in several domains and for different task-types. The knowledge acquisition process is based on the notion of templates, the knowledge-bearing units of ASKE. Templates provide a convenient way of representing domain knowledge.|Jitu Patel","14242|IJCAI|1985|Checking an Expert Systems Knowledge Base for Consistency and Completeness|In this paper we describe a program that verifies the consistency and completeness of expert system knowledge bases which utilize the Lookheed Expert System (LES) framework. The algorithms described here are not specific to LES and can be applied to most rule-based systems. The program, called CHECK, combines logical principles as well as specific information about the knowledge representation formalism of LES. The program checks for redundant rules, conflicting rules, subsumed rules, missing rules, circular rules, unreachable clauses, and deadend clauses. It also generates a dependency chart which shows the dependencies among the rules and between the rules and the goals. CHECK can help the knowledge engineer to detect many programming errors even before the knowledge base testing phase. It also helps detect gaps in the knowledge base which the knowledge engineer and the expert might have overlooked. A wide variety of knowledge bases have been analyzed using CHECK.|Tin A. Nguyen,Walton A. Perkins,Thomas J. Laffey,Deanne Pecora","15183|IJCAI|1995|A Hybrid Fuzzy-Neural Expert System for Diagnosis|Fuzzy Logic, a neural network and an expert system are combined to build a hybrid diagnosis system. With this system we introduce a new approach to the acquisition of knowledge bases. Our system consists of a fuzzy expert system with a dual source knowledge base. Two sets of rules are acquired, inductively from given examples and deductively formulated by a physician. A fuzzy neural network serves to learn from sample data and allows to extract fuzzy rules for the knowledge base. The diagnosis of electroencephalograms by interpretation of graphoelements serves as visualization for our approach. Preliminary results demonstrate the promising possibilities offered by our method.|Christoph S. Herrmann","66428|AAAI|2008|Horn Complements Towards Horn-to-Horn Belief Revision|Horn-to-Horn belief revision asks for the revision of a Horn knowledge base such that the revised knowledge base is also Horn. Horn knowledge bases are important whenever one is concerned with efficiency--of computing inferences, of knowledge acquisition, etc. Horn-to-Horn belief revision could be of interest, in particular, as a component of any efficient system requiring large commonsense knowledge bases that may need revisions because, for example, new contradictory information is acquired. Recent results on belief revision for general logics show that the existence of a belief contraction operator satisfying the generalized AGM postulates is equivalent to the existence of a complement. Here we provide a first step towards efficient Horn-to-Horn belief revision, by characterizing the existence of a complement of a Horn consequence of a Horn knowledge base. A complement exists if and only if the Horn consequence is not the consequence of a modified knowledge base obtained from the original by an operation called body building. This characterization leads to the efficient construction of a complement whenever it exists.|Marina Langlois,Robert H. Sloan,Balázs Szörényi,György Turán","14293|IJCAI|1985|Taxonomic Reasoning|In formalizing knowledge for common sense reasoning, one often needs to partition some domain. An instance of this from the Blocks World is the statement \"All blocks are either held, on the table, or on another block.\" Although we can write this axiom in predicate calculus or in clause form for input to a theorem prover, such representations are highly space inefficient. In this paper we present a generalized clause form that allows for the compact representation of arbitrary partitions, along with a set of corresponding inference rules. Additionally, a theorem prover implementing these rules is described that demonstrates their utility with certain kinds of common sense rule bases.|Josh D. Tenenberg","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"],["16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","65538|AAAI|2005|SAT-Based versus CSP-Based Constraint Weighting for Satisfiability|Recent research has focused on bridging the gap between the satisfiability (SAT) and constraint satisfaction problem (CSP) formalisms. One approach has been to develop a many-valued SAT formula (MV-SAT) as an intermediate paradigm between SAT and CSP, and then to translate existing highly efficient SAT solvers to the MV-SAT domain. Experimental results have shown this approach can achieve significant improvements in performance compared with the traditional SAT and CSP approaches. In this paper, we follow a different route, developing SAT solvers that can automatically recognise CSP structure hidden in SAT encodings. This allows us to look more closely at how constraint weighting can be implemented in the SAT and CSP domains. Our experimental results show that a SAT-based approach to handle weights, together with CSP-based approach to variable instantiation, is superior to other combinations of SAT and CSP-based approaches. A further experiment on the round robin scheduling problem indicates that this many-valued constraint weighting approach outperforms other state-of-the-art solvers.|Duc Nghia Pham,John Thornton,Abdul Sattar,Abdelraouf Ishtaiwi","16007|IJCAI|2003|Backdoors To Typical Case Complexity|There has been significant recent progress in reasoning and constraint processing methods. In areas such as planning and finite model-checking, current solution techniques can handle combinatorial problems with up to a million variables and five million constraints. The good scaling behavior of these methods appears to defy what one would expect based on a worst-case complexity analysis. In order to bridge this gap between theory and practice, we propose a new framework for studying the complexity of these techniques on practical problem instances. In particular, our approach incorporates general structural properties observed in practical problem instances into the formal complexity analysis. We introduce a notion of \"backdoors\", which are small sets of variables that capture the overall combinatorics of the problem instance. We provide empirical results showing the existence of such backdoors in real-world problems. We then present a series of complexity results that explain the good scaling behavior of current reasoning and constraint methods observed on practical problem instances.|Ryan Williams,Carla P. Gomes,Bart Selman","14677|IJCAI|1989|A Comparison of ATMS and CSP Techniques|A fundamental problem for most AI problem solvers is how to control search to avoid searching subspaces which previously have been determined to be inconsistent. Two of the standard approaches to this problem are embodied in the constraint satisfaction problem (CSP) techniques which evolved from vision tasks and assumption-based truth maintenance system (ATMS) techniques which evolved from applying constraint propagation techniques to reasoning about the physical world. This paper argues that both approaches embody similar intuitions for avoiding thrashing and shows how CSPs can be mapped to ATMS problems and vice versa. In particular, Mackworth's notions of node, arc, and path consistency, Freuder's notion of it-consistency, and Dechter and Pearl's notion of directed K-consistency have precise analogs in the ATMS framework.|Johan de Kleer","65206|AAAI|2004|Collapsibility and Consistency in Quantified Constraint Satisfaction|The concept of consistency has pervaded studies of the constraint satisfiction problem. We introduce two concepts, which are inspired by consistency, for the more general framework of the quantified constraint satisfaction problem (QCSP). We use these concepts to derive, in a uniform fashion, proofs of polynomial-time tractability and corresponding algorithms for certain cases of the QCSP where the types of allowed relations are restricted. We not only unify existing tractability results and algorithms, but also identify new classes of tractable QCSPs.|Hubie Chen","15450|IJCAI|1999|A Comparison of Structural CSP Decomposition Methods|We compare tractable classes of constraint satisfaction problems (CSPs). We first give a uniform presentation of the major structural CSP decomposition methods. We then introduce a new class of tractable CSPs based on the concept of hypertree decomposition recently developed in Database Theory. We introduce a framework for comparing parametric decomposition-based methods according to tractability criteria and compare the most relevant methods. We show that the method of hypertree decomposition dominates the others in the case of general (nonbinary) CSPs.|Georg Gottlob,Nicola Leone,Francesco Scarcello","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","15481|IJCAI|1999|Constraint Propagation and Value Acquisition Why we should do it Interactively|In Constraint Satisfaction Problems (CSPs) values belonging to variable domains should be completely known before the constraint propagation process starts. In many applications, however, the acquisition of domain values is a computational expensive process or some domain values could not be available at the beginning of the computation. For this purpose, we introduce an Interactive Constraint Satisfaction Problem (ICSP) model as extension of the widely used CSP model. The variable domain values can be acquired when needed during the resolution process by means of Interactive Constraints, which retrieve (possibly consistent) information. Experimental results on randomly generated CSPs and for D object recognition show the effectiveness of the proposed approach.|Evelina Lamma,Paola Mello,Michela Milano,Rita Cucchiara,Marco Gavanelli,Massimo Piccardi","66156|AAAI|2007|Counting CSP Solutions Using Generalized XOR Constraints|We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming.|Carla P. Gomes,Willem Jan van Hoeve,Ashish Sabharwal,Bart Selman"],["15346|IJCAI|1997|Active Diagnosis by Self-Organization An Approach by The Immune Network Metaphor|We propose a concept of active diagnosis that differs from the conventional passive (i.e. event-driven) diagnosis in temporal (diagnosis is carried out by always monitoring normal condition as opposed to identifying faulty only when abnormal condition is detected) sense as well as spatial (diagnosis is carried out by agents distributed in the sensor network) sense. As one way of realizing active diagnosis, we present immunity-based agents approach based on the self creating, monitoring, and maintaining feature of immune systems. We apply the approach to process diagnosis where the agents are defined on the sensor network. Each agent corresponding to sensor or process constraint evaluates a kind of reliability by communicating other agents. System level recognition of sensorprocess fault can be attained by continuously and mutually monitoring and maintaining consistency among sensor values and process constraints.|Yoshiteru Ishida","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","15575|IJCAI|2001|A software architecture for dynamically generated adaptive Web stores|We provide technical details about the software and hardware architecture of SETA, a prototype toolkit for the creation of Web stores which personalize the interaction with customers. SETA is based on a multi-agent architecture and on the use of MAS technologies, which support a seamless communication among the system agents and an easy distribution of such agents on different computers.|Liliana Ardissono,Anna Goy,Giovanna Petrone,Marino Segnan","65174|AAAI|1994|SodaBot A Software Agent Environment and Construction System|This thesis presents em SodaBot, a general-purpose software agent user- environment and construction system. Its primary component is the em basic software agent --- a computational framework for building agents which is essentially an em agent operating system. We also present a new language for programming the basic software agent whose primitives are designed around human-level descriptions of agent activity. Via this programming language, em users can easily implement a wide-range of typical software agent applications, e.g. personal on-line assistants and meeting scheduling agents. The SodaBot system has been implemented and tested, and its description comprises the bulk of this thesis.|Michael H. Coen","65883|AAAI|2006|Behaviosites Manipulation of Multiagent System Behavior through Parasitic Infection|In this paper we present the Behaviosite Paradigm, a new approach to coordination and control of distributed agents in a multiagent system, inspired by biological parasites with behavior manipulation properties. Behaviosites are code modules that \"infect\" a system, attaching themselves to agents and altering the sensory activity and actions of those agents. These behavioral changes can be used to achieve altered, potentially improved, performance of the overall system thus, Behaviosites provide a mechanism for distributed control over a distributed system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it. To demonstrate our approach, we use behaviosites to control the behavior of a swarm of simple agents. With a relatively low infection rate, a few behaviosites can engender desired behavior over the swarm as a whole keeping it in one place, leading it through checkpoints, or moving the swarm from one stable equilibrium to another. We contrast behaviosites as a distributed swarm control mechanism with alternatives, such as the use of group leaders, herders, or social norms.|Amit Shabtay,Zinovi Rabinovich,Jeffrey S. Rosenschein","57148|GECCO|2003|Coevolution and Linear Genetic Programming for Visual Learning|In this paper, a novel genetically-inspired visual learning method is proposed. Given the training images, this general approach induces a sophisticated feature-based recognition system, by using cooperative coevolution and linear genetic programming for the procedural representation of feature extraction agents. The paper describes the learning algorithm and provides a firm rationale for its design. An extensive experimental evaluation, on the demanding real-world task of object recognition in synthetic aperture radar (SAR) imagery, shows the competitiveness of the proposed approach with human-designed recognition systems.|Krzysztof Krawiec,Bir Bhanu","15735|IJCAI|2001|Reflective Negotiating Agents for Real-Time Multisensor Target Tracking|In this paper we describe a multiagent system in which agents negotiate to allocate resources and satisfy constraints in a real-time environment of multisensor target tracking. The agents attempt to optimize the use of their own consumable resources while adhering to the global goal, i.e., accurate and effective multisensor target tracking. Agents negotiate based on different strategies which are selected and instantiated using case-based reasoning (CBR). Agents are also fully reflective in that they are aware of all their resources including system-level ones such as CPU allocation, and this allows them to achieve real-time behavior. We focus our discussion on multisensor target racking, case-based negotiation, and real-time behavior, and present experimental results comparing our methodology to ones using either no negotiation or using a static negotiation protocol.|Leen-Kiat Soh,Costas Tsatsoulis","57779|GECCO|2006|Adaption in distributed systems an evolutionary approach|There is a trend towards networked and distributed systems, complicating the design process of self-adaptive software. Logistics networks can be seen as a distributed system that have to adapt to requirements of companies and customers in a flexible and fast manner. When constructing and planning logistic networks different aspects of complexity have to be considered the number of stores, intermediate stores and transport entities that are required at every stage in a supply chain as well as the sufficient size of every store or transport entity. This paper presents an approach that simulates adaptive logistic networks using a multi-agent system (MAS) based on Evolutionary Computation (EC). Our approach uses fully decentralized operators for reproduction like mutation, recombination and selection, regulated by market mechanisms. The novelty of this approach lies in the decentralized bottom-up adaption method for decentralized systems and we use a logistic scenario as an example. Our proposed method is based on a formal model explaining how adaption occurs in the number and strategies of agents and thus of logistic networks. The implementation and experimental results are given to illustrate the expected outcomes.|Stephan Otto,Stefan Kirn","65187|AAAI|2004|Intelligent Systems Demonstration The Secure Wireless Agent Testbed SWAT|We will demonstrate the Secure Wireless Agent Testbed (SWAT), a unique facility developed at Drexel University to study integration, networking and information assurance for next-generation wireless mobile agent systems. SWAT is an implemented system that fully integrates ) mobile agents, ) wireless ad hoc multi-hop networks, and ) security. The demonstration will show the functionality of a number of decentralized agent-based applications, including applications for authentication, collaboration, messaging, and remote sensor monitoring. The demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen nodes (PDAs, tablet PCs, and laptops) and hundreds of mobile software agents.|Gustave Anderson,Andrew Burnheimer,Vincent A. Cicirello,David Dorsey,Saturnino Garcia,Moshe Kam,Joseph Kopena,Kris Malfettone,Andrew Mroczkowski,Gaurav Naik,Maxim Peysakhov,William C. Regli,Joshua Shaffer,Evan Sultanik,Kenneth Tsang,Leonardo Urbano,Kyle Usbeck,Jacob Warren","15526|IJCAI|1999|Be Patient and Tolerate Imprecision How Autonomous Agents can Coordinate Effectively|A decentralized multiagent system comprises agents who act autonomously based on local knowledge. Achieving coordination in such a system is nontrivial, hut is essential in most applications, where disjointed or incoherent behavior would be undesirable. Coordination in decentralized systems is a richer phenomenon than previously believed. In particular, five major attributes are crucial the extent of the local knowledge and choices of the member agents, the extent of their shared knowledge, the level of their inertia, and the level of precision of the required coordination. Interestingly, precision and inertia turn out to control the coordination process. They define different regions within each of which the other attributes relate nicely with coordination, but among which their relationships are altered or even reversed. Based on our study, we propose simple design rules to obtain coordinated behavior in decentralized multiagent systems.|Sudhir K. Rustogi,Munindar P. Singh"],["66878|AAAI|2010|Using Bisimulation for Policy Transfer in MDPs|Much of the work on using Markov Decision Processes (MDPs) in artificial intelligence (AI) focuses on solving a single problem. However, AI agents often exist over a long period of time, during which they may be required to solve several related tasks. This type of scenario has motivated a significant amount of recent research in knowledge transfer methods for MDPs. The idea is to allow an agent to continue to re-use the expertise accumulated while solving past tasks over its lifetime (see Taylor & Stone, , for a comprehensive survey).|Pablo Samuel Castro,Doina Precup","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","15471|IJCAI|1999|A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes|An issue that is critical for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or even infinite state spaces, traditional planning and reinforcement learning algorithms are often inapplicable, since their running time typically scales linearly with the state space size. In this paper we present a new algorithm that, given only a generative model (simulator) for an arbitrary MDP, performs near-optimal planning with a running time that has no dependence on the number of states. Although the running time is exponential in the horizon time (which depends only on the discount factor  and the desired degree of approximation to the optimal policy), our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large, unstructured MDPs. Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs KMN.|Michael J. Kearns,Yishay Mansour,Andrew Y. Ng","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","16446|IJCAI|2007|The Value of Observation for Monitoring Dynamic Systems|We consider the fundamental problem of monitoring (i.e. tracking) the belief state in a dynamic system, when the model is only approximately correct and when the initial belief state might be unknown. In this general setting where the model is (perhaps only slightly) mis-specified, monitoring (and consequently planning) may be impossible as errors might accumulate over time. We provide a new characterization, the value of observation, which allows us to bound the error accumulation. The value of observation is a parameter that governs how much information the observation provides. For instance, in Partially Observable MDPs when it is  the POMDP is an MDP while for an unobservable Markov Decision Process the parameter is . Thus, the new parameter characterizes a spectrum from MDPs to unobservable MDPs depending on the amount of information conveyed in the observations.|Eyal Even-Dar,Sham M. Kakade,Yishay Mansour","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","66182|AAAI|2007|Situated Conversational Agents|A Situated Conversational Agent (SCA) is an agent that engages in dialog about the context within which it is embedded. Situated dialog is characterized by its deep connection to the embedding context, and the precise cross-timing of linguistic and non-linguistic actions. This paper describes initial research into the construction of an SCA that engages in dialog about collaborative physical tasks, in which agents engage in dialog with the joint goal of manipulating the physical context in some manner. Constructing an SCA that can interact naturally in such tasks requires an agent with the ability to interleave planning, action, and observation while operating in a partially observable environment. Consequently, I propose to model an SCA as a Partially Observable Markov Decision Process (POMDP).|William Thompson","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|Håkan L. S. Younes","65240|AAAI|2004|Dynamic Programming for Partially Observable Stochastic Games|We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.|Eric A. Hansen,Daniel S. Bernstein,Shlomo Zilberstein"]]},"title":{"entropy":5.775400731832895,"topics":["natural language, the web, language for, from, language and, knowledge representation, information extraction, for recognition, semantic for, artificial intelligence, support vector, sense disambiguation, language, and recognition, answer set, semantic web, learning from, web search, the language, for and","algorithm for, genetic algorithm, genetic programming, genetic for, the problem, particle swarm, evolutionary algorithm, for problem, for optimization, the algorithm, using genetic, evolutionary for, neural networks, for the, solving problem, algorithm with, and algorithm, using algorithm, evolutionary, and genetic","for system, data base, system, for data, reinforcement learning, learning for, the system, model for, data, and system, and data, expert system, mobile robot, data management, database system, data system, efficient for, data streams, the data, for database","the and, for and, reasoning about, constraint satisfaction, for logic, and logic, logic programming, with and, logic, for planning, description logic, and its, and reasoning, planning with, logic programs, for reasoning, method for, reasoning, planning and, the logic","natural language, language for, language and, for recognition, and recognition, the language, artificial intelligence, for natural, representation and, representation for, and image, for and, the and, language, and natural, pattern recognition, speech understanding, and object, for image, activity recognition","for knowledge, knowledge and, sense disambiguation, between and, answer set, word sense, situation calculus, word disambiguation, knowledge representation, knowledge, the knowledge, the role, for disambiguation, using knowledge, the calculus, common sense, answer programming, and their, human-robot interaction, knowledge acquisition","genetic programming, genetic algorithm, genetic for, particle swarm, using genetic, programming for, using algorithm, and genetic, genetic with, using programming, estimation distribution, for using, and programming, using, estimation algorithm, distribution algorithm, using and, with programming, the genetic, the programming","algorithm for, for problem, genetic algorithm, algorithm problem, search for, local search, algorithm with, the algorithm, genetic for, solving problem, and algorithm, search algorithm, for the, search, heuristic search, new for, evolutionary algorithm, local for, using search, genetic problem","learning for, reinforcement learning, learning, learning and, for robot, mobile robot, learning system, the learning, learning classifier, learning with, for distributed, for mobile, robot control, learning using, learning networks, active for, for diagnosis, resource allocation, efficient learning, learning model","for system, the system, and system, system, database system, expert system, architecture for, data system, management system, system using, classifier system, distributed system, knowledge system, system with, for the, the architecture, base system, for real-time, artificial system, immune system","and logic, for logic, logic programming, framework for, the logic, description logic, for programs, logic programs, logic, for the, reasoning about, belief and, the and, belief revision, and programs, for belief, description and, programs, for and, for description","the and, for and, and its, and, and application, combinatorial auctions, its application, with application, and problem, phase transition, the application, for application, general for, rules for, for auctions, model and, with and, the its, the concept, for combinatorial"],"ranking":[["15525|IJCAI|1999|Learning in Natural Language|Statistics-based classifiers in natural language are developed typically by assuming a generative model for the data, estimating its parameters from training data and then using Bayes rule to obtain a classifier. For many problems the assumptions made by the generative models are evidently wrong, leaving open the question of why these approaches work. This paper presents a learning theory account of the major statistical approaches to learning in natural language. A class of Linear Statistical Queries (LSQ) hypotheses is defined and learning with it is shown to exhibit some robustness properties. Many statistical learners used in natural language, including naive Bayes, Markov Models and Maximum Entropy models are shown to be LSQ hypotheses, explaining the robustness of these predictors even when the underlying probabilistic assumptions do not hold. This coherent view of when and why learning approaches work in this context may help to develop better learning methods and an understanding of the role of learning in natural language inferences.|Dan Roth","16582|IJCAI|2007|On Natural Language Processing and Plan Recognition|The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR.|Christopher W. Geib,Mark Steedman","16737|IJCAI|2007|Open Information Extraction from the Web|Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a ,, Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of % on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER's ,, highest probability tuples, and show that they contain over ,, concrete facts and over ,, more abstract assertions.|Michele Banko,Michael J. Cafarella,Stephen Soderland,Matthew Broadhead,Oren Etzioni","15304|IJCAI|1995|Semantic Inference in Natural Language Validating a Tractable Approach|This paper is concerned with an inferential approach to information extraction reporting in particular on the results of an empirical study that was performed to validate the approach. The study brings together two lines of research () the RHO framework for tractable terminological knowledge representation and () the Alembic message understanding system. There are correspondingly two principal aspects of interest to this work From the knowledge representation perspective the present study serves to validate experimentally a normal form hypothesis that guarantees tractability of inference in the RHO framework. From the message processing perspective this study substantiates the utility of limited inference to information extraction.|Marc B. Vilain","14382|IJCAI|1987|Representation and Interpretation of Determiners in Natural Language|Following the principles of locality and compositionality during semantic interpretation, we propose a semantic representation formalism which manages to deal with reference problems, i.e. with the interpretation of NPs (in particular, of those beginning with an article) the addressed problems are generic  specific  class readings, and collective  distributive interpretations. The formalism follows the semantic net approach, and uses different representation plans (semantic, content and reference) and particular structures, called ambiguity spaces, that hide the ambiguities from the other parts of the sentence and remain neutral with respect to the various interpretations until certain disambiguation clues are found. Besides presenting the formalism, the paper discusses which clues may be used to disambiguate among the various interpretations and shows the way this representation is used to update the hearer's knowledge base.|Barbara Di Eugenio,Leonardo Lesmo","14433|IJCAI|1987|A Knowledge Framework for Natural Language Analysis|Recent research in language analysis and language generation has highlighted the role of knowledge representation in both processes. Certain knowledge representation foundations, such as structured inheritance networks and feature-based linguistic representations, have proved useful in a variety of language processing tasks. Augmentations to this common framework, however, are required to handle particular issues, such as the ROLE RELATIONSHIP problem the task of determining how roles, or slots, of a given frame, are filled based on knowledge about other roles. Three knowledge structures are discussed that address this problem. The semantic interpreter of an analyzer called TRUMP (TRansportable Understanding Mechanism Package) uses these structures to determine the fillers of roles effectively without requiring excessive specialized information about each frame.|Paul S. Jacobs","13340|IJCAI|1971|Pattern Recognition by Quasi-Linguistic Translation Into Artificial Noise-Resistant Language|A new approach to the recognition problem is considered, which does not require clustering of the sampling space into the appropriate number of regions. Each sample mapped in multi-dimensional space is represented by a family of points (vectors), forming a certain configuration. Less compact location of the points leads to more simple and reliable pattern recognition. The input description is represented by the \"text\" formed by a Bet of binary pulse sequences. The recognition implies that the text fragments of different space-temporal structure should be related to one of the different classes. The recognition is carried out as an informational transformation of the input text into another one, invariant of any small variations in the space-temporal structure of the input text. Sufficiently different texts are translated into non-identical output texts. Repeated transformation of the input text by a number of translators is accompanied by an increasing \"abstraction\" (stabilization) of the output text relative to the input text variations. As a result, the output text structure can be considered as a list of classes, to which the input text fragments belong. It is pointed out that the information compression characterizing the recognition process can not necessarily be carried out a priori but it can always be carried out as a consequence of the recognition process. A correspondence is implied between the features of the \"translation\" process and some neuro-physiological phenomena.|A. N. Radchenko","14126|IJCAI|1985|COMODEL A Language for the Representation of Technical Knowledge|The component oriented language COMODEL for the description of technical systems is presented. It represents technical knowledge by means of functions and geometric properties of components in a uniform way based on a small number of elementary concepts. Components together with interactions between components can be composed to aggregates, and a whole technical system is viewed as a big aggregate consisting of a hierarchy of subaggregates. Functions and geometric properties of aggregates can be derived from those of the subaggregates. Thus, a technical system can be modelled by a set of COMODEL expressions, and diagnosis and prognosis in the system may be performed by means of these expressions.|Werner Dilger,Jörg Kippe","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons"],["58756|GECCO|2009|Cheating for problem solving a genetic algorithm with social interactions|We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, i.e. animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.|Rafael Lahoz-Beltra,Gabriela Ochoa,Uwe Aickelin","57057|GECCO|2003|A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem|We propose a hybrid genetic algorithm for the hexagonal tortoise problem. We combined the genetic algorithm with an efficient local heuristic and aging mechanism. Another search heuristic which focuses on the space around existing solutions is also incorporated into the genetic algorithm. With the proposed algorithm, we could find the optimal solutions of up to a fairly large problem.|Heemahn Choe,Sung-Soon Choi,Byung Ro Moon","57139|GECCO|2003|Circuit Bipartitioning Using Genetic Algorithm|In this paper, we propose a hybrid genetic algorithm for partitioning a VLSI circuit graph into two disjoint graphs of minimum cut size. The algorithm includes a local optimization heuristic which is a modification of Fiduccia-Matheses algorithm. Using well-known benchmarks (including ACMSIGDA benchmarks), the combination of genetic algorithm and the local heuristic outperformed hMetis , a representative circuit partitioning algorithm.|Jong-Pil Kim,Byung Ro Moon","59100|GECCO|2010|Optimization of the hlder image descriptor using a genetic algorithm|Local image features can provide the basis for robust and invariant recognition of objects and scenes. Therefore, compact and distinctive representations of local shape and appearance has become invaluable in modern computer vision. In this work, we study a local descriptor based on the Hlder exponent, a measure of signal regularity. The proposal is to find an optimal number of dimensions for the descriptor using a genetic algorithm (GA). To guide the GA search, fitness is computed based on the performance of the descriptor when applied to standard region matching problems. This criterion is quantified using the F-Measure, derived from recall and precision analysis. Results show that it is possible to reduce the size of the canonical Hlder descriptor without degrading the quality of its performance. In fact, the best descriptor found through the GA search is nearly % smaller and achieves similar performance on standard tests.|Leonardo Trujillo,Pierrick Legrand,Gustavo Olague,Cynthia B. Pérez","57116|GECCO|2003|Designing A Hybrid Genetic Algorithm for the Linear Ordering Problem|The Linear Ordering Problem(LOP), which is a well-known NP-hard problem, has numerous applications in various fields. Using this problem as an example, we illustrate a general procedure of designing a hybrid genetic algorithm, which includes the selection of crossovermutation operators, accelerating the local search module and tuning the parameters. Experimental results show that our hybrid genetic algorithm outperforms all other existing exact and heuristic algorithms for this problem.|Gaofeng Huang,Andrew Lim","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57037|GECCO|2003|A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem|Recently proved successful for variants of the vehicle routing problem (VRP) involving time windows, genetic algorithms have not yet shown to compete or challenge current best search techniques in solving the classical capacitated VRP. In this paper, a hybrid genetic algorithm to address the capacitated vehicle routing problem is proposed. The basic scheme consists in concurrently evolving two populations of solutions to minimize total traveled distance using genetic operators combining variations of key concepts inspired from routing techniques and search strategies used for a time-variant of the problem to further provide search guidance while balancing intensification and diversification. Results from a computational experiment over common benchmark problems report the proposed approach to be very competitive with the best-known methods.|Jean Berger,Mohamed Barkaoui","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","79860|VLDB|1977|A Data Base Design Decision Support System|The task of physical data base design in an DBTG enviornment is examined. Generation of an internal schema which considers questions of storage versus access costs, efficient implementation of data relationships, efficient placement of data within the data base and allocation of primary and secondary storage is shown to be a formidable task. Current data base design aids are reviewed. One aid, a sophisicated mathematical model of DBTG data bases (Gerritsen ) is shown to be a potentially valuable tool. A limitation of this model is that no optimization algorithm other than total enumeration has been found. This paper proposes an implementation of the Gerritsen model. An interactive design tool, based upon the Gerritsen model, is discussed. A Data Base Design Decision Support System (DBD-DSS) for use by the DBA is developed. The objectives and structure of the DBD-DSS are examined. A comprehensive example illustrating both the user interface and the potential benefits of the interactive tool is presented.|Thomas J. Gambino,Rob Gerritsen","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80033|VLDB|1981|A Commercial Back-End Data Base System|Back-end Data base machines receive considerable attention owing to their facility for improving performance of installations with heavy data base activity. Many prototype systems have been described in the litterature but few systems are commercially available. This paper presents a commercial back-end data base management machine called MIX (design code), which is developped by our company. The global architecture of this data base machine is described, and the principal issue, discussed here in, applies to its software.|J. P. Armisen,J. Y. Caleca","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","79831|VLDB|1976|An Enduser Guidance Component for an Intelligent Data Base System|A general application guidance system -GAGS- was developed to convenient enduser component for data base systems. It has the following four main features . Dialogue supported methods dictionary guides the user from a given application problem to the adequate solution method or program. . Dialogue supported data dictionary gives syntactic and semantic information about the data available in data administration systems. . Dialogue supported execution facility offers a system triggered execution of programs found with the help of the methods dictionary in order to process data identified by means of the data dictionary. . Dialogue supported system guidance enables the unexperienced user to work with software systems without being a DP-professional. The basic information structure for the interactive guidance which is the common basic concept of the four facilities listed above, is the information network. It is generated by breaking down the knowledge about the application area in question into information elements (network nodes) which are interconnected (network arcs).|R. Erbe,Georg Walch"],["13947|IJCAI|1983|Logic Modelling of Cognitive Reasoning|Logic modelling is presented as an approach for exploring cognitive reasoning. The notion of mental construction and execution of propositional models is introduced. A model is constructed through inclusions and exclusions of assertions and assumptions about the task. A constructed model is executed in a logical control structure. Formal rules of inference are argued to be an essential feature of this architecture. A few examples are given for purpose of illustration.|Göran Hagert,\u2026ke Hansson","15791|IJCAI|2003|A Logic For Causal Reasoning|We introduce a logical formalism of irreflexive casual production relations that possesses both a standard monotonic semantics, and a natural nonmonotonic semantics. The formalism is shown to provide a complete characterization for the casual reasoning behind casual theories from McCain and Turner, . It is shown also that any causal relation is reducible to its Horn sub-relation with respect to the nonmonotonic semantics. We describe also a general correspondence between casual relations and abductive systems, which shows, in effect, that casual relations allow to express abductive reasoning. The results of the study seem to suggest causal production relations as a viable general framework for nonmonotonic reasoning.|Alexander Bochman","13934|IJCAI|1983|A Description and Reasoning of Plant Controllers in Temporal Logic|This paper describes the methodology to deal with the behavior of a dynamical system such as plant controllers in the framework of Temporal Logic. Many important concepts of the dynamical system like stability or observability are represented in this framework. As a reasoning method, we present an w -graph approach which enables us to represent the dynamical behavior of a given system, and an automatic synthesis of control rules can be reduced to a simple decision procedure on the w -graph. Moreover, the typical reasoning about the time-dependent system such as a causal argument or a qualitative simulation can be also treated on the w -graph in the same way.|Akira Fusaoka,Hirohisa Seki,Kazuko Takahashi","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16866|IJCAI|2009|A Logic for Reasoning about Counterfactual Emotions|The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. An example of this kind of emotions is regret. In order to meet this objective, we exploit the well-known STIT logic Belnap et al.,  Horty, . STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable. In this work we study a decidable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions.|Emiliano Lorini,François Schwarzentruber","15013|IJCAI|1993|On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning and Logic Programming|The purpose of this paper is to study the fundamental mechanism humans use in argumentation and its role in different major approaches to commonsense reasoning in AI and logic programming. We present three novel results We develop a theory for argumentation in which the acceptability of arguments is precisely defined. We show that logic programming and nonmonotonic reasoning in AI are different forms of argumentation. We show that argumentation can be viewed as a special form of logic programming with negation as failure. This result introduces a general method for generating metainterpreters for argumentation systems.|Phan Minh Dung","15759|IJCAI|2001|EPDL A Logic for Causal Reasoning|This paper presents an extended system EPDL of propositional dynamic logic by allowing a proposition as a modality for representing and specifying direct and indirect effects of actions in a unified logical structure. A set of causal logics based on the framework are proposed to model causal propagations through logical relevancy and iterated effects of causation. It is shown that these logics capture the basic properties of causal reasoning.|Dongmo Zhang,Norman Y. Foo","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","66699|AAAI|2010|Ontological Reasoning with F-logic Lite and its Extensions|This paper presents a novel architecture for collaborative multi-agent functional modeling in design on the semantic Web. The proposed architecture is composed of two visiting levels, i.e., local level and global level. The local level is an ontology-based functional modeling framework, which uses Web ontology language (OWL) to build a domain-specific local functional design ontology repository. Using this local ontology repository, the requests coming from the functional design agent can be parsed and performed effectively. The global level is a distributed multi-agent collaborative virtual environment, in which, OWL is used as a content language within the standard FIPA agent communication language (FIPA ACL) messages for describing ontologies, enabling diverse functional design ontologies between different agents to be communicated freely. The proposed architecture facilitates the exchange between diverse knowledge representation schemes in different functional modeling environments, and supports computer supported cooperative work (CSCW) between multiple functional design agents|Andrea Calì,Georg Gottlob,Michael Kifer,Thomas Lukasiewicz,Andreas Pieris"],["13693|IJCAI|1977|PRUF - A Language for the Representation of Meaning in Natural Languages|PRUF--an acronym for Possibilistic Relational Universal Fuzzy--is a designation for a novel type of synthetic language which is intended to serve as a target language for the representation of meaning of expressions in a natural language.|Lotfi A. Zadeh","15376|IJCAI|1997|Name-It Naming and Detecting Faces in Video by the Integration of Image and Natural Language Processing|We have been developing Name-It, a system that associates faces and names in news videos. First, as the only knowledge source, the system is given news videos which include image sequences and transcripts obtained from audio tracks or closed caption texts. The system can then either infer the name of a given face and output the name candidates, or can locate the faces in news videos by a name. To accomplish this task, the system extracts faces from image sequences and names from transcripts, both of which might correspond to key persons in news topics. The proposed system takes full advantage of advanced image and natural language processing. The image processing contributes to the extraction of face sequences which provide rich information for face-name association. The processing also helps to select the best frontal view of a face in a face sequence to enhance the face identification which is required for the processing. On the other hand, the natural language processing effectively extracts names by using lexicalgrammatical analysis and knowledge of the news video topics structure. The success of our experiments demonstrates the benefits of the advanced image and natural language processing methods and their incorporation.|Shin'ichi Satoh,Yuichi Nakamura,Takeo Kanade","16582|IJCAI|2007|On Natural Language Processing and Plan Recognition|The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR.|Christopher W. Geib,Mark Steedman","14382|IJCAI|1987|Representation and Interpretation of Determiners in Natural Language|Following the principles of locality and compositionality during semantic interpretation, we propose a semantic representation formalism which manages to deal with reference problems, i.e. with the interpretation of NPs (in particular, of those beginning with an article) the addressed problems are generic  specific  class readings, and collective  distributive interpretations. The formalism follows the semantic net approach, and uses different representation plans (semantic, content and reference) and particular structures, called ambiguity spaces, that hide the ambiguities from the other parts of the sentence and remain neutral with respect to the various interpretations until certain disambiguation clues are found. Besides presenting the formalism, the paper discusses which clues may be used to disambiguate among the various interpretations and shows the way this representation is used to update the hearer's knowledge base.|Barbara Di Eugenio,Leonardo Lesmo","13287|IJCAI|1969|A Conceptual Parser for Natural Language|This paper describes an operable automatic parser for natural language. It is a conceptual parser, concerned with determining the underlying meaning of the input utilizing a network of concepts explicating the beliefs inherent in a piece of discourse.|Roger C. Schank,Lawrence G. Tesler","13340|IJCAI|1971|Pattern Recognition by Quasi-Linguistic Translation Into Artificial Noise-Resistant Language|A new approach to the recognition problem is considered, which does not require clustering of the sampling space into the appropriate number of regions. Each sample mapped in multi-dimensional space is represented by a family of points (vectors), forming a certain configuration. Less compact location of the points leads to more simple and reliable pattern recognition. The input description is represented by the \"text\" formed by a Bet of binary pulse sequences. The recognition implies that the text fragments of different space-temporal structure should be related to one of the different classes. The recognition is carried out as an informational transformation of the input text into another one, invariant of any small variations in the space-temporal structure of the input text. Sufficiently different texts are translated into non-identical output texts. Repeated transformation of the input text by a number of translators is accompanied by an increasing \"abstraction\" (stabilization) of the output text relative to the input text variations. As a result, the output text structure can be considered as a list of classes, to which the input text fragments belong. It is pointed out that the information compression characterizing the recognition process can not necessarily be carried out a priori but it can always be carried out as a consequence of the recognition process. A correspondence is implied between the features of the \"translation\" process and some neuro-physiological phenomena.|A. N. Radchenko","14012|IJCAI|1983|Event Models for Recognition and Natural Language Description of Events in Real-World Image Sequences|For an adequate interpretation of image sequences it is not only necessary to recognize objects and object positions but also certain interesting temporal developments of the scene, called events. In this paper we discuss event models for traffic scenes as high-level conceptual structures which permit interfacing to an existing natural language dialogue system. Event models are declarative descriptions of classes of events organized around verbs of locomotion. They involve components which are directly related to the deep case structure of a corresponding natural language description. Event models may be used for bottom-up scene description as well as top-down question-answering. They may also incorporate expectations about a scene, thus providing an interface to experience and common sense.|Bernd Neumann,Hans-Joachim Novak","13955|IJCAI|1983|Generation in a Natural Language Interface|The PHRED (PHR asal English Diction) generator produces the natural language output of Berkeley's UNIX Consultant system (UC). The generator shares its knowledge base with the language analyzer PHRAN (PHRasal ANalyser). The parser and generator, together a component of UC's user interface, draw from a database of pattern-concept pairs where the basic unit of the linguistic patterns is the phrase. Both are designed to provide multilingual capabilities, to facilitate linguistic paraphrases, and to be adaptable to the individual user's vocabulary and knowledge. The generator affords extensibility,simplicity, and processing speed while performing the task of producing natural language utterances from conceptual representations using a large knowledge base. This paper describes the implementation of the phrasal generator and discusses the role of generation in a user-friendly natural language interface.|Paul S. Jacobs","13573|IJCAI|1977|NLG - Natural Language Graphics|The goal of the NLG project is to enhance person-computer interaction by using more than one mode of communication. The project uses a combination of natural language and graphics for both input and output. This allows development of practical, habitable systems suitable for users who are naive about programming, and encourages productive use of computers by more people. Mixed graphical and linguistic interaction could be an important tool in a variety of application areas.|David C. Brown,Stanley C. Kwasny,H. William Buttelmann,B. Chandrasekaran,Norman K. Sondheimer","13771|IJCAI|1981|Variable-Depth Natural Language Understanding|Standard A I representations of knowledge operate at fixed depth (i.e. the objects manipulated are described by an amount of information which remains constant for every task). Contrary to this approach, Variable Depth Processing (VDP) uses a progressive description of objects, tries different strategies according to the quality of the result it needs, and continually controls this quality by means of an evaluation of the approximations it makes. Contextual Production Rules are shown to be an effective way to implement some features of VDP. We are currently developing a VDP question - answering system which works on texts concerning a non-technical subject, namely an excerpt of a general public - oriented encyclopaedia.|Daniel Kayser,Daniel Coulon"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","15251|IJCAI|1995|Discourse as a Knowledge Resource for Sentence Disambiguation|A consistent text contains rich resources of information such as collocation patterns that can be used to resolve ambiguities within its sentences For example attachment ambiguities in a sentence can be resolved by selecting a candidate attachment that matches attachments found in other sentences in the same discourse Thus, discourse can be regarded as a valuable knowledge resource for sentence analysis In this paper, we examine some features of discourse as a knowledge resource and propose a framework for natural language processing that provides a simple algorithm for using information extracted from discourse together with information stored in knowledge bases The experimental results of using our framework to disambiguate sentences in technical documents offer good prospects for improving the accuracy of a broad coverage natural language processing system that handles various texts without constructing knowledge bases for each text in advance Some noteworthy features of discourse information are also deduced from the results of our experiments.|Tetsuya Nasukawa,Naohiko Uramoto","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","66344|AAAI|2008|AnalogySpace Reducing the Dimensionality of Common Sense Knowledge|We are interested in the problem of reasoning over very large common sense knowledge bases. When such a knowledge base contains noisy and subjective data, it is important to have a method for making rough conclusions based on similarities and tendencies, rather than absolute truth. We present Analogy Space, which accomplishes this by forming the analogical closure of a semantic network through dimensionality reduction. It self-organizes concepts around dimensions that can be seen as making distinctions such as \"good vs. bad\" or \"easy vs. hard\", and generalizes its knowledge by judging where concepts lie along these dimensions. An evaluation demonstrates that users often agree with the predicted knowledge, and that its accuracy is an improvement over previous techniques.|Robert Speer,Catherine Havasi,Henry Lieberman","15818|IJCAI|2003|Hierarchical Semantic Classification Word Sense Disambiguation with World Knowledge|We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \"world knowledge\". The model compiles knowledge contained in a dictionary-ontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \"hierarchical classifier\" outperforms a state-of-the-art standard \"flat\" one.|Massimiliano Ciaramita,Thomas Hofmann,Mark Johnson","15542|IJCAI|1999|Combining Weak Knowledge Sources for Sense Disambiguation|There has been a tradition of combining different knowledge sources in Artificial Intelligence research. We apply this methodology to word sense disambiguation (WSD), a long-standing problem in Computational Linguistics. We report on an implemented sense tagger which uses a machine readable dictionary to provide both a set of senses and associated forms of information on which to base disambiguation decisions. The system is based on an architecture which makes use of different sources of lexical knowledge in two ways and optimises their combination using a learning algorithm. Tested accuracy of our approach on a general corpus exceeds %, demonstrating the viability of allword disambiguation as opposed to restricting oneself to a small sample.|Mark Stevenson,Yorick Wilks","65932|AAAI|2006|A Unified Knowledge Based Approach for Sense Disambiguation and Semantic Role Labeling|In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our approach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.|Peter Z. Yeh,Bruce W. Porter,Ken Barker","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Kornél G. Markó,Stefan Schulz,Udo Hahn","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["58970|GECCO|2010|The estimation of hlderian regularity using genetic programming|This paper presents a Genetic Programming (GP) approach to synthesize estimators for the pointwise Hlder exponent in D signals. It is known that irregularities and singularities are the most salient and informative parts of a signal. Hence, explicitly measuring these variations can be important in various domains of signal processing. The pointwise Hlder exponent provides a characterization of these types of features. However, current methods for estimation cannot be considered to be optimal in any sense. Therefore, the goal of this work is to automatically synthesize operators that provide an estimation for the Hlderian regularity in a D signal. This goal is posed as an optimization problem in which we attempt to minimize the error between a prescribed regularity and the estimated regularity given by an image operator. The search for optimal estimators is then carried out using a GP algorithm. Experiments confirm that the GP-operators produce a good estimation of the Hlder exponent in images of multifractional Brownian motions. In fact, the evolved estimators significantly outperform a traditional method by as much as one order of magnitude. These results provide further empirical evidence that GP can solve difficult problems of applied mathematics.|Leonardo Trujillo,Pierrick Legrand,Jacques Lévy Véhel","57020|GECCO|2002|Hyperspectral Image Analysis Using Genetic Programming|Genetic programming is used to evolve mineral identification functions for hyperspectral images. The input image set comprises  images from different wavelengths ranging from nm (visible blue) to nm (invisible shortwave in the infrared), taken over Cuprite, Nevada, with the AVIRIS hyperspectral sensor. A composite mineral image indicating the overall reflectance percentage of three minerals (alunite, kaolnite, buddingtonite) is used as a reference or ''solution'' image. The training set is manually selected from this composite image, and results are cross-validated with the remaining image data not used for training. The task of the GP system is to evolve mineral identifiers, where each identifier is trained to identify one of the three mineral specimens. A number of different GP experiments were undertaken, which parameterized features such as thresholded mineral reflectance intensity and target GP language. The results are promising, especially for minerals with higher reflectance thresholds, which indicate more intense concentrations.|Brian J. Ross,Anthony G. Gualtieri,Frank Fueten,Paul Budkewitsch","15740|IJCAI|2001|Neural Logic Network Learning using Genetic Programming|Neural Logic Network or Neulonet is a hybrid of neural network expert systems. Its strength lies in its ability to learn and to represent human logic in decision making using component net rules. The technique originally employed in neulonet learning is backpropagation. However, the resulting weight adjustments will lead to a loss in the logic of the net rules. A new technique is now developed that allows the neulonet to learn by composing net rules using genetic programming. This paper presents experimental results to demonstrate this new and exciting capability in capturing human decision logic from examples. Comparisons will also be made between the use of net rules, and the use of standard boolean logic of negation, disjunction and conjunction in evolutionary computation.|Chew Lim Tan,Henry Wai Kit Chia","57703|GECCO|2006|On evolving buffer overflow attacks using genetic programming|In this work, we employed genetic programming to evolve a \"white hat\" attacker that is to say, we evolve variants of an attack with the objective of providing better detectors. Assuming a generic buffer overflow exploit, we evolve variants of the generic attack, with the objective of evading detection by signature-based methods. To do so, we pay particular attention to the formulation of an appropriate fitness function and partnering instruction set. Moreover, by making use of the intron behavior inherent in the genetic programming paradigm, we are able to explicitly obfuscate the true intent of the code. All the resulting attacks defeat the widely used 'Snort' Intrusion Detection System.|Hilmi Günes Kayacik,Malcolm I. Heywood,A. Nur Zincir-Heywood","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57053|GECCO|2003|Data Classification Using Genetic Parallel Programming|A novel Linear Genetic Programming (LGP) paradigm called Genetic Parallel Programming (GPP) has been proposed to evolve parallel programs based on a Multi-ALU Processor. It is found that GPP can evolve parallel programs for Data Classification problems. In this paper, five binary-class UCI Machine Learning Repository databases are used to test the effectiveness of the proposed GPP-classifier. The main advantages of employing GPP for data classification are ) speeding up evolutionary process by parallel hardware fitness evaluation and ) discovering parallel algorithms automatically. Experimental results show that the GPP-classifier evolves simple classification programs with good generalization performance. The accuracies of these evolved classifiers are comparable to other existing classification algorithms.|Sin Man Cheang,Kin-Hong Lee,Kwong-Sak Leung","58433|GECCO|2008|Combining cartesian genetic programming with an estimation of distribution algorithm|This paper describes initial testing of a novel idea to combine a CGP with an EDA. In recent work a new improved crossover technique was successfully applied to a CGP. To implement the new method meant changing the traditional CGP representation. The new representation developed in that work lends itself very nicely to some probability distribution being implemented. The work in this paper has investigated this idea of incoporating estimated probability distributions into the new CGP method with crossover.|Janet Clegg","58332|GECCO|2008|Fault tolerant control using Cartesian genetic programming|The paper focuses on the evolution of algorithms for control of a machine in the presence of sensor faults, using Cartesian Genetic Programming. The key challenges in creating training sets and a fitness function that encourage a general solution are discussed. The evolved algorithms are analysed and discussed. It was found that highly novel, mathematically elegant and hitherto unknown solutions were found.|Yoshikazu Hirayama,Tim Clarke,Julian Francis Miller"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","58259|GECCO|2008|Comparing genetic algorithm and guided local search methods by symmetric TSP instances|This paper aims at comparing Genetic Algorithm (GA) and Guided Local Search (GLS) methods so as to scrutinize their behaviors. Authors apply the GLS program with the Fast Local Search (FLS), developed at University of Essex, and implement a genetic algorithm with partially-mapped and order crossovers, reciprocal and inversion mutations, and rank and tournament selections in order to experiment with various Travelling Salesman Problems. The paper then ends up with two prominent conclusions regarding the performance of these meta-heuristic techniques over wide range of symmetric-TSP instances. First, the GLS-FLS strategy on the s-TSP instances yields the most promising performance in terms of the near-optimality and the mean CPU time. Second, the GA results are comparable to GLS-FLS outcomes on the same s-TSP instances. In the other word, the GA is able to generate near optimal solutions with some compromise in the CPU time.|Mehrdad Nojoumian,Divya K. Nair","66248|AAAI|2007|Automatic Algorithm Configuration Based on Local Search|The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variablevalue ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at httpwww.cs.ubc.calabsbetaProjectsParamILS.|Frank Hutter,Holger H. Hoos,Thomas Stützle","58756|GECCO|2009|Cheating for problem solving a genetic algorithm with social interactions|We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, i.e. animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.|Rafael Lahoz-Beltra,Gabriela Ochoa,Uwe Aickelin","58959|GECCO|2010|Efficient stochastic local search algorithm for solving the shortest common supersequence problem|The Shortest Common Supersequence (SCS) problem is a well-known hard combinatorial optimization problem that formalizes many real world problems. Recently, an application of the iterative optimization method called Prototype Optimization with Evolved Improvement Steps (POEMS) to the SCS problem has been proposed. The POEMS seeks the best variation of the current solution in each iteration. The variations, considered as structured hypermutations, are evolved by means of an evolutionary algorithm. This approach has been shown to work very well on synthetic as well as real biological data. However, the approach exhibited rather low scalability which is caused by very time demanding evaluation function. This paper proposes a new time efficient evaluation procedure and a new moving-window strategy for constructing and refining the supersequence. These two enhancements significantly improve an efficiency of the approach. Series of experiments with the modified POEMS method have been carried out. Results presented in this paper show that the method is competitive with current state-of-the-art algorithms for solving the SCS problem. Moreover, there is a potential for further improvement as discussed in the conclusions.|Jirí Kubalík","58009|GECCO|2007|A gestalt genetic algorithm less details for better search|The basic idea to defend in this paper is that an adequate perception of the search space, sacrificing most of the precision, can paradoxically accelerate the discovery of the most promising solution zones. While any search space can be observed at any scale according to the level of details, there is nothing inherent to the classical metaheuristics to naturally account for this multi-scaling. Nevertheless, the wider the search space the longer the time needed by any metaheuristic to discover and exploit the \"promising\" zones. Any possibility to compress this time is welcome. Abstracting the search space during the search is one such possibility. For instance, a common Ordering Genetic Algorithm (o-GA) is not well suited to efficiently resolve very large instances of the Traveling Salesman Problem (TSP). The mechanism presented here (reminiscent of Gestalt psychology) aims at abstracting the search space by substituting the variables of the problems with macro-versions of them. This substitution allows any given metaheuristic to tackle the problem at various scales or through different multi-resolution lenses. In the TSP problem to be treated here, the towns will simply be aggregated into regions and the metaheuristics will apply on this new one-level-up search space. The whole problem becomes now how to discover the most appropriate regions and to merge this discovery with the running of the o-GA at the new level.|Christophe Philemotte,Hugues Bersini","59115|GECCO|2010|Adaptive genetic algorithm using harmony search|Evolutionary algorithm is one of the major classes of stochastic search methods. This algorithm searches the problem space by exploring and exploiting the search space. The balance between exploration and exploitation will change throughout the search process. Maintaining the right balance between the exploration and exploitation in the search process is crucial for the success of the search process. The parameter values of the algorithm play a crucial role in determining the nature of the search, whether explorative or exploitative. In this paper, we propose an adaptive parameter controlling approach using harmony search. During the search process, harmony search directs the search from the current state to a desired state by determining suitable parameter values such that the balance between exploration and exploitation is suitable for that state transition. The preliminary results of the proposed method is comparable with those from the literature.|Farhad Nadi,Ahamad Tajudin Abdul Khader,Mohammed Azmi Al-Betar","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","16437|IJCAI|2007|GUNSAT A Greedy Local Search Algorithm for Unsatisfiability|Local search algorithms for satisfiability testing are still the best methods for a large number of problems, despite tremendous progresses observed on complete search algorithms over the last few years. However, their intrinsic limit does not allow them to address UNSAT problems. Ten years ago, this question challenged the community without any answer was it possible to use local search algorithm for UNSAT formulae We propose here a first approach addressing this issue, that can beat the best resolution-based completemethods. We define the landscape of the search by approximating the number of filtered clauses by resolution proof. Furthermore, we add high-level reasoning mechanism, based on Extended Resolution and Unit Propagation Look-Ahead to make this new and challenging approach possible. Our new algorithm also tends to be the first step on two other challenging problems obtaining short proofs for UNSAT problems and build a real local-search algorithm for QBF.|Gilles Audemard,Laurent Simon"],["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","66465|AAAI|2008|Spatial Scaffolding for Sociable Robot Learning|Spatial scaffolding is a naturally occurring human teaching behavior, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner. Robotic systems can take advantage of simple, highly reliable spatial scaffolding cues to learn from human teachers. We present an integrated robotic architecture that combines social attention and machine learning components to learn tasks effectively from natural spatial scaffolding interactions with human teachers. We evaluate the performance of this architecture in comparison to human learning data drawn from a novel study of the use of embodied cues in human task learning and teaching behavior. This evaluation provides quantitative evidence for the utility of spatial scaffolding to learning systems. In addition, this evaluation supported the construction of a novel, interactive demonstration of a humanoid robot taking advantage of spatial scaffolding cues to learn from natural human teaching behavior.|Cynthia Breazeal,Matt Berlin","13591|IJCAI|1977|Robot Learning and Error Correction|We describe a learning paradigm designed to improve the performance of a robot in a partially unpredictable environment. The paradigm was suggested by phenomena observed in animal behavior and it models aspects of that behavior. (See FI for details.) An implementation as a working program is under way, intended for incorporation in the now operational JPL robot. Rieger's CSA system is the implementation language and includes a plan synthesizer (RI). A brief overview of the robot's existing system organization is given in Thompson's paper on robot navigation in these proceedings (TI).|Leonard Friedman","65590|AAAI|2005|Online Resource Allocation Using Decompositional Reinforcement Learning|This paper considers a novel application domain for reinforcement learning that of \"autonomic computing,\" i.e. selfmanaging computing systems. RL is applied to an online resource allocation task in a distributed multi-application computing environment with independent time-varying load in each application. The task is to allocate servers in real time so as to maximize the sum of performance-based expected utility in each application. This task may be treated as a composite MDP, and to exploit the problem structure, a simple localized RL approach is proposed, with better scalability than previous approaches. The RL approach is tested in a realistic prototype data center comprising real servers, real HTTP requests, and realistic time-varying demand. This domain poses a number of major challenges associated with live training in a real system, including the need for rapid training, exploration that avoids excessive penalties, and handling complex, potentially non-Markovian system effects. The early results are encouraging in overnight training, RL performs as well as or slightly better than heavily researched model-based approaches derived from queuing theory.|Gerald Tesauro","15344|IJCAI|1997|Learning to Coordinate Controllers - Reinforcement Learning on a Control Basis|Autonomous robot systems operating in an uncertain environment have to be reactive and adaptive in order to cope with changing environment conditions and task requirements. To achieve this, the hybrid control architecture presented in this paper uses reinforcement learning on top of a Discrete Event Dynamic System (DEDS) framework to learn to supervise a set of basis controllers in order to achieve a given task. The use of an abstract system model in the automatically derived supervisor reduces the complexity of the learning problem. In addition, safety constraints may be imposed a priori, such that the system learns on-line in a single trial without the need for an outside teacher. To demonstrate the applicability of the approach, the architecture is used to learn a turning gait on a four legged robot platform.|Manfred Huber,Roderic A. Grupen","15798|IJCAI|2003|Simultaneous Adversarial Multi-Robot Learning|Multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. There has been a great deal of recent research on multiagent reinforcement learning in stochastic games, which is the intuitive extension of MDPs to multiple agents. This recent work, although general, has only been applied to small games with at most hundreds of states. On the other hand robot tasks have continuous, and often complex, state and action spaces. Robot learning tasks demand approximation and generalization techniques, which have only received extensive attention in single-agent learning. In this paper we introduce GraWoLF, a general-purpose, scalable, multiagent learning algorithm. It combines gradient-based policy learning techniques with the WoLF (\"Win or Learn Fast\") variable learning rate. We apply this algorithm to an adversarial multi-robot task with simultaneous learning. We show results of learning both in simulation and on the real robots. These results demonstrate that GraWoLF can learn successful policies, overcoming the many challenges in multi-robot learning.|Michael H. Bowling,Manuela M. Veloso","16402|IJCAI|2007|Color Learning on a Mobile Robot Towards Full Autonomy under Changing Illumination|A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a \"curriculum,\" an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map.|Mohan Sridharan,Peter Stone","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","66617|AAAI|2010|Control Model Learning for Whole-Body Mobile Manipulation|The proliferation of chip multiprocessors (CMPs) has led to the integration of large on-chip caches. For scalability reasons, a large on-chip cache is often divided into smaller banks that are interconnected through packet-based network-on-chip (NoC). With increasing number of cores and cache banks integrated on a single die, the on-chip network introduces significant communication latency and power consumption. In this paper, we propose a novel scheme that exploits frequent value compression to optimize the power and performance of NoC. Our experimental results show that the proposed scheme reduces the router power by up to .%, with CPI reduction as much as .% in our setting. Comparing to the recent zero pattern compression scheme, the frequent value scheme saves up to .% more router power and has up to .% more CPI reduction. Hardware design of the FV table and its overhead are also presented.|Scott Kuindersma","13710|IJCAI|1981|Learning of Sensory-Motor Schemas in a Mobile Robot|A learning system is described which was used to control a simple robot vehicle and to autonomously learn behaviour patterns. The system is loosely based on Becker's model of Intermediate Cognition.|Alan H. Bond,David H. Mott"],["79938|VLDB|1978|The ADD System An Architecture for Distributed Databases|A global logical architecture for distributed databases is presented in the form of a series of logical layers which, taken together, link the user to the physical database. Associated with each layer is a schema or view of the database at the appropriate level, and (in priciple) a processor which handles inter-layer transactions. Each layer is defined in terms of its functions, which are constrained to be dependent only upon adjacent layers. An example, is presented.|K. C. Toth,Samy A. Mahmoud,J. Spruce Riordon,O. Sherif","14398|IJCAI|1987|CHARADE A Rule System Learning System|Designed for an operational prospect, the CHARADE system automatically learns consistent rule systems from a description language, a set of axioms reflecting the language semantics and a set of examples. The technique advocated below is based on a \"generate and test\" mechanism where the description space is explored from the more general to the more specific descriptions. Rules and properties to be obtained are translated into exploration procedure constraints thanks to formalization of the learning set with two Boolean lattices.The underlying theoretical framework allows to both justify the heuristics conventionnaly used similarity based-learning and to introduce global properties to be satisfied by a rule system during its construction.|Jean-Gabriel Ganascia","13822|IJCAI|1981|Application Design Issues in Expert System Architecture|We describe an expert system that has been applied to the task of application design. Users supply the system with problem specifications, such as the required output data, and the system produces a graphic representation of the completed application in the form of a flow diagram. The application design task has forced us to consider two important issues in expert system architecture constraint processing and the explicit representation of control flow. The resulting knowledge representation and control logic are discussed.|Harry C. Reinstein,Janice S. Aikins","79800|VLDB|1975|Hardware and System Architecture for a Very Large Database|This paper describes practical experience in structuring a large amount of data (about  reels of -track  binary code decimal magnetic tapes of  census data). The hardware and software systems were given. The goal was to structure the database for online devices and to make the retrieval as efficient as possible. This paper explains only the database architecture finally chosen.|Robert Healey,Bradford Heckman","14182|IJCAI|1985|Model Expert System MES|The comprehensive oil log interpretation is a procedure of numerical calculation and logical (or plausible) inference. In other words, the results of preliminary inference will provide a basis for the choice of parameters of calculation formula, and then the results of numerical calculation will be used as evidences for further inference, and so on and so forth. In view of this situation, we have designed a model expert system (tool system) MES and established a comprehensive oil log interpretation expert system WELIES based on the system MES. In MES, we introduced the concept of \"TASK\" to make numerical calculation intergrated with logical (or plausible) inference during the task solution. The execution of MES is a procedure of task solution. Ways of solving a task are defined by Task Transformation Rules. The Knowledge Representation is divided into two parts ) Task Transformation Rules, including Task Decomposition Rules and Task Derivation Rules ) Inference Rules, including Production Rules and Procedure Rules. We adopt DempsterShafer's Evidence Theory  to express uncertainty.|Guan Jiwen,Xu Ying,Chang Minche,Zhao Jizhi","65075|AAAI|1987|A Multiprocessor Architecture for Production System Matching|This paper presents a new, highly parallel algorithm for OPS production system matching, and a multiprocessor architecture to support it. The algorithm is based on a partitioning of the Rete algorithm at the comparison level, suitable for execution on an array of several hundred processing elements. The architecture' provides an execution environment which optimizes the algorithm's performance. Analysis of existing production systems and results of simulations indicate that an increase in match speed of two orders of magnitude or more over current implementations is possible.|Michael A. Kelly,Rudolph E. Seviora","57032|GECCO|2003|Artificial Immune System for Classification of Gene Expression Data|DNA microarray experiments generate thousands of gene expression measurement simultaneously. Analyzing the difference of gene expression in cell and tissue samples is useful in diagnosis of disease. This paper presents an Artificial Immune System for classifying microarray-monitored data. The system evolutionarily selects important features and optimizes their weights to derive classification rules. This system was applied to two datasets of cancerous cells and tissues. The primary result found few classification rules which correctly classified all the test samples and gave some interesting implications for feature selection.|Shin Ando,Hitoshi Iba","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","65585|AAAI|2005|The TaskTracker System|Knowledge workers spend the majority of their working hours processing and manipulating information. These users face continual costs as they switch between tasks to retrieve and create information. The TaskTracer project at Oregon State University investigates the possibilities of a desktop software system that will record in detail how knowledge workers complete tasks, and intelligently leverage that information to increase efficiency and productivity. Our approach assigns each observed user interface action to a task for which it is likely being performed. In this demonstration we show how we have applied machine learning in this environment.|Simone Stumpf,Xinlong Bao,Anton N. Dragunov,Thomas G. Dietterich,Jonathan L. Herlocker,Kevin Johnsrude,Lida Li,Jianqiang Shen","14121|IJCAI|1985|The Architecture of the FAIM- Symbolic Multiprocessing System|The FAIM -  is an ultra-concurrent symbolic multiprocessor which attempts to significantly improve the performance of AI systems. The system includes a language in which concurrent AI application programs can be written, a machine which provides direct hardware support for the language, and a resource allocation mechanism which maps programs onto the machine in order to exploit the program's concurrency in an efficient manner at run-time. The paper provides a brief synopsis of the nature of the language and resource allocation mechanism, but is primarily concerned with the description of the physical architecture of the machine. The architecture is consistent with high performance VLSI implementation and packaging technology, and is easily extended to include arbitrary numbers of processors.|Alan L. Davis,Shane V. Robison"],["80112|VLDB|1985|Functional Dependencies in Logic Programs|When logic programming is used for database access, there is a need to improve the backtracking behaviour of the interpreter. Rather than putting on the programmer the onus of using extra-logical operators such as cut to improve performance, we show that some uses of the cut can be automated by inferring them from functional dependencies. This requires some knowledge of which variables are guaranteed to be bound at query execution time we give a method for deriving such information using data flow analysis.|Alberto O. Mendelzon","15728|IJCAI|2001|A Comparative Study of Logic Programs with Preference|We are interested in semantical underpinnings for existing approaches to preference handling in extended logic programming (within the framework of answer set programming). As a starting point, we explore three different approaches that have been recently proposed in the literature. Because these approaches use rather different formal means, we furnish a series of uniform characterizations that allow us to gain insights into the relationships among these approaches. To be more precise, we provide different characterizations in terms of (i) fixpoints, (ii) order preservation, and (iii) translations into standard logic programs. While the two former provide semantics for logic programming with preference information, the latter furnishes implementation techniques for these approaches.|Torsten Schaub,Kewen Wang","65268|AAAI|2004|Logic Programs with Abstract Constraint Atoms|We propose and study extensions of logic programming with constraints represented as generalized atoms of the form C(X), where X is a finite set of atoms and C is an abstract constraint (formally, a collection of sets of atoms). Atoms C(X) are satisfied by an interpretation (set of atoms) M, if M  X  C. We focus here on monotone constraints, that is, those collections C that are closed under the superset. They include, in particular, weight (or pseudo-boolean) constraints studied both by the logic programming and SAT communities. We show that key concepts of the theory of normal logic programs such as the one-step provability operator, the semantics of supported and stable models, as well as several of their properties including complexity results, can be lifted to such case.|Victor W. Marek,Miroslaw Truszczynski","13879|IJCAI|1983|Integrating Logic Programs and Schemata|(orn clauseSchema Representation Language) is the result of an athor to combine the- tools of logic program-niinp. and schema based knowledge represention into a single hybrid system. knowledge, compressed in schemala can be accessed during the execution of logic programs, and the retrieval of the values of a SLOT in a schema can involve- the execution of logic programs that attempt. to declare the- values prior to presenting- to inheritence, should the slot be empty. ISRI. supports the implementation of programs that take advantage of the best controles. of the logical and objec oriented approaches to knowledge represenlation.|Bradley P. Allen,J. Mark Wright","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","15628|IJCAI|2001|A Framework for Declarative Update Specifications in Logic Programs|Recently, several approaches for updating knowledge bases represented as logic programs have been proposed. In this paper, we present a generic framework for declarative specifications of update policies, which is built upon such approaches. It extends the LUPS language for update specifications and incorporates the notion of events into the framework. An update policy allows an agent to flexibly react upon new information, arriving as an event, and perform suitable changes of its knowledge base. The framework compiles update policies to logic programs by means of generic translations, and can be instantiated in terms of different concrete update approaches. It thus provides a flexible tool for designing adaptive reasoning agents.|Thomas Eiter,Michael Fink,Giuliana Sabbatini,Hans Tompits","15653|IJCAI|2001|Ontology Reasoning in the SHOQD Description Logic|Ontologies are set to play a key rle in the \"Semantic Web\" by providing a source of shared and precisely defined terms that can be used in descriptions of web resources. Reasoning over such descriptions will be essential if web resources are to be more accessible to automated processes. SHOQ(D) is an expressive description logic equipped with named individuals and concrete datatypes which has almost exactly the same expressive power as the latest web ontology languages (e.g., OIL and DAML). We present sound and complete reasoning services for this logic.|Ian Horrocks,Ulrike Sattler","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","16108|IJCAI|2005|Strong Equivalence for Logic Programs with Preferences|Recently, strong equivalence for Answer Set Programming has been studied intensively, and was shown to be beneficial for modular programming and automated optimization. In this paper we define the novel notion of strong equivalence for logic programs with preferences. Based on this definition we give, for several semantics for preference handling, necessary and sufficient conditions for programs to be strongly equivalent. These results provide a clear picture of the relationship of these semantics with respect to strong equivalence, which differs considerably from their relationship with respect to answer sets. Finally, based on these results, we present for the first time simplification methods for logic programs with preferences.|Wolfgang Faber,Kathrin Konczak","13907|IJCAI|1983|AND Parallelism in Logic Programs|An interpreter for logic programs is defined which executes some goals in parallel. OR parallelism exploits the parallelism defined from nondeterministic choices, and is essentially a replacement for backtracking. AND parallelism comes from solving goals in the body of a single clause in parallel, and is the only way to exploit parallelism in deterministic functions written as logic programs. A unique feature of our model is that it allows both forms of parallelism for the same computation.|John S. Conery,Dennis F. Kibler"],["65868|AAAI|2006|Expressive Commerce and Its Application to Sourcing|Sourcing professionals buy several trillion dollars worth of goods and services yearly. We introduced a new paradigm called expressive commerce and applied it to sourcing. It combines the advantages of highly expressive human negotiation with the advantages of electronic reverse auctions. The idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auctions, and are algorithmically cleared. This creates a Pareto efficiency improvement in the allocation (a win-win between the buyer and the sellers) but the market clearing problem is a highly complex combinatorial optimization problem. We developed the world's fastest tree search algorithms for solving it. We have hosted $ billion of sourcing using the technology, and created $. billion of hard-dollar savings. The suppliers also benefited by being able to express production efficiencies and creativity, and through exposure problem removal. Supply networks were redesigned, with quantitative understanding of the tradeoffs, and implemented in weeks instead of months.|Tuomas Sandholm","57253|GECCO|2003|A Specialized Island Model and Its Application in Multiobjective Optimization|This paper discusses a new model of parallel evolutionary algorithms (EAs) called the specialized island model (SIM) that can be used to generate a set of diverse non-dominated solutions to multiobjective optimization problems. This model is derived from the island model, in which an EA is divided into several subEAs that exchange individuals among them. In SIM, each subEA is responsible (i.e., specialized) for optimizing a subset of the objective functions in the original problem. The efficacy of SIM is demonstrated using a three-objective optimization problem. Seven scenarios of the model with a different number of subEAs, communication topology, and specialization are tested, and their results are compared. The results suggest that SIM effectively finds non-dominated solutions to multiobjective optimization problems.|Ningchuan Xiao,Marc P. Armstrong","15560|IJCAI|1999|The LPSAT Engine  Its Application to Resource Planning|Compilation to boolean satisfiability has become a powerful paradigm for solving AI problems. However, domains that require metric reasoning cannot be compiled efficiently to SAT even if they would otherwise benefit from compilation. We address this problem by introducing the LCNF representation which combines propositional logic with metric constraints. We present LPSAT, an engine which solves LCNF problems by interleaving calls to an incremental simplex algorithm with systematic satisfaction methods. We describe a compiler which converts metric resource planning problems into LCNF for processing by LPSAT. The experimental section of the paper explores several optimizations to LPSAT, including learning from constraint failure and randomized cutoffs.|Steven A. Wolfman,Daniel S. Weld","66030|AAAI|2007|A Randomized String Kernel and Its Application to RNA Interference|String kernels directly model sequence similarities without the necessity of extracting numerical features in a vector space. Since they better capture complex traits in the sequences, string kernels often achieve better prediction performance. RNA interference is an important biological mechanism with many therapeutical applications, where strings can be used to represent target messenger RNAs and initiating short RNAs and string kernels can be applied for learning and prediction. However, existing string kernels are not particularly developed for RNA applications. Moreover, most existing string kernels are n-gram based and suffer from high dimensionality and inability of preserving subsequence orderings. We propose a randomized string kernel for use with support vector regression with a purpose of better predicting silencing efficacy scores for the candidate sequences and eventually improving the efficiency of biological experiments. We show the positive definiteness of this kernel and give an analysis of randomization error rates. Empirical results on biological data demonstrate that the proposed kernel performed better than existing string kernels and achieved significant improvements over kernels computed from numerical descriptors extracted according to structural and thermodynamic rules. In addition, it is computationally more efficient.|Shibin Qiu,Terran Lane,Ljubomir J. Buturovic","66299|AAAI|2008|A General Method for Reducing the Complexity of Relational Inference and its Application to MCMC|Many real-world problems are characterized by complex relational structure, which can be succinctly represented in first-order logic. However, many relational inference algorithms proceed by first fully instantiating the first-order theory and then working at the propositional level. The applicability of such approaches is severely limited by the exponential time and memory cost of propositionalization. Singla and Domingos () addressed this by developing a \"lazy\" version of the WalkSAT algorithm, which grounds atoms and clauses only as needed. In this paper we generalize their ideas to a much broader class of algorithms, including other types of SAT solvers and probabilistic inference methods like MCMC. Lazy inference is potentially applicable whenever variables and functions have default values (i.e., a value that is much more frequent than the others). In relational domains, the default is false for atoms and true for clauses. We illustrate our framework by applying it to MC-SAT, a state-of-the-art MCMC algorithm. Experiments on a number of real-world domains show that lazy inference reduces both space and time by several orders of magnitude, making probabilistic relational inference applicable in previously infeasible domains.|Hoifung Poon,Pedro Domingos,Marc Sumner","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","14637|IJCAI|1989|A Computational Framework for Granularity and its Application to Educational Diagnosis|Many artificial intelligence systems implicitly use notions of granularity in reasoning, but there is very little research into granularity itself. An exception is the work of Hobbs , which outlines several characteristics of granularity. In this paper we describe an approach to representing granularity which formalizes in computational terms most of Hobbs' notions, often refining and extending them. In particular two types of granularity have been delineated aggregation and abstraction. Objects can be described at various grain sizes and connected together into a granularity hierarchy which allows focus shifts along either aggregation or abstraction dimensions. We briefly discuss how we have used granularity hierarchies in the recognition of novice LISP programming strategies and show how this enhances the recognition process and can lead toward planning appropriate feedback for the student.|Jim E. Greer,Gordon I. McCalla","14184|IJCAI|1985|Type Inference in Prolog and Its Application|In this paper we present a type inference method for Prolog programs. The new idea is to describe a superset of the success set by associating a type substitution (an assignment of sets of ground terms to variables) with each head of definite clause. This approach not only conforms to the style of definition inherent to Prolog but also gives some accuracy to the types infered. We show the basic computation method of the superset by sequential approximation as well as an incremental method to utilize already obtained results. We also show its application to verification of Prolog programs.|Tadashi Kanamori,Kenji Horiuchi","14975|IJCAI|1991|An Augmented EBL and its Application to the Utility Problem|EBL can learn justified generalizations from only one example when the domain theory is perfect. However, it does not work when the domain theory is imperfect. Imperfectness of the domain theory can be classified into four levels, i.e. incomplete, intractable, inconsistent and non-operational ones. It is necessary to unify EBL and SBL to solve these problems. In this paper, we propose a framework of an augmented EBL to handle plural examples simultaneously. We formalize it on logic program, and introduce a concept of least EBG to extract similarities from plural examples. We discuss on an approach to solve utility problem with the augmented EBL. Utility problem is a problem to learn more efficient description under complete, tractable, consistent but nonoperational domain theories. We define operationality criteria with maximizing usage degree and minimizing backtracking number, and show they increase partial monotonically by generalization. Since this partial monotinicity is not preferable to search operational generalizations, least EBGs are more operational than usual EBGs. We design a simple incremental learner based on least EBGs, and show its usefulness in recursive domain theories. We also discuss on other imperfect theory problems.|Masayuki Yamamura,Shigenobu Kobayashi","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cedeño"]]}}