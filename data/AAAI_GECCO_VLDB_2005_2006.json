{"abstract":{"entropy":6.60366313549707,"topics":["systems, data, data mining, data stream, recent years, data management, xml data, search engine, growing interest, systems complex, mapping schema, efficient xml, web, materialized views, database, description logic, time series, query processing, wide range, business bp-ql","genetic algorithms, evolutionary algorithms, algorithms, present algorithms, particle swarm, optimization problem, algorithms problem, algorithms search, solving problem, present approach, problem, evolutionary computation, algorithms optimization, optimization, present novel, evolutionary problem, estimation distribution, ant colony, algorithms solving, particle pso","markov decision, artificial intelligence, agents, markov processes, decision processes, partially observable, planning domains, neural network, consider problem, processes mdps, based reasoning, markov mdps, sensor network, actions effects, decision mdps, cases reasoning, systems agents, bayesian network, aggregation voting, situations calculus","genetic programming, machine learning, cartesian genetic, genetic algorithms, learning classifier, classifier xcs, differential evolution, programming cartesian, fitness function, evolution strategies, embedded cartesian, classifier systems, negative selection, genetic evolve, symbolic regression, fitness landscape, spanning tree, activity recognition, support vector, best query","data, data stream, systems data, data management, management systems, xml data, stream systems, mapping schema, queries data, support data, schema matching, processing stream, information integration, stream applications, data model, continuous queries, emerging queries, stream management, applications emerging, systems matching","data mining, search engine, large data, web, web information, web services, knowledge base, current systems, web search, web systems, information retrieval, relational database, web pages, semantic web, knowledge systems, data web, large database, web database, knowledge, large","evolutionary algorithms, evolutionary computation, hybrid algorithms, ant colony, testing test, evolutionary solutions, algorithms eas, evolutionary optimization, evolutionary eas, algorithms population, hybrid genetic, optimization emo, algorithms test, multiobjective optimization, important optimization, genetic population, evolutionary emo, algorithms important, evolutionary first, algorithms first","problem, evolutionary problem, optimization problem, solving problem, evolutionary search, multi-objective problem, search heuristics, multi-objective evolutionary, algorithms problem, multi-objective optimization, approach evolutionary, solve problem, approach problem, combinatorial problem, paper approach, search, search problem, approach, constraint problem, multi-objective algorithms","based reasoning, cases reasoning, study problem, reasoning, problem different, study agents, world reasoning, different, study, model different, model, time, representation, state, utility, world, logic, good, deal, cognitive","partially observable, systems agents, learning systems, group agents, autonomous robots, recent sat, multi-agent agents, agents, autonomous robotics, multi-agent systems, teams robots, recent research, agents architecture, agents multiagent, distributed agents, robots, agents users, robots need, autonomous agents, goal agents","fitness function, fitness landscape, genetic fitness, algorithms fitness, different fitness, model landscape, fitness, store data, function, function approximation, solutions fitness, genetic sequence, algorithms function, genetic function, introduce algorithms, introduce, function problem, introduce genetic, genetic algorithms, scheme","genetic programming, genetic algorithms, cartesian genetic, programming cartesian, embedded cartesian, genetic evolve, paper genetic, problem programming, graph cartesian, graph genetic, genetic based, genetic used, embedded genetic, applications genetic, extension cartesian, introduce programming, based cartesian, used, present genetic, programming algorithms"],"ranking":[["80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","65623|AAAI|2006|Building Semantic Mappings from Databases to Ontologies|A recent special issue of AI Magazine (AAAI ) was dedicated to the topic of semantic integration -- the problem of sharing data across disparate sources. At the core of the solution lies the discovery the \"semantics\" of different data sources. Ideally, the semantics of data are captured by a formal ontology of the domain together with a semantic mapping connecting the schema describing the data to the ontology. However, establishing the semantic mapping from a database schema to a formal ontology in terms of formal logic expressions is inherently difficult to automate, so the task was left to humans. In this paper, we report on our study (An, Borgida, & Mylopoulos a b) of a semi-automatic tool, called MAPONTO, that assists users to discover plausible semantic relationships between a database schema (relational or XML) and an ontology, expressing them as logical formulasrules.|Yuan An,John Mylopoulos,Alexander Borgida","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Cal√¨,Michael Kifer","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80481|VLDB|2005|Database Publication Practices|There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.|Philip A. Bernstein,David J. DeWitt,Andreas Heuer,Zachary G. Ives,Christian S. Jensen,Holger Meyer,M. Tamer √\u2013zsu,Richard T. Snodgrass,Kyu-Young Whang,Jennifer Widom","80654|VLDB|2006|InteMon Intelligent System Monitoring on Large Clusters|InteMon is a prototype monitoring and mining system for large clusters. Currently, it monitors over  hosts of a prototype data center at CMU. It uses the SNMP protocol and it stores the monitoring data in an mySQL database. Then, it allows for visualization of the time-series data using a JSP web-based frontend interface for users.What sets it apart from other cluster monitoring systems is its ability to automatically analyze the monitoring data in real time and alert the users for potential anomalies. It uses state of the art stream mining methods, it has a sophisticated definition of anomalies (broken correlations among input streams), and it can also pinpoint the reason of the anomaly. InteMon has a user-friendly GUI, it allows the users to perform interactive mining tasks, and it is fully operational.|Evan Hoke,Jimeng Sun,Christos Faloutsos","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["57678|GECCO|2006|Exploring network topology evolution through evolutionary computations|We present an evolutionary methodology that explores the evolution of network topology when a uniform growth of the network traffic is considered. The network redesign problem is formulated as an optimization problem, subject to a set of design and performance constraints, while minimizing the redesign cost by maintaining as many as possible of the network devices that constitute the original topology. The experimental results for a -level network redesign problem (consisting of  client nodes) demonstrate the value of the search technique within the genetic algorithms in finding good solutions with respect to redesign cost and time.|Sami J. Habib,Alice C. Parker","57405|GECCO|2005|Theoretical analysis of a mutation-based evolutionary algorithm for a tracking problem in the lattice|Evolutionary algorithms are often applied for solving optimization problems that are too complex or different from classical problems so that the application of classical methods is difficult. One example are dynamic problems that change with time. An important class of dynamic problems is the class of tracking problems where an algorithm has to find an approximately optimal solution and insure an almost constant quality in spite of the changing problem. For the application of evolutionary algorithms to static optimization problems, the distribution of the optimization time and most often its expected value are most important. Adopting this perspective a simple tracking problem in the lattice is considered and the performance of a mutation-based evolutionary algorithm is evaluated. For the static case, asymptotically tight upper and lower bounds are proven. These results are applied to derive results on the tracking performance for different rates of change.|Thomas Jansen,Ulf Schellbach","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","57352|GECCO|2005|Ant colony optimization for power plant maintenance scheduling optimization|In order to maintain a reliable and economic electric power supply, the maintenance of power plants is becoming increasingly important. In this paper, a formulation that enables ant colony optimization (ACO) algorithms to be applied to the power plant maintenance scheduling optimization (PPMSO) problem is developed and tested on a -unit case study. A heuristic formulation is introduced and its effectiveness in solving the problem is investigated. The performance of two different ACO algorithms is compared, including Best Ant System (BAS) and Max-Min Ant System (MMAS), and a detailed sensitivity analysis is conducted on the parameters controlling the searching behavior of ACO algorithms. The results obtained indicate that the performance of the two ACO algorithms investigated is significantly better than that of a number of other metaheuristics, such as genetic algorithms and simulated annealing, which have been applied to the same case study previously. In addition, use of the heuristics significantly improves algorithm performance. Also, ACO is found to have similar performance for the case study considered across an identified range of parameter values.|Wai-Kuan Foong,Holger R. Maier,Angus R. Simpson","57584|GECCO|2005|MRI magnet design search space analysis EDAs and a real-world problem with significant dependencies|This paper introduces the design of superconductive magnet configurations in Magnetic Resonance Imaging (MRI) systems as a challenging real-world problem for Evolutionary Algorithms (EAs). Analysis of the problem structure is conducted using a general statistical method, which could be easily applied to other problems. The results suggest that the problem is highly multimodal and likely to present a significant challenge for many algorithms. Through a series of preliminary experiments, a continuous Estimation of Distribution Algorithm (EDA) is shown to be able to generate promising designs with a small computational effort. The importance of utilizing problem-specific knowledge and the ability of an algorithm to capture dependencies in solving complex real-world problems is also highlighted.|Bo Yuan,Marcus Gallagher,Stuart Crozier","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. G√ºrgen"],["57721|GECCO|2006|Genetic algorithms for action set selection across domains a demonstration|Action set selection in Markov Decision Processes (MDPs) is an area of research that has received little attention. On the other hand, the set of actions available to an MDP agent can have a significant impact on the ability of the agent to gain optimal rewards. Last year at GECCO', the first automated action set selection tool powered by genetic algorithms was presented. The demonstration of its capabilities, though intriguing, was limited to a single domain. In this paper, we apply the tool to a more challenging problem of oil sand image interpretation. In the new experiments, genetic algorithms evolved a compact high-performance set of image processing operators, decreasing interpretation time by % while improving image interpretation accuracy by %. These results exceed the original performance and suggest certain cross-domain portability of the approach.|Greg Lee,Vadim Bulitko","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,Fran√ßois Charpillet","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aur√©lie Beynier,Abdel-Illah Mouaddib","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong","65864|AAAI|2006|Targeting Specific Distributions of Trajectories in MDPs|We define TTD-MDPs, a novel class of Markov decision processes where the traditional goal of an agent is changed from finding an optimal trajectory through a state space to realizing a specified distribution of trajectories through the space. After motivating this formulation, we show how to convert a traditional MDP into a TTD-MDP. We derive an algorithm for finding non-deterministic policies by constructing a trajectory tree that allows us to compute locally-consistent policies. We specify the necessary conditions for solving the problem exactly and present a heuristic algorithm for constructing policies when an exact answer is impossible or impractical. We present empirical results for our algorithm in two domains a synthetic grid world and stories in an interactive drama or game.|David L. Roberts,Mark J. Nelson,Charles Lee Isbell Jr.,Michael Mateas,Michael L. Littman"],["57472|GECCO|2005|Extraction of informative genes from microarray data|Identification of those genes that might anticipate the clinical behavior of different types of cancers is challenging due to availability of a smaller number of patient samples compared to huge number of genes, and the noisy nature of microarray data. After selection of some good genes based on signal-to-noise ratio, unsupervised learning like clustering and supervised learning like k-nearest neighbor (kNN) classifier are widely used in cancer researches to correlate the pathological behavior of cancers with the gene expression levels' differences in cancerous and normal tissues. By applying adaptive searches like Probabilistic Model Building Genetic Algorithm (PMBGA), it may be possible to get a smaller size gene subset that would classify patient samples more accurately than the above methods. In this paper, we propose a new PMBGA based method to extract informative genes from microarray data using Support Vector Machine (SVM) as a classifier. We apply our method to three microarray data sets and present the experimental results. Our method with SVM obtains encouraging results on those data sets as compared with the rank based method using kNN as a classifier.|Topon Kumar Paul,Hitoshi Iba","57776|GECCO|2006|Investigation on artificial ant using analytic programming|The paper deals with a alternative tool for symbolic regression - Analytic Programming which is able to solve various problems from the symbolic regression domain. In this contribution main principles of Analytic Programming are described and explained. Then follows how Analytic Programming was used for setting an optimal trajectory for an artificial ant according to Koza. An ability to create so called programs, as well as Genetic Programming or Grammatical Evolution do, is shown in that part. Analytic Programming is a superstructure of evolutionary algorithms which are necessary to run Analytic Programming. In this contribution SelfOrganizing Migrating Algorithm and Differential Evolution as two evolutionary algorithms were used to carry simulations out.|Zuzana Oplatkov√°,Ivan Zelinka","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llor√†,David E. Goldberg","57360|GECCO|2005|A statistical learning theory approach of bloat|Code bloat, the excessive increase of code size, is an important issue in Genetic Programming (GP). This paper proposes a theoretical analysis of code bloat in the framework of symbolic regression in GP, from the viewpoint of Statistical Learning Theory, a well grounded mathematical toolbox for Machine Learning. Two kinds of bloat must be distinguished in that context, depending whether the target function lies in the search space or not. Then, important mathematical results are proved using classical results from Statistical Learning. Namely, the Vapnik-Chervonenkis dimension of programs is computed, and further results from Statistical Learning allow to prove that a parsimonious fitness ensures Universal Consistency (the solution minimizing the empirical error does converge to the best possible error when the number of examples goes to infinity). However, it is proved that the standard method consisting in choosing a maximal program size depending on the number of examples might still result in programs of infinitely increasing size with their accuracy a more complicated modification of the fitness is proposed that theoretically avoids unnecessary bloat while nevertheless preserving the Universal Consistency.|Sylvain Gelly,Olivier Teytaud,Nicolas Bredeche,Marc Schoenauer","57668|GECCO|2006|Improving GP classifier generalization using a cluster separation metric|Genetic Programming offers freedom in the definition of the cost function that is unparalleled among supervised learning algorithms. However, this freedom goes largely unexploited in previous work. Here, we revisit the design of fitness functions for genetic programming by explicitly considering the contribution of the wrapper and cost function. Within the context of supervised learning, as applied to classification problems, a clustering methodology is introduced using cost functions which encourage maximization of separation between in and out of class exemplars. Through a series of empirical investigations of the nature of these functions, we demonstrate that classifier performance is much more dependable than previously the case under the genetic programming paradigm.|Ashley George,Malcolm I. Heywood","57722|GECCO|2006|Pareto-coevolutionary genetic programming classifier|The conversion and extension of the Incremental Pareto-Coevolution Archive algorithm (IPCA) into the domain of Genetic Programming classifier evolution is presented. In order to accomplish efficiency in regards to classifier evaluation on training data, the coevolutionary aspect of the IPCA algorithm is utilized to simultaneously evolve a subset of the training data that provides distinctions between candidate classifiers. The algorithm is compared in terms of classification \"score\" (equal weight to detection rate, and  - false positive rate), and run-time against a traditional GP classifier using the entirety of the training data for evaluation, and a GP classifier which performs Dynamic Subset Selection. The results indicate that the presented algorithm outperforms the subset selection algorithm in terms of classification score, and outperforms the traditional classifier while requiring roughly over of the wall-clock time.|Michal Lemczyk,Malcolm I. Heywood","57792|GECCO|2006|Dynamics of evolutionary robustness|Recently there has been considerable interest in determining whether, and how much, evolutionary pressure for genetic robustness influences evolutionary processes. In this paper, we attempt to show that this evolutionary pressure does have a significant effect in typical genetic programming problems. Specifically we demonstrate that in a standard genetic programming implementation to solve a symbolic regression problem, pressure for genetic robustness forces the population away from high fitness, but less robust, solutions in favor of solutions with lower fitness, but higher genetic robustness.|Alan Piszcz,Terence Soule","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57850|GECCO|2006|A multi-chromosome approach to standard and embedded cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is an extension of Cartesian Genetic Programming (CGP) that can automatically acquire, evolve and re-use partial solutions in the form of modules. In this paper, we introduce for the first time a new multi-chromosome approach to CGP and ECGP that allows difficult problems with multiple outputs to be broken down into many smaller, simpler problems with single outputs, whilst still encoding the entire solution in a single genotype. We also propose a multi-chromosome evolutionary strategy which selects the best chromosomes from the entire population to form the new fittest individual, which may not have been present in the population. The multi-chromosome approach to CGP and ECGP is tested on a number of multiple output digital circuits. Computational Effort figures are calculated for each problem and compared against those for CGP and ECGP. The results indicate that the use of multiple chromosomes in both CGP and ECGP provide a significant performance increase on all problems tested.|James Alfred Walker,Julian Francis Miller,Rachel Cavill","57561|GECCO|2005|Investigating the performance of module acquisition in cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is a form of the graph based Cartesian Genetic Programming (CGP) in which modules are automatically acquired and evolved. In this paper we compare the efficiencies of the ECGP and CGP techniques on three classes of problem digital adders, digital multipliers and digital comparators. We show that in most cases ECGP shows a substantial improvement in performance over CGP and that the computational speedup is more pronounced on larger problems.|James Alfred Walker,Julian Francis Miller"],["80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80712|VLDB|2006|State-Slice New Paradigm of Multi-query Optimization of Window-based Stream Queries|Modern stream applications such as sensor monitoring systems and publishsubscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams. Efficient sharing of computations among multiple continuous queries, especially for the memory- and CPU-intensive window-based operations, is critical. A novel challenge in this scenario is to allow resource sharing among similar queries, even if they employ windows of different lengths. This paper first reviews the existing sharing methods in the literature, and then illustrates the significant performance shortcomings of these methods.This paper then presents a novel paradigm for the sharing of window join queries. Namely we slice window states of a join operator into fine-grained window slices and form a chain of sliced window joins. By using an elaborate pipelining methodology, the number of joins after state slicing is reduced from quadratic to linear. This novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes. Based on the state-slice sharing paradigm, two algorithms are proposed for the chain buildup. One minimizes the memory consumption while the other minimizes the CPU usage. The algorithms are proven to find the optimal chain with respect to memory or CPU usage for a given query workload. We have implemented the slice-share paradigm within the data stream management system CAPE. The experimental results show that our strategy provides the best performance over a diverse range of workload settings among all alternate solutions in the literature.|Song Wang,Elke A. Rundensteiner,Samrat Ganguly,Sudeept Bhatnagar","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80706|VLDB|2006|Window-Aware Load Shedding for Aggregation Queries over Data Streams|Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a \"Window Drop\". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.|Nesime Tatbul,Stanley B. Zdonik","80696|VLDB|2006|Efficient Scheduling of Heterogeneous Continuous Queries|Data Stream Management Systems (DSMS) typically host multiple Continuous Queries (CQ) that process streams of data. In this paper, we examine the problem of how to schedule CQs in a DSMS to optimize for average QoS. We show that unlike standard on-line systems, scheduling policies in DSMSs that optimize for average response time will be different than policies that optimize for average slowdown which is more appropriate metric to use in the presence of a heterogeneous workload. We also propose a hybrid scheduling policy based on slowdown that strikes a fine balance between performance and fairness. We further discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies outperform currently used ones.|Mohamed A. Sharaf,Panos K. Chrysanthis,Alexandros Labrinidis,Kirk Pruhs","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["80662|VLDB|2006|PARAgrab A Comprehensive Architecture for Web Image Management and Multimodal Querying|We demonstrate PARAgrab - a scalable Web image archival, retrieval, and annotation system that supports multiple querying modalities. The underlying architecture of our large-scale Web image database is described. Querying and visualization techniques used in the system are explained.|Dhiraj Joshi,Ritendra Datta,Ziming Zhuang,W. P. Weiss,Marc Friedenberg,Jia Li,James Ze Wang","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Cal√¨,Michael Kifer","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65852|AAAI|2006|Sensor-Based Understanding of Daily Life via Large-Scale Use of Common Sense|The use of large quantities of common sense has long been thought to be critical to the automated understanding of the world. To this end, various groups have collected repositories of common sense in machine-readable form. However, efforts to apply these large bodies of knowledge to enable correspondingly large-scale sensor-based understanding of the world have been few. Challenges have included semantic gaps between facts in the repositories and phenomena detected by sensors, fragility of reasoning in the face of noise, incompleteness of repositories, and slowness of reasoning with these large repositories. We show how to address these problems with a combination of novel sensors, probabilistic representation, web-scale information retrieval and approximate reasoning. In particular, we show how to use the ,-fact hand-entered Open-Mind Indoor Common Sense database to interpret sensor traces of day-to-day activities with % accuracy (which is easy) and % precisionrecall (which is not).|William Pentney,Ana-Maria Popescu,Shiaokai Wang,Henry A. Kautz,Matthai Philipose","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["57391|GECCO|2005|Toward evolved flight|We present the first hardware-in-the-loop evolutionary optimization on an ornithopter. Our experiments demonstrate the feasibility of evolving flight through genetic algorithms and adaptable hardware, without the requirement for a thorough knowledge of the aerodynamics of flapping flight. In this research we successfully optimized forward velocity and basic efficiency on an actual hardware ornithopter. The ornithopter was flown integrated to a \"whirling-arm\" test apparatus, allowing lengthy experimental flights without the risk of crashing. Flapping rate and tail position were controlled by an evolutionary algorithm with feedback of forward velocity and motor power. The system evolved an unexpected optimal configuration. This paper discusses the development of the test apparatus and experimental results from the initial phase of research.|Rusty Hunt,Gregory Hornby,Jason D. Lohn","65927|AAAI|2006|A New Approach to Estimating the Expected First Hitting Time of Evolutionary Algorithms|Evolutionary algorithms (EA) have been shown to be very effective in solving practical problems, yet many important theoretical issues of them are not clear. The expected first hitting time is one of the most important theoretical issues of evolutionary algorithms, since it implies the average computational time complexity. In this paper, we establish a bridge between the expected first hitting time and another important theoretical issue, i.e., convergence rate. Through this bridge, we propose a new general approach to estimating the expected first hitting time. Using this approach, we analyze EAs with different configurations, including three mutation operators, withwithout population, a recombination operator and a time variant mutation operator, on a hard problem. The results show that the proposed approach is helpful for analyzing a broad range of evolutionary algorithms. Moreover, we give an explanation of what makes a problem hard to EAs, and based on the recognition, we prove the hardness of a general problem.|Yang Yu,Zhi-Hua Zhou","57724|GECCO|2006|Biobjective evolutionary and heuristic algorithms for intersection of geometric graphs|Wire routing in a VLSI chip often requires minimization of ire-length as well as the number of intersections among multiple nets. Such an optimization problem is computationally hard for which no efficient algorithm or good heuristic is known to exist. Additionally, in a biobjective setting, the major challenge to solve a problem is to obtain representative diverse solutions across the (near-) Pareto-front.In this work, we consider the problem of constructing spanning trees of two geometric graphs corresponding to two nets, each with multiple terminals, with a goal to minimize the total edge cost and the number of intersections among the edges of the two trees. We first design simple heuristics to obtain the extreme points in the solution space, which however, could not produce diverse solutions. Search algorithms based on evolutionary multiobjective optimization (EMO) are then proposed to obtain diverse solutions in the feasible solution space. Each element of this solution set is a tuple of two spanning trees corresponding to the given geometric graphs. Empirical evidence shows that the proposed evolutionary algorithms cover a larger range and are much superior to the heuristics.|Rajeev Kumar,Pramod Kumar Singh,Bhargab B. Bhattacharya","57395|GECCO|2005|An empirical study on the handling of overlapping solutions in evolutionary multiobjective optimization|We focus on the handling of overlapping solutions in evolutionary multiobjective optimization (EMO) algorithms. First we show that there exist a large number of overlapping solutions in each population when EMO algorithms are applied to multiobjective combinatorial optimization problems with only a few objectives. Next we implement three strategies to handle overlapping solutions. One strategy is the removal of overlapping solutions in the objective space. In this strategy, overlapping solutions in the objective space are removed during the generation update phase except for only a single solution among them. As a result, each solution in the current population has a different location in the objective space. Another strategy is to remove overlapping solutions so that each solution in the current population has a different location in the decision space. The other strategy is the modification of Pareto ranking where overlapping solutions in the objective space are allocated to different fronts. As a result, each solution in each front has a different location in the objective space. Effects of each strategy on the performance of the NSGA-II algorithm are examined through computational experiments on multiobjective  knapsack problems, multiobjective flowshop scheduling problems, and multiobjective fuzzy rule selection problems.|Hisao Ishibuchi,Kaname Narukawa,Yusuke Nojima","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","57410|GECCO|2005|Genetic algorithms using low-discrepancy sequences|The random number generator is one of the important components of evolutionary algorithms (EAs). Therefore, when we try to solve function optimization problems using EAs, we must carefully choose a good pseudo-random number generator. In EAs, the pseudo-random number generator is often used for creating uniformly distributed individuals. As the low-discrepancy sequences allow us to create individuals more uniformly than the random number sequences, we apply the low-discrepancy sequence generator, instead of the pseudo-random number generator, to EAs in this study. The numerical experiments show that the low-discrepancy sequence generator improves the search performances of EAs.|Shuhei Kimura,Koki Matsumura","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57296|GECCO|2005|Diversity as a selection pressure in dynamic environments|Evolutionary algorithms (EAs) are widely used to deal with optimization problems in dynamic environments (DE) . When using EAs to solve DE problems, we are usually interested in the algorithm's ability to adapt and recover from the changes. One of the main problems facing an evolutionary method when solving DE problems is the loss of genetic diversity.In this paper, we investigate the use of evolutionary multi-objective optimization methods (EMOs) for single-objective DE problems. For that purpose, we introduce an artificial second objective with the aim to maintain useful diversity in the population. Six different artificial objectives are examined and compared.All the results will be compared against a traditional GA and the random immigrants algorithm. NSGA is employed as the evolutionary multi-objective technique.|Lam Thu Bui,J√ºrgen Branke,Hussein A. Abbass","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57283|GECCO|2005|Simple addition of ranking method for constrained optimization in evolutionary algorithms|During the optimization of a constrained problem using evolutionary algorithms (EAs), an individual in the population can be described using three important properties, i.e., objective function, the sum of squares of the constraint violation, and the number of constraints violated. However, the question of how to combine these three properties effectively is always difficult to solve due to the scaling and aggregation problems. In this paper, a simple addition of ranking method is proposed to handle constrained optimization problems in EAs. In this method, each individual is ranked based on the above three properties separately, resulting in three new properties which are in the same order of magnitude. Simple addition of the three new terms can then be performed and this produces a new global ranking for each individual. The algorithm was tested using  benchmark problems on the basis of evolution strategy and genetic algorithm. Results showed that the proposed algorithm performed well in all of the problems with inequality constraints, without requiring any parameter tuning for the constraint handling part. On the other hand, problems with equality constraints can be handled well through the addition of a simple diversity mechanism and a tolerance value adjustment scheme.|Pei Yee Ho,Kazuyuki Shimizu"],["57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","57315|GECCO|2005|Quality-time analysis of multi-objective evolutionary algorithms|A quality-time analysis of multi-objective evolutionary algorithms (MOEAs) based on schema theorem and building blocks hypothesis is developed. A bicriteria OneMax problem, a hypothesis of niche and species, and a definition of dissimilar schemata are introduced for the analysis. In this paper, the convergence time, the first and last hitting time models are constructed for analyzing the performance of MOEAs. Population sizing model is constructed for determining appropriate population sizes. The models are verified using the bicriteria OneMax problem. The theoretical results indicate how the convergence time and population size of a MOEA scale up with the problem size, the dissimilarity of Pareto-optimal solutions, and the number of Pareto-optimal solutions of a multi-objective optimization problem.|Jian-Hung Chen,Shinn-Ying Ho,David E. Goldberg","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,J√∂rn Mehnen,Thomas Michelitsch","57698|GECCO|2006|Multi-objective genetic algorithms for pipe arrangement design|This paper presents an automatic design method for piping arrangement. A pipe arrangement design problem is proposed for a space in which many pipes and objects co-exist. This problem includes large-scale numerical optimization and combinatorial optimization problems, as well as two criteria. For these reasons, it is difficult to optimize the problem using usual optimization techniques such as Random Search. Therefore, multi-objective genetic algorithms suitable for this problem are developed. The proposed method for optimizing a pipe arrangement efficiently is demonstrated through several experiments.|Satoshi Ikehira,Hajime Kimura","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57461|GECCO|2005|Minimum spanning trees made easier via multi-objective optimization|Many real-world problems are multi-objective optimization problems and evolutionary algorithms are quite successful on such problems. Since the task is to compute or approximate the Pareto front, multi-objective optimization problems are considered as more difficult than single-objective problems. One should not forget that the fitness vector with respect to more than one objective contains more information that in principle can direct the search of evolutionary algorithms. Therefore, it is possible that a single-objective problem can be solved more efficiently via a generalized multi-objective model of the problem. That this is indeed the case is proved by investigating the computation of minimum spanning trees.|Frank Neumann,Ingo Wegener","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","57671|GECCO|2006|Maximum cardinality matchings on trees by randomized local search|To understand the working principles of randomized search heuristics like evolutionary algorithms they are analyzed on optimization problems whose structure is well-studied. The idea is to investigate when it is possible to simulate clever optimization techniques for combinatorial optimization problems by random search. The maximum matching problem is well suited for this approach since long augmenting paths do not allow immediate improvements by local changes. It is known that randomized search heuristics like simulated annealing, the Metropolis algorithm, the (+) EA and randomized local search efficiently approximate maximum matchings for any graph however, there are graphs where they fail to find maximum matchings in polynomial time. In this paper, we examine randomized local search (RLS) for graphs whose structure is simple. We show that RLS finds maximum matchings on trees in expected polynomial time.|Oliver Giel,Ingo Wegener","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan","57336|GECCO|2005|Introducing a watermarking with a multi-objective genetic algorithm|We propose an evolutionary algorithm for the enhancement of digital semi-fragile watermaking based on the manipulation of the image discrete cosine transform (DCT). The algorithm searches for the optimal localization of the DCT of an image to place the mark image DCT coefficients. The problem is stated as a multi-objective optimization problem (MOP), that involves the simultaneous minimization of distortion and robustness criteria.|Diego Sal D√≠az,Manuel Grana Romay"],["65710|AAAI|2006|Inconsistencies Negations and Changes in Ontologies|Ontology management and maintenance are considered cornerstone issues in current Semantic Web applications in which semantic integration and ontological reasoning play a fundamental role. The ability to deal with inconsistency and to accommodate change is of utmost importance in real-world applications of ontological reasoning and management, wherein the need for expressing negated assertions also arises naturally. For this purpose, precise, formal definitions of the the different types of inconsistency and negation in ontologies are required. Unfortunately, ontology languages based on Description Logics (DLs) do not provide enough expressive power to represent axiom negations. Furthermore, there is no single, well-accepted notion of inconsistency and negation in the Semantic Web community, due to the lack of a common and solid foundational framework. In this paper, we propose a general framework accounting for inconsistency, negation and change in ontologies. Different levels of negation and inconsistency in DL-based ontologies are distinguished. We demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontologies.|Giorgos Flouris,Zhisheng Huang,Jeff Z. Pan,Dimitris Plexousakis,Holger Wache","65564|AAAI|2005|Diagnosing Terminologies|We present a framework for the debugging of logically contradicting terminologies, which is based on traditional model-based diagnosis. To study the feasibility of this highly general approach we prototypically implemented the hitting set algorithm presented in (Reiter ), and applied it in three different scenarios. First, we use a Description Logic reasoning system as a black-box to determine (necessarily maximal) conflict sets. Then we use our own non-optimized DL reasoning engine to produce small, and a specialized algorithm to determine minimal conflict sets. In a number of expenments we show that the first method already fails for relatively small terminologies. However, based on small, or minimal conflict sets, we can often calculate diagnoses in reasonable time.|Stefan Schlobach","65774|AAAI|2006|Towards an Axiom System for Default Logic|Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning.|Gerhard Lakemeyer,Hector J. Levesque","65836|AAAI|2006|Reasoning about Partially Observed Actions|Partially observed actions are observations of action executions in which we are uncertain about the identity of objects, agents, or locations involved in the actions (e.g., we know that action move(o, x, y) occurred, but do not know o, y). Observed-Action Reasoning is the problem of reasoning about the world state after a sequence of partial observations of actions and states. In this paper we formalize Observed-Action Reasoning, prove intractability results for current techniques, and find tractable algorithms for STRIPS and other actions. Our new algorithms update a representation of all possible world states (the belief state) in logic using new logical constants for unknown objects. A straightforward application of this idea is incorrect, and we identify and add two key amendments. We also present successful experimental results for our algorithm in Blocks-world domains of varying sizes and in Kriegspiel (partially observable chess). These results are promising for relating sensors with symbols, partial-knowledge games, multi-agent decision making, and AI planning.|Megan Nance,Adam Vogel,Eyal Amir","65474|AAAI|2005|Only-Knowing Taking It Beyond Autoepistemic Reasoning|The idea of only-knowing a collection of sentences has been previously shown to have a close connection with autoepistemic logic. Here we propose a more general account of only-knowing that captures not only autoepistemic logic but default logic as well. This allows us not only to study the properties of default logic in terms of an underlying model of belief, but also the relationship among different forms of nonmonotonic reasoning, all within a classical monotonic logic characterized semantically in terms of possible worlds.|Gerhard Lakemeyer,Hector J. Levesque","65665|AAAI|2006|Progress in Textual Case-Based Reasoning Predicting the Outcome of Legal Cases from Text|This paper reports on a project that explored reasoning with textual cases in the context of legal reasoning. The work is anchored in both Case-Based Reasoning (CBR) and AI and Law. It introduces the SMILE+IBP framework that generates a case-based analysis and prediction of the outcome of a legal case given a brief textual summary of the case facts. The focal research question in this work was to find a good text representation for text classification. An evaluation showed that replacing case-specific names by roles and adding NLP lead to higher performance for assigning CBR indices. The NLP-based representation produced the best results for reasoning with the automatically indexed cases.|Stefanie Br√ºninghaus,Kevin D. Ashley","65582|AAAI|2005|Managing the Life Cycle of Plans|The scalability of recent planning algorithms allows developers to automate planning tasks, which so far have been reserved to humans. However in real-world applications, synthesizing a plan is just the beginning of a complex life-cycle management process. Plans must be organized in large collections, where they can be grouped along different purposes and are amenable to the search, inspection, evaluation, and modification by human experts or automated reasoning systems. Eventually, plans will outlast their utility and be replaced. We present our solution to plan life cycle management for an autonomic computing application. We focus in particular on the automatic synthesis of plan metadata for plans containing conditional and parallel actions, well-structured loops, and non-deterministic choices. The plans are of unknown origin, i.e., their underlying action model. which could provide us with pre- and postconditions, is not available. New analysis techniques are presented that uniformly generate metadata for plans, thus allowing a system to embed plans into context and organize them in meaningfully structured plan repositories.|Biplav Srivastava,Jussi Vanhatalo,Jana Koehler","65385|AAAI|2005|Recovery Planning for Ambiguous Cases in Perceptual Anchoring|An autonomous robot using symbolic reasoning, sensing and acting in a real environment needs the ability to create and maintain the connection between symbols representing objects in the world and the corresponding perceptual representations given by its sensors. This connection has been named perceptual anchoring. In complex environments, anchoring is not always easy to establish the situation may often be ambiguous as to which percept actually corresponds to a given symbol. In this paper. we extend perceptual anchoring to deal robustly with ambiguous situations by providing general methods for detecting them and recovering from them. We consider different kinds of ambiguous situations and present planning-based methods to recover from them. We illustrate our approach by showing experiments involving a mobile robot equipped with a color camera and an electronic nose.|Mathias Broxvall,Silvia Coradeschi,Lars Karlsson,Alessandro Saffiotti","65486|AAAI|2005|Tractable Reasoning in First-Order Knowledge Bases with Disjunctive Information|This work proposes a new methodology for establishing the tractability of a reasoning service that deals with expressive first-order knowledge bases. It consists of defining a logic that is weaker than classical logic and that has two properties first, the entailment problem can be reduced to the model checking problem for a small number of characteristic models and second, the model checking problem itself is tractable for formulas with a bounded number of variables. We show this methodology in action for the reasoning service previously proposed by Liu, Lakemeyer and Levesque for dealing with disjunctive information. They show that their reasoning is tractable in the propositional case and decidable in the first-order case. Here we apply the methodology and prove that the reasoning is also tractable in the first-order case if the knowledge base and the query both use a bounded number of variables.|Yongmei Liu,Hector J. Levesque","65725|AAAI|2006|Bounded Treewidth as a Key to Tractability of Knowledge Representation and Reasoning|Several forms of reasoning in AI - like abduction, closed world reasoning, circumscription, and disjunctive logic programming - are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae or programs is bounded by some constant. Clearly, these theoretical tractability results as such do not immediately yield feasible algorithms. However, we have recently established a new method based on monadic datalog which allowed us to design an efficient algorithm for a related problem in the database area. In this work, we exploit the monadic datalog approach to construct new algorithms for logic-based abduction.|Georg Gottlob,Reinhard Pichler,Fang Wei"],["65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","65912|AAAI|2006|Contract Enactment in Virtual Organizations A Commitment-Based Approach|A virtual organization (VO) is a dynamic collection of entities (individuals, enterprises, and information resources) collaborating on some computational activity. VOs are an emerging means to model, enact, and manage large-scale computations. VOs consist of autonomous, heterogeneous members, often dynamic exhibiting complex behaviors. Thus, VOs are best modeled via multiagent systems. An agent can be an individual such as a person, business partner, or a resource. An agent may also be a VO. A VO is an agent that comprises other agents. Contracts provide a natural arms-length abstraction for modeling interaction among autonomous and heterogeneous agents. The interplay of contracts and VOs is the subject of this paper. The core of this paper is an approach to formalize VOs and contracts based on commitments. Our main contributions are () a formalization of VOs, () a discussion of certain key properties of VOs, and () an identification of a variety of VO structures and an analysis of how they support contract enactment. We evaluate our approach with an analysis of several scenarios involving the handling of exceptions and conflicts in contracts.|Yathiraj B. Udupi,Munindar P. Singh","65374|AAAI|2005|Coordination and Adaptation in Impromptu Teams|Coordinating a team of autonomous agents is one of the major challenges in building effective multiagcnt systems. Many techniques have been devised for this problem. and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques. though. is that the team members are developed together as a whole. In many multi agent scenarios. this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills. models. strategies. and coordination mechanisms. and no external organization is imposed upon them. In particular. we propose two techniques. one adaptive and one predictive. for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.|Michael H. Bowling,Peter McCracken","65695|AAAI|2006|Traffic Intersections of the Future|Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.|Kurt M. Dresner,Peter Stone","65651|AAAI|2006|Performance Evaluation Methods for the Trading Agent Competition|This paper proposes a novel method to characterize the performance of autonomous agents in the Trading Agent Competition for Supply Chain Management (TAC-SCM). We create benchmarking tools that manipulate market environments to control the conditions and provide guidelines to test trading agents. Using these tools, we show how developers can inspect their agents and unveil behaviors that might otherwise have gone undiscovered.|Brett Borghetti,Eric Sodomka","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","57788|GECCO|2006|The Baldwin effect under spatial isolation and autonomous reproduction|The impact of learning on evolution in dynamic environments undergoes recognized stages of the Baldwin Effect although its cause is not clear. To identify it experimentally, we devise spatial constraints and allowed autonomous reproduction for a multi-agent game play using Iterated Prisoner's Dilemma. In comparison to Arita and Suzuki's model, we exclude the selective bias of noise, reduce the genotypic and phenotypic correlation and explain how mutant strategies can survive under costs assumptions. -Simulation results show that plastic agents are exploited by Defecting, non-plastic agents during game-play. In the majority of the simulations, this Nash Equilibrium Phase occurs, resulting in a Defect-oriented population which eventually wipes itself out. However, with spatial isolation, a small number of cooperative plastic and non-plastic agents survive and proliferate in the simulation after the Defecting agents have died out the two transitions stages ascribed to the Baldwin Effect are then observed to occur.|H. L. Peng,J. C. Tay","65732|AAAI|2006|From Centralized to Distributed Selective Overhearing|Overhearing is an approach for monitoring open, distributed, multi-agent systems by listening to the routine communications taking place within them. Previous investigations of overhearing assumed that all inter-agent communications are accessible to a single overhearing agent. However, as multi-agent systems grow both in size and distribution two problems arise. First, in large-scale settings, an overhearing agent cannot monitor all agents and their conversations, and must therefore be selective in carefully choosing its targets. Second, a single overhearer would encounter difficulties overhearing agents acting in a geographically-distributed environment. This paper tackles these challenges by addressing distributed teams of overhearing agents involved in selective overhearing. Building on prior work on centralized selective overhearing, we consider the consequences of transitioning from overhearing teams working in a centrally-coordinated manner to distributed overhearing teams. In doing so, we distinguish the various factors influencing the level of distribution within these teams and determine their importance in terms of effective selective overhearing.|Gery Gutnik,Gal A. Kaminka"],["57436|GECCO|2005|Combating user fatigue in iGAs partial ordering support vector machines and synthetic fitness|One of the daunting challenges of interactive genetic algorithms (iGAs)---genetic algorithms in which fitness measure of a solution is provided by a human rather than by a fitness function, model, or computation---is user fatigue which leads to sub-optimal solutions. This paper proposes a method to combat user fatigue by augmenting user evaluations with a synthetic fitness function. The proposed method combines partial ordering concepts, notion of non-domination from multiobjective optimization, and support vector machines to synthesize a fitness model based on user evaluation. The proposed method is used in an iGA on a simple test problem and the results demonstrate that the method actively combats user fatigue by requiring -- times less user evaluation when compared to a simple iGA.|Xavier Llor√†,Kumara Sastry,David E. Goldberg,Abhimanyu Gupta,Lalitha Lakshmi","57545|GECCO|2005|Crossover is provably essential for the ising model on trees|Due to experimental evidence it is incontestable that crossover is essential for some fitness functions. However, theoretical results without assumptions are difficult. So-called real royal road functions are known where crossover is proved to be essential, i.e., mutation-based algorithms have an exponential expected runtime while the expected runtime of a genetic algorithm is polynomially bounded. However, these functions are artificial and have been designed in such a way that crossover is essential only at the very end (or at other well-specified points) of the optimization process.Here, a more natural fitness function based on a generalized Ising model is presented where crossover is essential throughout the whole optimization process. Mutation-based algorithms such as (+) EAs with constant population size are proved to have an exponential expected runtime while the expected runtime of a simple genetic algorithm with population size  and fitness sharing is polynomially bounded.|Dirk Sudholt","57335|GECCO|2005|Analysis and mathematical justification of a fitness function used in an intrusion detection system|Convergence to correct solutions in Genetic Algorithms depends largely on the fitness function. A fitness function that captures all goals and constraints can be difficult to find. This paper gives a mathematical justification for a fitness function that has previously been demonstrated experimentally to be effective.|Pedro A. Diaz-Gomez,Dean F. Hougen","57468|GECCO|2005|A hardware pipeline for function optimization using genetic algorithms|Genetic Algorithms (GAs) are very commonly used as function optimizers, basically due to their search capability. A number of different serial and parallel versions of GA exist. In this paper, a pipelined version of the commonly used Genetic Algorithms and a corresponding hardware platform is described. The main idea of achieving pipelined execution of different operations of GA is to use a stochastic selection function which works with the fitness value of the candidate chromosome only. The modified algorithm is termed PLGA (Pipelined Genetic Algorithm). When executed in a CGA (Classical Genetic Algorithm) framework, the stochastic selection gives comparable performances with the roulette-wheel selection. In the pipelined hardware environment, PLGA will be much faster than the CGA. When executed on similar hardware platforms, PLGA may attain a maximum speedup of four over CGA. However, if CGA is executed in a uniprocessor system the speedup is much more. A comparison of PLGA against PGA (Parallel Genetic Algorithms) shows that PLGA may be even more effective than PGAs. A scheme for realizing the hardware pipeline is also presented. Since a general function evaluation unit is essential, a detailed description of one such unit is presented.|Malay Kumar Pakhira,Rajat K. De","57357|GECCO|2005|On favoring positive correlations between form and quality of candidate solutions via the emergence of genomic self-similarity|A key property for the effectiveness of stochastic search techniques, including evolutionary algorithms, is the existence of a positive correlation between the form and the quality of candidate solutions. In this paper, we show that when the ordering of genomic symbols in a genetic algorithm is completely independent of the fitness function and therefore free to evolve along the candidate solutions it encodes, the resulting genomes self-organize into self-similar structures that favor this key stochastic search property.|Ivan I. Garibay,Annie S. Wu,Ozlem O. Garibay","57668|GECCO|2006|Improving GP classifier generalization using a cluster separation metric|Genetic Programming offers freedom in the definition of the cost function that is unparalleled among supervised learning algorithms. However, this freedom goes largely unexploited in previous work. Here, we revisit the design of fitness functions for genetic programming by explicitly considering the contribution of the wrapper and cost function. Within the context of supervised learning, as applied to classification problems, a clustering methodology is introduced using cost functions which encourage maximization of separation between in and out of class exemplars. Through a series of empirical investigations of the nature of these functions, we demonstrate that classifier performance is much more dependable than previously the case under the genetic programming paradigm.|Ashley George,Malcolm I. Heywood","57573|GECCO|2005|On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization|Estimation of distribution algorithms construct an explicit model of the problem to be solved, and then use this model to guide the search for good solutions. For an important class of fitness functions, namely those with k-bounded epistasis, it is possible to construct a complete explicit representation of the fitness function by sampling the fitness function. A very natural model of the problem to be solved is the Boltzmann distribution of the fitness function, which is an exponential of the fitness normalized to a probability distribution. As the exponentiation factor (inverse temperature) of the Boltzmann distribution is increased, probability is increasingly concentrated on the set of optimal points. We show that for fitness functions of k-bounded epistasis that satisfy an additional property called the running intersection property, an explicit computable exact factorization of the Boltzmann distribution with an arbitrary exponentiation factor can be constructed. This factorization allows the Boltzmann distribution to be efficiently sampled, which leads to an algorithm which finds the optimum with high probability.|Alden H. Wright,Sandeep Pulavarty","57491|GECCO|2005|Fractional dynamic fitness functions for GA-based circuit design|This paper proposes and analyses the performance of a Genetic Algorithm (GA) using two new concepts, namely a static fitness function including a discontinuity measure and a fractional-order dynamic fitness function. The GA is adopted for the synthesis of combinational logic circuits. In both cases, experiments reveal superior results in terms of speed and convergence to achieve a solution.|Cec√≠lia Reis,Jos√© Ant√≥nio Tenreiro Machado,J. Boaventura Cunha","57800|GECCO|2006|Analysis of the difficulty of learning goal-scoring behaviour for robot soccer|Learning goal-scoring behaviour from scratch for simulated robot soccer is considered to be a very difficult problem, and is often achieved by endowing players with an innate set of hand-coded skills, or by decomposing the problem into learning a set of simpler behaviours which are then aggregated into goal-scoring behaviour. When only basic skills are available to the player the fitness landscape is very flat, containing only a few thin peaks. As more human expertise is injected via hand-coded skills or a composite fitness function, more gradient information becomes apparent on the landscape and the genetic search is more successful. The work presented in this paper uses autocorrelation and information content measures to examine features of the fitness landscape to explain how the difficulty of the problem is changed by injecting human expertise.|Jeff Riley,Victor Ciesielski","57502|GECCO|2005|Latent variable crossover for k-tablet structures and its application to lens design problems|This paper presents the Real-coded Genetic Algorithms for high-dimensional ill-scaled structures, what is called, the k-tablet structure. The k-tablet structure is the landscape that the scale of the fitness function is different between a k-dimensional subspace and the orthogonal (n-k)-dimensional subspace. The search speed of traditional GAs degrades when a high dimensional k-tablet structure is included in the landscape of the fitness function.In this structure, offspring generated by crossovers are likely to spread wider region than the region where the parental population covers and this causes the stagnation of the search. To resolve this problem, we propose a new crossover LUNDX-m using only m-dimensional latent variables. The effectiveness of the proposal method is tested with several benchmark functions including k-tablet structures and we show that our proposed method performs better than traditional crossovers especially when the dimensionality n is higher than .As an example of a k-tablet structure in real world applications, we show that the lens design problem has a kind of k-tablet structures and that our proposed method also performs better than conventional crossovers in this problem.|Jun Sakuma,Shigenobu Kobayashi"],["57532|GECCO|2005|Unbiased tournament selection|Tournament selection is a popular form of selection which is commonly used with genetic algorithms, genetic programming and evolutionary programming. However, tournament selection introduces a sampling bias into the selection process. We review analytic results and present empirical evidence that shows this bias has a significant impact on search performance. We introduce two new forms of unbiased tournament selection that remove or reduce sampling bias in tournament selection.|Artem Sokolov,Darrell Whitley","57321|GECCO|2005|Finding needles in haystacks is harder with neutrality|This research presents an analysis of the reported successes of the Cartesian Genetic Programming method on a simplified form of the Boolean parity problem. We show the method of sampling used by the CGP is significantly less effective at locating solutions than the solution density of the corresponding formula space would warrant.We present results indicating that the loss of performance is caused by the sampling bias of the CGP, due to the neutrality friendly representation. We implement a simple intron free random sampling algorithm which performs considerably better on the same problem and then explain how such performance is possible.|M. Collins","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57850|GECCO|2006|A multi-chromosome approach to standard and embedded cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is an extension of Cartesian Genetic Programming (CGP) that can automatically acquire, evolve and re-use partial solutions in the form of modules. In this paper, we introduce for the first time a new multi-chromosome approach to CGP and ECGP that allows difficult problems with multiple outputs to be broken down into many smaller, simpler problems with single outputs, whilst still encoding the entire solution in a single genotype. We also propose a multi-chromosome evolutionary strategy which selects the best chromosomes from the entire population to form the new fittest individual, which may not have been present in the population. The multi-chromosome approach to CGP and ECGP is tested on a number of multiple output digital circuits. Computational Effort figures are calculated for each problem and compared against those for CGP and ECGP. The results indicate that the use of multiple chromosomes in both CGP and ECGP provide a significant performance increase on all problems tested.|James Alfred Walker,Julian Francis Miller,Rachel Cavill","57401|GECCO|2005|Determining equations for vegetation induced resistance using genetic programming|Inducing equations based on theory and data is a time-honoured technique in science. This is usually done manually, based on theoretical understanding and previously established equations. In this work, for a particular problem in hydraulics, human induction of equations is compared with the use of genetic programming. It will be shown that even with the use of synthetic data for training, genetic programming was capable of identifying a relationship that was more concise and more accurate than the relationship uncovered by scientists. As such this is a human-competitive result. Furthermore it will be shown that the genetic programming induced expression could be embedded in a line of theoretical work, filling in a few gaps in an already established line of reasoning. The resulting equation is the most accurate and elegant formulation of vegetation induced resistance to date.|Maarten Keijzer,Martin Baptist,Vladan Babovic,Javier Rodriguez Uthurburu","57385|GECCO|2005|Open-ended robust design of analog filters using genetic programming|Most existing research on robust design using evolutionary algorithms (EA) follows the paradigm of traditional robust design, in which parameters of a design solution are tuned to improve the robustness of the system. However, the topological structure of a system may set a limit on the possible robustness achievable through parameter tuning. This paper proposes a new robust design paradigm that exploits the open-ended topological synthesis capability of genetic programming to evolve more robust systems. As a case study, a methodology for automated synthesis of dynamic systems, based on genetic programming and bond graph modeling (GPBG), is applied to evolve robust low-pass and high-pass analog filters. Compared with a traditional robust design approach based on a state-of-the-art real-parameter genetic algorithm (GA), it is shown that open-ended topology search by genetic programming with a fitness criterion rewarding robustness can evolve more robust systems with respect to parameter perturbations than what was achieved through parameter tuning alone, for our test problems.|Jianjun Hu,Xiwei Zhong,Erik D. Goodman","57793|GECCO|2006|A survey of mutation techniques in genetic programming|The importance of mutation varies across evolutionary computation domains including genetic programming, evolution strategies, and genetic algorithms. In the genetic programming community, researchers' view of mutation's effectiveness spans the range from an ineffective or marginal operator, to a neutral operator, to a highly effective operator that evolves solutions more effectively than genetic programming with crossover alone. Mutation implementation and associated parameters are often under reported in genetic programming research and typically lack context that justifies the technique and parameter selection. In part, reporting variance stems from the adaptation of mutation developed by the genetic algorithm community, and the creation of new mutation techniques in genetic programming. This survey describes the controversial operator in genetic programming applications, mutation selection operators, mutation techniques and offers an organization of mutation characteristics. We suggest methodologies to improve reporting of mutation parameters and related individual selection methods.|Alan Piszcz,Terence Soule","57561|GECCO|2005|Investigating the performance of module acquisition in cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is a form of the graph based Cartesian Genetic Programming (CGP) in which modules are automatically acquired and evolved. In this paper we compare the efficiencies of the ECGP and CGP techniques on three classes of problem digital adders, digital multipliers and digital comparators. We show that in most cases ECGP shows a substantial improvement in performance over CGP and that the computational speedup is more pronounced on larger problems.|James Alfred Walker,Julian Francis Miller","57675|GECCO|2006|A tree-based genetic algorithm for building rectilinear Steiner arborescences|A rectilinear Steiner arborescence (RSA) is a tree, whose nodes include a prescribed set of points, termed the vertices, in the first quadrant of the Cartesian plane, and whose tree edges from parent to child nodes must head either straight to the right or straight above. A minimal RSA (a MRSA) is one for which the total path length of the edges in the tree is minimal. RSAs have application in VLSI design. Curiously, although a RSA is a tree, to our knowledge, previous genetic attacks on the MRSA problem have not used tree-based approaches to representation, nor to the operations of crossover and mutation. We show why some care is needed in the choice of such genetic operators. Then we present tree-based operators for crossover and mutation, which are successful in creating true RSAs from source RSAs without the need of repair steps. We compare our results to two earlier researches, and find that our approach gives good results, but not results that are consistently better than those earlier approaches.|William A. Greene"]]},"title":{"entropy":6.216550423040523,"topics":["genetic algorithm, algorithm for, genetic for, genetic programming, evolutionary algorithm, evolutionary for, using genetic, and algorithm, the algorithm, and genetic, genetic with, using algorithm, algorithm with, and its, evolutionary, for scheduling, neural networks, multi-objective optimization, its application, genetic the","particle swarm, for data, system for, for xml, and data, query for, data, support vector, sensor networks, human-robot interaction, model for, and system, efficient for, answering queries, bayesian networks, query, data stream, query processing, xml, decision processes","the web, learning for, reinforcement learning, sense disambiguation, semantic web, for and, planning with, for web, and search, learning with, the semantic, time series, web services, word disambiguation, natural language, word sense, learning, search for, learning and, with and","the problem, for the, the, and the, for problem, solving problem, the algorithm, differential evolution, artificial immune, classifier system, dynamic environments, the case, for optimization, analysis the, multiobjective optimization, method for, the performance, description logic, new for, local search","and its, based and, for scheduling, approach based, for and, its application, and mutation, and genetic, hybrid for, ant colony, genetic flexible, scheduling flexible, for flexible, and scheduling, its quality, mutation based, approach and, and, and quality, mutation approach","genetic programming, using genetic, using algorithm, and programming, programming for, algorithm design, for using, and design, and genetic, design genetic, using and, for design, optimization using, using programming, genetic for, genetic the, design optimization, evolving genetic, test problem, using the","for data, efficient for, and data, data stream, efficient and, efficient algorithm, data, for over, efficient, data management, over data, over stream, efficient over, efficient queries, efficient data, and over, for interactive, algorithm data, the data, over","particle swarm, human-robot interaction, for belief, particle optimization, swarm optimization, for information, load stream, information and, for interaction, information control, and mining, for stream, information, mining, pomdps, event, stream, filtering, sampling, oracle","for and, natural language, and, for agent, for application, planning and, the application, for planning, and the, and agent, and application, how and, language for, representation and, representation for, and relations, compact for, language and, and system, application","the web, for web, and web, semantic web, the semantic, using web, for semantic, web services, from the, semantic and, for services, and the, integrating and, services composition, for the, using semantic, engine for, services with, the services, for management","method for, for and, building block, assignment problem, and method, the programming, the assignment, heuristic for, solutions the, and the, for the, and problem, generalized for, for building, simple for, method the, the heuristic, block for, programming for, embedded and","dynamic environments, study the, for dynamic, for logic, the dynamic, the environments, description logic, the case, evolution strategies, knowledge for, comparative study, case study, and dynamic, knowledge and, for case, logic programming, reasoning about, dynamic, for environments, the evolution"],"ranking":[["57669|GECCO|2006|Segmentation of medical images using a genetic algorithm|Segmentation of medical images is challenging due to poor image contrast and artifacts that result in missing or diffuse organtissue boundaries. Consequently, this task involves incorporating as much prior information as possible (e.g., texture, shape, and spatial location of organs) into a single framework. In this paper, we present a genetic algorithm for automating the segmentation of the prostate on two-dimensional slices of pelvic computed tomography (CT) images. In this approach the segmenting curve is represented using a level set function, which is evolved using a genetic algorithm (GA). Shape and textural priors derived from manually segmented images are used to constrain the evolution of the segmenting curve over successive generations.We review some of the existing medical image segmentation techniques. We also compare the results of our algorithm with those of a simple texture extraction algorithm (Laws' texture measures) as well as with another GA-based segmentation tool called GENIE. Our preliminary tests on a small population of segmenting contours show promise by converging on the prostate region. We expect that further improvements can be achieved by incorporating spatial relationships between anatomical landmarks, and extending the methodology to three dimensions.|Payel Ghosh,Melanie Mitchell","57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","57269|GECCO|2005|Inexact pattern matching using genetic algorithm|A Genetic Algorithm for graphical pattern matching based on angle matching had been proposed. It has proven quite effective in matching simple patterns. However, the algorithm needs some modifications to enhance its accuracy on pattern matching when there are some differences between two patterns in terms of numbers of nodes, shapes and rotations. This paper presents the modifications, such as the introduction of node exemption, inexact matching between straight lines and curves in the patterns, and consideration of rotational degrees of the patterns. Each angle is also given with a weight to indicate the significant degree of the angle. A multi-objective function is used to reflect the similarity between two patterns. The experiments designed to evaluate the algorithm have shown very promising results. It is highly accurate on patterns matching with dissimilarities in shapes, numbers of nodes and rotational degrees.|Surapong Auwatanamongkol","57314|GECCO|2005|MDGA motif discovery using a genetic algorithm|Computationally identifying transcription factor binding sites in the promoter regions of genes is an important problem in computational biology and has been under intensive research for a decade. To predict the binding site locations efficiently, many algorithms that incorporate either approximate or heuristic techniques have been developed. However, the prediction accuracy is not satisfactory and binding site prediction thus remains a challenging problem. In this paper, we develop an approach that can be used to predict binding site motifs using a genetic algorithm. Based on the generic framework of a genetic algorithm, the approach explores the search space of all possible starting locations of the binding site motifs in different target sequences with a population that undergoes evolution. Individuals in the population compete to participate in the crossovers and mutations occur with a certain probability. Initial experiments demonstrated that our approach could achieve high prediction accuracy in a small amount of computation time. A promising advantage of our approach is the fact that the computation time does not explicitly depend on the length of target sequences and hence may not increase significantly when the target sequences become very long.|Dongsheng Che,Yinglei Song,Khaled Rasheed","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57432|GECCO|2005|Primer design for multiplex PCR using a genetic algorithm|Multiplex Polymerase Chain Reaction (PCR) experiments are used for amplifying several segments of the target DNA simultaneously and thereby to conserve template DNA, reduce the experimental time, and minimize the experimental expense. The success of the experiment is dependent on primer design. However, this can be a dreary task as there are many constrains such as melting temperatures, primer length, GC content and complementarity that need to be optimized to obtain a good PCR product. Motivated by the lack of primer design tools for multiplex PCR genotypic assay, we propose a multiplex PCR primer design tool using a genetic algorithm, which is a stochastic approach based on the concept of biological evolution, biological genetics and genetic operations on chromosomes, to find an optimal selection of primer pairs for multiplex PCR experiments. The presented experimental results indicate that the proposed algorithm is capable of finding a series of primer pairs that obeies the design properties in the same tube.|Feng-Mao Lin,Hsien-Da Huang,Hsi-Yuan Huang,Jorng-Tzong Horng","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","57336|GECCO|2005|Introducing a watermarking with a multi-objective genetic algorithm|We propose an evolutionary algorithm for the enhancement of digital semi-fragile watermaking based on the manipulation of the image discrete cosine transform (DCT). The algorithm searches for the optimal localization of the DCT of an image to place the mark image DCT coefficients. The problem is stated as a multi-objective optimization problem (MOP), that involves the simultaneous minimization of distortion and robustness criteria.|Diego Sal D√≠az,Manuel Grana Romay","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80723|VLDB|2006|Crimson A Data Management System to Support Evaluating Phylogenetic Tree Reconstruction Algorithms|Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a \"gold standard\" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.|Yifeng Zheng,Stephen Fisher,Shirley Cohen,Sheng Guo,Junhyong Kim,Susan B. Davidson","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80645|VLDB|2006|Efficient Discovery of XML Data Redundancies|As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often \"casually designed\" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.|Cong Yu,H. V. Jagadish","65842|AAAI|2006|Characterizing Data Complexity for Conjunctive Query Answering in Expressive Description Logics|Description Logics (DLs) are the formal foundations of the standard web ontology languages OWL-DL and OWL-Lite. In the Semantic Web and other domains, ontologies are increasingly seen also as a mechanism to access and query data repositories. This novel context poses an original combination of challenges that has not been addressed before (i) sufficient expressive power of the DL to capture common data modeling constructs (ii) well established and flexible query mechanisms such as Conjunctive Queries (CQs) (iii) optimization of inference techniques with respect to data size, which typically dominates the size of ontologies. This calls for investigating data complexity of query answering in expressive DLs. While the complexity of DLs has been studied extensively, data complexity has been characterized only for answering atomic queries, and was still open for answering CQs in expressive DLs. We tackle this issue and prove a tight CONP upper bound for the problem in SHIQ, as long as no transitive roles occur in the query. We thus establish that for a whole range of DLs from AL to SHIQ, answering CQs with no transitive roles has CONP-complete data complexity. We obtain our result by a novel tableaux-based algorithm for checking query entailment, inspired by the one in , but which manages the technical challenges of simultaneous inverse roles and number restrictions (which leads to a DL lacking the finite model property).|Magdalena Ortiz,Diego Calvanese,Thomas Eiter","65867|AAAI|2006|Closest Pairs Data Selection for Support Vector Machines|This paper presents data selection procedures for support vector machines (SVM). The purpose of data selection is to reduce the dataset by eliminating as many non support vectors (non-SVs) as possible. Based on the fact that support vectors (SVs) are those vectors close to the decision boundary, data selection keeps only the closest pair vectors of opposite classes. The selected dataset will replace the full dataset as the training component for any standard SVM algorithm.|Chaofan Sun","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","80690|VLDB|2006|An Algebraic Query Model for Effective and Efficient Retrieval of XML Fragments|Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.|Sujeet Pradhan"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","65393|AAAI|2005|Scaling Up Word Sense Disambiguation via Parallel Texts|A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL- English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL- English all-words task, and significantly outperforms the baseline of always choosing sense  of WordNet.|Yee Seng Chan,Hwee Tou Ng","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Korn√©l G. Mark√≥,Stefan Schulz,Udo Hahn","65754|AAAI|2006|An End-to-End Supervised Target-Word Sense Disambiguation System|We present an extensible supervised Target-Word Sense Disambiguation system that leverages upon GATE (General Architecture for Text Engineering), NSP (Ngram Statistics Package) and WEKA (Waikato Environment for Knowledge Analysis) to present an end-to-end solution that integrates feature identification, feature extraction, preprocessing and classification.|Mahesh Joshi,Serguei V. S. Pakhomov,Ted Pedersen,Richard Maclin,Christopher G. Chute","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57388|GECCO|2005|On the complexity of hierarchical problem solving|Competent Genetic Algorithms can efficiently address problems in which the linkage between variables is limited to a small order k. Problems with higher order dependencies can only be addressed efficiently if further problem properties exist that can be exploited. An important class of problems for which this occurs is that of hierarchical problems. Hierarchical problems can contain dependencies between all variables (kn) while being solvable in polynomial time.An open question so far is what precise properties a hierarchical problem must possess in order to be solvable efficiently. We study this question by investigating several features of hierarchical problems and determining their effect on computational complexity, both analytically and empirically. The analyses are based on the Hierarchical Genetic Algorithm (HGA), which is developed as part of this work. The HGA is tested on ranges of hierarchical problems, produced by a generator for hierarchical problems.|Edwin D. de Jong,Richard A. Watson,Dirk Thierens","57463|GECCO|2005|Enhancing differential evolution performance with local search for high dimensional function optimization|In this paper, we proposed Fittest Individual Refinement (FIR), a crossover based local search method for Differential Evolution (DE). The FIR scheme accelerates DE by enhancing its search capability through exploration of the neighborhood of the best solution in successive generations. The proposed memetic version of DE (augmented by FIR) is expected to obtain an acceptable solution with a lower number of evaluations particularly for higher dimensional functions. Using two different implementations DEfirDE and DEfirSPX we showed that proposed FIR increases the convergence velocity of DE for well known benchmark functions as well as improves the robustness of DE against variation of population. Experiments using multimodal landscape generator showed our proposed algorithms consistently outperformed their parent algorithms. A performance comparison with reported results of well known real coded memetic algorithms is also presented.|Nasimul Noman,Hitoshi Iba","57354|GECCO|2005|An artificial immune network for multimodal function optimization on dynamic environments|Multimodal optimization algorithms inspired by the immune system are generally characterized by a dynamic control of the population size and by diversity maintenance along the search. One of the most popular proposals is denoted opt-aiNet (artificial immune network for optimization) and is extended here to deal with time-varying fitness functions. Additional procedures are designed to improve the overall performance and the robustness of the immune-inspired approach, giving rise to a version for dynamic optimization, denoted dopt-aiNet. Firstly, challenging benchmark problems in static multimodal optimization are considered to validate the new proposal. No parameter adjustment is necessary to adapt the algorithm according to the peculiarities of each problem. In the sequence, dynamic environments are considered, and usual evaluation indices are adopted to assess the performance of dopt-aiNet and compare with alternative solution procedures available in the literature.|Fabr√≠cio Olivetti de Fran√ßa,Fernando J. Von Zuben,Leandro Nunes de Castro","57639|GECCO|2006|Selective self-adaptive approach to ant system for solving unit commitment problem|This paper presents a novel approach to solve the constrained unit commitment problem using Selective Self-Adaptive Ant System (SSAS) for improving search performance by automatically adapting ant populations and their transition probability parameters, which cooperates with Candidate Path Management Module (CPMM) and Effective Repairing Heuristic Module (ERHM) in reducing search space and recovering a feasible optimality region so that a high quality solution can be acquired in a very early iterative. The proposed SSAS algorithm not only enhances the convergence of search process, but also provides a suitable number of the population sharing which conducts a good guidance for trading-off between the importance of the visibility and the pheromone trail intensity. The proposed method has been performed on a test system up to  generating units with a scheduling time horizon of  hours. The numerical results show the most economical saving in the total operating cost when compared to the previous literature results. Moreover, the proposed SSAS topology can remarkably speed up the computation time of ant system algorithms, which is favorable for a large-scale unit commitment problem implementation.|Songsak Chusanapiputt,Dulyatat Nualhong,Sujate Jantarang,Sukumvit Phoomvuthisarn","57682|GECCO|2006|Local search for multiobjective function optimization pareto descent method|Genetic Algorithm (GA) is known as a potent multiobjective optimization method, and the effectiveness of hybridizing it with local search (LS) has recently been reported in the literature. However, there is a relatively small number of studies on LS methods for multiobjective function optimization. Although each of the existing LS methods has some strong points, they have respective drawbacks such as high computational cost and inefficiency in improving objective functions. Hence, a more effective and efficient LS method is being sought, which can be used to enhance the performance of the hybridization.Defining Pareto descent directions as descent directions to which no other descent directions are superior in improving all objective functions, this paper proposes a new LS method, Pareto Descent Method (PDM), which finds Pareto descent directions and moves solutions in such directions thereby improving all objective functions simultaneously. In the case part or all of them are infeasible, it finds feasible Pareto descent directions or descent directions as appropriate. PDM finds these directions by solving linear programming problems, which is computationally inexpensive. Experiments have shown PDM's superiority over existing methods.|Ken Harada,Jun Sakuma,Shigenobu Kobayashi","57681|GECCO|2006|Hybridization of genetic algorithm and local search in multiobjective function optimization recommendation of GA then LS|Hybridization with local search (LS) is known to enhance the performance of genetic algorithms (GA) in single objective optimization and have also been studied in the multiobjective combinatorial optimization literature. In most such studies, LS is applied to the solutions of each generation of GA, which is the scheme called \"GA with LS\" herein. Another scheme, in which LS is applied to the solutions obtained with GA, has also been studied, which is called \"GA then LS\" herein. It seems there is no consensus in the literature as to which scheme is better, let alone the reasoning for it. The situation in the multiobjective function optimization literature is even more unclear since the number of such studies in the field has been small.This paper, assuming that objective functions are differentiable, reveals the reasons why GA is not suitable for obtaining solutions of high precision, thereby justifying hybridization of GA and LS. It also suggests that the hybridization scheme which maximally exploits both GA and LS is GA then LS. Experiments conducted on many benchmark problems verified our claims.|Ken Harada,Kokolo Ikeda,Shigenobu Kobayashi","57364|GECCO|2005|Performance assessment of an artificial immune system multiobjective optimizer by two improved metrics|In this study, we introduce two improved assessment metrics of multiobjective optimizers, Nondominated Ratio and Spacing Distribution, and analyze their rationality and validity. Based on the concept of Immunodominance and Antibody Clonal Selection Theory, a novel multiobjective optimization algorithm, Immune Dominance Clonal Multiobjective Algorithm (IDCMA), is put forward. The simulation comparisons between IDCMA and the Strength Pareto Evolutionary Algorithm show that IDCMA has the best performance in popular metrics such as Spacing, Coverage of Two Sets and the two new metrics presented in this paper when low-dimensional multiobjective problems are concerned. The statistical results of the four metrics also show that Spacing Distribution conquers some limitations of Spacing triumphantly, and Nondominated Ratio conquers the limitation of Coverage of Two Sets that only compared between two sets.|Maoguo Gong,Licheng Jiao,Haifeng Du,Ronghua Shang,Bin Lu","57443|GECCO|2005|A theoretical analysis of the HIFF problem|We present a theoretical analysis of Watson's Hierarchical-if-and-only-if (HIFF) problem using a variety of tools. These include schema theory and course graining, the concept of effective fitness, and statistical analysis. We first review the use of Stephen's exact schema equations and schema basis to compute the changes in population distributions over time. We then use the tools described above to solve for the limit distributions of the  and -bit HIFF problems, and show that these limit distributions are essentially one-dimensional. We also show that a combination of fitness and the number of break points (a rough measure of distance in crossover space) in a string can be used to almost completely explain the limit distribution in the -bit HIFF problem.|Nicholas Freitag McPhee,Ellery Fussell Crane"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","65459|AAAI|2005|Flexible Teamwork in Behavior-Based Robots|A key challenge in deploying teams of robots in real-world applications is to automate the control of teamwork, such that the designer can focus on the taskwork. Existing teamwork architectures seeking to address this challenge are monolithic, in that they commit to interaction protocols at the architectural level, and do not allow the designer to mix and match protocols for a given task. We present BITE, a behavior-based teamwork architecture that automates collaboration in physical robots, in a distributed fashion. BITE separates task behaviors that control a robot's interaction with its task, from interaction behaviors that control a robot's interaction with its teammates. This distinction provides for flexibility and modularity in terms of the interactions used by teammates to collaborate effectively. It also allows BITE to synthesize and significantly extend existing teamwork architectures. BITE also incorporates key lessons learned in applying multi-agent teamwork architectures in physical robot teams. We present empirical results from experiments with teams of Sony AIBO robots executing BITE, and discuss the lessons learned.|Gal A. Kaminka,Inna Frenkel","57588|GECCO|2005|Adaptive crossover and mutation in genetic algorithms based on clustering technique|Instead of having fixed px and pm, this paper presents the use of fuzzy logic to adaptively tune px and pm for optimization of power electronic circuits throughout the process. By applying the K-means algorithm, distribution of the population in the search space is clustered in each training generation. Inferences of px and pm are performed by a fuzzy-based system that fuzzifies the relative sizes of the clusters containing the best and worst chromosomes. The proposed adaptation method is applied to optimize a buck regulator that requires satisfying some static and dynamic requirements. The optimized circuit component values, the regulator's performance, and the convergence rate in the training are favorably compared with the GA's using fixed px and p.|Jun Zhang,Henry Shu-Hung Chung,Jinghui Zhong","57307|GECCO|2005|An approach for QoS-aware service composition based on genetic algorithms|Web services are rapidly changing the landscape of software engineering. One of the most interesting challenges introduced by web services is represented by Quality Of Service (QoS)--aware composition and late--binding. This allows to bind, at run--time, a service--oriented system with a set of services that, among those providing the required features, meet some non--functional constraints, and optimize criteria such as the overall cost or response time. In other words, QoS--aware composition can be modeled as an optimization problem.We propose to adopt Genetic Algorithms to this aim. Genetic Algorithms, while being slower than integer programming, represent a more scalable choice, and are more suitable to handle generic QoS attributes. The paper describes our approach and its applicability, advantages and weaknesses, discussing results of some numerical simulations.|Gerardo Canfora,Massimiliano Di Penta,Raffaele Esposito,Maria Luisa Villani","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","57557|GECCO|2005|Improvements to penalty-based evolutionary algorithms for the multi-dimensional knapsack problem using a gene-based adaptive mutation approach|Knapsack problems are among the most common problems in literature tackled with evolutionary algorithms (EA). Their major advantage lies in the fact that they are relatively simple to implement while they allow generalizations for a wide range of real world problems. The multi-dimensional knapsack problem (MKP), which belongs to the class of NP-complete combinatorial optimization problems, is one of the variations of the knapsack problem. The MKP has a wide range of real world applications such as cargo loading, selecting projects to fund, budget management, cutting stock, etc. The MKP has been studied quite extensively in the EA community. Due to the constrained nature of the problem, constraint handling techniques gain great importance in the performance of the proposed EA approaches. In this study, the applicability of a generational EA that uses a penalty-based constraint handling technique and a gene locus based, asymmetric, adaptive mutation scheme is explored for the MKP. The effects of the parameters of the explored approach is determined through tests. Further experiments, using large MKP instances from commonly used benchmarks available through the Internet are performed. Comparison tables are given for the performance of the explored approach and other good performing EAs found in literature for the MKP. Results show that performance improves greatly when compared with other penalty-based techniques, but the explored approach is still not the best performer among all. However, unlike the explored technique, the EAs using the other constraint handling techniques require a great amount of extra computational effort and need heuristic information specific to the optimization problem. Based on these observations, and the fact that the performance difference between the explored scheme and the better performers is not too high, research on improving the explored approach is still in progress.|Sima Uyar,G√ºlsen Eryigit","57755|GECCO|2006|Extraction of landscape information based on a quality control approach and its applications to mutation in GAExtraction of landscape information based on a quality control approach and its applications to mutation in GA|We introduce an attraction hypothesis and repulsion hypothesis on combinations of genes and we characterize \"genelocus pair\" as a \"Unique Inheritance\" if the pair satisfies one of the hypotheses. We propose a method based on a statistical approach to extract a set of gene-locus pairs characterized as \"Unique Inheritance\", and also two new genetic operations, attraction mutation and repulsion mutation.|Mitsukuni Matayoshi,Morikazu Nakamura,Hayao Miyagi","57587|GECCO|2005|Search-based mutation testing for models|The efficient and effective generation of test-data from high-level models is of crucial importance in advanced modern software engineering. Empirical studies have shown that mutation testing is highly effective. This paper describes how search-based automatic test-data generation methods can be used to find mutation adequate test-sets for MatlabSimulink models.|Yuan Zhan,John A. Clark","57431|GECCO|2005|Combining competent crossover and mutation operators a probabilistic model building approach|This paper presents an approach to combine competent crossover and mutation operators via probabilistic model building. Both operators are based on the probabilistic model building procedure of the extended compact genetic algorithm (eCGA). The model sampling procedure of eCGA, which mimics the behavior of an idealized recombination---where the building blocks (BBs) are exchanged without disruption---is used as the competent crossover operator. On the other hand, a recently proposed BB-wise mutation operator---which uses the BB partition information to perform local search in the BB space---is used as the competent mutation operator. The resulting algorithm, called hybrid extended compact genetic algorithm (heCGA), makes use of the problem decomposition information for () effective recombination of BBs and () effective local search in the BB neighborhood. The proposed approach is tested on different problems that combine the core of three well known problem difficulty dimensions deception, scaling, and noise. The results show that, in the absence of domain knowledge, the hybrid approach is more robust than either single-operator-based approach.|Cl√°udio F. Lima,Kumara Sastry,David E. Goldberg,Fernando G. Lobo","57348|GECCO|2005|Knowledge insertion an efficient approach to reduce effort in simple genetic algorithms for unrestricted parallel equal machines scheduling|Simple Genetic Algorithms (SGAs) are blind search algorithms which only make use of the relative fitness of solutions and completely ignore the nature of the problem. The SGAs have been used to solve different scheduling problems but in large search spaces, a considerable number of evaluations are required to obtain solutions nearer to the optimum (known or estimated). Our purpose was to try to reduce the number of evaluations by introducing problem specific knowledge through the insertion of good seeds (solutions) obtained with other conventional heuristics. This work shows how the knowledge insertion in a SGA, reduces the cost in solving due-date based problems in parallel machines scheduling systems.|Edgardo Ferretti,Susana C. Esquivel"],["57602|GECCO|2006|Using genetic programming to classify node positive patients in bladder cancer|Nodal staging has been identified as an independent indicator of prognosis. Quantitative RT-PCR data was taken for  genes associated with bladder cancer and genetic programming was used to develop classification rules associated with nodal stages of bladder cancer. This study suggests involvement of several key genes for discriminating between samples with and without nodal metastasis.|Arpit A. Almal,Anirban P. Mitra,Ram H. Datar,Peter F. Lenehan,David W. Fry,Richard J. Cote,William P. Worzel","57703|GECCO|2006|On evolving buffer overflow attacks using genetic programming|In this work, we employed genetic programming to evolve a \"white hat\" attacker that is to say, we evolve variants of an attack with the objective of providing better detectors. Assuming a generic buffer overflow exploit, we evolve variants of the generic attack, with the objective of evading detection by signature-based methods. To do so, we pay particular attention to the formulation of an appropriate fitness function and partnering instruction set. Moreover, by making use of the intron behavior inherent in the genetic programming paradigm, we are able to explicitly obfuscate the true intent of the code. All the resulting attacks defeat the widely used 'Snort' Intrusion Detection System.|Hilmi G√ºnes Kayacik,Malcolm I. Heywood,A. Nur Zincir-Heywood","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57518|GECCO|2005|Design of air pump system using bond graph and genetic programming method|This paper introduces a redesign method for an air pump system using bond graphs and genetic programming to maximize outflow subject to a constraint specifying maximum power consumption. The redesign process can alter the topological connections among components and can introduce additional components. The air pump system is a mixed-domain system that includes electromagnetic, mechanical and pneumatic elements. Bond graphs are domain independent, allow free composition, and are efficient for classification and analysis of models. Genetic programming is well recognized as a powerful tool for open-ended search. The combination of these two powerful methods, BGGP, was applied for redesign of an air pump system.|Kisung Seo,Erik D. Goodman,Ronald C. Rosenberg","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57428|GECCO|2005|Multiplex PCR primer design for gene family using genetic algorithm|The multiplex PCR experiment is to amplify multiple regions of a DNA sequence at the same time by using different primer pairs. Designing feasible primer pairs for multiplex PCR is a tedious task since there are too many constraints to be satisfied. In this paper, a new method for multiplex PCR primer design strategy using genetic algorithm is proposed. The proposed algorithm is able to find a set of suitable primer pairs more efficient and uses a MAP model to speed up the examination of the specificity constraint that is important for gene family sequences. The dry-dock experiment shows that the proposed algorithm finds several sets of primer pairs of gene family sequences for multiplex PCR that not only obey the design properties, but also have specificity.|Hong-Long Liang,Chungnan Lee,Jain-Shing Wu","57385|GECCO|2005|Open-ended robust design of analog filters using genetic programming|Most existing research on robust design using evolutionary algorithms (EA) follows the paradigm of traditional robust design, in which parameters of a design solution are tuned to improve the robustness of the system. However, the topological structure of a system may set a limit on the possible robustness achievable through parameter tuning. This paper proposes a new robust design paradigm that exploits the open-ended topological synthesis capability of genetic programming to evolve more robust systems. As a case study, a methodology for automated synthesis of dynamic systems, based on genetic programming and bond graph modeling (GPBG), is applied to evolve robust low-pass and high-pass analog filters. Compared with a traditional robust design approach based on a state-of-the-art real-parameter genetic algorithm (GA), it is shown that open-ended topology search by genetic programming with a fitness criterion rewarding robustness can evolve more robust systems with respect to parameter perturbations than what was achieved through parameter tuning alone, for our test problems.|Jianjun Hu,Xiwei Zhong,Erik D. Goodman","57401|GECCO|2005|Determining equations for vegetation induced resistance using genetic programming|Inducing equations based on theory and data is a time-honoured technique in science. This is usually done manually, based on theoretical understanding and previously established equations. In this work, for a particular problem in hydraulics, human induction of equations is compared with the use of genetic programming. It will be shown that even with the use of synthetic data for training, genetic programming was capable of identifying a relationship that was more concise and more accurate than the relationship uncovered by scientists. As such this is a human-competitive result. Furthermore it will be shown that the genetic programming induced expression could be embedded in a line of theoretical work, filling in a few gaps in an already established line of reasoning. The resulting equation is the most accurate and elegant formulation of vegetation induced resistance to date.|Maarten Keijzer,Martin Baptist,Vladan Babovic,Javier Rodriguez Uthurburu","57432|GECCO|2005|Primer design for multiplex PCR using a genetic algorithm|Multiplex Polymerase Chain Reaction (PCR) experiments are used for amplifying several segments of the target DNA simultaneously and thereby to conserve template DNA, reduce the experimental time, and minimize the experimental expense. The success of the experiment is dependent on primer design. However, this can be a dreary task as there are many constrains such as melting temperatures, primer length, GC content and complementarity that need to be optimized to obtain a good PCR product. Motivated by the lack of primer design tools for multiplex PCR genotypic assay, we propose a multiplex PCR primer design tool using a genetic algorithm, which is a stochastic approach based on the concept of biological evolution, biological genetics and genetic operations on chromosomes, to find an optimal selection of primer pairs for multiplex PCR experiments. The presented experimental results indicate that the proposed algorithm is capable of finding a series of primer pairs that obeies the design properties in the same tube.|Feng-Mao Lin,Hsien-Da Huang,Hsi-Yuan Huang,Jorng-Tzong Horng","57309|GECCO|2005|Optimization of passenger car design for the mitigation of pedestrian head injury using a genetic algorithm|The problem of pedestrian injury is a significant one throughout the world. In , there were  pedestrian fatalities in Europe and  in the US. Significant advances have been made by automotive safety researchers and vehicle manufacturers to address this issue with respect to the design of vehicles, but the complex nature of pedestrian accident scenarios has resulted in great difficulty when using traditional statistical methods. Specifically, problems have been encountered when attempting to study the effects of individual parameters of vehicle front-end geometry on pedestrian head injury. This paper attempts to demonstrate the feasibility of applying the field of evolutionary computation to the problem of pedestrian safety by using a simple genetic algorithm to optimize the centre-line geometry of a car's front-end for the reduction of pedestrian head and thoracic injury. The fitness of each design is assessed by creating a multi-body mathematical model of the vehicle front and simulating impacts with models of different sized pedestrians, and ranking according to the combined injury scores.|Emma Carter,Steve Ebdon,Clive Neal-Sturgess"],["80625|VLDB|2006|Efficient Allocation Algorithms for OLAP Over Imprecise Data|Recent work proposed extending the OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform a given imprecise fact table into a form, called the Extended Database, that can be readily used to answer OLAP aggregation queries.In this work, we present scalable, efficient algorithms for creating the Extended Database (i.e., performing allocation) for a given imprecise fact table. Many allocation policies require multiple iterations over the imprecise fact table, and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, and has not been considered previously to the best of our knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straight-foward approaches.|Douglas Burdick,Prasad M. Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80518|VLDB|2005|ConQuer A System for Efficient Querying Over Inconsistent Databases|Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.|Ariel Fuxman,Diego Fuxman,Ren√©e J. Miller","80674|VLDB|2006|Safety Guarantee of Continuous Join Queries over Punctuated Data Streams|Continuous join queries (CJQ) are needed for correlating data from multiple streams. One fundamental problem for processing such queries is that since the data streams are infinite, this would require the join operator to store infinite states and eventually run out of space. Punctuation semantics has been proposed to specifically address this problem. In particular, punctuations explicitly mark the end of a subset of data and, hence, enable purging of the stored data which will not contribute to any new query results. Given a set of available punctuation schemes, if one can identify that a CJQ still requires unbounded storage, then this query can be flagged as unsafe and can be prevented from running. Unfortunately, while punctuation semantics is clearly useful, the mechanisms to identify if and how a particular CJQ could benefit from a given set of punctuation schemes are not yet known. In this paper, we provide sufficient and necessary conditions for checking whether a CJQ can be safely executed under a given set of punctuation schemes or not. In particular, we introduce a novel punctuation graph to aid the analysis of the safety for a given query. We show that the safety checking problem can be done in polynomial time based on this punctuation graph construct. In addition, various issues and challenges related to the safety checking of CJQs are highlighted.|Hua-Gang Li,Songting Chen,Jun'ichi Tatemura,Divyakant Agrawal,K. Sel√ßuk Candan,Wang-Pin Hsiung","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80706|VLDB|2006|Window-Aware Load Shedding for Aggregation Queries over Data Streams|Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a \"Window Drop\". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.|Nesime Tatbul,Stanley B. Zdonik","65666|AAAI|2006|B-ROC Curves for the Assessment of Classifiers over Imbalanced Data Sets|The class imbalance problem appears to be ubiquitous to a large portion of the machine learning and data mining communities. One of the key questions in this setting is how to evaluate the learning algorithms in the case of class imbalances. In this paper we introduce the Bayesian Receiver Operating Characteristic (B-ROC) curves, as a set of tradeoff curves that combine in an intuitive way, the variables that are more relevant to the evaluation of classifiers over imbalanced data sets. This presentation is based on section  of (Crdenas, Baras, & Seamon ).|Alvaro A. C√°rdenas,John S. Baras","80645|VLDB|2006|Efficient Discovery of XML Data Redundancies|As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often \"casually designed\" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.|Cong Yu,H. V. Jagadish","80672|VLDB|2006|Efficient Incremental Maintenance of Data Cubes|The data cube provides users with aggregated results that are group-bys for all possible combinations of dimension attributes. When the number of dimension attributes is n, the data cube computes n group-bys, each of which is called a cuboid. A data cube is often precomputed and stored as materialized views in data warehouses. The data cube needs to be updated when source relations change. The incremental maintenance of a data cube is to compute and propagate only changes of source relations rather than recompute the entire data cube from the source relations.To maintain a data cube incrementally, previous methods compute a delta cube which represents the change of the data cube. We call a cuboid in a delta cube a delta cuboid. For a data cube with n cuboids, a delta cube consists of n delta cuboids. Thus, as the number of dimension attributes increases, the cost of computing the delta cube increases significantly. In this paper, we propose an incremental maintenance method for data cubes that can maintain a data cube by using only (n n) delta cuboids. As a result, the cost of computing delta cuboids is substantially reduced. Through various experiments, we show the performance advantages of our method over the previous methods.|Ki Yong Lee,Myoung-Ho Kim","80489|VLDB|2005|OLAP Over Uncertain and Imprecise Data|We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.|Douglas Burdick,Prasad Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["57492|GECCO|2005|Particle swarm optimization for analysis of mass spectral serum profiles|Serum profiling using mass spectrometry is an emerging technology with a great potential to provide biomarkers for complex diseases such as cancer. However, protein profiles obtained from current mass spectrometric technologies are characterized by their high dimensionality and complex spectra with substantial level of noise. These characteristics have generated challenges in discovery of proteins and protein-profiles that distinguish cancer patients from healthy individuals. This paper proposes a novel machine learning method that combines support vector machines with particle swarm optimization for biomarker discovery. Prior to applying the proposed biomarker selection algorithm, low-level analysis methods are used for smoothing, baseline correction, normalization, and peak detection. The proposed method is applied for biomarker discovery from serum mass spectral profiles of liver cancer patients and controls.|Habtom W. Ressom,Rency S. Varghese,Daniel Saha,Eduard Orvisky,Lenka Goldman,Emanuel F. Petricoin,Thomas P. Conrads,Timothy D. Veenstra,Mohamed Abdel-Hamid,Christopher A. Loffredo,Radoslav Goldman","57323|GECCO|2005|A modified particle swarm optimization predicted by velocity|In standard particle swarm optimization (PSO), the velocity only provides a position displacement contrast with the longer computational time. To avoid premature convergence, a new modified PSO is proposed in which the velocity considered as a predictor, while the position considered as a corrector. The algorithm gives some balance between global and local search capability, and results the high computational efficiency. The optimization computing of some examples is made to show the new algorithm has better global search capacity and rapid convergence rate.|Zhihua Cui,Jianchao Zeng","80493|VLDB|2005|Loadstar Load Shedding in Data Stream Mining|In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.|Yun Chi,Haixun Wang,Philip S. Yu","57489|GECCO|2005|An effective use of crowding distance in multiobjective particle swarm optimization|In this paper, we present an approach that extends the Particle Swarm Optimization (PSO) algorithm to handle multiobjective optimization problems by incorporating the mechanism of crowding distance computation into the algorithm of PSO, specifically on global best selection and in the deletion method of an external archive of nondominated solutions. The crowding distance mechanism together with a mutation operator maintains the diversity of nondominated solutions in the external archive. The performance of this approach is evaluated on test functions and metrics from literature. The results show that the proposed approach is highly competitive in converging towards the Pareto front and generates a well distributed set of nondominated solutions.|Carlo R. Raquel,Prospero C. Naval Jr.","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57329|GECCO|2005|Improving particle swarm optimization with differentially perturbed velocity|This paper introduces a novel scheme of improving the performance of particle swarm optimization (PSO) by a vector differential operator borrowed from differential evolution (DE). Performance comparisons of the proposed method are provided against (a) the original DE, (b) the canonical PSO, and (c) three recent, high-performance PSO-variants. The new algorithm is shown to be statistically significantly better on a seven-function test suite for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57708|GECCO|2006|Introducing recombination with dynamic linkage discovery to particle swarm optimization|In this paper, we introduce the recombination operator with the technique of dynamic linkage discovery to particle swarm optimization (PSO) in order to improve the performance of PSO. Numerical experiments are conducted on a set of carefully designed benchmark functions and demonstrate good performance achieved by the proposed methodology.|Ming-chung Jian,Ying-Ping Chen","57813|GECCO|2006|Dynamic fitness inheritance proportion for multi-objective particle swarm optimization|In this paper, we propose a dynamic mechanism to vary the probability by which fitness inheritance is applied throughout the run of a multi-objective particle swarm optimizer, in order to obtain a greater reduction in computational cost (than the obtained with a fixed probability), without dramatically affecting the quality of the results. The results obtained show that it is possible to reduce the computational cost by % without affecting the quality of the obtained Pareto front.|Margarita Reyes Sierra,Carlos A. Coello Coello","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["65408|AAAI|2005|State Agnostic Planning Graphs and the Application to Belief-Space Planning|Planning graphs have been shown to be a rich source of heuristic information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states. The naive technique enumerates the graphs individually. This is equivalent to solving an all-pairs shortest path problem by iterating a single-source algorithm over each source. We introduce a structure, the state agnostic planning graph, that directly, solves the all-pairs problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in classical planning. The more prominent application of tnis technique is in belief-space planning, where an optimization results in drastically improved theoretical complexity. Our experimental evaluation quantifies this performance boost. and demonstrates that heuristic belief-space progression planning using our technique is competitive with the state of t the art.|William Cushing,Daniel Bryce","65868|AAAI|2006|Expressive Commerce and Its Application to Sourcing|Sourcing professionals buy several trillion dollars worth of goods and services yearly. We introduced a new paradigm called expressive commerce and applied it to sourcing. It combines the advantages of highly expressive human negotiation with the advantages of electronic reverse auctions. The idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auctions, and are algorithmically cleared. This creates a Pareto efficiency improvement in the allocation (a win-win between the buyer and the sellers) but the market clearing problem is a highly complex combinatorial optimization problem. We developed the world's fastest tree search algorithms for solving it. We have hosted $ billion of sourcing using the technology, and created $. billion of hard-dollar savings. The suppliers also benefited by being able to express production efficiencies and creativity, and through exposure problem removal. Supply networks were redesigned, with quantitative understanding of the tradeoffs, and implemented in weeks instead of months.|Tuomas Sandholm","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80528|VLDB|2005|Parallel Execution of Test Runs for Database Application Systems|In a recent paper , it was shown how tests for database application systems can be executed efficiently. The challenge was to control the state of the database during testing and to order the test runs in such a way that expensive reset operations that bring the database into the right state need to be executed as seldom as possible. This work extends that work so that test runs can be executed in parallel. The goal is to achieve linear speed-up andor exploit the available resources as well as possible. This problem is challenging because parallel testing can involve interference between the execution of concurrent test runs.|Florian Haftmann,Donald Kossmann,Eric Lo","65579|AAAI|2005|Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation|I propose a representation formalism and algorithms to be used in a new language generation mechanism for text-to-text applications. The generation process is driven by both text-specific information encoded via probability distributions over words and phrases derived from the input text, and general language knowledge captured by n-gram and syntactic language models.|Radu Soricut","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Ravi Vijay","65530|AAAI|2005|Identifying Similar Words and Contexts in Natural Language with SenseClusters|SenseClusters is a freely available intelligent system that clusters together similar contexts in natural language text. Thereafter it assigns identifying labels to these clusters based on their content. It is a purely unsupervised approach that is language independent, and uses no knowledge other than what is available in raw un-annotated corpora. In addition to clustering similar contexts, it can be used to identify synonyms and sets of related words. It has been applied to a diverse range of problems, including proper name disambiguation, word sense discrimination, email organization, and document clustering. SenseClusters is a complete system that supports feature selection from large corpora, several different context representation schemes, various clustering algorithms, the creation of descriptive and discriminating labels for the discovered clusters, and evaluation relative to gold standard data.|Ted Pedersen,Anagha Kulkarni","57465|GECCO|2005|The application of antigenic search techniques to time series forecasting|Time series have been a major topic of interest and analysis for hundreds of years, with forecasting a central problem. A large body of analysis techniques has been developed, particularly from methods in statistics and signal processing. Evolutionary techniques have only recently have been applied to time series problems. To date, applications of artificial immune system (AIS) techniques have been in the area of anomaly detection. In this paper we apply AIS techniques to the forecasting problem. We characterize a class of search algorithms we call antigenic search and show their ability to give a good forecast of next elements in series generated from Mackey-Glass and Lorenz equations.|Ian Nunn,Tony White"],["80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65742|AAAI|2006|Deciding Semantic Matching of Stateless Services|We present a novel approach to describe and reason about stateless information processing services. It can be seen as an extension of standard descriptions which makes explicit the relationship between inputs and outputs and takes into account OWL ontologies to fix the meaning of the terms used in a service description. This allows us to define a notion of matching between services which yields high precision and recall for service location. We explain why matching is decidable, and provide biomedical example services to illustrate the utility of our approach.|Duncan Hull,Evgeny Zolin,Andrey Bovykin,Ian Horrocks,Ulrike Sattler,Robert Stevens","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["57581|GECCO|2005|An evolutionary lagrangian method for the  multiple knapsack problem|We propose a new evolutionary approach to solve the  multiple knapsack problem. We approach the problem from a new viewpoint different from traditional methods. The most remarkable feature is the Lagrangian method. Lagrange multipliers transform the problem, keeping the optimality as well as decreasing the complexity. However, it is not easy to find Lagrange multipliers nearest to the constraints of the problem. We propose an evolution strategy to find the optimal Lagrange multipliers. Also, we improve the evolution strategy by adjusting its objective function properly. We show the efficiency of the proposed methods by the experiments. We make comparisons with existing general approach on well-known benchmark data.|Yourim Yoon,Yong-Hyuk Kim,Byung Ro Moon","57834|GECCO|2006|A GA-based method to produce generalized hyper-heuristics for the D-regular cutting stock problem|The idea behind hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. This paper presents a GA-based method that produces general hyper-heuristics that solve two-dimensional cutting stock problems. The GA uses a variable-length representation, which evolves combinations of condition-action rules producing hyper-heuristics after going through a learning process which includes training and testing phases. Such hyper-heuristics, when tested with a large set of benchmark problems, produce outstanding results (optimal and near-optimal) for most of the cases. The testbed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Mar√≠n,Cl√°udia J. Far√≠as Z√°rate,Peter Ross,Manuel Valenzuela-Rend√≥n","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llor√†,David E. Goldberg","57326|GECCO|2005|Probing for limits to building block mixing with a tunably-difficult problem for genetic programming|This paper describes a tunably-difficult problem for genetic programming (GP) that probes for limits to building block mixing and assembly. The existence of such a problem can be used to garner insight into the dynamics of what happens during the course of a GP run. The results indicate that the amount of mixing is fairly low in comparison to the amount of content that could be present in an initial population.|Jason M. Daida,Michael E. Samples,Matthew J. Byom","57358|GECCO|2005|Genetic algorithms for the sailor assignment problem|This paper examines a real-world application of genetic algorithms -- solving the United States Navy's Sailor Assignment Problem (SAP). The SAP is a complex assignment problem in which each of n sailors must be assigned one job drawn from a set of m jobs. The goal is to find a set of these assignments such that the overall desirability of the match is maximized while the cost of the match is minimized. We compare genetic algorithms to an existing algorithm, the Gale-Shapley algorithm, for generating these assignments and present empirical results showing that the GA is able to produce good solutions with significant savings in cost. Finally, we examine the possibility of using the GA to generate multiple different solutions for presentation to a human decision maker called a detailer, and we show that the GA can be used to provide a sample of good solutions.|Deon Garrett,Joseph Vannucci,Rodrigo Silva,Dipankar Dasgupta,James Simien","57518|GECCO|2005|Design of air pump system using bond graph and genetic programming method|This paper introduces a redesign method for an air pump system using bond graphs and genetic programming to maximize outflow subject to a constraint specifying maximum power consumption. The redesign process can alter the topological connections among components and can introduce additional components. The air pump system is a mixed-domain system that includes electromagnetic, mechanical and pneumatic elements. Bond graphs are domain independent, allow free composition, and are efficient for classification and analysis of models. Genetic programming is well recognized as a powerful tool for open-ended search. The combination of these two powerful methods, BGGP, was applied for redesign of an air pump system.|Kisung Seo,Erik D. Goodman,Ronald C. Rosenberg","57274|GECCO|2005|Heuristic rules embedded genetic algorithm to solve in-core fuel management optimization problem|Because of the large number of possible combinations for the fuel assembly loading in the core, the design of the loading pattern (LP) is a complex optimization problem. It requires finding an optimal fuel arrangement in order to achieve maximum cycle length while satisfying the safety constraints. The objective of this study is to develop a loading pattern optimization code. Generally in-core fuel management codes are written for specific cores and limited fuel inventory. One of the goals of this study is to develop a loading pattern optimization code, which is applicable for all types of Pressurized Water Reactor (PWR) core structures with unlimited number of fuel assembly types in the inventory. To reach this goal an innovative genetic algorithm is developed with modifying the classical representation of the genotype. To obtain the best result in a shorter time not only the representation is changed but also the algorithm is changed to use in-core fuel management heuristics rules. The improved GA code was tested demonstrating the advantages of the introduced enhancements. The core physics code used in this research is Moby-Dick, which was developed to analyze the VVER reactors by SKODA Inc.|Fatih Alim,Kostadin Ivanov","65812|AAAI|2006|A Simple and Effective Method for Incorporating Advice into Kernel Methods|We propose a simple mechanism for incorporating advice (prior knowledge), in the form of simple rules, into support-vector methods for both classification and regression. Our approach is based on introducing inequality constraints associated with datapoints that match the advice. These constrained datapoints can be standard examples in the training set, but can also be unlabeled data in a semi-supervised, advice-taking approach. Our new approach is simpler to implement and more efficiently solved than the knowledge-based support vector classification methods of Fung, Mangasarian and Shavlik ( ) and the knowledge-based support vector regression method of Mangasarian, Shavlik, and Wild (), while performing approximately as well as these more complex approaches. Experiments using our new approach on a synthetic task and a reinforcement-learning problem within the RoboCup soccer simulator show that our advice-taking method can significantly outperform a method without advice and perform similarly to prior advice-taking, support-vector machines.|Richard Maclin,Jude W. Shavlik,Trevor Walker,Lisa Torrey","57847|GECCO|2006|Comparative analysis of the sailor assignment problem|In this work the performance of several local search and metaheuristic methods is compared to previously reported work using evolutionary algorithms. The results show that while multiple algorithms are competitive on the Sailor Assignment Problem, the state-of-the-art evolutionary algorithm and a simulated annealing algorithm tend to provide the best performance. Additionally, some relevant features of the Sailor Assignment Problem are analyzed and used to explain the observed performance characteristics.|Joseph Vannucci,Deon Garrett,Dipankar Dasgupta","57686|GECCO|2006|The quadratic multiple knapsack problem and three heuristic approaches to it|The quadratic multiple knapsack problem extends the quadratic knapsack problem with K knapsacks, each with its own capacity Ck. A greedy heuristic fills the knapsacks one at a time with objects whose contributions are likely to be large relative to their weights. A hill-climber and a genetic algorithm encode candidate solutions as strings over ,,...,K with length equal to the number of objects. The hill-climber's neighbor operator is also the GA's mutation. In tests on  problem instances, the GA performed better than the greedy heuristic on the smaller instances, but it fell behind as the numbers of objects and knapsacks grew. The hill-climber always outperformed the greedy heuristic, and on the larger instances, also the GA.|Amanda Hiley,Bryant A. Julstrom"],["57798|GECCO|2006|The effect of crossover on the behavior of the GA in dynamic environments a case study using the shaky ladder hyperplane-defined functions|One argument as to why the hyperplane-defined functions (hdf's) are a good testbed for the genetic algorithm (GA) is that the hdf's are built in the same way that the GA works. In this paper we test that hypothesis in a new setting by exploring the GA on a subset of the hdf's which are dynamic---the shaky ladder hyperplane-defined functions (sl-hdf's). In doing so we gain insight into how the GA makes use of crossover during its traversal of the sl-hdf search space. We begin this paper by explaining the sl-hdf's. We then conduct a series of experiments with various crossover rates and various rates of environmental change. Our results show that the GA performs better with than without crossover in dynamic environments. Though these results have been shown on some static functions in the past, they are re-confirmed and expanded here for a new type of function (the hdf) and a new type of environment (dynamic environments). Moreover we show that crossover is even more beneficial in dynamic environments than it is in static environments. We discuss how these results can be used to develop a richer knowledge about the use of building blocks by the GA.|William Rand,Rick L. Riolo,John H. Holland","57488|GECCO|2005|The problem with a self-adaptative mutation rate in some environments a case study using the shaky ladder hyperplane-defined functions|Dynamic environments have periods of quiescence and periods of change. In periods of quiescence a Genetic Algorithm (GA) should (optimally) exploit good individuals while in periods of change the GA should (optimally) explore new solutions. Self-adaptation is a mechanism which allows individuals in the GA to choose their own mutation rate, and thus allows the GA to control when it explores new solutions or exploits old ones. We examine the use of this mechanism on a recently devised dynamic test suite, the Shaky Ladder Hyperplane-Defined Functions (sl-hdf's). This test suite can generate random problems with similar levels of difficulty and provides a platform allowing systematic controlled observations of the GA in dynamic environments. We show that in a variety of circumstances self-adaptation fails to allow the GA to perform better on this test suite than fixed mutation, even when the environment is static. We also show that mutation is beneficial throughout the run of a GA, and that seeding a population with known good genetic material is not always beneficial to the results. We provide explanations for these observations, with particular emphasis on comparing our results to other results  which have shown the GA to work in static environments. We conclude by giving suggestions as to how to change the simple GA to solve these problems.|William Rand,Rick L. Riolo","57868|GECCO|2006|A comparative study of immune system based genetic algorithms in dynamic environments|Diversity and memory are two major mechanisms used in biology to keep the adaptability of organisms in the ever-changing environment in nature. These mechanisms can be integrated into genetic algorithms to enhance their performance for problem optimization in dynamic environments. This paper investigates several GAs inspired by the ideas of biological immune system and transformation schemes for dynamic optimization problems. An aligned transformation operator is proposed and combined to the immune system based genetic algorithm to deal with dynamic environments. Using a series of systematically constructed dynamic test problems, experiments are carried out to compare several immune system based genetic algorithms, including the proposed one, and two standard genetic algorithms enhanced with memory and random immigrants respectively. The experimental results validate the efficiency of the proposed aligned transformation and corresponding immune system based genetic algorithm in dynamic environments.|Shengxiang Yang","57577|GECCO|2005|Memory-based immigrants for genetic algorithms in dynamic environments|Investigating and enhancing the performance of genetic algorithms in dynamic environments have attracted a growing interest from the community of genetic algorithms in recent years. This trend reflects the fact that many real world problems are actually dynamic, which poses serious challenge to traditional genetic algorithms. Several approaches have been developed into genetic algorithms for dynamic optimization problems. Among these approches, random immigrants and memory schemes have shown to be beneficial in many dynamic problems. This paper proposes a hybrid memory and random immigrants scheme for genetic algorithms in dynamic environments. In the hybrid scheme, the best solution in memory is retrieved and acts as the base to create random immigrants to replace the worst individuals in the population. In this way, not only can diversity be maintained but it is done more efficiently to adapt the genetic algorithm to the changing environment. The experimental results based on a series of systematically constructed dynamic problems show that the proposed memory-based immigrants scheme efficiently improves the performance of genetic algorithms in dynamic environments.|Shengxiang Yang","57354|GECCO|2005|An artificial immune network for multimodal function optimization on dynamic environments|Multimodal optimization algorithms inspired by the immune system are generally characterized by a dynamic control of the population size and by diversity maintenance along the search. One of the most popular proposals is denoted opt-aiNet (artificial immune network for optimization) and is extended here to deal with time-varying fitness functions. Additional procedures are designed to improve the overall performance and the robustness of the immune-inspired approach, giving rise to a version for dynamic optimization, denoted dopt-aiNet. Firstly, challenging benchmark problems in static multimodal optimization are considered to validate the new proposal. No parameter adjustment is necessary to adapt the algorithm according to the peculiarities of each problem. In the sequence, dynamic environments are considered, and usual evaluation indices are adopted to assess the performance of dopt-aiNet and compare with alternative solution procedures available in the literature.|Fabr√≠cio Olivetti de Fran√ßa,Fernando J. Von Zuben,Leandro Nunes de Castro","57285|GECCO|2005|Towards an analysis of dynamic environments|Although the interest in nature-inspired optimization of dynamic problems has been growing constantly over the past decade, very little has been done to analyze and characterize a changing fitness landscape. However, it would be very helpful for algorithm development to have a better understanding of the nature of fitness changes in dynamic real-world problems. In this paper, we propose a number of measures that can be used to analyze and characterize the dynamism in a problem changing over time. Additionally, we introduce a new dynamic multi-dimensional knapsack problem as a close-to-real-world test problem.|J√ºrgen Branke,Erdem Salihoglu,Sima Uyar","57556|GECCO|2005|Coordinating multi-rover systems evaluation functions for dynamic and noisy environments|This paper addresses the evolution of control strategies for a collective a set of entities that collectively strives to maximize a global evaluation function that rates the performance of the full system. Directly addressing such problems by having a population of collectives and applying the evolutionary algorithm to that population is appealing, but the search space is prohibitively large in most cases. Instead, we focus on evolving control policies for each member of the collective. The main difficulty with this approach is creating an evaluation function for each member of the collective that is both aligned with the global evaluation function and sensitive to the fitness changes of the member. We show how to construct evaluation functions in dynamic, noisy and communication-limited collective environments. On a rover coordination problem, a control policy evolved using aligned and member-sensitive evaluations outperforms global evaluation methods by up to %. More notably, in the presence of a larger number of rovers or rovers with noisy and communication limited sensors, the improvements due to the proposed method become significantly more pronounced.|Kagan Tumer,Adrian K. Agogino","57296|GECCO|2005|Diversity as a selection pressure in dynamic environments|Evolutionary algorithms (EAs) are widely used to deal with optimization problems in dynamic environments (DE) . When using EAs to solve DE problems, we are usually interested in the algorithm's ability to adapt and recover from the changes. One of the main problems facing an evolutionary method when solving DE problems is the loss of genetic diversity.In this paper, we investigate the use of evolutionary multi-objective optimization methods (EMOs) for single-objective DE problems. For that purpose, we introduce an artificial second objective with the aim to maintain useful diversity in the population. Six different artificial objectives are examined and compared.All the results will be compared against a traditional GA and the random immigrants algorithm. NSGA is employed as the evolutionary multi-objective technique.|Lam Thu Bui,J√ºrgen Branke,Hussein A. Abbass","65501|AAAI|2005|On the Evaluation of Dynamic Critiquing A Large-Scale User Study|Critiquing is an important form of feedback in conversational recommender systems. However, in these systems the user is usually limited to critiquing a single product feature at a time. Recently dynamic critiquing has been proposed to address this shortcoming, by automatically generating compound critiques over multiple features that may be presented to the user at recommendation time. To date a number of different versions of dynamic critiquing have been evaluated in isolation, and with reference to artificial users. In this paper we bring together the main flavors of dynamic critiquing and perform a large-scale comparative evaluation as part of an extensive real-user trial. This evaluation reveals some interesting facts about the way real users interact with critique-based recommenders.|Kevin McCarthy,Lorraine McGinty,Barry Smyth,James Reilly","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. G√ºrgen"]]}}