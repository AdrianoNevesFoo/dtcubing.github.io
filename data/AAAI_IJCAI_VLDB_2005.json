{"abstract":{"entropy":6.91180323562071,"topics":["natural language, data, present novel, scene understanding, data mining, recent years, management data, language generation, data stream, vector machine, applications data, present algorithms, emerging queries, support vector, machine data, database, machine learning, query, xml documents, natural generation","bayesian networks, description logic, belief change, reinforcement learning, learning approach, machine learning, logic programs, based reasoning, decision making, textual given, local space, disjunctive temporal, present based, knowledge base, learning, present learning, task classification, classification technique, learning models, text given","model-based diagnosis, combinatorial auctions, agents, sensor networks, self-interested agents, consider agents, key challenge, applications require, agents coalitions, task environments, robot, autonomous robot, mobile robot, systems complex, negotiation agents, agents environments, autonomous systems, multi-agent systems, environments, need systems","constraint satisfaction, constraint problem, web search, satisfaction problem, problem, consider problem, markov decision, planning domains, quantified boolean, planning, partially observable, optimization problem, constraint, markov mdps, search algorithms, boolean satisfiability, decision mdps, value variables, algorithms solving, search problem","natural language, scene understanding, language generation, natural generation, describe systems, context natural, language, information, systems, information systems, current systems, language information, current, language systems, automatic, research, developed, inference, querying, addresses","query optimization, different ontologies, query, best query, database systems, xml repositories, database, large number, large database, relational database, relational, database query, large, number, repositories, available, strategies, source, growing, answering","disjunctive temporal, classification technique, task classification, present dynamic, classification, technique, finding, task, temporal, formula, dynamic, tools, fundamental, search, question, selection, estimation, aims, instances, uncertain","bayesian networks, machine learning, learning, problem learning, local space, learning formally, study learning, study, analysis, features, probability, introduce, language, used, linear, related, capable, idea","sensor networks, networks agents, task agents, task, multiagent, based, complexity, communication, world, building, distributed, investigate, social, people, real-world, unknown, common, computational, paper, cost","model-based diagnosis, combinatorial auctions, systems use, models systems, models, use, state, predictive, dynamical, representation, protocol, perform, human, device, properties, address, settings, present, information, challenge","consider problem, markov decision, problem solving, partially observable, study problem, optimization problem, markov mdps, problem backbone, problem agents, decision mdps, markov processes, decision problem, decision processes, knowledge, decision, allows, framework, paper, introduce, effective","search problem, web search, search constraint, algorithms constraint, present algorithms, search, algorithms solving, search algorithms, search solutions, algorithms, problem algorithms, algorithms used, search used, used, propositional, heuristic, solve, recently, need, answer"],"ranking":[["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","80491|VLDB|2005|Flexible Database Generators|Evaluation and applicability of many database techniques, ranging from access methods, histograms, and optimization strategies to data normalization and mining, crucially depend on their ability to cope with varying data distributions in a robust way. However, comprehensive real data is often hard to come by, and there is no flexible data generation framework capable of modelling varying rich data distributions. This has led individual researchers to develop their own ad-hoc data generators for specific tasks. As a consequence, the resulting data distributions and query workloads are often hard to reproduce, analyze, and modify, thus preventing their wider usage. In this paper we present a flexible, easy to use, and scalable framework for database generation. We then discuss how to map several proposed synthetic distributions to our framework and report preliminary results.|Nicolas Bruno,Surajit Chaudhuri","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","16304|IJCAI|2005|Evaluating an NLG System using Post-Editing|Computer-generated texts, whether from Natural Language Generation (NLG) or Machine Translation (MT) systems, are often post-edited by humans before being released to users. The frequency and type of post-edits is a measure of how well the system works, and can be used for evaluation. We describe how we have used post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces weather forecasts.|Somayajulu Sripada,Ehud Reiter,Lezan Hawizy","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["16169|IJCAI|2005|Learning with Labeled Sessions|Traditional supervised learning deals with labeled instances. In many applications such as physiological data modeling and speaker identification, however, training examples are often labeled objects and each of the labeled objects consists of multiple unlabeled instances. When classifying a new object, its class is determined by the majority of its instance classes. As a consequence of this decision rule, one challenge to learning with labeled objects (or sessions) is to determine during training which subset of the instances inside an object should belong to the class of the object. We call this type of learning 'session-based learning' to distinguish it from the traditional supervised learning. In this paper, we introduce session-based learning problems, give a formal description of session-based learning in the context of related work, and propose an approach that is particularly designed for session-based learning. Empirical studies with UCI datasets and real-world data show that the proposed approach is effective for session-based learning.|Rong Jin,Huan Liu","65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","16029|IJCAI|2005|Exploiting Informative Priors for Bayesian Classification and Regression Trees|A general method for defining informative priors on statistical models is presented and applied specifically to the space of classification and regression trees. A Bayesian approach to learning such models from data is taken, with the Metropolis-Hastings algorithm being used to approximately sample from the posterior. By only using proposal distributions closely tied to the prior, acceptance probabilities are easily computable via marginal likelihood ratios, whatever the prior used. Our approach is empirically tested by varying (i) the data, (ii) the prior and (iii) the proposal distribution. A comparison with related work is given.|Nicos Angelopoulos,James Cussens","65616|AAAI|2005|Hidden Naive Bayes|The conditional independence assumption of naive Bayes essentially ignores attribute dependencies and is often violated. On the other hand, although a Bayesian network can represent arbitrary attribute dependencies, learning an optimal Bayesian network from data is intractable. The main reason is that learning the optimal structure of a Bayesian network is extremely time consuming. Thus, a Bayesian model without structure learning is desirable. In this paper, we propose a novel model, called hidden naive Bayes (HNB). In an HNB, a hidden parent is created for each attribute which combines the influences from all other attributes. We present an approach to creating hidden parents using the average of weighted one-dependence estimators. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. We propose an algorithm for learning HNB based on conditional mutual information. We experimentally test HNB in terms of classification accuracy, using the  UCI data sets recommended by Weka (Witten & Frank ), and compare it to naive Bayes (Langley, Iba, & Thomas ), C. (Quinlan ), SBC (Langley & Sage ), NBTree (Kohavi ), CL-TAN (Friedman, Geiger, & Goldszmidt ), and AODE (Webb, Boughton, & Wang ). The experimental results show that HNB outperforms naive Bayes, C., SBC, NBTree, and CL-TAN, and is competitive with AODE.|Harry Zhang,Liangxiao Jiang,Jiang Su","65576|AAAI|2005|Discriminative Training of Markov Logic Networks|Many machine learning applications require a combination of probability and first-order logic. Markov logic networks (MLNs) accomplish this by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Model parameters (i.e., clause weights) can be learned by maximizing the likelihood of a relational database, but this can be quite costly and lead to suboptimal results for any given prediction task. In this paper we propose a discriminative approach to training MLNs, one which optimizes the conditional likelihood of the query predicates given the evidence ones, rather than the joint likelihood of all predicates. We extend Collins's () voted perceptron algorithm for HMMs to MLNs by replacing the Viterbi algorithm with a weighted satisfiability solver. Experiments on entity resolution and link prediction tasks show the advantages of this approach compared to generative MLN training, as well as compared to purely probabilistic and purely logical approaches.|Parag Singla,Pedro Domingos","65546|AAAI|2005|Robust Textual Inference Via Learning and Abductive Reasoning|We present a system for textual inference (the task of inferring whether a sentence follows from another text) that uses learning and a logical-formula semantic representation of the text. More precisely, our system begins by parsing and then transforming sentences into a logical formula-like representation similar to the one used by (Harabagiu et al., ). An abductive theorem prover then tries to find the minimum \"cost\" set of assumptions necessary to show that one statement follows from the other. These costs reflect how likely different assumptions are, and are learned automatically using information from syntacticsemantic features and from linguistic resources such as WordNet. If one sentence follows from the other given only highly plausible, low cost assumptions, then we conclude that it can be inferred. Our approach can be viewed as combining statistical machine learning and classical logical reasoning, in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving. We give experimental results from the recent PASCAL RTE  challenge competition on recognizing textual inferences, where a system using this inference algorithm achieved the highest confidence weighted score.|Rajat Raina,Andrew Y. Ng,Christopher D. Manning","65497|AAAI|2005|Giving Advice about Preferred Actions to Reinforcement Learners Via Knowledge-Based Kernel Regression|We present a novel formulation for providing advice to a reinforcement learner that employs support-vector regression as its function approximator. Our new method extends a recent advice-giving technique, called Knowledge-Based Kernel Regression (KBKR), that accepts advice concerning a single action of a reinforcement learner. In KBKR, users can say that in some set of states, an action's value should be greater than some linear expression of the current state. In our new technique, which we call Preference KBKR (Pref-KBKR), the user can provide advice in a more natural manner by recommending that some action is preferred over another in the specified set of states. Specifying preferences essentially means that users are giving advice about policies rather than Q values, which is a more natural way for humans to present advice. We present the motivation for preference advice and a proof of the correctness of our extension to KBKR. In addition, we show empirical results that our method can make effective use of advice on a novel reinforcement-learning task, based on the RoboCup simulator, which we call Breakaway. Our work demonstrates the significant potential of advice-giving techniques for addressing complex reinforcement learning problems, while further demonstrating the use of support-vector regression for reinforcement learning.|Richard Maclin,Jude W. Shavlik,Lisa Torrey,Trevor Walker,Edward W. Wild","16240|IJCAI|2005|Inverse Resolution as Belief Change|Belief change is concerned with modelling the way in which a rational reasoner maintains its beliefs as it acquires new information. Of particular interest is the way in which new beliefs are acquired and determined and old beliefs are retained or discarded. A parallel can be drawn to symbolic machine learning approaches where examples to be categorised are presented to the learning system and a theory is subsequently derived, usually over a number of iterations. It is therefore not surprising that the term 'theory revision' is used to describe this process Ourston and Mooney, . Viewing a machine learning system as a rational reasoner allows us to begin seeing these seemingly disparate mechanisms in a similar light. In this paper we are concerned with characterising the well known inverse resolution operations Muggleton,   (and more recently, inverse entailment Muggleton, ) as AGM-style belief change operations. In particular, our account is based on the abductive expansion operation Pagnucco et al.,  Pagnucco,  and characterised by using the notion of epistemic entrenchment Grdenfors and Makinson,  extended for this operation. This work provides a basis for reconciling work in symbolic machine learning and belief revision. Moreover, it allows machine learning techniques to be understood as forms of nonmonotonic reasoning.|Maurice Pagnucco,David Rajaratnam","16139|IJCAI|2005|Learning Coordination Classifiers|We present a new approach to ensemble classification that requires learning only a single base classifier. The idea is to learn a classifier that simultaneously predicts pairs of test labels--as opposed to learning multiple predictors for single test labels-- then coordinating the assignment of individual labels by propagating beliefs on a graph over the data. We argue that the approach is statistically well motivated, even for independent identically distributed (iid) data. In fact, we present experimental results that show improvements in classification accuracy over single-example classifiers, across a range of iid data sets and over a set of base classifiers. Like boosting, the technique increases representational capacity while controlling variance through a principled form of classifier combination.|Yuhong Guo,Russell Greiner,Dale Schuurmans","65490|AAAI|2005|Distribution-Free Learning of Bayesian Network Structure in Continuous Domains|In this paper we present a method for learning the structure of Bayesian networks (BNs) without making any assumptions on the probability distribution of the domain. This is mainly useful for continuous domains, where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the Bayesian network, and only a few have been examined analytically. We therefore focus on BN structure learning in continuous domains. We address the problem by developing a conditional independence test for continuous variables, which can be readily used by any existing independence-based BN structure learning algorithm. Our test is non-parametric, making no assumptions on the distribution of the domain. We also provide an effective and computationally efficient method for calculating it from data. We demonstrate the learning of the structure of graphical models in continuous domains from real-world data, to our knowledge for the first time using independence-based methods and without distributional assumptions. We also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization, and verify desirable properties such as statistical consistency.|Dimitris Margaritis"],["65452|AAAI|2005|Non-Stationary Policy Learning in -Player Zero Sum Games|A key challenge in multiagent environments is the construction of agents that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting. These domains require on-line learning methods without the benefit of repeated training examples, as well as the ability to adapt to the evolving behavior of other agents in the environment. The difficulty is further exacerbated when the agents are in an adversarial relationship, demanding that a robust (i.e. winning) non-stationary policy be rapidly learned and adapted. We propose an on-line sequence learning algorithm, ELPH, based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non-stationary policies. We demonstrate the performance of this method in a non-stationary learning environment of adversarial zero-sum matrix games.|Steven Jensen,Daniel Boley,Maria L. Gini,Paul R. Schrater","65458|AAAI|2005|Towards Model-Based Diagnosis of Coordination Failures|With increasing deployment of multi-agent and distributed systems, there is an increasing need for failure diagnosis systems. While successfully tackling key challenges in multi-agent settings, model-based diagnosis has left open the diagnosis of coordination failures, where failures often lie in the boundaries between agents, and thus the inputs to the model--with which the diagnoser simulates the system to detect discrepancies--are not known. However, it is possible to diagnose such failures using a model of the coordination between agents. This paper formalizes model-based coordination diagnosis, using two coordination primitives (concurrence and mutual exclusion). We define the consistency-based and abductive diagnosis problems within this formalization, and show that both are NP-Hard by mapping them to other known problems.|Meir Kalech,Gal A. Kaminka","16252|IJCAI|2005|Learning against opponents with bounded memory|Recently, a number of authors have proposed criteria for evaluating learning algorithms in multiagent systems. While well-justified, each of these has generally given little attention to one of the main challenges of a multi-agent setting the capability of the other agents to adapt and learn as well. We propose extending existing criteria to apply to a class of adaptive opponents with bounded memory. We then show an algorithm that provably achieves an o-best response against this richer class of opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play. This new algorithm also demonstrates strong performance in empirical tests against a variety of opponents in a wide range of environments.|Rob Powers,Yoav Shoham","65561|AAAI|2005|Solving the Auction-Based Task Allocation Problem in an Open Environment|In this paper we analyze the process of allocating tasks to self-interested agents in uncertain changing open environments. The allocator in our model is responsible for the performance of dynamically arriving tasks using a second price reverse auction as the allocation protocol. Since the agents are self-interested (i.e. each agent attempts to maximize its own revenue), previous models concerning cooperative agents aiming for a joint goal are not applicable. Thus the main challenge is to identify a set of equilibrium strategies - a stable solution where no agent can benefit from changing its strategy given the other agents' strategies - for any specific environmental settings. We formulate the model and discuss the difficulty in extracting the agents' equilibrium strategies directly from the model's equations. Consequently we propose an efficient algorithm to accurately approximate the agents' equilibrium strategies. A comparative illustration through simulation of the system performance in a closed and open environments is given, emphasizing the advantage of the allocator operating in the latter environment, reaching results close to those obtained by a central enforceable allocation.|David Sarne,Sarit Kraus","65428|AAAI|2005|Agent-Organized Networks for Multi-Agent Production and Exchange|As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.|Matthew E. Gaston,Marie desJardins","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","16335|IJCAI|2005|Coalitional Games in Open Anonymous Environments|Coalition formation is a key aspect of automated negotiation among self-interested agents. In order for coalitions to be stable, a key question that must be answered is how the gains from cooperation are to be distributed. Various solution concepts (such as the Shapley value, core, least core, and nucleolus) have been proposed. In this paper, we demonstrate how these concepts are vulnerable to various kinds of manipulations in open anonymous environments such as the Internet. These manipulations include submitting false names (one acting as many), collusion (many acting as one), and the hiding of skills. To address these threats, we introduce a new solution concept called the anonymity-proof core, which is robust to these manipulations. We show that the anonymity-proof core is characterized by certain simple axiomatic conditions. Furthermore, we show that by relaxing these conditions, we obtain a concept called the least anonymity-proof core, which is guaranteed to be non-empty. We also show that computational hardness of manipulation may provide an alternative barrier to manipulation.|Makoto Yokoo,Vincent Conitzer,Tuomas Sandholm,Naoki Ohta,Atsushi Iwasaki","65588|AAAI|2005|Stable Service Placement on Dynamic Peer-to-Peer Networks A Heuristic for the Distributed Center Problem|The proliferation of wireless networks has underscored the need for systems capable of coping with sporadic network connectivity. The restriction of communication to neighboring hosts makes determining the global state especially difficult, if not impractical. This paper addresses the problem of coordinating the positions of an arbitrary number of services, encapsulated by mobile agents, in a dynamic peer-to-peer network. The agents' collective goal is to minimize the distance between hosts and services, even if the topology is changing constantly. We propose a distributed algorithm to efficiently calculate the stationary distribution of the network. This can be used as a hill climbing heuristic for agents to find near-optimal locations at which to provide services. Finally, we show that the agent-based hill climbing approach is temporally-stable relative to the instantaneous optimum.|Evan Sultanik,William C. Regli","65374|AAAI|2005|Coordination and Adaptation in Impromptu Teams|Coordinating a team of autonomous agents is one of the major challenges in building effective multiagcnt systems. Many techniques have been devised for this problem. and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques. though. is that the team members are developed together as a whole. In many multi agent scenarios. this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills. models. strategies. and coordination mechanisms. and no external organization is imposed upon them. In particular. we propose two techniques. one adaptive and one predictive. for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.|Michael H. Bowling,Peter McCracken","65611|AAAI|2005|Coalitional Games in Open Anonymous Environments|Coalition formation is a key aspect of automated negotiation among self-interested agents. In order for coalitions to be stable, a key question that must be answered is how the gains from cooperation are to be distributed. Various solution concepts (such as the Shapley value, core, least core, and nucleolus) have been proposed. In this paper, we demonstrate how these concepts are vulnerable to various kinds of manipulations in open anonymous environments such as the Internet. These manipulations include submitting false names (one acting as many), collusion (many acting as one), and the hiding of skills. To address these threats, we introduce a new solution concept called the anonymity-proof core, which is robust to these manipulations. We show that the anonymity-proof core is characterized by certain simple axiomatic conditions. Furthermore, we show that by relaxing these conditions, we obtain a concept called the least anonymity-proof core, which is guaranteed to be non-empty. We also show that computational hardness of manipulation may provide an alternative barrier to manipulation.|Makoto Yokoo,Vincent Conitzer,Tuomas Sandholm,Naoki Ohta,Atsushi Iwasaki"],["65485|AAAI|2005|Risk-Sensitive Planning with One-Switch Utility Functions Value Iteration|Decision-theoretic planning with nonlinear utility functions is important since decision makers are often risk-sensitive in high-stake planning situations. One-switch utility functions are an important class of nonlinear utility functions that can model decision makers whose decisions change with their wealth level. We study how to maximize the expected utility of a Markov decision problem for a given one-switch utility function, which is difficult since the resulting planning problem is not decomposable. We first study an approach that augments the states of the Markov decision problem with the wealth level. The properties of the resulting infinite Markov decision problem then allow us to generalize the standard risk-neutral version of value iteration from manipulating values to manipulating functions that map wealth levels to values. We use a probabilistic blocks-world example to demonstrate that the resulting risk-sensitive version of value iteration is practical.|Yaxin Liu,Sven Koenig","65351|AAAI|2005|The Achilles Heel of QBF|In recent years we have seen significant progress in the area of Boolean satisfiability (SAT) solving and its applications. As a new challenge, the community is now moving to investigate whether similar advances can be made in the use of Quantified Boolean Formulas (QBF). QBF provides a natural framework for capturing problem solving and planning in multi-agent settings. However, contrarily to single-agent planning, which can be effectively formulated as SAT, we show that a QBF approach to planning in a multi-agent setting leads to significant unexpected computational difficulties. We identify as a key difficulty of the QBF approach the fact that QBF solvers often end up exploring a much larger search space than the natural search space of the original problem. This is in contrast to the experience with SAT approaches. We also show how one can alleviate these problems by introducing two special QBF formulations and a new QBF solution strategy. We present experiments that show the effectiveness of our approach in terms of a significant improvement in performance compared to earlier work in this area. Our work also provides a general methodology for formulating adversarial scenarios in QBF.|Carlos Ans√≥tegui,Carla P. Gomes,Bart Selman","16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","16229|IJCAI|2005|Combination of Local Search Strategies for Rotating Workforce Scheduling Problem|Rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work places (e.g. industrial plants). Solving this problem is of a high practical relevance. In this paper we propose the combination of tabu search with random walk and min conflicts strategy to solve this problem. Computational results for benchmark examples in literature and other real life examples show that combination of tabu search with random walk and min conflicts strategy improves the performance of tabu search for this problem. The methods proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedules.|Nysret Musliu","16130|IJCAI|2005|QCSP-Solve A Solver for Quantified Constraint Satisfaction Problems|The Quantified Constraint Satisfaction Problem (QCSP) is a generalization of the CSP in which some variables are universally quantified. It has been shown that a solver based on an encoding of QCSP into QBF can outperform the existing direct QCSP approaches by several orders of magnitude. In this paper we introduce an efficient QCSP solver. We show how knowledge learned from the successful encoding of QCSP into QBF can be utilized to enhance the existing QCSP techniques and speed up search by orders of magnitude. We also show how the performance of the solver can be further enhanced by incorporating advanced look-back techniques such as CBJ and solution-directed pruning. Experiments demonstrate that our solver is several orders of magnitude faster than existing direct approaches to QCSP solving, and significantly outperforms approaches based on encoding QCSPs as QBFs.|Ian P. Gent,Peter Nightingale,Kostas Stergiou","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","16286|IJCAI|2005|Structural Symmetry Breaking|Symmetry breaking has been shown to be an important method to speed up the search in constraint satisfaction problems that contain symmetry. When breaking symmetry by dominance detection, a computationally efficient symmetry breaking scheme can be achieved if we can solve the dominance detection problem in polynomial time. We study the complexity of dominance detection when value and variable symmetry appear simultaneously in constraint satisfaction problems (CSPs) with single-valued variables and set-CSPs. We devise an efficient dominance detection algorithm for CSPs with single-valued variables that yields symmetry-free search trees and that is based on the abstraction to the actual, intuitive structure of a symmetric CSP.|Meinolf Sellmann,Pascal Van Hentenryck","16299|IJCAI|2005|Value Ordering for Finding All Solutions|In finding all solutions to a constraint satisfaction problem, or proving that there are none, with a search algorithm that backtracks chronologically and forms k-way branches, the order in which the values are assigned is immaterial. However, we show that if the values of a variable are assigned instead via a sequence of binary choice points, and the removal of the value just tried from the domain of the variable is propagated before another value is selected, the value ordering can affect the search effort. We show that this depends on the problem constraints for some types of constraints, we show that the savings in search effort can be significant, given a good value ordering.|Barbara M. Smith,Paula Sturdy"],["16172|IJCAI|2005|Automatic Text-to-Scene Conversion in the Traffic Accident Domain|In this paper, we describe a system that automatically converts narratives into D scenes. The texts, written in Swedish, describe road accidents. One of the program's key features is that it animates the generated scene using temporal relations between the events. We believe that this system is the first text-to-scene converter that is not restricted to invented narratives. The system consists of three modules natural language interpretation based on information extraction (IE) methods, a planning module that produces a geometric description of the accident, and finally a visualization module that renders the geometric description as animated graphics. An evaluation of the system was carried out in two steps First, we used standard IE scoring methods to evaluate the language interpretation. The results are on the same level as for similar systems tested previously. Secondly, we performed a small user study to evaluate the quality of the visualization. The results validate our choice of methods, and since this is the first evaluation of a text-to-scene conversion system, they also provide a baseline for further studies.|Richard Johansson,Anders Berglund,Magnus Danielsson,Pierre Nugues","16122|IJCAI|2005|Feature Generation for Text Categorization Using World Knowledge|We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.|Evgeniy Gabrilovich,Shaul Markovitch","16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins","80506|VLDB|2005|Bridging the Gap between OLAP and SQL|In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N , ranking , , spreadsheet computations , grouping sets , data cube , and moving sums  in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the clientserver architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.|Jens-Peter Dittrich,Donald Kossmann,Alexander Kreutz","16046|IJCAI|2005|A language for functional interpretation of model based simulation|Functional modeling is in use for the interpretation of the results of model based simulation of engineered systems for design analysis, enabling the automatic generation of a textual design analysis report that expresses the results of the simulation in terms of the system's purpose. We present a novel functional description language that increases the expressiveness of this approach, allowing a system function to be decomposed in terms of subsidiary functions as well as required effects, increasing the range both of systems and design analysis tasks for which the approach can be used.|Jonathan Bell,Neal Snooke,Chris Price","16329|IJCAI|2005|Appropriate Microplanning Choices for Low-Skilled Readers|We have developed a set of microplanning choice rules which are intended to enable Natural Language Generation (NLG) systems to generate appropriate texts for readers with below-average literacy, focusing in particular on choices related to how discourse structure is expressed (cue phrases, ordering, sentence structure). Evaluation experiments suggest that our rules do enhance the readability of texts for low-skilled readers, although there is still room for improvement.|Sandra Williams,Ehud Reiter","65579|AAAI|2005|Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation|I propose a representation formalism and algorithms to be used in a new language generation mechanism for text-to-text applications. The generation process is driven by both text-specific information encoded via probability distributions over words and phrases derived from the input text, and general language knowledge captured by n-gram and syntactic language models.|Radu Soricut","16304|IJCAI|2005|Evaluating an NLG System using Post-Editing|Computer-generated texts, whether from Natural Language Generation (NLG) or Machine Translation (MT) systems, are often post-edited by humans before being released to users. The frequency and type of post-edits is a measure of how well the system works, and can be used for evaluation. We describe how we have used post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces weather forecasts.|Somayajulu Sripada,Ehud Reiter,Lezan Hawizy","16058|IJCAI|2005|Viewing Referring Expression Generation as Search|Almost all natural language generation (NLG) systems are faced with the problem of the generation of referring expressions (GRE) given a symbol corresponding to an intended referent, how do we work out the semantic content of a referring expression that uniquely identifies the entity in question This is now one of the most widely explored problems in NLG over the last  years, a number of algorithms have been proposed for addressing different aspects of this problem, but the different approaches taken make it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we show how viewing the problem of referring expression generation as a search problem allows us to recast existing algorithms in a way that makes their similarities and differences clear.|Bernd Bohnet,Robert Dale","16085|IJCAI|2005|Learning to Understand Web Site Update Requests|Although Natural Language Processing (NLP) for requests for information has been well-studied, there has been little prior work on understanding requests to update information. In this paper, we propose an intelligent system that can process natural language website update requests semi-automatically. In particular, this system can analyze requests, posted via email, to update the factual content of individual tuples in a database-backed website. Users' messages are processed using a scheme decomposing their requests into a sequence of entity recognition and text classification tasks. Using a corpus generated by human-subject experiments, we experimentally evaluate the performance of this system, as well as its robustness in handling request types not seen in training, or user-specific language styles not seen in training.|William W. Cohen,Einat Minkov,Anthony Tomasic"],["65620|AAAI|2005|External-Memory Pattern Databases Using Structured Duplicate Detection|A pattern database is a lookup table that stores an exact evaluation function for a relaxed search problem, which provides an admissible heuristic for the original search problem. In general, the larger the pattern database, the more accurate the heuristic function. We consider how to build large pattern databases that are stored in external memory, such as disk, and how to use an external-memory pattern database efficiently in heuristic search. To limit the number of slow disk IO operations needed to construct and query an external-memory pattern data-base, we adapt an approach to external-memory graph search called structured duplicate detection that localizes memory references by leveraging an abstraction of the state space. We present results that show this approach increases the scalability of heuristic search by allowing larger and more accurate pattern database heuristics.|Rong Zhou,Eric A. Hansen","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80506|VLDB|2005|Bridging the Gap between OLAP and SQL|In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N , ranking , , spreadsheet computations , grouping sets , data cube , and moving sums  in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the clientserver architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.|Jens-Peter Dittrich,Donald Kossmann,Alexander Kreutz","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80577|VLDB|2005|General Purpose Database Summarization|In this paper, a message-oriented architecture for large database summarization is presented. The summarization system takes a database table as input and produces a reduced version of this table through both a rewriting and a generalization process. The resulting table provides tuples with less precision than the original but yet are very informative of the actual content of the database. This reduced form can be used as input for advanced data mining processes as well as some specific application presented in other works. We describe the incremental maintenance of the summarized table, the system capability to directly deal with XML database systems, and finally scalability which allows it to handle very large datasets of a million record.|R√©gis Saint-Paul,Guillaume Raschia,Noureddine Mouaddib","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["65503|AAAI|2005|Augmenting Disjunctive Temporal Problems with Finite-Domain Constraints|We present a general framework for augmenting instances of the Disjunctive Temporal Problem (DTP) with finite-domain constraints. In this new formalism, the bounds of the temporal constraints become conditional on the finite-domain assignment. This hybridization makes it possible to reason simultaneously about temporal relationships between events as well as their nontemporal properties. We provide a special case of this hybridization that allows reasoning about a limited form of spatial constraints namely, the travel time induced by the locations of a set of activities. We develop a least-commitment algorithm for efficiently finding solutions to this combined constraint system and provide empirical results demonstrating the effectiveness of our approach.|Michael D. Moffitt,Bart Peintner,Martha E. Pollack","16213|IJCAI|2005|Active Cost-Sensitive Learning|For many classification tasks a large number of instances available for training are unlabeled and the cost associated with the labeling process varies over the input space. Meanwhile, virtually all these problems require classifiers that minimize a nonuniform loss function associated with the classification decisions (rather than the accuracy or number of errors). For example, to train pattern classification models for a network intrusion detection task, experts need to analyze network events and assign them labels. This can be a very costly procedure if the instances to be labeled are selected at random. In the meantime, the loss associated with mislabeling an intrusion is much higher than the loss associated with the opposite error (i.e., labeling a legal event as being an intrusion). As a result, to address these types of tasks, practitioners need tools that minimize the total cost computed as a sum of the cost of labeling and the loss associated with the decisions. This paper describes an approach for addressing this problem.|Dragos D. Margineantu","65562|AAAI|2005|Dependency Parsing with Dynamic Bayesian Network|Exact parsing with finite state automata is deemed in-apropriate because of the unbounded non-locality languages overwhelmingly exhibit. We propose a way to structure the parsing task in order to make it amenable to local classification methods. This allows us to build a Dynamic Bayesian Network which uncovers the syntactic dependency structure of English sentences. Experiments with the Wall Street Journal demonstrate that the model successfully learns from labeled data.|Virginia Savova,Leonid Peshkin","65448|AAAI|2005|Clustering and Classifying Person Names by Origin|In natural language processing, information about a person's geographical origin is an important feature for name entity transliteration and question answering. We propose a language-independent name origin clustering and classification framework. Provided with a small amount of bilingual name translation pairs with labeled origins, we measure origin similarities based on the perplexities of name character language and translation models. We group similar origins into clusters, then train a Bayesian classifier with different features. It achieves % classification accuracy with source names only, and % with both source and target name pairs. We apply the origin clustering and classification technique to a name transliteration task. The cluster-specific transliteration model dramatically improves the transliteration accuracy from .% to %, reducing the transliteration character error rate from . to .. Adding more unlabeled name pairs to the cluster-specific name transliteration model further improves the transliteration accuracy.|Fei Huang,Stephan Vogel,Alex Waibel","16209|IJCAI|2005|Identifying Conflicts in Overconstrained Temporal Problems|We describe a strong connection between maximally satisfiable and minimally unsatisfiable subsets of constraint systems. Using this relationship, we develop a two-phase algorithm, employing powerful constraint satisfaction techniques, for the identification of conflicting sets of constraints in infeasible constraint systems. We apply this technique to overconstrained instances of the Disjunctive Temporal Problem (DTP), an expressive form of temporal constraint satisfaction problems. Using randomly-generated benchmarks, we provide experimental results that demonstrate how the algorithm scales with problem size and constraint density.|Mark H. Liffiton,Michael D. Moffitt,Martha E. Pollack,Karem A. Sakallah","16226|IJCAI|2005|Applying Local Search to Disjunctive Temporal Problems|We present a method for applying local search to overconstrained instances of the Disjunctive Temporal Problem (DTP). Our objective is to generate high quality solutions (i.e., solutions that violate few constraints) in as little time as possible. The technique presented here differs markedly from previous work on DTPs, as it operates within the total assignment space of the underlying CSP rather than the partial assignment space of the related meta-CSP. We provide experimental results demonstrating that the use of local search leads to substantially improved performance over systematic methods.|Michael D. Moffitt,Martha E. Pollack","16205|IJCAI|2005|Supervised Local Tangent Space Alignment for Classification|Supervised local tangent space alignment (SLTSA) is an extension of local tangent space alignment (LTSA) to supervised feature extraction. Two algorithmic improvements are made upon LTSA for classification. First a simple technique is proposed to map new data to the embedded low-dimensional space and make LTSA suitable in a changing, dynamic environment. Then SLTSA is introduced to deal with data sets containing multiple classes with class membership information.|Hongyu Li,Wenbin Chen,I-Fan Shen","65370|AAAI|2005|Weighted One-Against-All|The one-against-all reduction from multiclass classification to binary classification is a standard technique used to solve multiclass problems with binary classifiers. We show that modifying this technique in order to optimize its error transformation properties results in a superior technique, both experimentally and theoretically. This algorithm can also be used to solve a more general classification problem \"multi-label classification,\" which is the same as multiclass classification except that it allows multiple correct labels for a given example.|Alina Beygelzimer,John Langford,Bianca Zadrozny","80515|VLDB|2005|Scaling and Time Warping in Time Series Querying|The last few years have seen an increasing understanding that Dynamic Time Warping (DTW), a technique that allows local flexibility in aligning time series, is superior to the ubiquitous Euclidean Distance for time series classification, clustering, and indexing. More recently, it has been shown that for some problems, Uniform Scaling (US), a technique that allows global scaling of time series, may just be as important for some problems. In this work, we note that for many real world problems, it is necessary to combine both DTW and US to achieve meaningful results. This is particularly true in domains where we must account for the natural variability of human action, including biometrics, query by humming, motion-captureanimation, and handwriting recognition. We introduce the first technique which can handle both DTW and US simultaneously, and demonstrate its utility and effectiveness on a wide range of problems in industry, medicine, and entertainment.|Ada Wai-Chee Fu,Eamonn J. Keogh,Leo Yung Hang Lau,Chotirat (Ann) Ratanamahatana","65510|AAAI|2005|Temporal Dynamic Controllability Revisited|An important issue for temporal planners is the ability to handle temporal uncertainty. We revisit the question of how to determine whether a given set of temporal requirements are feasible in the light of uncertain durations of some processes. In particular, we consider how best to determine whether a network is Dynamically Controllable, i.e., whether a dynamic strategy exisls for executing the network that is guaranteed to satisfy the requirements. Previous work has shown the existence of a pseudo-polynomial algorithm for testing Dynamic Controllability. Here, we simplify the previous framework, and present a strongly polynomial algorithm with a termination criterion based on the structure of the network.|Paul H. Morris,Nicola Muscettola"],["16029|IJCAI|2005|Exploiting Informative Priors for Bayesian Classification and Regression Trees|A general method for defining informative priors on statistical models is presented and applied specifically to the space of classification and regression trees. A Bayesian approach to learning such models from data is taken, with the Metropolis-Hastings algorithm being used to approximately sample from the posterior. By only using proposal distributions closely tied to the prior, acceptance probabilities are easily computable via marginal likelihood ratios, whatever the prior used. Our approach is empirically tested by varying (i) the data, (ii) the prior and (iii) the proposal distribution. A comparison with related work is given.|Nicos Angelopoulos,James Cussens","16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando","16198|IJCAI|2005|Approximating Pseudo-Boolean Functions on Non-Uniform Domains|In Machine Learning (ML) and Evolutionary Computation (EC), it is often beneficial to approximate a complicated function by a simpler one, such as a linear or quadratic function, for computational efficiency or feasibility reasons (cf. Jin, ). A complicated function (the target function in ML or the fitness function in EC) may require an exponential amount of computation to learnevaluate, and thus approximations by simpler functions are needed. We consider the problem of approximating pseudo-Boolean functions by simpler (e.g., linear) functions when the instance space is associated with a probability distribution. We consider , n as a sample space with a (possibly nonuniform) probability measure on it, thus making pseudo-Boolean functions into random variables. This is also in the spirit of the PAC learning framework of Valiant Valiant,  where the instance space has a probability distribution on it. The best approximation to a target function f is then defined as the function g (from all possible approximating functions of the simpler form) that minimizes the expected distance to f. In an example, we use methods from linear algebra to find, in this more general setting, the best approximation to a given pseudo-Boolean function by a linear function.|Robert F. Lax,Guoli Ding,Peter P. Chen,Jianhua Chen","65613|AAAI|2005|Learning Measures of Progress for Planning Domains|We study an approach to learning heuristics for planning domains from example solutions. There has been little work on learning heuristics for the types of domains used in deterministic and stochastic planning competitions. Perhaps one reason for this is the challenge of providing a compact heuristic language that facilitates learning. Here we introduce a new representation for heuristics based on lists of set expressions described using taxonomic syntax. Next, we review the idea of a measure of progress (parmar ), which is any heuristic that is guaranteed to be improvable at every state. We take finding a measure of progress as our learning goal, and describe a simple learning algorithm for this purpose. We evaluate our approach across a range of deterministic and stochastic planning-competition domains. The results show that often greedily following the learned heuristic is highly effective. We also show our heuristic can be combined with learned rule-based policies, producing still stronger results.|Sung Wook Yoon,Alan Fern,Robert Givan","65525|AAAI|2005|Redescription Mining Structure Theory and Algorithms|We introduce a new data mining problem--redescription mining--that unifies considerations of conceptual clustering, constructive induction, and logical formula discovery. Redescription mining begins with a collection of sets, views it as a propositional vocabulary, and identifies clusters of data that can be defined in at least two ways using this vocabulary. The primary contributions of this paper are conceptual and theoretical (i) we formally study the space of redescriptions underlying a dataset and characterize their intrinsic structure, (ii) we identify impossibility as well as strong possibility results about when mining redescriptions is feasible, (iii) we present several scenarios of how we can custom-build redescription mining solutions for various biases, and (iv) we outline how many problems studied in the larger machine learning community are really special cases of redescription mining. By highlighting its broad scope and relevance. we aim to establish the importance of redescription mining and make the case for a thrust in this new line of research.|Laxmi Parida,Naren Ramakrishnan","16284|IJCAI|2005|Evolino Hybrid NeuroevolutionOptimal Linear Search for Sequence Learning|Current Neural Network learning algorithms are limited in their ability to model non-linear dynamical systems. Most supervised gradient-based recurrent neural networks (RNNs) suffer from a vanishing error signal that prevents learning from inputs far in the past. Those that do not, still have problems when there are numerous local minima. We introduce a general framework for sequence learning, EVOlution of recurrent systems with LINear outputs (Evolino). Evolino uses evolution to discover good RNN hidden node weights, while using methods such as linear regression or quadratic programming to compute optimal linear mappings from hidden state to output. Using the Long Short-Term Memory RNN Architecture, the method is tested in three very different problem domains ) context-sensitive languages, ) multiple superimposed sine waves, and ) the Mackey-Glass system. Evolino performs exceptionally well across all tasks, where other methods show notable deficiencies in some.|J√ºrgen Schmidhuber,Daan Wierstra,Faustino J. Gomez","65372|AAAI|2005|Semi-Supervised Sequence Modeling with Syntactic Topic Models|Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or content word. We then use this projection as additional input features to a linear-chain conditional random field trained on limited labeled training data. On standard part-of-speech tagging and Chinese word segmentation data sets we show as much as % error reduction due to the unlabeled data, and also statistically-significant improvements over a related semi-supervised sequence tagging method due to Miller et al.|Wei Li 0010,Andrew McCallum","65416|AAAI|2005|Learning CPG Sensory Feedback with Policy Gradient for Biped Locomotion for a Full-Body Humanoid|This paper describes a learning framework for a central pattern generator based biped locomotion controller using a policy gradient method. Our goals in this study are to achieve biped walking with a D hardware humanoid, and to develop an efficient learning algorithm with CPG by reducing the dimensionality of the state space used for learning. We demonstrate that an appropriate feed-back controller can be acquired within a thousand trials by numerical simulations and the obtained controller in numerical simulation achieves stable walking with a physical robot in the real world. Numerical simulations and hardware experiments evaluated walking velocity and stability. Furthermore, we present the possibility of an additional online learning using a hardware robot to improve the controller within  iterations.|Gen Endo,Jun Morimoto,Takamitsu Matsubara,Jun Nakanishi,Gordon Cheng","65437|AAAI|2005|Transforming between Propositions and Features Bridging the Gap|It is notoriously difficult to simultaneously deal with both probabilistic and structural representations in A.I., particularly because probability necessitates a uniform representation of the training examples. In this paper, we show how to build fully-specified probabilistic models from arbitrary propositional case descriptions about terrorist activities. Our method facilitates both reasoning and learning. Our solution is to use structural analogy to build probabilistic generalizations about those cases. We use these generalizations as a framework for mapping the structural representations, which are well-suited for reasoning, into features, which are well-suited for learning, and back again. Finally, we demonstrate how probabilistic generalizations are an excellent bridge for joining reasoning and learning by using them to perform a traditional machine learning technique, Bayesian network modeling, over arbitrarily high order structural data about terrorist actions, and further, we discuss how this might be used to facilitate automatic knowledge acquisition.|Daniel T. Halstead,Kenneth D. Forbus","65490|AAAI|2005|Distribution-Free Learning of Bayesian Network Structure in Continuous Domains|In this paper we present a method for learning the structure of Bayesian networks (BNs) without making any assumptions on the probability distribution of the domain. This is mainly useful for continuous domains, where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the Bayesian network, and only a few have been examined analytically. We therefore focus on BN structure learning in continuous domains. We address the problem by developing a conditional independence test for continuous variables, which can be readily used by any existing independence-based BN structure learning algorithm. Our test is non-parametric, making no assumptions on the distribution of the domain. We also provide an effective and computationally efficient method for calculating it from data. We demonstrate the learning of the structure of graphical models in continuous domains from real-world data, to our knowledge for the first time using independence-based methods and without distributional assumptions. We also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization, and verify desirable properties such as statistical consistency.|Dimitris Margaritis"],["16230|IJCAI|2005|Networked Distributed POMDPs A Synergy of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present a distributed policy generation algorithm that performs local search.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","65392|AAAI|2005|Multiple-Goal Recognition from Low-Level Signals|Researchers and practitioners from both the artificial intelligence and pervasive computing communities have been paying increasing attention to the task of inferring users' high-level goals from low-level sensor readings. A common assumption made by most approaches is that a user either has a single goal in mind, or achieves several goals sequentially. However, in real-world environments, a user often has multiple goals that are concurrently carried out, and a single action can serve as a common step towards multiple goals. In this paper, we formulate the multiple-goal recognition problem and exemplify it in an indoor environment where an RF-based wireless network is available. We propose a goal-recognition algorithm based on a dynamic model set and show how goal models evolve over time based on pre-defined states. Experiments with real data demonstrate that our method can accurately and efficiently recognize multiple interleaving goals in a user's trace.|Xiaoyong Chai,Qiang Yang","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","65428|AAAI|2005|Agent-Organized Networks for Multi-Agent Production and Exchange|As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.|Matthew E. Gaston,Marie desJardins","16261|IJCAI|2005|Using Predictive Representations to Improve Generalization in Reinforcement Learning|The predictive representations hypothesis holds that particularly good generalization will result from representing the state of the world in terms of predictions about possible future experience. This hypothesis has been a central motivation behind recent research in, for example, PSRs and TD networks. In this paper we present the first explicit investigation of this hypothesis. We show in a reinforcement-learning example (a grid-world navigation task) that a predictive representation in tabular form can learn much faster than both the tabular explicit-state representation and a tabular history-based method.|Eddie J. Rafols,Mark B. Ring,Richard S. Sutton,Brian Tanner","16255|IJCAI|2005|The Role of Clustering on the Emergence of Efficient Social Conventions|Multiagent models of the emergence of social conventions have demonstrated that global conventions can arise from local coordination processes without a central authority. We further develop and extend previous work to address how and under what conditions emerging conventions are also socially efficient, i.e. better for all agents than potential alternative conventions. We show with computational experiments that the clustering coefficient of the networks within which agents interact is an important condition for efficiency. We also develop an analytical approximation of the simulation model that sheds some light to the original model behavior. Finally, we combine two decision mechanisms, local optimization and imitation, to study the competition between efficient and attractive actions. Our main result is that in clustered networks a society converges to an efficient convention and is stable against invasion of sub-optimal conventions under a much larger range of conditions than in a non-clustered network. On the contrary, in non-clustered networks the convention finally established heavily depends on its initial support.|Josep M. Pujol,Jordi Delgado,Ramon Sang√ºesa,Andreas Flache","65588|AAAI|2005|Stable Service Placement on Dynamic Peer-to-Peer Networks A Heuristic for the Distributed Center Problem|The proliferation of wireless networks has underscored the need for systems capable of coping with sporadic network connectivity. The restriction of communication to neighboring hosts makes determining the global state especially difficult, if not impractical. This paper addresses the problem of coordinating the positions of an arbitrary number of services, encapsulated by mobile agents, in a dynamic peer-to-peer network. The agents' collective goal is to minimize the distance between hosts and services, even if the topology is changing constantly. We propose a distributed algorithm to efficiently calculate the stationary distribution of the network. This can be used as a hill climbing heuristic for agents to find near-optimal locations at which to provide services. Finally, we show that the agent-based hill climbing approach is temporally-stable relative to the instantaneous optimum.|Evan Sultanik,William C. Regli","16258|IJCAI|2005|PsychSim Modeling Theory of Mind with Decision-Theoretic Agents|Agent-based modeling of human social behavior is an increasingly important research area. A key factor in human social interaction is our beliefs about others, a theory of mind. Whether we believe a message depends not only on its content but also on our model of the communicator. How we act depends not only on the immediate effect but also on how we believe others will react. In this paper, we discuss PsychSim, an implemented multiagent-based simulation tool for modeling interactions and influence. While typical approaches to such modeling have used first-order logic, Psych-Sim agents have their own decision-theoretic model of the world, including beliefs about its environment and recursive models of other agents. Using these quantitative models of uncertainty and preferences, we have translated existing psychological theories into a decision-theoretic semantics that allow the agents to reason about degrees of believability in a novel way. We discuss PsychSim's underlying architecture and describe its application to a school violence scenario for illustration.|David V. Pynadath,Stacy Marsella","16296|IJCAI|2005|Decentralized Search in Networks Using Homophily and Degree Disparity|We propose a new algorithm for finding a target node in a network whose topology is known only locally. We formulate this task as a problem of decision making under uncertainty and use the statistical properties of the graph to guide this decision. This formulation uses the homophily and degree structure of the network simultaneously, differentiating our algorithm from those previously proposed in the literature. Because homophily and degree disparity are characteristics frequently observed in real-world networks, the algorithm we propose is applicable to a wide variety of networks, including two families that have received much recent attention small-world and scale-free networks.|√\u2013zg√ºr Simsek,David Jensen"],["16093|IJCAI|2005|Learning Forward Models for Robots|Forward models enable a robot to predict the effects of its actions on its own motor system and its environment. This is a vital aspect of intelligent behaviour, as the robot can use predictions to decide the best set of actions to achieve a goal. The ability to learn forward models enables robots to be more adaptable and autonomous this paper describes a system whereby they can be learnt and represented as a Bayesian network. The robot's motor system is controlled and explored using 'motor babbling'. Feedback about its motor system comes from computer vision techniques requiring no prior information to perform tracking. The learnt forward model can be used by the robot to imitate human movement.|Anthony M. Dearden,Yiannis Demiris","65375|AAAI|2005|Combining Stochastic and Greedy Search in Hybrid Estimation|Techniques for robot monitoring and diagnosis have been developed that perform state estimation using probabilistic hybrid discretecontinuous models. Exact inference in hybrid dynamic systems is, in general, intractable. Approximate algorithms are based on either ) greedy search, as in the case of k-best enumeration or ) stochastic search, as in the case of Rao-Blackwellised Particle Filtering (RBPF). In this paper we propose a new method for hybrid state estimation. The key insight is that stochastic and greedy search methods, taken together, are often particularly effective in practice. The new method combines the stochastic methods of RBPF with the greedy search of k-best in order to create a method that is effective for a wider range of estimation problems than the individual methods alone. We demonstrate this robustness on a simulated acrobatic robot, and show that this benefit comes at only a small performance penalty.|Lars Blackmore,Stanislav Funiak,Brian C. Williams","65509|AAAI|2005|Model-Based Monitoring and Diagnosis of Systems with Software-Extended Behavior|Model-based diagnosis has largely operated on hard-ware systems. However, in most complex systems today, hardware is augmented with software functions that influence the system's behavior. In this paper, hard-ware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Prior work introduced probabilistic, hierarchical, constraint-based automata (PHCA) to allow the uniform and compact encoding of both hard-ware and software behavior. This paper focuses on PHCA-based monitoring and diagnosis to ensure the robustness of complex systems. We introduce a novel approach that frames diagnosis over a finite time horizon as a soft constraint optimization problem (COP), allowing us to leverage an extensive body of efficient solution methods for COPs. The solutions to the COP correspond to the most likely evolutions of the complex system. We demonstrate our approach on a vision-based rover navigation system, and models of the SPHERES and Earth Observing One spacecraft.|Tsoline Mikaelian,Brian C. Williams,Martin Sachenbacher","65353|AAAI|2005|Mechanism Design for Single-Value Domains|In \"Single-Value domains\", each agent has the same private value for all desired outcomes. We formalize this notion and give new examples for such domains. including a \"SAT domain\" and a \"single-value combinatorial auctions\" domain. We study two informational models where the set of desired outcomes is public information (the \"known\" case). and where it is private information (the \"unknown\" case). Under the \"known\" assumption, we present several truthful approximation mechanisms. Additionally, we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain (where agent values are either zero or one) to a truthful mechanism, with only a small approximation loss. In contrast, we show that even positive results from the \"unknown single minded combinatorial auctions\" literature fail to extend to the \"unknown\" single-value case. We give a characterization of truthfulness in this case, demonstrating that the difference is subtle and surprising.|Moshe Babaioff,Ron Lavi,Elan Pavlov","16241|IJCAI|2005|Accurate and Low-cost Location Estimation Using Kernels|We present a novel method for indoor-location estimation using a vector-space model based on signals received from a wireless client. Our aim is to obtain an accurate mapping between the signal space and the physical space without incurring too much human calibration effort. This problem has traditionally been tackled through probabilistic models trained on manually labeled data, which are expensive to obtain. In this paper, we present a novel approach to building a mapping between the signalvector space and the physical location space using kernel canonical correlation analysis (KCCA). Its training requires much less human labor. Moreover, unlike traditional location-estimation systems that treat grid points as independent and discrete target classes during training, we use the physical location as a continuous feedback to build a similarity mapping using KCCA. We test our algorithm in a . wireless LAN environment, and demonstrate the advantage of our method in both accuracy and its ability to utilize a much smaller set of labeled training data than previous methods.|Jeffrey Junfeng Pan,James T. Kwok,Qiang Yang,Yiqiang Chen","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","80532|VLDB|2005|Personalized Systems Models and Methods from an IR and DB Perspective|In today's knowledge-driven society, information abundance and personal electronic device ubiquity have made it difficult for users to find the right information at the right time and at the right level of detail. To solve this problem, researchers have developed systems that adapt their behavior to the goals, tasks, interests, and other characteristics of their users. Based on models that capture important user characteristics, these personalized systems maintain their users' profiles and take them into account to customize the content generated or its presentation to the different individuals.|Yannis E. Ioannidis,Georgia Koutrika","16164|IJCAI|2005|Combining Memory and Landmarks with Predictive State Representations|It has recently been proposed that it is advantageous to have models of dynamical systems be based solely on observable quantities. Predictive state representations (PSRs) are a type of model that uses predictions about future observations to capture the state of a dynamical system. However, PSRs do not use memory of past observations. We propose a model called memory-PSRs that uses both memories of the past, and predictions of the future. We show that the use of memories provides a number of potential advantages. It can reduce the size of the model (in comparison to a PSR model). In addition many dynamical systems have memories that can serve as landmarks that completely determine the current state. The detection and recognition of landmarks is advantageous because they can serve to reset a model that has gotten off-track, as often happens when the model is learned from samples. This paper develops both memory-PSRs and the use and detection of landmarks.|Michael R. James,Britton Wolfe,Satinder P. Singh","16162|IJCAI|2005|Model minimization by linear PSR|Predictive state representation (PSR), proposed by Littman et al.,  Singh et al., , are a general representation for controlled dynamical systems. We present a sufficient condition under which a linear PSR compresses a POMDP representation.|Masoumeh T. Izadi,Doina Precup","16194|IJCAI|2005|Automatic Evaluation of Text Coherence Models and Representations|This paper investigates the automatic evaluation of text coherence for machine-generated texts. We introduce a fully-automatic, linguistically rich model of local coherence that correlates with human judgments. Our modeling approach relies on shallow text properties and is relatively inexpensive. We present experimental results that assess the predictive power of various discourse representations proposed in the linguistic literature. Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance.|Mirella Lapata,Regina Barzilay"],["16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","16030|IJCAI|2005|Language Learning in Multi-Agent Systems|We present the problem of learning to communicate in decentralized and stochastic environments, analyzing it formally in a decision-theoretic context and illustrating the concept experimentally. Our approach allows agents to converge upon coordinated communication and action over time.|Martin Allen,Claudia V. Goldman,Shlomo Zilberstein","16147|IJCAI|2005|Solving POMDPs with Continuous or Large Discrete Observation Spaces|We describe methods to solve partially observable Markov decision processes (POMDPs) with continuous or large discrete observation spaces. Realistic problems often have rich observation spaces, posing significant problems for standard POMDP algorithms that require explicit enumeration of the observations. This problem is usually approached by imposing an a priori discretisation on the observation space, which can be sub-optimal for the decision making task. However, since only those observations that would change the policy need to be distinguished, the decision problem itself induces a lossless partitioning of the observation space. This paper demonstrates how to find this partition while computing a policy, and how the resulting discretisation of the observation space reveals the relevant features of the application domain. The algorithms are demonstrated on a toy example and on a realistic assisted living task.|Jesse Hoey,Pascal Poupart","65485|AAAI|2005|Risk-Sensitive Planning with One-Switch Utility Functions Value Iteration|Decision-theoretic planning with nonlinear utility functions is important since decision makers are often risk-sensitive in high-stake planning situations. One-switch utility functions are an important class of nonlinear utility functions that can model decision makers whose decisions change with their wealth level. We study how to maximize the expected utility of a Markov decision problem for a given one-switch utility function, which is difficult since the resulting planning problem is not decomposable. We first study an approach that augments the states of the Markov decision problem with the wealth level. The properties of the resulting infinite Markov decision problem then allow us to generalize the standard risk-neutral version of value iteration from manipulating values to manipulating functions that map wealth levels to values. We use a probabilistic blocks-world example to demonstrate that the resulting risk-sensitive version of value iteration is practical.|Yaxin Liu,Sven Koenig","16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","16267|IJCAI|2005|Conditional Planning in the Discrete Belief Space|Probabilistic planning with observability restrictions, as formalized for example as partially observable Markov decision processes (POMDP), has a wide range of applications, but it is computationally extremely difficult. For POMDPs, the most general decision problems about existence of policies satisfying certain properties are undecidable. We consider a computationally easier form of planning that ignores exact probabilities, and give an algorithm for a class of planning problems with partial observability. We show that the basic backup step in the algorithm is NP-complete. Then we proceed to give an algorithm for the backup step, and demonstrate how it can be used as a basis of an efficient algorithm for constructing plans.|Jussi Rintanen","65422|AAAI|2005|Efficient Maximization in Solving POMDPs|We present a simple, yet effective improvement to the dynamic programming algorithm for solving partially observable Markov decision processes. The technique targets the vector pruning operation during the maximization step, a key source of complexity in POMDP algorithms. We identify two types of structures in the belief space and exploit them to reduce significantly the number of constraints in the linear programs used for pruning. The benefits of the new technique are evaluated both analytically and experimentally, showing that it can lead to significant performance improvement. The results open up new research opportunities to enhance the performance and scalability of several POMDP algorithms.|Zhengzhu Feng,Shlomo Zilberstein","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes"],["65581|AAAI|2005|Domain-Dependent Parameter Selection of Search-based Algorithms Compatible with User Performance Criteria|Search-based algorithms, like planners, schedulers and satisfiability solvers, are notorious for having numerous parameters with a wide choice of values that can affect their performance drastically. As a result, the users of these algorithms, who may not be search experts, spend a significant time in tuning the values of the parameters to get acceptable performance on their particular problem domains. In this paper, we present a learning-based approach for automatic tuning of search-based algorithms to help such users. The benefit of our methodology is that it handles diverse parameter types, performs effectively for a broad range of systematic as well as non-systematic search based solvers (the selected parameters could make the algorithms solve up to % problems while the bad parameters would lead to none being solved), incorporates user-specified performance criteria () and is easy to implement. Moreover, the selected parameter will satisfy  in the first try or the ranked candidates can be used along with  to minimize the number of times the parameter settings need to he adjusted until a problem is solved.|Biplav Srivastava,Anupam Mediratta","16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann","16153|IJCAI|2005|Optimal Refutations for Constraint Satisfaction Problems|Variable ordering heuristics have long been an important component of constraint satisfaction search algorithms. In this paper we study the behaviour of standard variable ordering heuristics when searching an insoluble (sub)problem. We employ the notion of an optimal refutation of an insoluble (sub)problem and describe an algorithm for obtaining it. We propose a novel approach to empirically looking at problem hardness and typical-case complexity by comparing optimal refutations with those generated by standard search heuristics. It is clear from our analysis that the standard variable orderings used to solve CSPs behave very differently on real-world problems than on random problems of comparable size. Our work introduces a potentially useful tool for analysing the causes of the heavy-tailed phenomenon observed in the runtime distributions of backtrack search procedures.|Tudor Hulubei,Barry O'Sullivan","16223|IJCAI|2005|Reducing Checks and Revisions in Coarse-grained MAC Algorithms|Arc consistency algorithms are widely used to prune the search space of Constraint Satisfaction Problems (CSPs). Coarse-grained arc consistency algorithms like AC-, AC-d and AC- are efficient when it comes to transforming a CSP to its arc-consistent equivalent. These algorithms repeatedly carry out revisions. Revisions require support checks for identifying and deleting all unsupported values from the domain of a variable. In revisions for difficult problems most values have some support. Indeed, most revisions are ineffective, i.e. they cannot delete any value and consume a lot of checks and time. We propose two solutions to overcome these problems. First we introduce the notion of a Support Condition (SC) which guarantees that a value has some support. SCs reduce support checks while maintaining arc consistency during search. Second we introduce the notion of a Revision Condition (RC) which guarantees that all values have support. A RC avoids a candidate revision and queue maintenance overhead. For random problems, SCs reduce the checks required by MAC- (MAC-) up to % (%). RCs avoid at least % of the total revisions. Combining the two results in reducing % of the solution time.|Deepak Mehta,Marc R. C. van Dongen","65414|AAAI|2005|Cost-Algebraic Heuristic Search|Heuristic search is used to efficiently solve the single-node shortest path problem in weighted graphs. In practice, however, one is not only interested in finding a short path, but an optimal path, according to a certain cost notion. We propose an algebraic formalism that captures many cost notions, like typical Quality of Service attributes. We thus generalize A*, the popular heuristic search algorithm. for solving optimal-path problem. The paper provides an answer to a fundamental question for AI search, namely to which general notion of cost, heuristic search algorithms can be applied. We proof correctness of the algorithms and provide experimental results that validate the feasibility of the approach.|Stefan Edelkamp,Shahid Jabbar,Alberto Lluch-Lafuente","16249|IJCAI|2005|Counting Solutions of CSPs A Structural Approach|Determining the number of solutions of a CSP has several applications in AI, in statistical physics, and in guiding backtrack search heuristics. It is a P-complete problem for which some exact and approximate algorithms have been designed. Successful CSP models often use high-arity, global constraints to capture the structure of a problem. This paper exploits such structure and derives polytime evaluations of the number of solutions of individual constraints. These may be combined to approximate the total number of solutions or used to guide search heuristics. We give algorithms for several of the main families of constraints and discuss the possible uses of such solution counts.|Gilles Pesant","16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji","16145|IJCAI|2005|LRTAk|LRTA* is a real-time heuristic search algorithm widely used. In each iteration it updates the heuristic estimate of the current state. Here we present LRTA*(k), a new LRTA*-based algorithm that is able to update the heuristic estimates of up to k states, not necessarily distinct. Based on bounded propagation, this updating strategy maintains heuristic admissibility, so the new algorithm keeps the good theoretical properties of LRTA*. Experimentally, we show that LRTA*(k) produces better solutions in the first trial and converges faster when compared with other state-of-the-art algorithms on benchmarks for real-time search.|Carlos Hern√°ndez,Pedro Meseguer","16044|IJCAI|2005|Proactive Algorithms for Scheduling with Probabilistic Durations|Proactive scheduling seeks to generate high quality solutions despite execution time uncertainty. Building on work in Beck and Wilson, , we conduct an empirical study of a number of algorithms for the job shop scheduling problem with probabilistic durations. The main contributions of this paper are the introduction and empirical analysis of a novel constraint-based search technique that can be applied beyond probabilistic scheduling problems, the introduction and empirical analysis of a number of deterministic filtering algorithms for probabilistic job shop scheduling, and the identification of a number of problem characteristics that contribute to algorithm performance.|J. Christopher Beck,Nic Wilson","16199|IJCAI|2005|A Greedy Approach to Establish Singleton Arc Consistency|In this paper, we propose a new approach to establish Singleton Arc Consistency (SAC) on constraint networks. While the principle of existing SAC algorithms involves performing a breadth-first search up to a depth equal to , the principle of the two algorithms introduced in this paper involves performing several runs of a greedy search (where at each step, arc consistency is maintained). It is then an original illustration of applying inference (i.e. establishing singleton arc consistency) by search. Using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. Further-more, both space and time complexities are quite competitive.|Christophe Lecoutre,St√©phane Cardon"]]},"title":{"entropy":6.363968864331424,"topics":["word disambiguation, word sense, sense disambiguation, learning for, combinatorial auction, activity recognition, learning, value functions, reinforcement learning, for robot, mobile robot, the web, the semantic, word with, between and, semantic for, and dynamic, the between, agents and, using for","for and, and, natural language, algorithms for, constraint satisfaction, logic programs, description logic, representation and, and constraint, with and, the and, the complexity, for constraint, and algorithms, and complexity, constraint problems, approach for, for, the case, logic","system for, bayesian networks, search for, framework for, local search, planning with, for planning, and system, and time, web search, solving problems, search, planning domains, the problems, heuristic search, and search, search problems, monitoring system, heuristic for, for control","distributed pomdps, for xml, for data, constraint pomdps, vector machine, management system, constraint optimization, distributed optimization, support vector, data stream, support machine, system data, for database, database system, optimization pomdps, adaptive for, data, query for, for stream, for optimization","for robot, learning robot, mobile robot, semantic for, the semantic, learning for, robot, semantic, for, application, environments, robust, autonomous, performance, role, relation, and","using for, using, using and, networks, classification, syntactic, approach, domains, study, model","for and, and, mapping for, model and, and programming, and consistency, logical, generation, integration","and reasoning, with and, description logic, logic programs, and logic, for logic, knowledge for, for programs, description for, with, for with, for reasoning, logic, reasoning with, reasoning, with logic, formulas, interactive, ordering, propositional","framework for, and time, decision for, for based, based, decision, tree, transitions, rule, phase, diagrams, querying, processes, modelling, markov","system for, and system, for control, monitoring system, for diagnosis, belief, multi-agent, behavior, space, complex, language, base, training, new","for xml, distributed pomdps, for optimization, constraint pomdps, constraint optimization, distributed optimization, distributed and, and pomdps, optimization pomdps, optimization and, adaptive for, for views, for text, distributed for, and xml, text, based, method, learning","the, query, queries, matching, through, with, feature, approximate, probability, documents, observation, segmentation"],"ranking":[["16200|IJCAI|2005|Growth of Motor Coordination in Early Robot Learning|We present an implementation of a model of very early sensory-motor development, guided by results from developmental psychology. Behavioral acquisition and growth is demonstrated through constraint-lifting mechanisms initiated by global state variables. The results show how staged competence can be shaped by qualitative behavior changes produced by anatomical, computational and maturational constraints.|Mark H. Lee,Qinggang Meng","65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","65393|AAAI|2005|Scaling Up Word Sense Disambiguation via Parallel Texts|A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL- English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL- English all-words task, and significantly outperforms the baseline of always choosing sense  of WordNet.|Yee Seng Chan,Hwee Tou Ng","65603|AAAI|2005|Unsupervised Activity Recognition Using Automatically Mined Common Sense|A fundamental difficulty in recognizing human activities is obtaining the labeled data needed to learn models of those activities. Given emerging sensor technology, however, it is possible to view activity data as a stream of natural language terms. Activity models are then mappings from such terms to activity names, and may be extracted from text corpora such as the web. We show that models so extracted are sufficient to automatically produce labeled segmentations of activity data with an accuracy of % over  activities, well above the .% baseline. The segmentation so obtained is sufficient to bootstrap learning, with accuracy of learned models increasing to %. To our knowledge, this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned, even for labeling.|Danny Wyatt,Matthai Philipose,Tanzeem Choudhury","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Korn√©l G. Mark√≥,Stefan Schulz,Udo Hahn","16081|IJCAI|2005|Unsupervised Learning of Semantic Relations between Concepts of a Molecular Biology Ontology|In this paper we present an unsupervised model for learning arbitrary relations between concepts of a molecular biology ontology for the purpose of supporting text mining and manual ontology building. Relations between named-entities are learned from the GENIA corpus by means of several standard natural language processing techniques. An in-depth analysis of the output of the system shows that the model is accurate and has good potentials for text mining and ontology building applications.|Massimiliano Ciaramita,Aldo Gangemi,Esther Ratsch,Jasmin Saric,Isabel Rojas","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","65570|AAAI|2005|Learning to Prevent Failure States for a Dynamically Balancing Robot|To achieve robust autonomy, robots must avoid getting stuck in states from which they cannot recover without external aid. While this is the role of the robot's control algorithms, these are often imperfect. We examine how to detect failures by observing the robot's internal sensors over time. For such cases, triggering a response when detecting the onset of a failure can increase the operational range of the robot. Concretely, we explore the use of supervised learning techniques to create a classifier that can detect a potential failure and trigger a response for a dynamically balancing robot. We present a fully implemented system, where the results clearly demonstrate an improved safety margin for the robot.|Jeremy Searock,Brett Browning","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["65542|AAAI|2005|Constraint-Based Preferential Optimization|We first show that the optimal and undominated outcomes of an unconstrained (and possibly cyclic) CP-net are the solutions of a set of hard constraints. We then propose a new algorithm for finding the optimal outcomes of a constrained CP-net which makes use of hard constraint solving. Unlike previous algorithms, this new algorithm works even with cyclic CP-nets. In addition. the algorithm is not tied to CP-nets, but can work with any preference formalism which produces a preorder over the outcomes. We also propose an approximation method which weakens the preference ordering induced by the CP-net, returning a larger set of outcomes, but provides a significant computational advantage. Finally, we describe a weighted constraint approach that allows to find good solutions even when optimals do not exist.|Steven David Prestwich,Francesca Rossi,Kristen Brent Venable,Toby Walsh","16133|IJCAI|2005|The Complexity of Quantified Constraint Satisfaction Problems under Structural Restrictions|We give a clear picture of the tractabilityintractability frontier for quantified constraint satisfaction problems (QCSPs) under structural restrictions. On the negative side, we prove that checking QCSP satisfiability remains PSPACE-hard for all known structural properties more general than bounded treewidth and for the incomparable hypergraph acyclicity. Moreover, if the domain is not fixed, the problem is PSPACE-hard even for tree-shaped constraint scopes. On the positive side, we identify relevant tractable classes, including QCSPs with prefix  having bounded hypertree width, and QCSPs with a bounded number of guards. The latter are solvable in polynomial time without any bound on domains or quantifier alternations.|Georg Gottlob,Gianluigi Greco,Francesco Scarcello","65446|AAAI|2005|Weighted Super Solutions for Constraint Programs|Super solutions to constraint programs guarantee that if a limited number of variables lose their values, repair solutions can be found by modifying a bounded number of assignments. However, in many application domains the classical super solutions framework is not expressive enough since it only reasons about the number of breaks in a solution and the number of changes that are necessary to find a repair. For example, in combinatorial auctions we may wish to guarantee that we can always find a repair solution whose revenue exceeds some threshold while limiting the cost associated with forming such a repair. In this paper we present the weighted super solution framework that involves two important extensions. Firstly, the set of variables that may lose their values is determined using a probabilistic approach enabling us to find repair solutions for assignments that are most likely to fail. Secondly, we include a mechanism for reasoning about the cost of repair. The proposed framework has been successfully used to find robust solutions to combinatorial auctions.|Alan Holland,Barry O'Sullivan","16083|IJCAI|2005|A Unified Theory of Structural Tractability for Constraint Satisfaction and Spread Cut Decomposition|In this paper we introduce a generic form of structural decomposition for the constraint satisfaction problem, which we call a guarded decomposition. We show that many existing decomposition methods can be characterized in terms of finding guarded decompositions satisfying certain specified additional conditions. Using the guarded decomposition framework we are also able to define a new form of decomposition, which we call a spread cut. We show that discovery of width k spread-cut decompositions is tractable for each k, and that the spread cut decomposition strongly generalize all existing decompositions except hypertrees. Finally we exhibit a family of hypergraphs Hn, for n  , ,  ..., where the width of the best hypertree decomposition of each Hn is at least n, but the width of the best spreadcut decomposition is at most n.|David A. Cohen,Peter Jeavons,Marc Gyssens","16153|IJCAI|2005|Optimal Refutations for Constraint Satisfaction Problems|Variable ordering heuristics have long been an important component of constraint satisfaction search algorithms. In this paper we study the behaviour of standard variable ordering heuristics when searching an insoluble (sub)problem. We employ the notion of an optimal refutation of an insoluble (sub)problem and describe an algorithm for obtaining it. We propose a novel approach to empirically looking at problem hardness and typical-case complexity by comparing optimal refutations with those generated by standard search heuristics. It is clear from our analysis that the standard variable orderings used to solve CSPs behave very differently on real-world problems than on random problems of comparable size. Our work introduces a potentially useful tool for analysing the causes of the heavy-tailed phenomenon observed in the runtime distributions of backtrack search procedures.|Tudor Hulubei,Barry O'Sullivan","16130|IJCAI|2005|QCSP-Solve A Solver for Quantified Constraint Satisfaction Problems|The Quantified Constraint Satisfaction Problem (QCSP) is a generalization of the CSP in which some variables are universally quantified. It has been shown that a solver based on an encoding of QCSP into QBF can outperform the existing direct QCSP approaches by several orders of magnitude. In this paper we introduce an efficient QCSP solver. We show how knowledge learned from the successful encoding of QCSP into QBF can be utilized to enhance the existing QCSP techniques and speed up search by orders of magnitude. We also show how the performance of the solver can be further enhanced by incorporating advanced look-back techniques such as CBJ and solution-directed pruning. Experiments demonstrate that our solver is several orders of magnitude faster than existing direct approaches to QCSP solving, and significantly outperforms approaches based on encoding QCSPs as QBFs.|Ian P. Gent,Peter Nightingale,Kostas Stergiou","65506|AAAI|2005|A Constraint Satisfaction Approach to Geospatial Reasoning|The large number of data sources on the Internet can be used to augment and verify the accuracy of geospatial sources, such as gazetteers and annotated satellite imagery. Data sources such as satellite imagery, maps, gazetteers and vector data have been traditionally used in geographic infonnation systems (GIS), but nontraditional geospatial data, such as online phone books and property records are more difficult to relate to imagery. In this paper, we present a novel approach to combining extracted information from imagery, road vector data, and online data sources. We represent the problem of identifying buildings in satellite images as a constraint satisfing problem (CSP) and use constraint programming to solve it. We apply this technique to real-world data sources in EI Segundo, CA and our experimental evaluation shows how this approach can accurately identify buildings when provided with both traditional and nontraditional data sources.|Martin Michalowski,Craig A. Knoblock","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder","16108|IJCAI|2005|Strong Equivalence for Logic Programs with Preferences|Recently, strong equivalence for Answer Set Programming has been studied intensively, and was shown to be beneficial for modular programming and automated optimization. In this paper we define the novel notion of strong equivalence for logic programs with preferences. Based on this definition we give, for several semantics for preference handling, necessary and sufficient conditions for programs to be strongly equivalent. These results provide a clear picture of the relationship of these semantics with respect to strong equivalence, which differs considerably from their relationship with respect to answer sets. Finally, based on these results, we present for the first time simplification methods for logic programs with preferences.|Wolfgang Faber,Kathrin Konczak"],["65429|AAAI|2005|Fast Planning in Domains with Derived Predicates An Approach Based on Rule-Action Graphs and Local Search|The ability to express \"derived predicates\" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of \"Rule-Action Graphs\", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina,Paolo Toninelli","65618|AAAI|2005|Simultaneous Heuristic Search for Conjunctive Subgoals|We study the problem of building effective heuristics for achieving cunjunctive goals from heuristics for individual goals. We consider a straightforward method for building conjunctive heuristics that smoothly trades off between previous common methods. In addition to first explicitly formulating the problem of designing conjunctive heuristics. our major contribution is the discovery that this straightforward method substantially outperforms previously used methods across a wide range of domains. Based on a single positive real parameter k, our heuristic measure sums the individual heuristic values for the subgoal conjuncts, each raised to the k'th power. Varying k allows loose approximation and combination of the previous min, max. and sum approaches, while mitigating some of the weaknesses in those approaches. Our empirical work shows that for many benchmark planning domains there exist fixed parameter values that perform well-- we give evidence that these values can be found automatically by training. Our method, applied to top-level conjunctive goals, shows dramatic improvements over the heuristic used in the FF planner across a wide range of planning competition benchmarks. Also, our heuristic, without computing landmarks, consistently improves upon the success ratio of a recently published landmark-based planner FF-L.|Lin Zhu,Robert Givan","80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","16045|IJCAI|2005|Improved Knowledge Acquisition for High-Performance Heuristic Search|We present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm. The approach addresses the known difficulty of tuning probabilistic search algorithms, such as genetic algorithms or simulated annealing, for a given search problem by the introduction of domain knowledge. We show that our approach is effective for developing heuristic algorithms for difficult combinatorial problems by solving benchmarks from the industrially relevant domain of VLSI detailed routing. In this paper we present advanced techniques for improving our knowledge acquisition approach. We also present a novel method that uses domain knowledge for the prioritisation of mutation operators, increasing the GA's efficiency noticeably.|J. P. Bekmann,Achim G. Hoffmann","16226|IJCAI|2005|Applying Local Search to Disjunctive Temporal Problems|We present a method for applying local search to overconstrained instances of the Disjunctive Temporal Problem (DTP). Our objective is to generate high quality solutions (i.e., solutions that violate few constraints) in as little time as possible. The technique presented here differs markedly from previous work on DTPs, as it operates within the total assignment space of the underlying CSP rather than the partial assignment space of the related meta-CSP. We provide experimental results demonstrating that the use of local search leads to substantially improved performance over systematic methods.|Michael D. Moffitt,Martha E. Pollack","65414|AAAI|2005|Cost-Algebraic Heuristic Search|Heuristic search is used to efficiently solve the single-node shortest path problem in weighted graphs. In practice, however, one is not only interested in finding a short path, but an optimal path, according to a certain cost notion. We propose an algebraic formalism that captures many cost notions, like typical Quality of Service attributes. We thus generalize A*, the popular heuristic search algorithm. for solving optimal-path problem. The paper provides an answer to a fundamental question for AI search, namely to which general notion of cost, heuristic search algorithms can be applied. We proof correctness of the algorithms and provide experimental results that validate the feasibility of the approach.|Stefan Edelkamp,Shahid Jabbar,Alberto Lluch-Lafuente","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","16274|IJCAI|2005|An Heuristic Search based Approach for Moving Objects Tracking|Fast and accurate tracking of moving objects in video streams is a critical process in computer vision. This problem can be formulated as exploration problems and thus can be expressed as a search into a state space based representation approach. However, these search problems are hard to solve because they involve search through a high dimensional space. In this paper, we describe an A* heuristic search for computing efficient search through a space of transformations corresponding to the D motion of the object, where most promising search alternatives are computed by means of integrating target dynamics into the search process, and ideas from information theory are used to guide the search. The paper includes evaluations with video streams that illustrate the efficiency and suitability for real-time vision tasks on general purpose hardware. Moreover, the computational cost to carry out the tracking task is smaller than real time requirements ( ms).|Elena S√°nchez-Nielsen,Mario Hern√°ndez-Tejera","65465|AAAI|2005|Search versus Knowledge for Solving Life and Death Problems in Go|In games research, Go is considered the classical board game that is most resistant to current AI techniques. Large-scale knowledge engineering has been considered indispensable for building state of the art programs, even for subproblems such as Life and Death, or tsume-Go. This paper describes the technologies behind TSUMEGO EXPLORER, a high-performance tsume-Go search engine for enclosed problems. In empirical testing. this engine outperforms GoTools. which has been the undisputedly best tsume-Go program for  years.|Akihiro Kishimoto,Martin M√ºller 0003","65473|AAAI|2005|Using Domain-Configurable Search Control for Probabilistic Planning|We describe how to improve the performance of MDP planning algorithms by modifying them to use the search-control mechanisms of planners such as TLPlan, SHOP, and TALplanner. In our experiments, modified versions of RTDP, LRTDP, and Value Iteration were exponentially faster than the original algorithms. On the largest problems the original algorithms could solve, the modified ones were about , times faster. On another set. of problems whose state spaces were more than , times larger than the original algorithms could solve, the modified algorithms took only about  second.|Ugur Kuter,Dana S. Nau"],["16230|IJCAI|2005|Networked Distributed POMDPs A Synergy of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present a distributed policy generation algorithm that performs local search.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","16250|IJCAI|2005|A Scalable Method for Multiagent Constraint Optimization|We present in this paper a new, complete method for distributed constraint optimization, based on dynamic programming. It is a utility propagation method, inspired by the sum-product algorithm, which is correct only for tree-shaped constraint networks. In this paper, we show how to extend that algorithm to arbitrary topologies using a pseudotree arrangement of the problem graph. Our algorithm requires a linear number of messages, whose maximal size depends on the induced width along the particular pseudotree chosen. We compare our algorithm with backtracking algorithms, and present experimental results. For some problem types we report orders of magnitude fewer messages, and the ability to deal with arbitrarily large problems. Our algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well.|Adrian Petcu,Boi Faltings","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","65542|AAAI|2005|Constraint-Based Preferential Optimization|We first show that the optimal and undominated outcomes of an unconstrained (and possibly cyclic) CP-net are the solutions of a set of hard constraints. We then propose a new algorithm for finding the optimal outcomes of a constrained CP-net which makes use of hard constraint solving. Unlike previous algorithms, this new algorithm works even with cyclic CP-nets. In addition. the algorithm is not tied to CP-nets, but can work with any preference formalism which produces a preorder over the outcomes. We also propose an approximation method which weakens the preference ordering induced by the CP-net, returning a larger set of outcomes, but provides a significant computational advantage. Finally, we describe a weighted constraint approach that allows to find good solutions even when optimals do not exist.|Steven David Prestwich,Francesca Rossi,Kristen Brent Venable,Toby Walsh","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","80556|VLDB|2005|The Integrated Microbial Genomes IMG System A Case Study in Biological Data Management|Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.|Victor M. Markowitz,Frank Korzeniewski,Krishna Palaniappan,Ernest Szeto,Natalia Ivanova,Nikos Kyrpides","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla"],["65583|AAAI|2005|Mobile Robot Mapping and Localization in Non-Static Environments|Whenever mobile robots act in the real world, they need to be able to deal with non-static objects. In the context of mapping, a common technique to deal with dynamic objects is to filter out the spurious measurements corresponding to such objects. In this paper, we present a novel approach to estimate typical configurations of dynamic areas in the environment of a mobile robot. Our approach clusters local grid maps to identify the possible configurations. We furthermore describe how these clusters can be utilized within a Rao-Blackwellized particle filter to localize a mobile robot in a non-static environment. In practical experiments carried out with a mobile robot in a typical office environment, we demonstrate the advantages of our approach compared to alternative techniques for mapping and localization in dynamic environments.|Cyrill Stachniss,Wolfram Burgard","16200|IJCAI|2005|Growth of Motor Coordination in Early Robot Learning|We present an implementation of a model of very early sensory-motor development, guided by results from developmental psychology. Behavioral acquisition and growth is demonstrated through constraint-lifting mechanisms initiated by global state variables. The results show how staged competence can be shaped by qualitative behavior changes produced by anatomical, computational and maturational constraints.|Mark H. Lee,Qinggang Meng","65556|AAAI|2005|Semantic Place Classification of Indoor Environments with Mobile Robots Using Boosting|Indoor environments can typically be divided into places with different functionalities like kitchens, offices, or seminar rooms. We believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of tasks such as human-robot interaction, path-planning, or localization. This paper presents a supervised learning approach to label different locations using boosting. We train a classifier using features extracted from vision and laser range data. Furthermore, we apply a Hidden Markov Model to increase the robustness of the final classification. Our technique has been implemented and tested on real robots as well as in simulation. The experiments demonstrate that our approach can be utilized to robustly classify places into semantic categories. We also present an example of localization using semantic labeling.|Axel Rottmann,√\u201Cscar Mart√≠nez Mozos,Cyrill Stachniss,Wolfram Burgard","65619|AAAI|2005|Semantic Scene Concept Learning by an Autonomous Agent|Scene understanding addresses the issue of \"what a scene contains\". Existing research on scene understanding is typically focused on classifying a scene into classes that are of the same category type. These approaches, although they solve some scene-understanding tasks successfully, in general fail to address the semantics in scene understanding. For example, how does an agent learn the concept label \"red\" and \"ball\" without being told that it is a color or a shape label in advance To cope with this problem, we have proposed a novel research called semantic scene concept learning. Our proposed approach models the task of scene understanding as a \"multi-labeling\" classification problem. Each scene instance perceived by the agent may receive multiple labels coming from different concept categories, where the goal of learning is to let the agent discover the semantic meanings, i.e., the set of relevant visual features, of the scene labels received. Our preliminary experiments have shown the effectiveness of our proposed approach in solving this special intra- and inter-category mixing learning task.|Weiyu Zhu","65410|AAAI|2005|Scavenging with a Laptop Robot|This synopsis presents Harvey Mudd College's entry into the  AAAI scavenger hunt competition. We are submiting a lap-controlled robot which uses commodity parts and limited sensors to localize itself and perform arrow following and object recognition.|Alan Davidson,Mac Mason,Susanna Ricco,Ben Tribelhorn,Zachary Dodds","16165|IJCAI|2005|Path-Planning for Autonomous Training on Robot Manipulators in Space|This paper describes the integration of robot path-planning and spatial task modeling into a software system that teaches the operation of a robot manipulator deployed on International Space Station (ISS). The system addresses the complexity of the manipulator, the limited direct view of the ISS exterior and the unpredictability of lighting conditions in the workspace. Robot path planning is used not for controlling the manipulator, but for automatically checking errors of a student learning to operate the manipulator and for automatically producing illustrations of good and bad motions in training.|Froduald Kabanza,Roger Nkambou,Khaled Belghith","65554|AAAI|2005|Remote Supervisory Control of a Humanoid Robot|For this demonstration, participants have the opportunity to control a humanoid robot located hundreds of miles away. The general task is to reach, grasp, and transport various objects in the vicinity of the robot. Although remote \"pick-and-place\" operations of this sort form the basis of numerous practical applications, they are frequently error-prone and fatiguing for human operators. Participants can experience the relative difficulty of remote manipulation both with and without the use of an assistive interface. This interface simplifies the task by injecting artificial intelligence in key places without seizing higher-level control from the operator. In particular, we demonstrate the benefits of two key components of the system a video display of predicted operator intentions, and a haptic-based controller for automated grasping.|Michael T. Rosenstein,Andrew H. Fagg,Robert Platt Jr.,John Sweeney,Roderic A. Grupen","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","65570|AAAI|2005|Learning to Prevent Failure States for a Dynamically Balancing Robot|To achieve robust autonomy, robots must avoid getting stuck in states from which they cannot recover without external aid. While this is the role of the robot's control algorithms, these are often imperfect. We examine how to detect failures by observing the robot's internal sensors over time. For such cases, triggering a response when detecting the onset of a failure can increase the operational range of the robot. Concretely, we explore the use of supervised learning techniques to create a classifier that can detect a potential failure and trigger a response for a dynamically balancing robot. We present a fully implemented system, where the results clearly demonstrate an improved safety margin for the robot.|Jeremy Searock,Brett Browning","65491|AAAI|2005|NavBot The Navigational Search-and-Rescue Robot|The Stony Brook Robot Design Team has focused on two main areas of research in the creation of NavBot, our new robot created for the American Association of Artificial Intelligence's (AAAI) Scavenger Hunt Event navigation and computer vision. The purpose is to create an intelligent machine that is able to navigate the conference floor for specific objects at the AAAI Conference in Pittsburgh, Pennsylvania. To achieve the desirable speeds required in a rescue robot, NavBot utilizes high-rpm servo motors with a manually adjustable gear train. The two V servo motors enhance the maneuverability of the robot by providing minor adjustments in the robot's speed as it navigates an area. The manually adjustable gear trains allow NavBot to function in different environments. This is achieved by controlling the force and the speed required for maneuverability in the terrain. NavBot also utilizes a dual-track motion system to handle rough terrain. To enhance the power of NavBot, it was designed with a light aluminum skeleton. In addition to the large space provided for necessary electrical and computer equipment, the skeleton serves as a secure casing that prevent, damage to vital parts.|Matthew Marge,Ayman Sawas,Juan Carlos Liberato,Murtaza M. Karim,Manish Muttreja,Nader Alrawahi,Brian Fink"],["65556|AAAI|2005|Semantic Place Classification of Indoor Environments with Mobile Robots Using Boosting|Indoor environments can typically be divided into places with different functionalities like kitchens, offices, or seminar rooms. We believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of tasks such as human-robot interaction, path-planning, or localization. This paper presents a supervised learning approach to label different locations using boosting. We train a classifier using features extracted from vision and laser range data. Furthermore, we apply a Hidden Markov Model to increase the robustness of the final classification. Our technique has been implemented and tested on real robots as well as in simulation. The experiments demonstrate that our approach can be utilized to robustly classify places into semantic categories. We also present an example of localization using semantic labeling.|Axel Rottmann,√\u201Cscar Mart√≠nez Mozos,Cyrill Stachniss,Wolfram Burgard","16183|IJCAI|2005|Using Neutral Examples for Learning Polarity|Sentiment analysis is an example of polarity learning. Most research on learning to identify sentiment ignores \"neutral\" examples and instead performs training and testing using only examples of significant polarity. We show that it is crucial to use neutral examples in learning polarity for a variety of reasons and show how neutral examples help us obtain superior classification results in two sentiment analysis test-beds.|Moshe Koppel,Jonathan Schler","16206|IJCAI|2005|Location-Based Activity Recognition using Relational Markov Networks|In this paper we define a general framework for activity recognition by building upon and extending Relational Markov Networks. Using the example of activity recognition from location data, we show that our model can represent a variety of features including temporal information such as time of day, spatial information extracted from geographic databases, and global constraints such as the number of homes or workplaces of a person. We develop an efficient inference and learning technique based on MCMC. Using GPS location data collected by multiple people we show that the technique can accurately label a person's activity locations. Furthermore, we show that it is possible to learn good models from less data by using priors extracted from other people's data.|Lin Liao,Dieter Fox,Henry A. Kautz","80558|VLDB|2005|Using a Fuzzy Classification Query Language for Customer Relationship Management|A key challenge for companies is to manage customer relationships as an asset. To create an effective toolkit for the analysis of customer relationships, a combination of relational databases and fuzzy logic is proposed. The fuzzy Classification Query Language allows marketers to improve customer equity, launch loyalty programs, automate mass customization, and refine marketing campaigns.|Andreas Meier,Nicolas Werro,Martin Albrecht,Miltiadis Sarakinos","65362|AAAI|2005|Using SAT and Logic Programming to Design Polynomial-Time Algorithms for Planning in Non-Deterministic Domains|We show that a Horn SAT and logic programming approach to obtain polynomial time algorithms for problem solving can be fruitfully applied to finding plans for various kinds of goals in a non-deterministic domain. We particularly focus on finding weak, strong, and strong cyclic plans for planning problems, as they are the most studied ones in the literature. We describe new algorithms for these problems and show how non-monotonic logic programming can be used to declaratively compute strong cyclic plans. As a further benefit, preferred plans among alternative candidate plans may be singled out this way. We give complexity results for weak. strong, and strong cyclic planning. Finally, we briefly discuss some of the kinds of goals in non-deterministic domains for which the approach in the paper can be used.|Chitta Baral,Thomas Eiter,Jicheng Zhao","16170|IJCAI|2005|A Novel Approach to Model Generation for Heterogeneous Data Classification|Ensemble methods such as bagging and boosting have been successfully applied to classification problems. Two important issues associated with an ensemble approach are how to generate models to construct an ensemble, and how to combine them for classification. In this paper, we focus on the problem of model generation for heterogeneous data classification. If we could partition heterogeneous data into a number of homogeneous partitions, we will likely generate reliable and accurate classification models over the homogeneous partitions. We examine different ways of forming homogeneous subsets and propose a novel method that allows a data point to be assigned multiple times in order to generate homogeneous partitions for ensemble learning. We present the details of the new algorithm and empirical studies over the UCI benchmark datasets and datasets of image classification, and show that the proposed approach is effective for heterogeneous data classification.|Rong Jin,Huan Liu","16305|IJCAI|2005|Inter-Agent Communication A Cost-Reduction Approach Using an Autonomous Mobile Mailbox|In this paper, we consider a mobile mailbox communication scheme to reduce inter-agent communication costs. We employ a mailbox mobility strategy based on the ability of the mailbox to predict variations in inbound message rates and to migrate, if necessary, to a potentially better position in the network.|Armin Stranjak,Igor Cavrak,Mario Zagar","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","16296|IJCAI|2005|Decentralized Search in Networks Using Homophily and Degree Disparity|We propose a new algorithm for finding a target node in a network whose topology is known only locally. We formulate this task as a problem of decision making under uncertainty and use the statistical properties of the graph to guide this decision. This formulation uses the homophily and degree structure of the network simultaneously, differentiating our algorithm from those previously proposed in the literature. Because homophily and degree disparity are characteristics frequently observed in real-world networks, the algorithm we propose is applicable to a wide variety of networks, including two families that have received much recent attention small-world and scale-free networks.|√\u2013zg√ºr Simsek,David Jensen","65473|AAAI|2005|Using Domain-Configurable Search Control for Probabilistic Planning|We describe how to improve the performance of MDP planning algorithms by modifying them to use the search-control mechanisms of planners such as TLPlan, SHOP, and TALplanner. In our experiments, modified versions of RTDP, LRTDP, and Value Iteration were exponentially faster than the original algorithms. On the largest problems the original algorithms could solve, the modified ones were about , times faster. On another set. of problems whose state spaces were more than , times larger than the original algorithms could solve, the modified algorithms took only about  second.|Ugur Kuter,Dana S. Nau"],["80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","16103|IJCAI|2005|A Uniform Integration of Higher-Order Reasoning and External Evaluations in Answer-Set Programming|We introduce HEX programs, which are nonmonotonic logic programs admitting higher-order atoms as well as external atoms, and we extend the well-known answer-set semantics to this class of programs. Higher-order features are widely acknowledged as useful for performing meta-reasoning, among other tasks. Furthermore, the possibility to exchange knowledge with external sources in a fully declarative framework such as Answer-Set Programming (ASP) is nowadays important, in particular in view of applications in the Semantic Web area. Through external atoms, HEX programs can model some important extensions to ASP, and are a useful KR tool for expressing various applications. Finally, complexity and implementation issues for a preliminary prototype are discussed.|Thomas Eiter,Giovambattista Ianni,Roman Schindlauer,Hans Tompits","16170|IJCAI|2005|A Novel Approach to Model Generation for Heterogeneous Data Classification|Ensemble methods such as bagging and boosting have been successfully applied to classification problems. Two important issues associated with an ensemble approach are how to generate models to construct an ensemble, and how to combine them for classification. In this paper, we focus on the problem of model generation for heterogeneous data classification. If we could partition heterogeneous data into a number of homogeneous partitions, we will likely generate reliable and accurate classification models over the homogeneous partitions. We examine different ways of forming homogeneous subsets and propose a novel method that allows a data point to be assigned multiple times in order to generate homogeneous partitions for ensemble learning. We present the details of the new algorithm and empirical studies over the UCI benchmark datasets and datasets of image classification, and show that the proposed approach is effective for heterogeneous data classification.|Rong Jin,Huan Liu","16038|IJCAI|2005|Propagating Logical Combinations of Constraints|Many constraint toolkits provide logical connectives like disjunction, negation and implication. These permit complex constraint expressions to be built from primitive constraints. However, the propagation of such complex constraint expressions is typically limited. We therefore present a simple and light weight method for propagating complex constraint expressions. We provide a precise characterization of when this method enforces generalized arc-consistency. In addition, we demonstrate that with our method many different global constraints can be easily implemented.|Fahiem Bacchus,Toby Walsh","16320|IJCAI|2005|Analogy Generation with HowNet|Analogy is a powerful boundary-transcending process that exploits a conceptual system's ability to perform controlled generalization in one domain and re-specialization into another. The result of this semantic leap is the transference of meaning from one concept to another from which metaphor derives its name (literally to carry over). Such generalization and respecialization can be achieved using a variety of representations and techniques, most notably abstraction via a taxonomic backbone, or selective projection via structure-mapping on propositional content. In this paper we explore the extent to which a bilingual lexical ontology for English and Chinese, called HowNet, can support both approaches to analogy.|Tony Veale","65505|AAAI|2005|Knowledge Integration for Description Logics|Description logic reasoners are able to detect incoherences (such as logical inconsistency and concept unsatisfiability) in knowledge bases, but provide little support for resolving them. We propose to recast techniques for propositional inconsistency management into the description logic setting. We show that the additional structure afforded by description logic statements can be used to refine these techniques. Our focus in this paper is on the formal semantics for such techniques, although we do provide high-level decision procedures for the knowledge integration strategies discussed.|Thomas Andreas Meyer,Kevin Lee,Richard Booth","16341|IJCAI|2005|A Logical Model of Nash Bargaining Solution|This paper presents a logical extension of Nash's Cooperative Bargaining Theory. We introduce a concept of entrenchment measurement, which maps propositions to real numbers, as a vehicle to represent agent's belief states and attitudes towards bargaining situations. We show that Nash's bargaining solution can be restated in terms of bargainers belief states. Negotiable items, bargaining outcomes and conflicting arguments can then be explicitly expressed in propositional logic meanwhile Nash's numerical solution to bargaining problem is still applicable.|Dongmo Zhang","16294|IJCAI|2005|First-Order Logical Filtering|Logical filtering is the process of updating a belief state (set of possible world states) after a sequence of executed actions and perceived observations. In general, it is intractable in dynamic domains that include many objects and relationships. Still, potential applications for such domains (e.g., semantic web, autonomous agents, and partial-knowledge games) encourage research beyond immediate intractability results. In this paper we present polynomial-time algorithms for filtering belief states that are encoded as First-Order Logic (FOL) formulae. We sidestep previous discouraging results, and show that our algorithms are exact in many cases of interest. These algorithms accept belief states in full FOL, which allows natural representation with explicit references to unidentified objects, and partially known relationships. Our algorithms keep the encoding compact for important classes of actions, such as STRIPS actions. These results apply to most expressive modeling languages, such as partial databases and belief revision in FOL.|Afsaneh Shirazi,Eyal Amir","65531|AAAI|2005|A Framework for Bayesian Network Mapping|This research is motivated by the need to support inference across multiple intelligence systems involving uncertainty. Our objective is to develop a theoretical framework and related inference methods to map semantically similar variables between separate Bayesian networks in a principled way. The work is to be conducted in two steps. In the first step, we investigate the problem of formalizing the mapping between variables in two separate BNs with different semantics and distributions as pair-wise linkages. In the second step, we aim to justify the mapping between networks as a set of selected variable linkages, and then conduct inference along it.|Rong Pan,Yun Peng"],["65598|AAAI|2005|A Theory of Forgetting in Logic Programming|The study of forgetting for reasoning has attracted considerable attention in AI. However, much of the work on forgetting, and other related approaches such as independence, irrelevance and novelty, has been restricted to the classical logics. This paper describes a detailed theoretical investigation of the notion of forgetting in the context of logic programming. We first provide a semantic definition of forgetting under the answer sets for extended logic programs. We then discuss the desirable properties and some motivating examples. An important result of this study is an algorithm for computing the result of forgetting in a logic program. Furthermore, we present a modified version of the algorithm and show that the time complexity of the new algorithm is polynomial with respect to the size of the given logic program if the size of certain rules is fixed. We show how the proposed theory of forgetting can be used to characterize the logic program updates.|Kewen Wang,Abdul Sattar,Kaile Su","16211|IJCAI|2005|Discovering Classes of Strongly Equivalent Logic Programs|In this paper we apply computer-aided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics. Our discovered theorems capture new classes of strongly equivalent logic programs that can lead to new program simplification rules that preserve strong equivalence. Specifically, with the help of computers, we discovered exact conditions that capture the strong equivalence between a rule and the empty set, between two rules, between two rules and one of the two rules, between two rules and another rule, and between three rules and two of the three rules.|Fangzhen Lin,Yin Chen","16269|IJCAI|2005|Asymptotic Conditional Probability in Modal Logic A Probabilistic Reconstruction of Nonmonotonic Logic|We analyze the asymptotic conditional validity of modal formulas, i.e., the probability that a formula  is valid in the finite Kripke structures in which a given modal formula  is valid, when the size of these Kripke structures grows to infinity. We characterize the formulas  that are almost surely valid (i.e., with probability ) in case  is a flat, S- consistent formula, and show that these formulas  are exactly those which follow from  according to the nonmonotonic modal logic SG. Our results provide - for the first time - a probabilistic semantics to a well-known nonmonotonic modal logic, establishing a new bridge between nonmonotonic and probabilistic reasoning, and give a computational account of the asymptotic conditional validity problem in Kripke structures.|Riccardo Rosati,Georg Gottlob","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","16067|IJCAI|2005|Declarative and Computational Properties of Logic Programs with Aggregates|We investigate the properties of logic programs with aggregates. We mainly focus on programs with monotone and antimonotone aggregates (LPm,aA programs). We define a new notion of unfounded set for (LPm,aA programs, and prove that it is a sound generalization of the standard notion of unfounded set for aggregate-free programs. We show that the answer sets of an LPm,aA program are precisely its unfounded-free models. We define a well-founded operator WP for LPm,aA programs we prove that its total fixpoints are precisely the answer sets of P, and its least fixpoint WPw() is contained in the intersection of all answer sets (if P admits an answer set). WPW() is efficiently computable, and for aggregate-free programs it coincides with the well-founded model. We carry out an in-depth complexity analysis in the general framework, including also nonmonotone aggregates. We prove that monotone and anti-monotone aggregates do not increase the complexity of cautious reasoning, which remains in co-NP. Nonmonotone aggregates, instead, do increase the complexity by one level in the polynomial hierarchy. Our results allow also to generalize and speed-up ASP systems with aggregates.|Francesco Calimeri,Wolfgang Faber,Nicola Leone,Simona Perri","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16160|IJCAI|2005|Equivalence in Abductive Logic|We consider the problem of identifying equivalence of two knowledge bases which are capable of abductive reasoning. Here, a knowledge base is written in either first-order logic or nonmonotonic logic programming. In this work, we will give two definitions of abductive equivalence. The first one, explainable equivalence, requires that two abductive programs have the same explainability for any observation. Another one, explanatory equivalence, guarantees that any observation has exactly the same explanations in each abductive framework. Explanatory equivalence is a stronger notion than explainable equivalence. In first-order abduction, explainable equivalence can be verified by the notion of extensional equivalence in default theories. In nonmonotonic logic programs, explanatory equivalence can be checked by means of the notion of relative strong equivalence. We also show the complexity results for abductive equivalence.|Katsumi Inoue,Chiaki Sakama","16108|IJCAI|2005|Strong Equivalence for Logic Programs with Preferences|Recently, strong equivalence for Answer Set Programming has been studied intensively, and was shown to be beneficial for modular programming and automated optimization. In this paper we define the novel notion of strong equivalence for logic programs with preferences. Based on this definition we give, for several semantics for preference handling, necessary and sufficient conditions for programs to be strongly equivalent. These results provide a clear picture of the relationship of these semantics with respect to strong equivalence, which differs considerably from their relationship with respect to answer sets. Finally, based on these results, we present for the first time simplification methods for logic programs with preferences.|Wolfgang Faber,Kathrin Konczak","16049|IJCAI|2005|Encoding formulas with partially constrained weights in a possibilistic-like many-sorted propositional logic|Possibilistic logic offers a convenient tool for handling uncertain or prioritized formulas and coping with inconsistency. Propositional logic formulas are thus associated with weights belonging to a linearly ordered scale. However, especially in case of multiple source information, only partial knowledge may be available about the relative ordering between weights of formulas. In order to cope with this problem, a two-sorted counterpart of possibilistic logic is introduced. Pieces of information are encoded as clauses where special literals refer to the weights. Constraints between weights translate into logical formulas of the corresponding sort and are gathered in a distinct auxiliary knowledge base. An inference relation, which is sound and complete with respect to preferential model semantics, enables us to draw plausible conclusions from the two knowledge bases. The inference process is characterized by using \"forgetting variables\" for handling the symbolic weights, and hence an inference process is obtained by means of a DNF compilation of the two knowledge bases.|Salem Benferhat,Henri Prade","65544|AAAI|2005|Towards Learning Stochastic Logic Programs from Proof-Banks|Stochastic logic programs combine ideas from probabilistic grammars with the expressive power of definite clause logic as such they can be considered as an extension of probabilistic context-free grammars. Motivated by an analogy with learning tree-bank grammars, we study how to learn stochastic logic programs from proof-trees. Using proof-trees as examples imposes strong logical constraints on the structure of the target stochastic logic program. These constraints can be integrated in the least general generalization (lgg) operator, which is employed to traverse the search space. Our implementation employs a greedy search guided by the maximum likelihood principle and failure-adjusted maximization. We also report on a number of simple experiments that show the promise of the approach.|Luc De Raedt,Kristian Kersting,Sunna Torge"],["16247|IJCAI|2005|Phase Transitions within Grammatical Inference|It is now well-known that the feasibility of inductive learning is ruled by statistical properties linking the empirical risk minimization principle and the \"capacity\" of the hypothesis space. The discovery, a few years ago, of a phase transition phenomenon in inductive logic programming proves that other fundamental characteristics of the learning problems may similarly affect the very possibility of learning under very general conditions. Our work examines the case of grammatical inference. We show that while there is no phase transition when considering the whole hypothesis space, there is a much more severe \"gap\" phenomenon affecting the effective search space of standard grammatical induction algorithms for deterministic finite automata (DFA). Focusing on the search heuristics of the RPNI and RED-BLUE algorithms, we show that they overcome this problem to some extent, but that they are subject to overgeneralization. The paper last suggests some directions for new generalization operators, suited to this Phase Transition phenomenon.|Nicolas Pernot,Antoine Cornu√©jols,Mich√®le Sebag","16253|IJCAI|2005|ROCCER An Algorithm for Rule Learning Based on ROC Analysis|We introduce a rule selection algorithm called ROCCER, which operates by selecting classification rules from a larger set of rules - for instance found by Apriori - using ROC analysis. Experimental comparison with rule induction algorithms shows that ROCCER tends to produce considerably smaller rule sets with compatible Area Under the ROC Curve (AUC) values. The individual rules that compose the rule set also have higher support and stronger association indexes.|Ronaldo C. Prati,Peter A. Flach","16060|IJCAI|2005|Regret-based Utility Elicitation in Constraint-based Decision Problems|We propose new methods of preference elicitation for constraint-based optimization problems based on the use of minimax regret. Specifically, we assume a constraintbased optimization problem (e.g., product configuration) in which the objective function (e.g., consumer preferences) are unknown or imprecisely specified. Assuming a graphical utility model, we describe several elicitation strategies that require the user to answer only binary (bound) queries on the utility model parameters. While a theoretically motivated algorithm can provably reduce regret quickly (in terms of number of queries), we demonstrate that, in practice, heuristic strategies perform much better, and are able to find optimal (or near-optimal) configurations with far fewer queries.|Craig Boutilier,Relu Patrascu,Pascal Poupart,Dale Schuurmans","65395|AAAI|2005|Constrained Decision Diagrams|A general n-ary constraint is usually represented explicitly as a set of its solution tuples, which may need exponential space. In this paper, we introduce a new representation for general n-ary constraints called Constrained Decision Diagram (CDD). CDD generalizes BDD-style representations and the main feature is that it combines constraint reasoningconsistency techniques with a compact data structure. We present an application of CDD for recording all solutions of a conjunction of constraints. Instead of an explicit representation, we can implicitly encode the solutions by means of constraint propagation. Our experiments confirm the scalability and demonstrate that CDDs can drastically reduce the space needed over explicit and ZBDD representations.|Kenil C. K. Cheng,Roland H. C. Yap","16279|IJCAI|2005|Affine Algebraic Decision Diagrams AADDs and their Application to Structured Probabilistic Inference|We propose an affine extension to ADDs (AADD) capable of compactly representing context-specific, additive, and multiplicative structure. We show that the AADD has worst-case time and space performance within a multiplicative constant of that of ADDs, but that it can be linear in the number of variables in cases where ADDs are exponential in the number of variables. We provide an empirical comparison of tabular, ADD, and AADD representations used in standard Bayes net and MDP inference algorithms and conclude that the AADD performs at least as well as the other two representations, and often yields an exponential performance improvement over both when additive or multiplicative structure can be exploited. These results suggest that the AADD is likely to yield exponential time and space improvements for a variety of probabilistic inference algorithms that currently use tables or ADDs.|Scott Sanner,David A. McAllester","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","80497|VLDB|2005|An Efficient SQL-based RDF Querying Scheme|Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings ) They are difficult to integrate with SQL queries used in database applications, and ) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDFMATCH to query RDF data. The results of RDFMATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDFMATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDFMATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using  million RDF triples from UniProt protein and annotation data.|Eugene Inseok Chong,Souripriya Das,George Eadon,Jagannathan Srinivasan","16330|IJCAI|2005|Decision Diagrams for the Computation of Semiring Valuations|This paper describes a new approach to computation in a semiring-based system, which includes semiring-based CSPs (in particular weighted CSPs, fuzzy CSPs and standard CSPs) as well as Bayesian networks. The approach to computation is based on what we call semiring-labelled decision diagrams (SLDDs). These can be generated in a similar way to a standard search tree (decision tree) for solving a CSP, but some nodes are merged, creating a more compact representation for certain classes of CSPs, the number of nodes in the resulting network will be a tiny fraction of the number of nodes in the corresponding search tree. A method is given for generating an SLDD that represents e.g., a particular instance of a semiring-based CSP it is shown how this can be used to perform various computations of interest, such as solving a semiring-based CSP, finding optimal solutions, determining the possible values of each variable and counting solutions of a CSP.|Nic Wilson","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes"],["65569|AAAI|2005|The DEFACTO System Training Tool for Incident Commanders|Techniques for augmenting the automation of routine coordination are rapidly reaching a level of effectiveness where they can simulate realistic coordination on the ground for large numbers of emergency response entities (e.g. fire engines. police cars) for the sake of training. Furthermore, it seems inevitable that future disaster response systems will utilize such technology. We have constructed a new system, DE-FACTO (Demonstrating Effective Flexible Agent Coordination of Teams through Omnipresence), that integrates state-of-the-art agent reasoning capabilities and D visualization into a unique high fidelity system for training incident commanders. The DEFACTO system achieves this goal via three main components (i) Omnipresent Viewer - intuitive interface, (ii) Proxy Framework - for team coordination, and (iii) Flexible Interaction - between the incident commander and the team. We have performed detailed preliminary experiments with DEFACTO in the fire-fighting domain. In addition, DEFACTO has been repeatedly demonstrated to key police and fire department personnel in Los Angeles area, with very positive feedback.|Nathan Schurr,Janusz Marecki,John P. Lewis,Milind Tambe,Paul Scerri","80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","65447|AAAI|2005|On Compiling System Models for Faster and More Scalable Diagnosis|Knowledge compilation is one of the more traditional approaches to model-based diagnosis, where a compiled system model is obtained in an off-line phase, and then used to efficiently answer diagnostic queries on-line. The choice of a suitable representation for the compiled model is critical to the success of this approach, and two of the main proposals have been Decomposable Negation Normal Form (DNNF) and Ordered Binary Decision Diagram (OBDD). The contribution of this paper is twofold. First, we show that in the current state of the art, DNNF dominates OBDD in efficiency and scalability for some typical diagnostic tasks. This result is based on a step-by-step comparison of the complexities of diagnostic algorithms for DNNF and OBDD, together with a known succinctness relation between the two representations. Second, we present a tool for model-based diagnosis, which is based on a state-of-the-art DNNF compiler and our implementations of DNNF diagnostic algorithms. We demonstrate the efficiency of this tool against recent results reported on diagnosis using OBDD.|Jinbo Huang,Adnan Darwiche","65509|AAAI|2005|Model-Based Monitoring and Diagnosis of Systems with Software-Extended Behavior|Model-based diagnosis has largely operated on hard-ware systems. However, in most complex systems today, hardware is augmented with software functions that influence the system's behavior. In this paper, hard-ware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Prior work introduced probabilistic, hierarchical, constraint-based automata (PHCA) to allow the uniform and compact encoding of both hard-ware and software behavior. This paper focuses on PHCA-based monitoring and diagnosis to ensure the robustness of complex systems. We introduce a novel approach that frames diagnosis over a finite time horizon as a soft constraint optimization problem (COP), allowing us to leverage an extensive body of efficient solution methods for COPs. The solutions to the COP correspond to the most likely evolutions of the complex system. We demonstrate our approach on a vision-based rover navigation system, and models of the SPHERES and Earth Observing One spacecraft.|Tsoline Mikaelian,Brian C. Williams,Martin Sachenbacher","16154|IJCAI|2005|Iterated Belief Change A Transition System Approach|We use a transition system approach to reason about the evolution of an agent's beliefs as actions are executed. Some actions cause an agent to perform belief revision and some actions cause an agent to perform belief update, but the interaction between revision and update can be non-elementary. We present a set of basic postulates describing the interaction of revision and update, and we introduce a new belief evolution operator that gives a plausible interpretation to alternating sequences of revisions and updates.|Aaron Hunter,James P. Delgrande","16088|IJCAI|2005|A rule language for modelling and monitoring social expectations in multi-agent systems|This paper proposes a rule language for defining social expectations based on a metric interval temporal logic with past and future modalities and a current-time binding operator. An algorithm for run-timemonitoring compliance of rules in this language based on formula progression is also outlined.|Stephen Cranefield","65597|AAAI|2005|CORMS AI Decision Support System for Monitoring US Maritime Environment|Rule based reasoning and case based reasoning have emerged as two important and complementary reasoning methodologies in artificial intelligence (AI). This paper describes the approach for the development of CORMS AI, a decision support system which employs rule-based and case-based reasoning to assist NOAA's Center for Operational Oceanographic Products and Services watch standing personnel in monitoring the quality of marine environmental data and information. CORMS AI has been in operation since July . The system accurately and reliably identifies suspect data and network disruptions, and has decreased the amount of time it takes to identify and troubleshoot sensor, network, and server failures. CORMS AI has proven to be robust, extendable, and cost effective. It is estimated that CORMS AI will save government over one million dollars per year when its full range of quality control monitoring capabilities is implemented.|Haleh Vafaie,Carl Cecere","65577|AAAI|2005|Minimizing Environmental Swings with a Recurrent Neural Network Control System|Maintaining environmental stability in a dynamic system is a difficult challenge. In your living room, when you set your thermostat to  degrees the actual temperature cycles above and below  degrees. We attempt to use a Recurrent Neural Network (RNN) in an Aquarium Control System that reduces such environmental swings.|Sam Skrivan,Jianna Zhang,Debra S. Jusak","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65585|AAAI|2005|The TaskTracker System|Knowledge workers spend the majority of their working hours processing and manipulating information. These users face continual costs as they switch between tasks to retrieve and create information. The TaskTracer project at Oregon State University investigates the possibilities of a desktop software system that will record in detail how knowledge workers complete tasks, and intelligently leverage that information to increase efficiency and productivity. Our approach assigns each observed user interface action to a task for which it is likely being performed. In this demonstration we show how we have applied machine learning in this environment.|Simone Stumpf,Xinlong Bao,Anton N. Dragunov,Thomas G. Dietterich,Jonathan L. Herlocker,Kevin Johnsrude,Lida Li,Jianqiang Shen"],["65533|AAAI|2005|Superstabilizing Fault-Containing Distributed Combinatorial Optimization|Self stabilization in distributed systems is the ability of a system to respond to transient failures by eventually reaching a legal state, and maintaining it afterwards. This makes such systems particularly interesting because they can tolerate faults, and are able to cope with dynamic environments. We propose the first self stabilizing mechanism for multiagent combinatorial optimization, which works on general networks and stabilizes in a state corresponding to the optimal solution of the optimization problem. Our algorithm is based on dynamic programming, and requires a linear number of messages to find the optimal solution in the absence of faults. We show how our algorithm can be made super-stabilizing, in the sense that while transiting from one stable state to the next, our system preserves the assignments from the previous optimal state, until the new optimal solution is found. We offer equal bounds for the stabilization and the superstabilization time. Furthermore, we describe a general scheme for fault containment and fast response time upon low impact failures. Multiple, isolated failures are handled effectively. To show the merits of our approach we report on experiments with practically sized distributed meeting scheduling problems in a multiagent system.|Adrian Petcu,Boi Faltings","16272|IJCAI|2005|Bounded Search and Symbolic Inference for Constraint Optimization|Constraint optimization underlies many problems in AI. We present a novel algorithm for finite domain constraint optimization that generalizes branch-and-bound search by reasoning about sets of assignments rather than individual assignments. Because in many practical cases, sets of assignments can be represented implicitly and compactly using symbolic techniques such as decision diagrams, the set-based algorithm can compute bounds faster than explicitly searching over individual assignments, while memory explosion can be avoided by limiting the size of the sets. Varying the size of the sets yields a family of algorithms that includes known search and inference algorithms as special cases. Furthermore, experiments on random problems indicate that the approach can lead to significant performance improvements.|Martin Sachenbacher,Brian C. Williams","16230|IJCAI|2005|Networked Distributed POMDPs A Synergy of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present a distributed policy generation algorithm that performs local search.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","16250|IJCAI|2005|A Scalable Method for Multiagent Constraint Optimization|We present in this paper a new, complete method for distributed constraint optimization, based on dynamic programming. It is a utility propagation method, inspired by the sum-product algorithm, which is correct only for tree-shaped constraint networks. In this paper, we show how to extend that algorithm to arbitrary topologies using a pseudotree arrangement of the problem graph. Our algorithm requires a linear number of messages, whose maximal size depends on the induced width along the particular pseudotree chosen. We compare our algorithm with backtracking algorithms, and present experimental results. For some problem types we report orders of magnitude fewer messages, and the ability to deal with arbitrarily large problems. Our algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well.|Adrian Petcu,Boi Faltings","65542|AAAI|2005|Constraint-Based Preferential Optimization|We first show that the optimal and undominated outcomes of an unconstrained (and possibly cyclic) CP-net are the solutions of a set of hard constraints. We then propose a new algorithm for finding the optimal outcomes of a constrained CP-net which makes use of hard constraint solving. Unlike previous algorithms, this new algorithm works even with cyclic CP-nets. In addition. the algorithm is not tied to CP-nets, but can work with any preference formalism which produces a preorder over the outcomes. We also propose an approximation method which weakens the preference ordering induced by the CP-net, returning a larger set of outcomes, but provides a significant computational advantage. Finally, we describe a weighted constraint approach that allows to find good solutions even when optimals do not exist.|Steven David Prestwich,Francesca Rossi,Kristen Brent Venable,Toby Walsh","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","80596|VLDB|2005|AReNA Adaptive Distributed Catalog Infrastructure Based On Relevance Networks|Wide area applications (WAAs) utilize a WAN infrastructure (e.g., the Internet) to connect a federation of hundreds of servers with tens of thousands of clients. Earlier generations of WAA relied on Web accessible sources and the http protocol for data delivery. Recent developments such as the PlanetLab  testbed is now demonstrating an emerging class of data- and compute- intensive wide area applications.|Vladimir Zadorozhny,Avigdor Gal,Louiqa Raschid,Qiang Ye","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","16342|IJCAI|2005|Learning Global Models Based on Distributed Data Abstractions|Due to the increasing demand of massive and distributed data analysis, achieving highly accurate global data analysis results with local data privacy preserved becomes an increasingly important research issue. In this paper, we propose to adopt a model-based method (Gaussian mixture model) for local data abstraction and aggregate the local model parameters for learning global models. To support global model learning based on solely local GMM parameters instead of virtual data generated from the aggregated local model, a novel EM-like algorithm is derived. Experiments have been performed using synthetic datasets and the proposed method was demonstrated to be able to achieve the global model accuracy comparable to that of using the data regeneration approach at a much lower computational cost.|Xiaofeng Zhang,William K. Cheung","16106|IJCAI|2005|Reinforcement Learning in POMDPs Without Resets|We consider the most realistic reinforcement learning setting in which an agent starts in an unknown environment (the POMDP) and must follow one continuous and uninterrupted chain of experience with no access to \"resets\" or \"offline\" simulation. We provide algorithms for general connected POMDPs that obtain near optimal average reward. One algorithm we present has a convergence rate which depends exponentially on a certain horizon time of an optimal policy, but has no dependence on the number of (unobservable) states. The main building block of our algorithms is an implementation of an approximate reset strategy, which we show always exists in every POMDP. An interesting aspect of our algorithms is how they use this strategy when balancing exploration and exploitation.|Eyal Even-Dar,Sham M. Kakade,Yishay Mansour"],["65599|AAAI|2005|Approximate Strategic Reasoning through Hierarchical Reduction of Large Symmetric Games|To deal with exponential growth in the size of a game with the number of agents, we propose an approximation based on a hierarchy of reduced games. The reduced game achieves savings by restricting the number of agents playing any strategy to fixed multiples. We validate the idea through experiments on randomly generated local-effect games. An extended application to strategic reasoning about a complex trading scenario motivates the approach, and demonstrates methods for game-theoretic reasoning over incompletely-specified games at multiple levels of granularity.|Michael P. Wellman,Daniel M. Reeves,Kevin M. Lochner,Shih-Fen Cheng,Rahul Suri","65571|AAAI|2005|Spotting Subsequences Matching an HMM Using the Average Observation Probability Criteria with Application to Keyword Spotting|This paper addresses the problem of detecting keywords in unconstrained speech. The proposed algorithms search for the speech segment maximizing the average observation probability along the most likely path in the hypothesized keyword model. As known, this approach (sometimes referred to as sliding model method) requires a relaxation of the beginendpoints of the Viterbi matching, as well as a time normalization of the resulting score. This makes solutions complex (i.e., LN basic operations for keyword HMM models with L states and utterances with N frames). We present here two altemative (quite simple and efficient) solutions to this problem. a) First we provide a method that finds the optimal segmentation according to the criteria of maximizing the average observation probability. It uses Dynamic Programming as a step, but does not require scoring for all possible beginendpoints. While the worst case remains O(LN), this technique converged in at most (L+)N basic operations in each experiment for two very different applications. b) The second proposed algorithm does not provide a segmentation but can be used for the decision problem of whether the utterance should be classified as containing the keyword or not (provided a predefined threshold on the acceptable average observation probability). This allows the algorithm to be even faster, with fix cost of (L+)N.|Marius-Calin Silaghi","80503|VLDB|2005|Sketching Streams Through the Net Distributed Approximate Query Tracking|Emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Effective solutions have to be simultaneously spacetime efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and spacetime-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.|Graham Cormode,Minos N. Garofalakis","65587|AAAI|2005|Observation-based Model for BDI-Agents|We present a new computational model of BDI-agents, called the observation-based BDI-model. The key point of this BDI-model is to express agents' beliefs, desires and intentions as a set of runs (computing paths), which is exactly a system in the interpreted system model, a well-known agent model due to Halpern and his colleagues. Our BDI-model is computationally grounded in that we are able to associate the BDI-agent model with a computer program, and formulas, involving agents' beliefs, desires (goals) and intentions, can be understood as properties of program computations. We present a sound and complete proof system with respect to our BDI-model and explore how symbolic model checking techniques can be applied to model checking BDI-agents. In order to make our BDI-model more flexible and practically realistic, we generalize it so that agents can have multiple sources of beliefs, goals and intentions.|Kaile Su,Abdul Sattar,Kewen Wang,Xiangyu Luo,Guido Governatori,Vineet Padmanabhan","16262|IJCAI|2005|InterActive Feature Selection|We study the effects of feature selection and human feedback on features in active learning settings. Our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features. Consistent with previous findings, we find that feature selection based on the labeled training set has little effect. But our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (%) of the most relevant features. Furthermore, these experiments show that feature labeling takes much less (about th) time than document labeling. We propose an algorithm that interleaves labeling features and documents which significantly accelerates active learning.|Hema Raghavan,Omid Madani,Rosie Jones","65591|AAAI|2005|Approximate Inference of Bayesian Networks through Edge Deletion|In this paper, we introduce two new algorithms for approximate inference of Bayesian networks that use edge deletion techniques. The first reduces a network to its maximal weight spanning tree using the Kullback-Leibler information divergence as edge weights, and then runs Pearl's algorithm on the resulting tree for linear-time inference. The second algorithm deletes edges from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound, thus placing a polynomial time bound on inference. When tested for efficiency, these two algorithms perform up to , times faster than exact techniques. See www.cis.ksu.edujasresearch.html for more information.|Julie Thornton","65610|AAAI|2005|Activity Recognition through Goal-Based Segmentation|A major issue in activity recognition in a sensor network is how to automatically segment the low-level signal sequences in order to optimize the probabilistic recognition models for goals and activities. Past efforts have relied on segmenting the signal sequences by hand, which is both time-consuming and error-prone. In our view, segments should correspond to atomic human activities that enable a goal-recognizer to operate optimally the two are intimately related. In this paper, we present a novel method for building probabilistic activity models at the same time as we segment signal sequences into motion patterns. We model each motion pattern as a linear dynamic model and the transitions between motion patterns as a Markov process conditioned on goals. Our EM learning algorithm simultaneously learns the motion-pattern boundaries and probabilistic models for goals and activities, which in turn can be used to accurately recognize activities in an online phase. A major advantage of our algorithm is that it can reduce the human effort in segmenting and labeling signal sequences. We demonstrate the effectiveness of our algorithm using the data collected in a real wireless environment.|Jie Yin,Dou Shen,Qiang Yang,Ze-Nian Li","65555|AAAI|2005|Learning Static Object Segmentation from Motion Segmentation|Dividing an image into its constituent objects can be a useful first step in many visual processing tasks, such as object classification or determining the arrangement of obstacles in an environment. Motion segmentation is a rich source of training data for learning to segment objects by their static image properties. Background subtraction can distinguish between moving objects and their surroundings, and the techniques of statistical machine learning can capture information about objects' shape, size. color, brightness, and texture properties. Presented with a new, static image, the trained model can infer the proper segmentation of the objects present in a scene. The algorithm presented in this work uses the techniques of Markov random field modeling and belief propagation inference, outperforms a standard segmentation algorithm on an object segmentation task, and outperforms a learned boundary detector at determining object boundaries on the test data.|Michael G. Ross,Leslie Pack Kaelbling","65378|AAAI|2005|Genre Classification of Web Documents|Retrieving relevant documents over the Web is an overwhelming task when search engines return thousands of Web documents. Sifting through these documents is time-consuming and sometimes leads to an unsuccessful search. One problem is that most search engines rely on matching a query to documents based solely on topical keywords. However, many users of search engines have a particular genre in mind for the desired documents. The genre of a document concerns aspects of the document such as the style or readability, presentation layout, and meta-content such as words in the title or the existence of graphs or photos. By including genre in Web searches, we hypothesize that Web document retrieval could greatly improve accuracy by better matching documents to the user's information needs. Before implementing a search engine capable of discriminating on both genre and topic, a feasibility analysis of genre classification is needed. Our previous research achieved % classification accuracy across ten genres, whereas similar research range between  and % accuracy. However, the ten genres used in our research were mostly distinct and only exemplar Web documents (consisting of only one genre) were chosen. This paper discusses our current work which involves an in-depth analysis of maintaining high accuracy rates among genres that are very similar.|Elizabeth Sugar Boese,Adele E. Howe","80478|VLDB|2005|Approximate Matching of Hierarchical Data Using pq-Grams|When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.|Nikolaus Augsten,Michael H. B√∂hlen,Johann Gamper"]]}}