{"abstract":{"entropy":6.662056616168054,"topics":["neural networks, data, data mining, web search, data stream, search engine, scene understanding, data management, machine data, support vector, xml data, information, web, efficient xml, wide range, xml document, self-interested agents, web services, mapping schema, query processing","evolutionary algorithms, genetic algorithms, optimization problem, algorithms search, particle swarm, problem, constraint satisfaction, algorithms, algorithms problem, solving problem, optimization, algorithms optimization, evolutionary computation, constraint problem, search, fitness function, widely used, differential evolution, evolution strategies, algorithms solving","genetic programming, present novel, machine learning, genetic algorithms, present algorithms, learning, present approach, present, partially observable, present based, cartesian genetic, description logic, reinforcement learning, present learning, present model, programming cartesian, learning classifier, bayesian networks, embedded cartesian, estimation distribution","markov decision, recent years, markov processes, classifier systems, artificial intelligence, decision processes, systems, natural language, markov mdps, decision mdps, time series, processes mdps, learning systems, growing interest, model-based diagnosis, systems complex, xcs systems, classifier xcs, activity recognition, autonomous robots","consider problem, information integration, aggregation preferences, aggregation voting, preferences voting, general voting, aggregation issue, changing software, multiple voting, agents voting, software systems, query answering, software, consider agents, problem agents, multiple agents, preferences agents, multiple, agents aggregation, systems multiple","data, data stream, data management, xml data, machine data, support vector, query processing, vector svm, support data, processing stream, management systems, problem data, systems data, stream systems, support machine, vector data, stream management, data model, xml repositories, applications data","evolutionary algorithms, evolutionary problem, evolutionary computation, evolutionary optimization, widely used, genetic algorithms, ant colony, hybrid algorithms, algorithms used, paper presented, testing test, approach evolutionary, multiobjective optimization, evolutionary solving, search test, algorithms applied, evolutionary eas, algorithms eas, present evolutionary, genetic used","particle swarm, solve problem, swarm optimization, optimization problem, particle pso, algorithms optimization, swarm pso, address problem, particle optimization, paper algorithms, algorithms dynamic, introduce problem, swarm algorithms, introduce algorithms, problem finding, paper problem, optimization pso, solve optimization, present optimization, paper introduce","learning classifier, based reasoning, genetic selection, selection algorithms, negative selection, cellular genetic, cases reasoning, algorithms cellular, selection learning, learning concept, analyze algorithms, computational model, form reasoning, introduce learning, classification used, selection, work model, work, learning classification, reasoning","description logic, learning sequential, planning domains, knowledge base, logic programs, temporal problem, belief change, logic reasoning, complexity dominance, disjunctive problem, learning temporal, temporal reasoning, disjunctive temporal, domains learning, logic, reinforcement difference, temporal reinforcement, sequential planning, temporal difference, difference methods","markov decision, markov processes, decision processes, natural language, markov mdps, time series, decision mdps, processes mdps, systems developed, query bp-ql, continuous queries, querying bp-ql, last years, novel bp-ql, present bp-ql, language systems, language generation, bp-ql language, processes bp-ql, natural generation","artificial intelligence, activity recognition, structure protein, sensor networks, networks artificial, number systems, planning problem, problem activity, important recent, number, networks, structure, mobile networks, important, applications, planning, applications problem, structure networks, problem number, problem networks"],"ranking":[["80548|VLDB|2005|Hubble An Advanced Dynamic Folder Technology for XML|A significant amount of information is stored in computer systems today, but people are struggling to manage their documents such that the information is easily found. XML is a de-facto standard for content publishing and data exchange. The proliferation of XML documents has created new challenges and opportunities for managing document collections. Existing technologies for automatically organizing document collections are either imprecise or based on only simple criteria. Since XML documents are self describing, it is now possible to automatically categorize XML documents precisely, according to their content. With the availability of the standard XML query languages, e.g. XQuery, much more powerful folder technologies are now feasible. To address this new challenge and exploit this new opportunity, this paper proposes a new and powerful dynamic folder mechanism, called Hubble. Hubble fully exploits the rich data model and semantic information embedded in the XML documents to build folder hierarchies dynamically and to categorize XML collections precisely. Besides supporting basic folder operations, Hubble also provides advanced features such as multi-path navigation and folder traversal across multiple document collections. Our performance study shows that Hubble is both efficient and scalable. Thus, it is an ideal technology for automating the process of organizing and categorizing XML documents.|Ning Li,Joshua Hui,Hui-I Hsiao,Kevin S. Beyer","80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80585|VLDB|2005|An Efficient and Versatile Query Engine for TopX Search|This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in ) the need to consider scores for XML elements while aggregating them at the document level, ) the combination of vague content conditions with XML path conditions, ) the need to relax query conditions if too few results satisfy all conditions, and ) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.|Martin Theobald,Ralf Schenkel,Gerhard Weikum","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani"],["57678|GECCO|2006|Exploring network topology evolution through evolutionary computations|We present an evolutionary methodology that explores the evolution of network topology when a uniform growth of the network traffic is considered. The network redesign problem is formulated as an optimization problem, subject to a set of design and performance constraints, while minimizing the redesign cost by maintaining as many as possible of the network devices that constitute the original topology. The experimental results for a -level network redesign problem (consisting of  client nodes) demonstrate the value of the search technique within the genetic algorithms in finding good solutions with respect to redesign cost and time.|Sami J. Habib,Alice C. Parker","57447|GECCO|2005|Promising infeasibility and multiple offspring incorporated to differential evolution for constrained optimization|In this paper, we incorporate a diversity mechanism to the differential evolution algorithm to solve constrained optimization problems without using a penalty function. The aim is twofold () to allow infeasible solutions with a promising value of the objective function to remain in the population and also () to increase the probabilities of an individual to generate a better offspring while promoting collaboration of all the population to generate better solutions. These goals are achieved by allowing each parent to generate more than one offspring. The best offspring is selected using a comparison mechanism based on feasibility and this child is compared against its parent. To maintain diversity, the proposed approach uses a mechanism successfully adopted with other evolutionary algorithms where, based on a parameter Sr a solution (between the best offspring and the current parent) with a better value of the objective function can remain in the population, regardless of its feasibility. The proposed approach is validated using test functions from a well-known benchmark commonly adopted to validate constraint-handling techniques used with evolutionary algorithms. The statistical results obtained by the proposed approach are highly competitive (based on quality, robustness and number of evaluations of the objective function) with respect to other constraint-handling techniques, either based on differential evolution or on other evolutionary algorithms, that are representative of the state-of-the-art in the area. Finally, a small set of experiments were made to detect sensitivity of the approach to its parameters.|Efr√©n Mezura-Montes,Jes√∫s Vel√°zquez-Reyes,Carlos A. Coello Coello","57485|GECCO|2005|Understanding cooperative co-evolutionary dynamics via simple fitness landscapes|Cooperative co-evolution is often used to solve difficult optimization problems by means of problem decomposition. Its performance for such tasks can vary widely from good to disappointing. One of the reasons for this is that attempts to improve co-evolutionary performance using traditional EC analysis techniques often fail to provide the necessary insights into the dynamics of co-evolutionary systems, a key factor affecting performance. In this paper we use two simple fitness landscapes to illustrate the importance of taking a dynamical systems approach to analyzing co-evolutionary algorithms in order to understand them better and to improve their problem solving performance.|Elena Popovici,Kenneth A. De Jong","57429|GECCO|2005|The molecule evoluator an interactive evolutionary algorithm for designing drug molecules|To help chemists design new drugs, we created a tool that uses interactive evolution to design drug molecules, the \"Molecule Evoluator\". In contrast to most other evolutionary de novo design programs, the molecule representation and the set of mutations enable it to both search the chemical space of all drug like molecules extensively and to fine-tune molecular structures to the problem at hand. Additionally, we use interaction with the user as a fitness function, which is new in evolutionary algorithms in drug design. This interactivity allows the Molecule Evoluator to use the domain knowledge of the chemist to estimate the ease of synthesis and the biological activity of the compound. This knowledge can guide the optimization process and thereby improve its results. Chemists of our department using the Molecule Evoluator were able to find six novel and synthesizable druglike core structures, indicating that the Molecule Evoluator can be used as a tool to enhance the chemist's creativity.|Eric-Wubbo Lameijer,Adriaan P. IJzerman,Joost N. Kok","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57461|GECCO|2005|Minimum spanning trees made easier via multi-objective optimization|Many real-world problems are multi-objective optimization problems and evolutionary algorithms are quite successful on such problems. Since the task is to compute or approximate the Pareto front, multi-objective optimization problems are considered as more difficult than single-objective problems. One should not forget that the fitness vector with respect to more than one objective contains more information that in principle can direct the search of evolutionary algorithms. Therefore, it is possible that a single-objective problem can be solved more efficiently via a generalized multi-objective model of the problem. That this is indeed the case is proved by investigating the computation of minimum spanning trees.|Frank Neumann,Ingo Wegener","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. G√ºrgen"],["65905|AAAI|2006|Cross-Domain Knowledge Transfer Using Structured Representations|Previous work in knowledge transfer in machine learning has been restricted to tasks in a single domain. However, evidence from psychology and neuroscience suggests that humans are capable of transferring knowledge across domains. We present here a novel learning method, based on neuroevolution, for transferring knowledge across domains. We use many-layered, sparsely-connected neural networks in order to learn a structural representation of tasks. Then we mine frequent sub-graphs in order to discover sub-networks that are useful for multiple tasks. These sub-networks are then used as primitives for speeding up the learning of subsequent related tasks, which may be in different domains.|Samarth Swarup,Sylvian R. Ray","57472|GECCO|2005|Extraction of informative genes from microarray data|Identification of those genes that might anticipate the clinical behavior of different types of cancers is challenging due to availability of a smaller number of patient samples compared to huge number of genes, and the noisy nature of microarray data. After selection of some good genes based on signal-to-noise ratio, unsupervised learning like clustering and supervised learning like k-nearest neighbor (kNN) classifier are widely used in cancer researches to correlate the pathological behavior of cancers with the gene expression levels' differences in cancerous and normal tissues. By applying adaptive searches like Probabilistic Model Building Genetic Algorithm (PMBGA), it may be possible to get a smaller size gene subset that would classify patient samples more accurately than the above methods. In this paper, we propose a new PMBGA based method to extract informative genes from microarray data using Support Vector Machine (SVM) as a classifier. We apply our method to three microarray data sets and present the experimental results. Our method with SVM obtains encouraging results on those data sets as compared with the rank based method using kNN as a classifier.|Topon Kumar Paul,Hitoshi Iba","65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","57321|GECCO|2005|Finding needles in haystacks is harder with neutrality|This research presents an analysis of the reported successes of the Cartesian Genetic Programming method on a simplified form of the Boolean parity problem. We show the method of sampling used by the CGP is significantly less effective at locating solutions than the solution density of the corresponding formula space would warrant.We present results indicating that the loss of performance is caused by the sampling bias of the CGP, due to the neutrality friendly representation. We implement a simple intron free random sampling algorithm which performs considerably better on the same problem and then explain how such performance is possible.|M. Collins","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57850|GECCO|2006|A multi-chromosome approach to standard and embedded cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is an extension of Cartesian Genetic Programming (CGP) that can automatically acquire, evolve and re-use partial solutions in the form of modules. In this paper, we introduce for the first time a new multi-chromosome approach to CGP and ECGP that allows difficult problems with multiple outputs to be broken down into many smaller, simpler problems with single outputs, whilst still encoding the entire solution in a single genotype. We also propose a multi-chromosome evolutionary strategy which selects the best chromosomes from the entire population to form the new fittest individual, which may not have been present in the population. The multi-chromosome approach to CGP and ECGP is tested on a number of multiple output digital circuits. Computational Effort figures are calculated for each problem and compared against those for CGP and ECGP. The results indicate that the use of multiple chromosomes in both CGP and ECGP provide a significant performance increase on all problems tested.|James Alfred Walker,Julian Francis Miller,Rachel Cavill","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57865|GECCO|2006|Behavioural GP diversity for dynamic environments an application in hedge fund investment|We present a new mechanism for preserving phenotypic behavioural diversity in a Genetic Programming application for hedge fund portfolio optimization, and provide experimental results on real-world data that indicate the importance of phenotypic behavioural diversity both in achieving higher fitness and in improving the adaptability of the GP population for continuous learning.|Wei Yan,Christopher D. Clack","57561|GECCO|2005|Investigating the performance of module acquisition in cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is a form of the graph based Cartesian Genetic Programming (CGP) in which modules are automatically acquired and evolved. In this paper we compare the efficiencies of the ECGP and CGP techniques on three classes of problem digital adders, digital multipliers and digital comparators. We show that in most cases ECGP shows a substantial improvement in performance over CGP and that the computational speedup is more pronounced on larger problems.|James Alfred Walker,Julian Francis Miller","57604|GECCO|2006|Genetic programming for human oral bioavailability of drugs|Automatically assessing the value of bioavailability from the chemical structure of a molecule is a very important issue in biomedicine and pharmacology. In this paper, we present an empirical study of some well known Machine Learning techniques, including various versions of Genetic Programming, which have been trained to this aim using a dataset of molecules with known bioavailability. Genetic Programming has proven the most promising technique among the ones that have been considered both from the point of view of the accurateness of the solutions proposed, of the generalization capabilities and of the correlation between predicted data and correct ones. Our work represents a first answer to the demand for quantitative bioavailability estimation methods proposed in literature, since the previous contributions focus on the classification of molecules into classes with similar bioavailability.|Francesco Archetti,Stefano Lanzeni,Enza Messina,Leonardo Vanneschi"],["16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","57721|GECCO|2006|Genetic algorithms for action set selection across domains a demonstration|Action set selection in Markov Decision Processes (MDPs) is an area of research that has received little attention. On the other hand, the set of actions available to an MDP agent can have a significant impact on the ability of the agent to gain optimal rewards. Last year at GECCO', the first automated action set selection tool powered by genetic algorithms was presented. The demonstration of its capabilities, though intriguing, was limited to a single domain. In this paper, we apply the tool to a more challenging problem of oil sand image interpretation. In the new experiments, genetic algorithms evolved a compact high-performance set of image processing operators, decreasing interpretation time by % while improving image interpretation accuracy by %. These results exceed the original performance and suggest certain cross-domain portability of the approach.|Greg Lee,Vadim Bulitko","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","65773|AAAI|2006|Learning Basis Functions in Hybrid Domains|Markov decision processes (MDPs) with discrete and continuous state and action components can be solved efficiently by hybrid approximate linear programming (HALP). The main idea of the approach is to approximate the optimal value function by a Set of basis functions and optimize their weights by linear programming. The quality of this approximation naturally depends on its basis functions. However, basis functions leading to good approximations are rarely known in advance. In this paper, we propose a new approach that discovers these functions automatically. The method relies on a class of parametric basis function models, which are optimized using the dual formulation of a relaxed HALP. We demonstrate the performance of our method on two hybrid optimization problems and compare it to manually selected basis functions.|Branislav Kveton,Milos Hauskrecht","80481|VLDB|2005|Database Publication Practices|There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.|Philip A. Bernstein,David J. DeWitt,Andreas Heuer,Zachary G. Ives,Christian S. Jensen,Holger Meyer,M. Tamer √\u2013zsu,Richard T. Snodgrass,Kyu-Young Whang,Jennifer Widom","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aur√©lie Beynier,Abdel-Illah Mouaddib","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65360|AAAI|2005|Mixed-Initiative Approach to Collaboration in the Mathematical Domain|Using smart-phones for ad-hoc mathematical collaboration poses multiple user interface challenges. In this paper, software agents are used to lessen the cognitive load through automatic line labeling. Researchers in the human-computer interaction community have attempted to alleviate the problem of general usability through user interface engineering conventions (Myers ). However, these engineering approaches can be improved upon through the application of mixed-initiative principles.|Nadya Belov,Joshua Shaffer","16031|IJCAI|2005|On the Axiomatic Foundations of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. In this paper we present an extensive axiomatic study of ranking systems. In particular, we consider two fundamental axioms Transitivity, and Ranked Independence of Irrelevant Alternatives. Surprisingly, we find that there is no general social ranking rule that satisfies both requirements. Furthermore, we show that our impossibility result holds under various restrictions on the class of ranking problems considered. Each of these axioms can be individually satisfied. Moreover, we show a complete axiomatization of approval voting using one of these axioms.|Alon Altman,Moshe Tennenholtz","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","65442|AAAI|2005|Anyone but Him The Complexity of Precluding an Alternative|Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as votercandidate additionsuppressionpartition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study--plurality, Condorcet, and approval voting--we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.|Edith Hemaspaandra,Lane A. Hemaspaandra,J√∂rg Rothe","16061|IJCAI|2005|Efficiency and envy-freeness in fair division of indivisible goods logical representation and complexity|We consider the problem of allocating fairly a set of indivisible goods among agents from the point of view of compact representation and computational complexity. We start by assuming that agents have dichotomous preferences expressed by propositional formulae. We express efficiency and envy-freeness in a logical setting, which reveals unexpected connections to nonmonotonic reasoning. Then we identify the complexity of determining whether there exists an efficient and envy-free allocation, for several notions of efficiency, when preferences are represented in a succinct way (as well as restrictions of this problem). We first study the problem under the assumption that preferences are dichotomous, and then in the general case.|Sylvain Bouveret,J√©r√¥me Lang","16099|IJCAI|2005|Explaining preferences with argument positions|When deciding what to do agents must choose among alternative actions and different agents may make different choices according to what they wish to achieve in the light of their preferences and values. It cannot be assumed, however, that agents have a conscious understanding of their value preferences independent of the reasoning situations in which they engage. In this paper we consider an extension to a generic framework for reasoning about arguments justifying actions in terms of values in which the preferences amongst values emerge from the reasoning process.|Sylvie Doutre,Trevor J. M. Bench-Capon,Paul E. Dunne","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm"],["80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80696|VLDB|2006|Efficient Scheduling of Heterogeneous Continuous Queries|Data Stream Management Systems (DSMS) typically host multiple Continuous Queries (CQ) that process streams of data. In this paper, we examine the problem of how to schedule CQs in a DSMS to optimize for average QoS. We show that unlike standard on-line systems, scheduling policies in DSMSs that optimize for average response time will be different than policies that optimize for average slowdown which is more appropriate metric to use in the presence of a heterogeneous workload. We also propose a hybrid scheduling policy based on slowdown that strikes a fine balance between performance and fairness. We further discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies outperform currently used ones.|Mohamed A. Sharaf,Panos K. Chrysanthis,Alexandros Labrinidis,Kirk Pruhs","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["57563|GECCO|2005|Using evolutionary algorithms for the unit testing of object-oriented software|As the paradigm of object orientation becomes more and more important for modern IT development projects, the demand for an automated test case generation to dynamically test object-oriented software increases. While search-based test case generation strategies, such as evolutionary testing, are well researched for procedural software, relatively little research has been done in the area of evolutionary object-oriented software testing.This paper presents an approach with which to apply evolutionary algorithms for the automatic generation of test cases for the white-box testing of object-oriented software. Test cases for testing object-oriented software include test programs which create and manipulate objects in order to achieve a certain test goal. Strategies for the encoding of test cases to evolvable data structures as well as ideas about how the objective functions could allow for a sophisticated evaluation are proposed. It is expected that the ideas herein can be adapted for other unit testing methods as well.The approach has been implemented by a prototype for empirical validation. In experiments with this prototype, evolutionary testing outperformed random testing. Evolutionary algorithms could be successfully applied for the white-box testing of object-oriented software.|Stefan Wappler,Frank Lammermann","57337|GECCO|2005|Some theoretical results about the computation time of evolutionary algorithms|This paper focuses on the computation time of evolutionary algorithms. First, some exact expressions of the mean first hitting times of general evolutionary algorithms in finite search spaces are obtained theoretically by using the properties of Markov chain. Then, by introducing drift analysis and applying Dynkin's Formula, the general upper and lower bounds of the mean first hitting times of evolutionary algorithms are given rigorously under some mild conditions. These results obtained in this paper, and the analytic methods used in this paper, are widely valid for analyzing the computation time of evolutionary algorithms in any search space(finite or infinite)as long as some simple technique processes are introduced.|Lixin X. Ding,Jinghu Yu","57842|GECCO|2006|Pairwise sequence comparison for fitness evaluation in evolutionary structural software testing|Evolutionary algorithms are among the metaheuristic search methods that have been applied to the structural test data generation problem. Fitness evaluation methods play an important role in the performance of evolutionary algorithms and various methods have been devised for this problem. In this paper, we propose a new fitness evaluation method based on pairwise sequence comparison also used in bioinformatics. Our preliminary study shows that this method is easy to implement and produces promising results.|H. Turgut Uyar,A. Sima Etaner-Uyar,A. Emre Harmanci","57485|GECCO|2005|Understanding cooperative co-evolutionary dynamics via simple fitness landscapes|Cooperative co-evolution is often used to solve difficult optimization problems by means of problem decomposition. Its performance for such tasks can vary widely from good to disappointing. One of the reasons for this is that attempts to improve co-evolutionary performance using traditional EC analysis techniques often fail to provide the necessary insights into the dynamics of co-evolutionary systems, a key factor affecting performance. In this paper we use two simple fitness landscapes to illustrate the importance of taking a dynamical systems approach to analyzing co-evolutionary algorithms in order to understand them better and to improve their problem solving performance.|Elena Popovici,Kenneth A. De Jong","57534|GECCO|2005|An enhanced GA to improve the search process reliability in tuning of control systems|Evolutionary Algorithms (EAs) have been largely applied to optimisation and synthesis of controllers. In spite of several successful applications and competitive solutions, the stochastic nature of EAs and the uncertainty of the results have considerably hindered their use in industrial applications. In this paper we propose a Genetic Algorithm (GA) for tuning controllers for classical first and second order plants with actuator nonlinearities. To increase the robustness of the algorithm we introduce two features ) genetic operators that perform directional mutations, ) selection tournaments organized by genome vicinity. The experiment results show that the proposed GA is able to guarantee high performance and low variance in the results from different runs. The increased reliability, compared to the results from a classical GA, seems to favour particularly the application of Evolutionary Computation (EC) in tuning of control systems, where, thanks to this approach, a large search space can be searched repeatedly with high consistency in the solutions.|Andrea Soltoggio","65927|AAAI|2006|A New Approach to Estimating the Expected First Hitting Time of Evolutionary Algorithms|Evolutionary algorithms (EA) have been shown to be very effective in solving practical problems, yet many important theoretical issues of them are not clear. The expected first hitting time is one of the most important theoretical issues of evolutionary algorithms, since it implies the average computational time complexity. In this paper, we establish a bridge between the expected first hitting time and another important theoretical issue, i.e., convergence rate. Through this bridge, we propose a new general approach to estimating the expected first hitting time. Using this approach, we analyze EAs with different configurations, including three mutation operators, withwithout population, a recombination operator and a time variant mutation operator, on a hard problem. The results show that the proposed approach is helpful for analyzing a broad range of evolutionary algorithms. Moreover, we give an explanation of what makes a problem hard to EAs, and based on the recognition, we prove the hardness of a general problem.|Yang Yu,Zhi-Hua Zhou","57411|GECCO|2005|Benefits of software measures for evolutionary white-box testing|White-box testing is an important method for the early detection of errors during software development. In this process test case generation plays a crucial role, defining appropriate and error-sensitive test data. The evolutionary white-box testing is a promising approach for the complete automation of structure-oriented test case generation. Here, test case generation can be completely automated with the help of evolutionary algorithms. However, problem cases exist in which the evolutionary test is not able to find valid test data. Thus, in the case of not achieving a test goal, it is not known whether this is due to non-executable program code or a problem case. This paper will investigate how successfully a software measure can support an evolutionary white-box test if the measure can predict the test effort. Hence, the termination criteria of evolutionary white-box testing can be adapted to test goals with problem cases in such a way that problematic test goals are either excluded from the test in advance or can be covered due to an adequate termination criteria according to a software measure. This could lead to an increase in efficiency and effectiveness of evolutionary white-box testing.|Frank Lammermann,Stefan Wappler","57410|GECCO|2005|Genetic algorithms using low-discrepancy sequences|The random number generator is one of the important components of evolutionary algorithms (EAs). Therefore, when we try to solve function optimization problems using EAs, we must carefully choose a good pseudo-random number generator. In EAs, the pseudo-random number generator is often used for creating uniformly distributed individuals. As the low-discrepancy sequences allow us to create individuals more uniformly than the random number sequences, we apply the low-discrepancy sequence generator, instead of the pseudo-random number generator, to EAs in this study. The numerical experiments show that the low-discrepancy sequence generator improves the search performances of EAs.|Shuhei Kimura,Koki Matsumura","57296|GECCO|2005|Diversity as a selection pressure in dynamic environments|Evolutionary algorithms (EAs) are widely used to deal with optimization problems in dynamic environments (DE) . When using EAs to solve DE problems, we are usually interested in the algorithm's ability to adapt and recover from the changes. One of the main problems facing an evolutionary method when solving DE problems is the loss of genetic diversity.In this paper, we investigate the use of evolutionary multi-objective optimization methods (EMOs) for single-objective DE problems. For that purpose, we introduce an artificial second objective with the aim to maintain useful diversity in the population. Six different artificial objectives are examined and compared.All the results will be compared against a traditional GA and the random immigrants algorithm. NSGA is employed as the evolutionary multi-objective technique.|Lam Thu Bui,J√ºrgen Branke,Hussein A. Abbass","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li"],["57521|GECCO|2005|Breeding swarms a GAPSO hybrid|In this paper we propose a novel hybrid (GAPSO) algorithm, Breeding Swarms, combining the strengths of particle swarm optimization with genetic algorithms. The hybrid algorithm combines the standard velocity and position update rules of PSOs with the ideas of selection, crossover and mutation from GAs. We propose a new crossover operator, Velocity Propelled Averaged Crossover (VPAC), incorporating the PSO velocity vector. The VPAC crossover operator actively disperses the population preventing premature convergence. We compare the hybrid algorithm to both the standard GA and PSO models in evolving solutions to five standard function minimization problems. Results show the algorithm to be highly competitive, often outperforming both the GA and PSO.|Matthew Settles,Terence Soule","57826|GECCO|2006|PSO and multi-funnel landscapes how cooperation might limit exploration|Particle Swarm Optimization (PSO) is a population-based optimization method in which search points employ a cooperative strategy to move toward one another. In this paper we show that PSO appears to work well on \"single-funnel\" optimization functions. On more complex optimization problems, PSO tends to converge too quickly and then fail to make further progress. We contend that most benchmarks for PSO have classically been demonstrated on single-funnel functions. However, in practice, optimization tasks are more complex and possess higher problem dimensionality. We present empirical results that support our conjecture that PSO performs well on single-funnel functions but tends to stagnate on more complicated landscapes.|Andrew M. Sutton,Darrell Whitley,Monte Lunacek,Adele E. Howe","57331|GECCO|2005|An efficient evolutionary algorithm applied to the design of two-dimensional IIR filters|This paper presents an efficient technique of designing two-dimensional IIR digital filters using a new algorithm involving the tightly coupled synergism of particle swarm optimization and differential evolution. The design task is reformulated as a constrained minimization problem and is solved by our newly developed PSO-DV (Particle Swarm Optimizer with Differentially perturbed Velocity) algorithm. Numerical results are presented. The paper also demonstrates the superiority of the proposed design method by comparing it with two recently published filter design methods.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57611|GECCO|2006|Adaptively choosing niching parameters in a PSO|Niching techniques play an important role in evolutionary algorithms. Existing niching methods often require user-specified parameters, limiting their usefulness. This paper proposes a niching method for Particle Swarm Optimisation (PSO) where population statistics are used to adaptively determine the niching parameters during a run. The proposed niching method is compared to another niching based PSO, SPSO. Our results show that the proposed adaptive niching PSO can solve difficult multimodal functions more reliably and with fewer evaluations.|Stefan Bird,Xiaodong Li","57708|GECCO|2006|Introducing recombination with dynamic linkage discovery to particle swarm optimization|In this paper, we introduce the recombination operator with the technique of dynamic linkage discovery to particle swarm optimization (PSO) in order to improve the performance of PSO. Numerical experiments are conducted on a set of carefully designed benchmark functions and demonstrate good performance achieved by the proposed methodology.|Ming-chung Jian,Ying-Ping Chen","57330|GECCO|2005|Two improved differential evolution schemes for faster global search|Differential evolution (DE) is well known as a simple and efficient scheme for global optimization over continuous spaces. In this paper we present two new, improved variants of DE. Performance comparisons of the two proposed methods are provided against (a) the original DE, (b) the canonical particle swarm optimization (PSO), and (c) two PSO-variants. The new DE-variants are shown to be statistically significantly better on a seven-function test bed for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57329|GECCO|2005|Improving particle swarm optimization with differentially perturbed velocity|This paper introduces a novel scheme of improving the performance of particle swarm optimization (PSO) by a vector differential operator borrowed from differential evolution (DE). Performance comparisons of the proposed method are provided against (a) the original DE, (b) the canonical PSO, and (c) three recent, high-performance PSO-variants. The new algorithm is shown to be statistically significantly better on a seven-function test suite for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule"],["57532|GECCO|2005|Unbiased tournament selection|Tournament selection is a popular form of selection which is commonly used with genetic algorithms, genetic programming and evolutionary programming. However, tournament selection introduces a sampling bias into the selection process. We review analytic results and present empirical evidence that shows this bias has a significant impact on search performance. We introduce two new forms of unbiased tournament selection that remove or reduce sampling bias in tournament selection.|Artem Sokolov,Darrell Whitley","65778|AAAI|2006|Quantifying the Impact of Learning Algorithm Parameter Tuning|The impact of learning algorithm optimization by means of parameter tuning is studied. To do this, two quality attributes, sensitivity and classification performance, are investigated, and two metrics for quantifying each of these attributes are suggested. Using these metrics, a systematic comparison has been performed between four induction algorithms on eight data sets. The results indicate that parameter tuning is often more important than the choice of algorithm and there does not seem to be a trade-off between the two quality attributes. Moreover, the study provides quantitative support to the assertion that some algorithms are more robust than others with respect to parameter configuration. Finally, it is briefly described how the quality attributes and their metrics could be used for algorithm selection in a systematic way.|Niklas Lavesson,Paul Davidsson","57362|GECCO|2005|Feature influence for evolutionary learning|This paper presents an approach that deals with the feature selection problem, and includes two main aspects first, the selection is done during the evolutionary learning process, i.e., it is a dynamic approach and second, the selection is local, i.e., the algorithm selects the best features from the best space region to learn at a given time of the exploration process. While the traditional feature selection is based on the attribute relevance, our approach is based on a new concept, called feature influence, which is aware of the dynamics and locality of the concept. The feature influence provides a measure of the attribute relevance at a certain instant of the evolutionary learning process, since it depends on each generation. Experimental results have been obtained by comparing an EA--based supervised learning algorithm to its modified version to include the concept approached. The results show an excellent performance, as the new adapted algorithm achieves the same classification results while using less rules, less conditions in rules and much less generations. The experiments include the statistical significance of the improvement over a set of sixteen datasets from the UCI repository.|Ra√∫l Gir√°ldez,Jes√∫s S. Aguilar-Ruiz","16150|IJCAI|2005|Reasoning with Inconsistent Ontologies|In this paper we present a framework of reasoning with inconsistent ontologies, in which pre-defined selection functions are used to deal with concept relevance. We examine how the notion of \"concept relevance\" can be used for reasoning with inconsistent ontologies. We have implemented a prototype called PION (Processing Inconsistent ONtologies), which is based on a syntactic relevance-based selection function. In this paper, we also report the experiments with PION.|Zhisheng Huang,Frank van Harmelen,Annette ten Teije","65471|AAAI|2005|Evolution of an Empathetic Digital Entity Phase One|This demonstration highlights the first of seven segments designed to develop a digital entity that will possess the potential for human empathy. The experiences in the first phase of the digital entity Zoe (Zero-One Entity) parallel a subset of learning and development activities encountered by human beings during their first nine months of existence. A website has been created to provide a window to observe Zoe's experiences and action selection process in order to make her basic learning observable, cumulative, and evolutionary. The human observer is invited to influence her action selection by setting the intensity of Zoe's digital personality traits such as assertiveness, reasoning ability, and disposition. Actions generate body-based and emotive-based feelings which are stored in memory structures. Significantly, these structures serve as a foundation for later stages of learning, understanding and reasoning.|Margaret Manella Kozak","57375|GECCO|2005|Applying both positive and negative selection to supervised learning for anomaly detection|This paper presents a novel approach of applying both positive selection and negative selection to supervised learning for anomaly detection. It first learns the patterns of the normal class via co-evolutionary genetic algorithm, which is inspired from the positive selection, and then generates synthetic samples of the anomaly class, which is based on the negative selection in the immune system. Two algorithms about synthetic generation of the anomaly class are proposed. One deals with data sets containing a few anomalous samples while the other deals with data sets containing no anomalous samples at all. The experimental results on some benchmark data sets from UCI data set repertory show that the detection rate is improved evidently, accompanied by a slight increase in false alarm rate via introducing novel synthetic samples of the anomaly class. The advantages of our method are the increased ability of classifiers in identifying both previously known and innovative anomalies, and the maximal degradation of overfitting phenomenon.|Xiaoshu Hang,Honghua Dai","57668|GECCO|2006|Improving GP classifier generalization using a cluster separation metric|Genetic Programming offers freedom in the definition of the cost function that is unparalleled among supervised learning algorithms. However, this freedom goes largely unexploited in previous work. Here, we revisit the design of fitness functions for genetic programming by explicitly considering the contribution of the wrapper and cost function. Within the context of supervised learning, as applied to classification problems, a clustering methodology is introduced using cost functions which encourage maximization of separation between in and out of class exemplars. Through a series of empirical investigations of the nature of these functions, we demonstrate that classifier performance is much more dependable than previously the case under the genetic programming paradigm.|Ashley George,Malcolm I. Heywood","57541|GECCO|2005|Is negative selection appropriate for anomaly detection|Negative selection algorithms for hamming and real-valued shape-spaces are reviewed. Problems are identified with the use of these shape-spaces, and the negative selection algorithm in general, when applied to anomaly detection. A straightforward self detector classification principle is proposed and its classification performance is compared to a real-valued negative selection algorithm and to a one-class support vector machine. Earlier work suggests that real-value negative selection requires a single class to learn from. The investigations presented in this paper reveal, however, that when applied to anomaly detection, the real-valued negative selection and self detector classification techniques require positive and negative examples to achieve a high classification accuracy. Whereas, one-class SVMs only require examples from a single class.|Thomas Stibor,Philipp H. Mohr,Jonathan Timmis,Claudia Eckert","57734|GECCO|2006|Analyzing active interactive genetic algorithms using visual analytics|This paper builds introduces visual-analytic techniques to aggregate, summarize, and visualize the information generated during interactive evolutionary processes. Special visualizations of the user-provided partial ordering of solutions, the synthetic fitness surrogates induced, and the model of user preferences were prepared. The proposed visual-analytic techniques point out potential pitfalls, strengths, and possible improvements in a non-trivial case study where the hierarchical tournament selection scheme of an active interactive genetic algorithm is replaced by an incremental selection scheme. Visual analytics provided an intuitive reasoning environment that unveiled important properties that greatly affect the performance of active interactive genetic algorithms that could not have been easily reveled otherwise.|Xavier Llor√†,Kumara Sastry,Francesc Al√≠as,David E. Goldberg,Michael Welge","57814|GECCO|2006|Anisotropic selection in cellular genetic algorithms|In this paper we introduce a new selection scheme in cellular genetic algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows accurate control of the selective pressure. First we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure we can obtain the same takeover time with the two techniques although the spreading of the best individual is different. We then give experimental results that show to what extent AS promotes the emergence of niches that support low coupling and high cohesion. Finally, using a cGA with anisotropic selection on a Quadratic Assignment Problem we show the existence of an anisotropic optimal value for which the best average performance is observed. Further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme.|David Simoncini,S√©bastien V√©rel,Philippe Collard,Manuel Clergue"],["16322|IJCAI|2005|Disjunctive Temporal Planning with Uncertainty|Driven by planning problems with both disjunctive constraints and contingency, we define the Disjunctive Temporal Problem with Uncertainty (DTPU), an extension of the DTP that includes contingent events. Generalizing existing work on Simple Temporal Problems with Uncertainty, we divide the time-points into controllable and uncontrollable classes, and propose varying notions of controllability to replace the notion of consistency.|Kristen Brent Venable,Neil Yorke-Smith","16132|IJCAI|2005|Integrating Planning and Temporal Reasoning for Domains with Durations and Time Windows|The treatment of exogenous events in planning is practically important in many domains. In this paper we focus on planning with exogenous events that happen at known times, and affect the plan actions by imposing that the execution of certain plan actions must be during some time windows. When actions have durations, handling such constraints adds an extra difficulty to planning, which we address by integrating temporal reasoning into planning. We propose a new approach to planning in domains with durations and time windows, combining graph-based planning and disjunctive constraint-based temporal reasoning. Our techniques are implemented in a planner that took part in the th International Planning Competition showing very good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina","65718|AAAI|2006|Incremental Least-Squares Temporal Difference Learning|Approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning, usually solved using temporal difference (TD) algorithms. In this paper we introduce a new variant of linear TD learning, called incremental least-squares TD learning, or iLSTD. This method is more data efficient than conventional TD algorithms such as TD() and is more computationally efficient than non-incremental least-squares TD methods such as LSTD (Bradtke & Barto  Boyan ). In particular, we show that the per-time-step complexities of iLSTD and TD() are O(n), where n is the number of features, whereas that of LSTD is O(n). This difference can be decisive in modern applications of reinforcement learning where the use of a large number features has proven to be an effective solution strategy. We present empirical comparisons, using the test problem introduced by Boyan (), in which iLSTD converges faster than TD() and almost as fast as LSTD.|Alborz Geramifard,Michael H. Bowling,Richard S. Sutton","65614|AAAI|2005|Exploiting the Structure of Hierarchical Plans in Temporal Constraint Propagation|Quantitative temporal constraints are an essential requirement for many planning domains. The HTN planning paradigm has proven to be better suited than other approaches to many applications. To date, however, efficiently integrating temporal reasoning with HTN planning has been little explored. This paper describes a means to exploit the structure of a HTN plan in performing temporal propagation on an associated Simple Temporal Network. By exploiting the natural restriction on permitted temporal constraints, the time complexity of propagation can be sharply reduced, while completeness of the inference is maintained. Empirical results indicate an order of magnitude improvement on real-world plans.|Neil Yorke-Smith","65589|AAAI|2005|Value Functions for RL-Based Behavior Transfer A Comparative Study|Temporal difference (TD) learning methods (Sutton & Barto ) have hecome popular reinforcement learning techniques in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have heen shown to exhibit some desirable properties in theory, but have often been found slow in practice. This paper presents methods for further generalizing across tasks, thereby speeding up learning. via a novel form of behavior transfer. We compare learning on a complex task with three function approximators, a CMAC, a neural network, and an RBF, and demonstrate that behavior transfer works well with all three. Using behavior transfer, agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup-soccer keepaway domain.|Matthew E. Taylor,Peter Stone,Yaxin Liu","16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando","57855|GECCO|2006|On-line evolutionary computation for reinforcement learning in stochastic domains|In reinforcement learning, an agent interacting with its environment strives to learn a policy that specifies, for each state it may encounter, what action to take. Evolutionary computation is one of the most promising approaches to reinforcement learning but its success is largely restricted to off-line scenarios. In on-line scenarios, an agent must strive to maximize the reward it accrues while it is learning. Temporal difference (TD) methods, another approach to reinforcement learning, naturally excel in on-line scenarios because they have selection mechanisms for balancing the need to search for better policies exploration) with the need to accrue maximal reward (exploitation). This paper presents a novel way to strike this balance in evolutionary methods by borrowing the selection mechanisms used by TD methods to choose individual actions and using them in evolution to choose policies for evaluation. Empirical results in the mountain car and server job scheduling domains demonstrate that these techniques can substantially improve evolution's on-line performance in stochastic domains.|Shimon Whiteson,Peter Stone","65600|AAAI|2005|Improving Reinforcement Learning Function Approximators via Neuroevolution|Reinforcement learning problems are commonly tackled with temporal difference methods, which use dynamic programming and statistical sampling to estimate the long-term value of taking each action in each state. In most problems of real-world interest, learning this value function requires a function approximator. which represents the mapping from stateaction pairs to values via a concise, parameterized function and uses supervised learning methods to set its parameters. Function approximators make it possible to use temporal difference methods on large problems but, in practice, the feasibility of doing so depends on the ability of the human designer to select an appropriate representation for the value function. My thesis presents a new approach to function approximation that automates some of these difficult design choices by coupling temporal difference methods with policy search methods such as evolutionary computation. It also presents a particular implementation which combines NEAT, a neuroevolutionary policy search method, and Q-learning, a popular temporal difference method, to yield a new method called NEAT+Q that automatically learns effective representations for neural network function approximators. Empirical results in a server job scheduling task demonstrate that NEAT+Q can outperform both NEAT and Q-learning with manually designed neural networks.|Shimon Whiteson","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","65568|AAAI|2005|Improving Action Selection in MDPs via Knowledge Transfer|Temporal-difference reinforcement learning (RL) has been successfully applied in several domains with large state sets. Large action sets, however, have received considerably less attention. This paper demonstrates the use of knowledge transfer between related tasks to accelerate learning with large action sets. We introduce action transfer, a technique that extracts the actions from the (near-)optimal solution to the first task and uses them in place of the full action set when learning any subsequent tasks. When optimal actions make up a small fraction of the domain's action set, action transfer can substantially reduce the number of actions and thus the complexity of the problem. However, action transfer between dissimilar tasks can be detrimental. To address this difficulty, we contribute randomized task perturbation (RTP), an enhancement to action transfer that makes it robust to unrepresentative source tasks. We motivate RTP action transfer with a detailed theoretical analysis featuring a formalism of related tasks and a bound on the suboptimality of action transfer. The empirical results in this paper show the potential of RTP action transfer to substantially expand the applicability of RL to problems with large action sets.|Alexander A. Sherstov,Peter Stone"],["80616|VLDB|2006|Querying Business Processes|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (Business Process Execution Language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers(peers).We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","57721|GECCO|2006|Genetic algorithms for action set selection across domains a demonstration|Action set selection in Markov Decision Processes (MDPs) is an area of research that has received little attention. On the other hand, the set of actions available to an MDP agent can have a significant impact on the ability of the agent to gain optimal rewards. Last year at GECCO', the first automated action set selection tool powered by genetic algorithms was presented. The demonstration of its capabilities, though intriguing, was limited to a single domain. In this paper, we apply the tool to a more challenging problem of oil sand image interpretation. In the new experiments, genetic algorithms evolved a compact high-performance set of image processing operators, decreasing interpretation time by % while improving image interpretation accuracy by %. These results exceed the original performance and suggest certain cross-domain portability of the approach.|Greg Lee,Vadim Bulitko","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","65773|AAAI|2006|Learning Basis Functions in Hybrid Domains|Markov decision processes (MDPs) with discrete and continuous state and action components can be solved efficiently by hybrid approximate linear programming (HALP). The main idea of the approach is to approximate the optimal value function by a Set of basis functions and optimize their weights by linear programming. The quality of this approximation naturally depends on its basis functions. However, basis functions leading to good approximations are rarely known in advance. In this paper, we propose a new approach that discovers these functions automatically. The method relies on a class of parametric basis function models, which are optimized using the dual formulation of a relaxed HALP. We demonstrate the performance of our method on two hybrid optimization problems and compare it to manually selected basis functions.|Branislav Kveton,Milos Hauskrecht","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aur√©lie Beynier,Abdel-Illah Mouaddib"],["16073|IJCAI|2005|Compiling Bayesian Networks with Local Structure|Recent work on compiling Bayesian networks has reduced the problem to that of factoring CNF encodings of these networks, providing an expressive framework for exploiting local structure. For networks that have local structure, large CPTs, yet no excessive determinism, the quality of the CNF encodings and the amount of local structure they capture can have a significant effect on both the offline compile time and online inference time. We examine the encoding of such Bayesian networks in this paper and report on new findings that allow us to significantly scale this compilation approach. In particular, we obtain order-of-magnitude improvements in compile time, compile some networks successfully for the first time, and obtain ordersof-magnitude improvements in online inference for some networks with local structure, as compared to baseline jointree inference, which does not exploit local structure.|Mark Chavira,Adnan Darwiche","65869|AAAI|2006|Identification and Evaluation of Weak Community Structures in Networks|Identifying intrinsic structures in large networks is a fundamental problem in many fields, such as engineering, social science and biology. In this paper, we are concerned with communities, which are densely connected sub-graphs in a network, and address two critical issues for finding community structures from large experimental data. First, most existing network clustering methods assume sparse networks and networks with strong community structures. In contrast, we consider sparse and dense networks with weak community structures. We introduce a set of simple operations that capture local neighborhood information of a node to identify weak communities. Second, we consider the issue of automatically determining the most appropriate number of communities, a crucial problem for all clustering methods. This requires to properly evaluate the quality of community structures. Built atop a function for network cluster evaluation by Newman and Girvan, we extend their work to weighted graphs. We have evaluated our methods on many networks of known structures, and applied them to analyze a collaboration network and a genetic network. The results showed that our methods can find superb community structures and correct numbers of communities. Comparing to the existing approaches, our methods performed significantly better on networks with weak community structures and equally well on networks with strong community structures.|Jianhua Ruan,Weixiong Zhang","65739|AAAI|2006|Solving MAP Exactly by Searching on Compiled Arithmetic Circuits|The MAP (maximum a posteriori hypothesis) problem in Bayesian networks is to find the most likely states of a set of variabls given partial evidence on the complement of that set. Standard structure-based inference methods for finding exact solutions to MAP, such as variable elimination and join-tree algorithms, have complexities that are exponential in the constrained treewidth of the network. A more recent algorithm, proposed by Park and Darwiche, is exponential only in the treewidth and has been shown to handle networks whose constrained treewidth is quite high. In this paper we present a new algorithm for exact MAP that is not necessarily limited in scalability even by the treewidth. This is achieved by leveraging recent advances in compilation of Bayesian networks into arithmetic circuits, which can circumvent treewidth-imposed limits by exploiting the local structure present in the network. Specifically, we implement a branch-and-bound search where the bounds are computed using linear-time operations on the compiled arithmetic circuit. On networks with local structure, we observe orders-of-magnitude improvements over the algorithm of Park and Darwiche. In particular, we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high treewidth.|Jinbo Huang,Mark Chavira,Adnan Darwiche","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,G√©rard Verfaillie","65363|AAAI|2005|Hybrid Possibilistic Networks|Possibilistic networks are important tools for dealing with uncertain pieces of information. For multiply-connected networks, it is well known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of standard propagation algorithm.|Salem Benferhat,Salma Smaoui","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","57427|GECCO|2005|Nonlinear feature extraction using a neuro genetic hybrid|Feature extraction is a process that extracts salient features from observed variables. It is considered a promising alternative to overcome the problems of weight and structure optimization in artificial neural networks. There were many nonlinear feature extraction methods using neural networks but they still have the same difficulties arisen from the fixed network topology. In this paper, we propose a novel combination of genetic algorithm and feedforward neural networks for nonlinear feature extraction. The genetic algorithm evolves the feature space by utilizing characteristics of hidden neurons. It improved remarkably the performance of neural networks on a number of real world regression and classification problems.|Yung-Keun Kwon,Byung Ro Moon","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","16296|IJCAI|2005|Decentralized Search in Networks Using Homophily and Degree Disparity|We propose a new algorithm for finding a target node in a network whose topology is known only locally. We formulate this task as a problem of decision making under uncertainty and use the statistical properties of the graph to guide this decision. This formulation uses the homophily and degree structure of the network simultaneously, differentiating our algorithm from those previously proposed in the literature. Because homophily and degree disparity are characteristics frequently observed in real-world networks, the algorithm we propose is applicable to a wide variety of networks, including two families that have received much recent attention small-world and scale-free networks.|√\u2013zg√ºr Simsek,David Jensen","80705|VLDB|2006|Online Outlier Detection in Sensor Data Using Non-Parametric Models|Sensor networks have recently found many popular applications in a number of different settings. Sensors at different locations can generate streaming data, which can be analyzed in real-time to identify events of interest. In this paper, we propose a framework that computes in a distributed fashion an approximation of multi-dimensional data distributions in order to enable complex applications in resource-constrained sensor networks.We motivate our technique in the context of the problem of outlier detection. We demonstrate how our framework can be extended in order to identify either distance- or density-based outliers in a single pass over the data, and with limited memory requirements. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques. We also demonstrate the applicability of our technique to other related problems in sensor networks.|Sharmila Subramaniam,Themis Palpanas,Dimitris Papadopoulos,Vana Kalogeraki,Dimitrios Gunopulos"]]},"title":{"entropy":6.167134650319735,"topics":["genetic algorithm, algorithm for, the problem, genetic for, genetic programming, the algorithm, evolutionary algorithm, for problem, for the, using genetic, the, evolutionary for, the and, particle swarm, using algorithm, for optimization, and algorithm, solving problem, algorithm problem, evolutionary the","the web, sense disambiguation, description logic, for data, the semantic, for xml, semantic web, web search, for web, word disambiguation, word sense, web services, and system, data stream, query for, the and, from, time series, decision processes, human-robot interaction","for and, local search, search for, and its, differential evolution, model for, and, artificial immune, with and, and search, evolution strategies, its application, estimation distribution, evolution for, differential optimization, search, with, and constraint, and evolution, model and","learning for, reinforcement learning, system for, learning, neural networks, constraint satisfaction, for networks, classifier system, vector machine, support vector, method for, support machine, natural language, distributed pomdps, networks, based approach, bayesian networks, learning classifier, ant colony, learning system","algorithm for, genetic for, for scheduling, hybrid genetic, hybrid for, hybrid algorithm, algorithm scheduling, for problem, for and, and flexible, genetic scheduling, for, software testing, and scheduling, scheduling flexible, genetic flexible, hybrid problem, genetic quadratic, scheduling problem, evolutionary for","genetic algorithm, algorithm for, genetic programming, genetic the, genetic for, the algorithm, using genetic, and genetic, programming for, genetic problem, genetic with, and algorithm, algorithm with, and programming, the programming, algorithm problem, using programming, genetic selection, algorithm selection, programming problem","the web, for web, sense disambiguation, description logic, and web, web services, word disambiguation, word sense, and complexity, services composition, web search, using web, and context, for logic, the services, semantic web, text categorization, for services, for description, the and","the semantic, semantic for, from the, from data, engine for, and retrieval, for retrieval, load stream, from, image and, image using, data sources, the query, from and, query for, semantic and, search engine, for image, semantic web, challenge and","model for, with and, model and, for with, with, algorithm with, and inference, for graph, genetic with, model, representation and, combining and, the coevolution, probabilistic for, clustering and, using model, and probabilistic, predictive representation, scheme for, model algorithm","for and, and, and algorithm, generation for, the and, for approximation, value iteration, crossover operator, and genetic, techniques for, order crossover, crossover for, and programming, linear for, value function, optimal for, for function, operator for, and evolutionary, and function","reinforcement learning, approach for, vector machine, support vector, support for, support machine, natural language, based approach, language for, adaptive for, language and, evolutionary learning, approach, approach and, machine learning, for reinforcement, learning for, for data, for interactive, learning via","new for, selection for, for sets, negative selection, statistical learning, incorporating into, for detection, theory and, selection and, for task, and sets, theory for, selection learning, selection, for statistical, with preferences, memory for, real-time for, for solving, sets"],"ranking":[["57306|GECCO|2005|A hybrid evolutionary algorithm for the p-median problem|A hybrid evolutionary algorithm (EA) for the p-median problem consist of two stages, each of which is a steady-state hybrid EA. These EAs encode selections of medians as subsets of the candidate sites, apply a recombination operator tailored to the problem, and select symbols in chromosomes to mutate based on an explicit collective memory (named virtual loser). They also apply a sequence of two or three local search procedures to each new solution. Tests e.g. on the benchmark problem instances of ORLIB returned results within .% of the best solutions known.|Istv√°n Borgulya","57405|GECCO|2005|Theoretical analysis of a mutation-based evolutionary algorithm for a tracking problem in the lattice|Evolutionary algorithms are often applied for solving optimization problems that are too complex or different from classical problems so that the application of classical methods is difficult. One example are dynamic problems that change with time. An important class of dynamic problems is the class of tracking problems where an algorithm has to find an approximately optimal solution and insure an almost constant quality in spite of the changing problem. For the application of evolutionary algorithms to static optimization problems, the distribution of the optimization time and most often its expected value are most important. Adopting this perspective a simple tracking problem in the lattice is considered and the performance of a mutation-based evolutionary algorithm is evaluated. For the static case, asymptotically tight upper and lower bounds are proven. These results are applied to derive results on the tracking performance for different rates of change.|Thomas Jansen,Ulf Schellbach","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57654|GECCO|2006|The snake in the box problem mathematical conjecture and a genetic algorithm approach|With applications in coding theory and hypercube-based computing and networking, the \"snake in the box\" problem is of great practical importance. Moreover, it is both mathematically elegant and highly difficult. The problem is simply to find the longest \"snake\" in a hypercube. However, as the hypercube grows in dimensionality, the size of the search space increases exponentially. Moreover, as the maximum snake length is only known for the smallest dimensions (where the snakes themselves have already been identified), there is no known stopping criterion for the search in higher dimensions. In this paper we make a mathematical conjecture about the possible maximum length of a snake in a hypercube of dimension d. We use a genetic algorithm for finding snakes in a  hypercube to show some results.|Pedro A. Diaz-Gomez,Dean F. Hougen","57274|GECCO|2005|Heuristic rules embedded genetic algorithm to solve in-core fuel management optimization problem|Because of the large number of possible combinations for the fuel assembly loading in the core, the design of the loading pattern (LP) is a complex optimization problem. It requires finding an optimal fuel arrangement in order to achieve maximum cycle length while satisfying the safety constraints. The objective of this study is to develop a loading pattern optimization code. Generally in-core fuel management codes are written for specific cores and limited fuel inventory. One of the goals of this study is to develop a loading pattern optimization code, which is applicable for all types of Pressurized Water Reactor (PWR) core structures with unlimited number of fuel assembly types in the inventory. To reach this goal an innovative genetic algorithm is developed with modifying the classical representation of the genotype. To obtain the best result in a shorter time not only the representation is changed but also the algorithm is changed to use in-core fuel management heuristics rules. The improved GA code was tested demonstrating the advantages of the introduced enhancements. The core physics code used in this research is Moby-Dick, which was developed to analyze the VVER reactors by SKODA Inc.|Fatih Alim,Kostadin Ivanov","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum"],["16121|IJCAI|2005|Image Retrieval and Disambiguation for Encyclopedic Web Search|To produce multimedia encyclopedic content, we propose a method to search the Web for images associated with a specific word sense. We use text in an HTML file which links to an image as a pseudocaption for the image and perform text-based indexing and retrieval. We use term descriptions in a Web search site called \"CYCLONE\" as queries and correspond images and texts based on word senses.|Atsushi Fujii,Tetsuya Ishikawa","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65498|AAAI|2005|Searching for Common Sense Populating Cyc from the Web|The Cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information - what is known informally as \"common sense.\" Over the last twenty years, a sufficient core of common sense knowledge has been entered into Cyc to allow it to begin effectively and flexibly supporting its most important task increasing its own store of world knowledge. In this paper, we present initial work on a method of using a combination of Cyc and the World Wide Web, accessed via Google, to assist in entering knowledge into Cyc. The long-term goal is automating the process of building a consistent, formalized representation of the world in the Cyc knowledge base via machine learning. We present preliminary results of this work and describe how we expect the knowledge acquisition process to become more accurate, faster, and more automated in the future.|Cynthia Matuszek,Michael J. Witbrock,Robert C. Kahlert,John Cabral,David Schneider,Purvesh Shah,Douglas B. Lenat","65754|AAAI|2006|An End-to-End Supervised Target-Word Sense Disambiguation System|We present an extensible supervised Target-Word Sense Disambiguation system that leverages upon GATE (General Architecture for Text Engineering), NSP (Ngram Statistics Package) and WEKA (Waikato Environment for Knowledge Analysis) to present an end-to-end solution that integrates feature identification, feature extraction, preprocessing and classification.|Mahesh Joshi,Serguei V. S. Pakhomov,Ted Pedersen,Richard Maclin,Christopher G. Chute","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Korn√©l G. Mark√≥,Stefan Schulz,Udo Hahn","16244|IJCAI|2005|Building the Semantic Web Tower from RDF Straw|A same-syntax extension of RDF to first-order logic results in a collapse of the model theory due to logical paradoxes resulting from diagonalization. RDF is thus the wrong material for building the Semantic Web tower.|Peter F. Patel-Schneider","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["57281|GECCO|2005|Optimization with constraints using a cultured differential evolution approach|In this paper we propose a cultural algorithm, where different knowledge sources modify the variation operator of a differential evolution algorithm. Differential evolution is used as a basis for the population, variation and selection processes. The experiments performed show that the cultured differential evolution is able to reduce the number of fitness function evaluations needed to obtain a good aproximation of the optimum value in constrained real-parameter optimization. Comparisons are provided with respect to three techniques that are representative of the state-of-the-art in the area.|Ricardo Landa Becerra,Carlos A. Coello Coello","57463|GECCO|2005|Enhancing differential evolution performance with local search for high dimensional function optimization|In this paper, we proposed Fittest Individual Refinement (FIR), a crossover based local search method for Differential Evolution (DE). The FIR scheme accelerates DE by enhancing its search capability through exploration of the neighborhood of the best solution in successive generations. The proposed memetic version of DE (augmented by FIR) is expected to obtain an acceptable solution with a lower number of evaluations particularly for higher dimensional functions. Using two different implementations DEfirDE and DEfirSPX we showed that proposed FIR increases the convergence velocity of DE for well known benchmark functions as well as improves the robustness of DE against variation of population. Experiments using multimodal landscape generator showed our proposed algorithms consistently outperformed their parent algorithms. A performance comparison with reported results of well known real coded memetic algorithms is also presented.|Nasimul Noman,Hitoshi Iba","57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","57447|GECCO|2005|Promising infeasibility and multiple offspring incorporated to differential evolution for constrained optimization|In this paper, we incorporate a diversity mechanism to the differential evolution algorithm to solve constrained optimization problems without using a penalty function. The aim is twofold () to allow infeasible solutions with a promising value of the objective function to remain in the population and also () to increase the probabilities of an individual to generate a better offspring while promoting collaboration of all the population to generate better solutions. These goals are achieved by allowing each parent to generate more than one offspring. The best offspring is selected using a comparison mechanism based on feasibility and this child is compared against its parent. To maintain diversity, the proposed approach uses a mechanism successfully adopted with other evolutionary algorithms where, based on a parameter Sr a solution (between the best offspring and the current parent) with a better value of the objective function can remain in the population, regardless of its feasibility. The proposed approach is validated using test functions from a well-known benchmark commonly adopted to validate constraint-handling techniques used with evolutionary algorithms. The statistical results obtained by the proposed approach are highly competitive (based on quality, robustness and number of evaluations of the objective function) with respect to other constraint-handling techniques, either based on differential evolution or on other evolutionary algorithms, that are representative of the state-of-the-art in the area. Finally, a small set of experiments were made to detect sensitivity of the approach to its parameters.|Efr√©n Mezura-Montes,Jes√∫s Vel√°zquez-Reyes,Carlos A. Coello Coello","57860|GECCO|2006|The LEM implementation of learnable evolution model and its testing on complex function optimization problems|Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to ). In every experiment, LEM outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.|Janusz Wojtusiak,Ryszard S. Michalski","57746|GECCO|2006|A comparative study of differential evolution variants for global optimization|In this paper, we present an empirical comparison of some Differential Evolution variants to solve global optimization problems. The aim is to identify which one of them is more suitable to solve an optimization problem, depending on the problem's features and also to identify the variant with the best performance, regardless of the features of the problem to be solved. Eight variants were implemented and tested on  benchmark problems taken from the specialized literature. These variants vary in the type of recombination operator used and also in the way in which the mutation is computed. A set of statistical tests were performed in order to obtain more confidence on the validity of the results and to reinforce our discussion. The main aim is that this study can help both researchers and practitioners interested in using differential evolution as a global optimizer, since we expect that our conclusions can provide some insights regarding the advantages or limitations of each of the variants studied.|Efr√©n Mezura-Montes,Jes√∫s Vel√°zquez-Reyes,Carlos A. Coello Coello","57330|GECCO|2005|Two improved differential evolution schemes for faster global search|Differential evolution (DE) is well known as a simple and efficient scheme for global optimization over continuous spaces. In this paper we present two new, improved variants of DE. Performance comparisons of the two proposed methods are provided against (a) the original DE, (b) the canonical particle swarm optimization (PSO), and (c) two PSO-variants. The new DE-variants are shown to be statistically significantly better on a seven-function test bed for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57770|GECCO|2006|A new generation alternation model for differential evolution|We present a modified version of Differential Evolution (DE) for locating the global minimum at a higher convergence velocity. The proposed model differs from conventional DE by applying selection both for reproduction and survival, whereas the original model applies exclusively \"knock-out\" selection mechanism for survival. Because of its one-to-one reproduction strategy DE often consumes too many fitness evaluations to locate the global optimum. In this work we show that selecting parents for breeding and offspring for survival, DE's search capability can be further accelerated, which will be particularly useful for expensive function optimizations. Computational results using many benchmark functions are reported which show significant improvements in the convergence characteristics of the proposed algorithm over the original one.|Nasimul Noman,Hitoshi Iba","57423|GECCO|2005|ARGEN  AREPO mixing the artificial genetic engineering and artificial evolution of populations to improve the search process|In this paper we analyze the performance of several evolutionary algorithms in the feature and instance selection problem. It is also introduced the ARGEN + AREPO search algorithm which has been tested in the same problem. There is no need to adapt parameters in this genetic algorithm, except the population size. The reported preliminary results show that using this technique in a wrapper model to search data subsets, we can obtain accuracy similar to the obtained with some of the genetic algorithms models here presented, but with less data.|Agust√≠n Le√≥n-Barranco,Sandra E. Barajas,Carlos A. Reyes Garc√≠a","57399|GECCO|2005|Efficient differential evolution using speciation for multimodal function optimization|In this paper differential evolution is extended by using the notion of speciation for solving multimodal optimization problems. The proposed species-based DE (SDE) is able to locate multiple global optima simultaneously through adaptive formation of multiple species (or subpopulations) in an DE population at each iteration step. Each species functions as an DE by itself. Successive local improvements through species formation can eventually transform into global improvements in identifying multiple global optima. In this study the performance of SDE is compared with another recently proposed DE variant CrowdingDE. The computational complexity of SDE, the effect of population size and species radius on SDE are investigated. SDE is found to be more computationally efficient than CrowdingDE over a number of benchmark multimodal test functions.|Xiaodong Li"],["57264|GECCO|2005|Analysis of the initialization stage of a Pittsburgh approach learning classifier system|This paper is focused on studying the initialization stage of learning classifier systems (LCS) applying the Pittsburgh approach. It has a theoretical part where the covering probability of a random rule set is modelled and a practical part. The practical part has the objective of developing general initialization policies that have competent performance on a broad range of datasets. Two kinds of policies are tested () ways of tuning the initialization probability of the system and () smart initialization operators that create rules that are generalized versions of randomly sampled training instances. The results identify a subset of settings that are robust enough to be considered candidates to be the default initialization policy. These settings have competent performance compared to several alternative machine learning systems. Beside identifying the good policies, the experimentation made is also useful to give hints about what kind of initial solutions is the system able to process successfully to create well generalized solutions.|Jaume Bacardit","65380|AAAI|2005|A Learning and Reasoning System for Intelligence Analysis|This paper presents a personal cognitive assistant, called Disciple-LTA, that can acquire expertise in intelligence analysis directly from intelligence analysts, can train new analysts, and can help analysts find solutions to complex problems through mixed-initiative reasoning, making possible the synergistic integration of a human's experience and creativity with an automated agent's knowledge and speed, and facilitating the collaboration with complementary experts and their agents.|Mihai Boicu,Gheorghe Tecuci,Cindy Ayers,Dorin Marcu,Cristina Boicu,Marcel Barbulescu,Bogdan Stanescu,William Wagner,Vu Le,Denitsa Apostolova,Adrian Ciubotariu","16233|IJCAI|2005|A Machine Learning Approach to Identification and Resolution of One-Anaphora|We present a machine learning approach to identifying and resolving one-anaphora. In this approach, the system first learns to distinguish different uses of instances of the word one in the second stage, the antecedents of those instances of one that are classified as anaphoric are then determined. We evaluated our approach on written texts drawn from the informative domains of the British National Corpus (BNC), and achieved encouraging results. To our knowledge, this is the first learning-based system for the identification and resolution of one-anaphora.|Hwee Tou Ng,Yu Zhou,Robert Dale,Mary Gardiner","57594|GECCO|2006|Smart crossover operator with multiple parents for a Pittsburgh learning classifier system|This paper proposes a new smart crossover operator for a Pittsburgh Learning Classifier System. This operator, unlike other recent LCS approaches of smart recombination, does not learn the structure of the domain, but it merges the rules of N parents (N  ) to generate a new offspring. This merge process uses an heuristic that selects the minimum subset of candidate rules that obtains maximum training accuracy. Moreover the operator also includes a rule pruning scheme to avoid the inclusion of over-specific rules, and to guarantee as much as possible the robust behaviour of the LCS. This operator takes advantage from the fact that each individual in a Pittsburgh LCS is a complete solution, and the system has a global view of the solution space that the proposed rule selection algorithm exploits. We have empirically evaluated this operator using a recent LCS called GAssist. First with the standard LCS benchmark, the  bits multiplexer, and later using  standard real datasets. The results of the experiments over these datasets indicate that the new operator manages to increase the accuracy of the system over the classical crossover in  of the  datasets, and never having a significantly worse performance than the classic operator.|Jaume Bacardit,Natalio Krasnogor","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llor√†,Kumara Sastry","65765|AAAI|2006|Detecting Spam Blogs A Machine Learning Approach|Weblogs or blogs are an important new way to publish information, engage in discussions, and form communities on the Internet. The Blogosphere has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from splogs - false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features can be used to detect splogs. We present an evaluation of learned models and their utility to blog search engines systems that employ techniques differing from those of conventional web search engines.|Pranam Kolari,Akshay Java,Tim Finin,Tim Oates,Anupam Joshi","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","57803|GECCO|2006|Reward allotment in an event-driven hybrid learning classifier system for online soccer games|This paper describes our study into the concept of using rewards in a classifier system applied to the acquisition of decision-making algorithms for agents in a soccer game. Our aim is to respond to the changing environment of video gaming that has resulted from the growth of the Internet, and to provide bug-free programs in a short time. We have already proposed a bucket brigade algorithm (a reinforcement learning method for classifiers) and a procedure for choosing what to learn depending on the frequency of events with the aim of facilitating real-time learning while a game is in progress. We have also proposed a hybrid system configuration that combines existing algorithm strategies with a classifier system, and we have reported on the effectiveness of this hybrid system. In this paper, we report on the results of performing reinforcement learning with different reward values assigned to reflect differences in the roles performed by forward, midfielder and defense players, and we describe the results obtained when learning is performed with different combinations of success rewards for various type of play such as dribbling and passing. In  matches played against an existing soccer game incorporating an algorithm devised by humans, a better win ratio and better convergence were observed compared with the case where learning was performed with no roles assigned to all of the in-game agents.|Yuji Sato,Yosuke Akatsuka,Takenori Nishizono","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","57306|GECCO|2005|A hybrid evolutionary algorithm for the p-median problem|A hybrid evolutionary algorithm (EA) for the p-median problem consist of two stages, each of which is a steady-state hybrid EA. These EAs encode selections of medians as subsets of the candidate sites, apply a recombination operator tailored to the problem, and select symbols in chromosomes to mutate based on an explicit collective memory (named virtual loser). They also apply a sequence of two or three local search procedures to each new solution. Tests e.g. on the benchmark problem instances of ORLIB returned results within .% of the best solutions known.|Istv√°n Borgulya","57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","57647|GECCO|2006|Comparison of genetic representation schemes for scheduling soft real-time parallel applications|This paper presents a hybrid technique that combines List Scheduling (LS) with Genetic Algorithms (GA) for constructing non-preemptive schedules for soft real-time parallel applications represented as directed acyclic graphs (DAGs). The execution time requirements of the applications' tasks are assumed to be stochastic and are represented as probability distribution functions. The performance in terms of schedule lengths for three different genetic representation schemes are evaluated and compared for a number of different DAGs.The approaches presented here produce shorter schedules than HLFET, a popular LS approach for all of the sample problems. Of the three genetic representation schemes investigated, PosCT, the technique that allows the GA to learn which tasks to delay in order to allow other tasks to complete produced the shortest schedules for a majority of the sample DAGs.|Yoginder S. Dandass,Amit C. Bugde","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","65457|AAAI|2005|The Deep Space Network Scheduling Problem|We describe the Deep Space Network's scheduling problem based on a user requirement language. The problem is difficult to encode by almost all existing planning and scheduling systems. We describe how it can be mapped into a system that supports metric resources, durative action, simple temporal network constraints, and task hierarchy among other language features. We also describe how we adapted a local search scheduler to generate schedules. However, we argue that the application will best serve the users if local search is combined with systematic search. We describe how an implemented systematic search can be effectively applied to rescheduling.|Bradley J. Clement,Mark D. Johnston","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius"],["57311|GECCO|2005|Multi-chromosomal genetic programming|This paper introduces an evolutionary algorithm which uses multiple chromosomes to evolve solutions to a symbolic regression problem. Inspiration for this algorithm is provided by the existence of multiple chromosomes in natural evolution, particularly in plants. A multi-chromosomal system usually requires a dominance system and subsequently dominance in nature and in previous artificial evolutionary systems has also been considered. An implementation of a multi-chromosomal system is presented with initial results which support the use of multi-chromosomal techniques in evolutionary algorithms.|Rachel Cavill,Stephen L. Smith,Andrew M. Tyrrell","57603|GECCO|2006|Instance similarity and the effectiveness of case injection in a genetic algorithm for binary quadratic programming|When an evolutionary algorithm addresses a sequence of instances of the same problem, it can seed its population with solutions that it found for previous instances. This technique is called case injection. How similar must the instances be for case injection to help an EA's search We consider this question by applying a genetic algorithm, without and with case injection, to sequences of instances of binary quadratic programming. When the instances are similar, case injection helps when the instances differ sufficiently, case injection is no help at all.|Jason Amunrud,Bryant A. Julstrom","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57688|GECCO|2006|ORDERTREE a new test problem for genetic programming|In this paper, we describe a new test problem for genetic programming (GP), ORDERTREE. We argue that it is a natural analogue of ONEMAX, a popular GA test problem, and that it also avoids some of the known weaknesses of other benchmark problems for Genetic Programming. Through experiments, we show that the difficulty of the problem can be tuned not only by increasing the size of the problem, but also by increasing the non-linearity in the fitness structure.|Tuan Hao Hoang,Nguyen Xuan Hoai,Nguyen Thi Hien,Robert I. McKay,Daryl Essam","57279|GECCO|2005|Evolutionary tree genetic programming|We introduce a clustering-based method of subpopulation management in genetic programming (GP) called Evolutionary Tree Genetic Programming (ETGP). The biological motivation behind this work is the observation that the natural evolution follows a tree-like phylogenetic pattern. Our goal is to simulate similar behavior in artificial evolutionary systems such as GP. To test our model we use three common GP benchmarks the Ant Algorithm, -Multiplexer, and Parity problems.The performance of the ETGP system is empirically compared to those of the GP system. Code size and variance are consistently reduced by a small but statistically significant percentage, resulting in a slight speedup in the Ant and -Multiplexer problems, while the same comparisons on the Parity problem are inconclusive.|J√°n Antol√≠k,William H. Hsu","57484|GECCO|2005|Backward-chaining genetic programming|This paper presents a backward-chaining version of GP.|Riccardo Poli,William B. Langdon","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57645|GECCO|2006|Relaxed genetic programming|A study on the performance of solutions generated by Genetic Programming (GP) when the training set is relaxed (in order to allow for a wider definition of the desired solution) is presented. This performance is assessed through  important features of a solution its generalization error and its bloat, a common problem of GP individuals. We show how a small degree of relaxation improves the generalization error of the best solutions we also show how the variation of this parameter affects the bloat of the solutions generated.|Luis E. Da Costa,Jacques-Andr√© Landry","57637|GECCO|2006|Genetic programming for agricultural purposes|Nitrogen is one of the most important chemical intakes to ensure the healthy growth of agricultural crops. However, some environmental concerns emerge (soil and water pollution) when a farmer applies nitrogen in excess. In this study, we propose a new method called GP-SVI to search for the best descriptive model of nitrogen content in a cornfield (Zea mays), thanks to airborne hyperspectral data and ground truth nitrogen measurements. Coupling the output of this descriptive model with variable-rate technologies (VRT) would allow farmers to practice site-specific management ensuring them economical savings and ecological benefits. GP-SVI is a parallel search of the best spectral vegetation index (SVI) describing a crop biophysical variable, derived from Genetic Programming (GP). Compared to statistical regression methods on our datasets, GP-SVI improves results obtained with classical approaches, in term of explained-variance and generalization error. We also show that the spectral bands selected by GP-SVI match those selected by Partial Least Square regression optimized by Genetic Algorithms (GA-PLS) as proposed by Leardi in \"Application of genetic algorithm-PLS for feature extraction in spectral data sets\", in Journal of Chemometrics.|Cl√©ment Chion,Luis E. Da Costa,Jacques-Andr√© Landry"],["16251|IJCAI|2005|Automated Composition of Web Services by Planning at the Knowledge Level|In this paper, we address the problem of the automated composition of web services by planning on their \"knowledge level\" models. We start from descriptions of web services in standard process modeling and execution languages, like BPELWS, and automatically translate them into a planning domain that models the interactions among services at the knowledge level. This allows us to avoid the explosion of the search space due to the usually large and possibly infinite ranges of data values that are exchanged among services, and thus to scale up the applicability of state-of-the-art techniques for the automated composition of web services. We present the theoretical framework, implement it, and provide an experimental evaluation that shows the practical advantage of our approach w.r.t. techniques that are not based on a knowledgelevel representation.|Marco Pistore,Annapaola Marconi,Piergiorgio Bertoli,Paolo Traverso","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","16121|IJCAI|2005|Image Retrieval and Disambiguation for Encyclopedic Web Search|To produce multimedia encyclopedic content, we propose a method to search the Web for images associated with a specific word sense. We use text in an HTML file which links to an image as a pseudocaption for the image and perform text-based indexing and retrieval. We use term descriptions in a Web search site called \"CYCLONE\" as queries and correspond images and texts based on word senses.|Atsushi Fujii,Tetsuya Ishikawa","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65498|AAAI|2005|Searching for Common Sense Populating Cyc from the Web|The Cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information - what is known informally as \"common sense.\" Over the last twenty years, a sufficient core of common sense knowledge has been entered into Cyc to allow it to begin effectively and flexibly supporting its most important task increasing its own store of world knowledge. In this paper, we present initial work on a method of using a combination of Cyc and the World Wide Web, accessed via Google, to assist in entering knowledge into Cyc. The long-term goal is automating the process of building a consistent, formalized representation of the world in the Cyc knowledge base via machine learning. We present preliminary results of this work and describe how we expect the knowledge acquisition process to become more accurate, faster, and more automated in the future.|Cynthia Matuszek,Michael J. Witbrock,Robert C. Kahlert,John Cabral,David Schneider,Purvesh Shah,Douglas B. Lenat","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","80702|VLDB|2006|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data \"chunks\" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.|Utkarsh Srivastava,Kamesh Munagala,Jennifer Widom,Rajeev Motwani"],["16121|IJCAI|2005|Image Retrieval and Disambiguation for Encyclopedic Web Search|To produce multimedia encyclopedic content, we propose a method to search the Web for images associated with a specific word sense. We use text in an HTML file which links to an image as a pseudocaption for the image and perform text-based indexing and retrieval. We use term descriptions in a Web search site called \"CYCLONE\" as queries and correspond images and texts based on word senses.|Atsushi Fujii,Tetsuya Ishikawa","80612|VLDB|2006|SIREN A Similarity Retrieval Engine for Complex Data|This paper presents a similarity retrieval engine - SIREN-that allows posing similarity queries in a relational DBMS using an extended syntax that adds the support for such type of queries in the SQL language. It discusses the main architecture of SIREN, describes some key features and provides a description of the demo.|Maria Camila Nardini Barioni,Humberto Luiz Razente,Agma J. M. Traina,Caetano Traina Jr.","65623|AAAI|2006|Building Semantic Mappings from Databases to Ontologies|A recent special issue of AI Magazine (AAAI ) was dedicated to the topic of semantic integration -- the problem of sharing data across disparate sources. At the core of the solution lies the discovery the \"semantics\" of different data sources. Ideally, the semantics of data are captured by a formal ontology of the domain together with a semantic mapping connecting the schema describing the data to the ontology. However, establishing the semantic mapping from a database schema to a formal ontology in terms of formal logic expressions is inherently difficult to automate, so the task was left to humans. In this paper, we report on our study (An, Borgida, & Mylopoulos a b) of a semi-automatic tool, called MAPONTO, that assists users to discover plausible semantic relationships between a database schema (relational or XML) and an ontology, expressing them as logical formulasrules.|Yuan An,John Mylopoulos,Alexander Borgida","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","80637|VLDB|2006|HISA A Query System Bridging The Semantic Gap For Large Image Databases|We propose a novel system called HISA for organizing very large image databases. HISA implements the first known data structure to capture both the ontological knowledge and visual features for effective and effcient retrieval of images by either keywords, image examples, or both. HISA employs automatic image annotation technique, ontology analysis and statistical analysis of domain knowledge to precompute the data structure. Using these techniques, HISA is able to bridge the gap between the image semantics and the visual features, therefore providing more user-friendly and high-performance queries. We demonstrate the novel data structure employed by HISA, the query algorithms, and the pre-computation process.|Gang Chen,Xiaoyan Li,Lidan Shou,Jinxiang Dong,Chun Chen","80721|VLDB|2006|Automatic Extraction of Dynamic Record Sections From Search Engine Result Pages|A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features () it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and () it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.|Hongkun Zhao,Weiyi Meng,Clement T. Yu","16244|IJCAI|2005|Building the Semantic Web Tower from RDF Straw|A same-syntax extension of RDF to first-order logic results in a collapse of the model theory due to logical paradoxes resulting from diagonalization. RDF is thus the wrong material for building the Semantic Web tower.|Peter F. Patel-Schneider","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","16095|IJCAI|2005|Inferring Image Templates from Classification Decisions|Assuming human image classification decisions are based on estimating the degree of match between a small number of stored internal templates and certain regions of the input images, we present an algorithm which infers observers classification templates from their classification decisions on a set of test images. The problem is formulated as learning prototypes from labeled data under an adjustable, prototype-specific elliptical metric. The matrix of the elliptical metric indicates the pixels that the template responds to. The model was applied to human psychophysical data collected in a simple image classification experiment.|Arnab Dhua,Florin Cutzu"],["57635|GECCO|2006|Adaptive discretization for probabilistic model building genetic algorithms|This paper proposes an adaptive discretization method, called Split-on-Demand (SoD), to enable the probabilistic model building genetic algorithm (PMBGA) to solve optimization problems in the continuous domain. The procedure, effect, and usage of SoD are described in detail. As an example, the integration of SoD and the extended compact genetic algorithm (ECGA), named real-coded ECGA (rECGA), is presented and numerically examined. The experimental results indicate that rECGA works well and SoD is effective. The behavior of SoD is analyzed and discussed, followed by the potential future work for SoD.|Chao-Hong Chen,Wei-Nan Liu,Ying-Ping Chen","57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","65706|AAAI|2006|A Two-Step Hierarchical Algorithm for Model-Based Diagnosis|For many large systems the computational complexity of complete model-based diagnosis is prohibitive. In this paper we investigate the speedup of the diagnosis process by exploiting the hierarchylocality as is typically present in well-engineered systems. The approach comprises a compile-time and a run-time step. In the first step, a hierarchical CNF representation of the system is compiled to hierarchical DNF of adjustable hierarchical depth. In the second step, the diagnoses are computed from the hierarchical DNF and the actual observations. Our hierarchical algorithm, while sound and complete, allows large models to be diagnosed, where compiletime investment directly translates to run-time speedup. The benefits of our approach are illustrated by using weak-fault models of real-world systems, including the ISCAS- combinatorial circuits. Even for these non-optimally partitioned problems the speedup compared to traditional approaches ranges in the hundreds.|Alexander Feldman,Arjan J. C. van Gemund","16100|IJCAI|2005|A Probabilistic Model of Redundancy in Information Extraction|Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without using hand-tagged training examples. A fundamental problem for both UIE and supervised IE is assessing the probability that extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness This paper introduces a combinatorial \"balls-andurns\" model that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating the model's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are  times better, on average, than those obtained by Pointwise Mutual Information (PMI) and the noisy-or model used in previous work. For supervised IE, the model's performance is comparable to that of Support Vector Machines, and Logistic Regression.|Doug Downey,Oren Etzioni,Stephen Soderland","65455|AAAI|2005|A Variational Learning Algorithm for the Abstract Hidden Markov Model|We present a fast algorithm for learning the parameters of the abstract hidden Markov model, a type of hierarchical activity recognition model. Learning using exact inference scales poorly as the number of levels in the hierarchy increases therefore, an approximation is required for large models. We demonstrate that variational inference is well suited to solve this problem. Not only does this technique scale. but it also offers a natural way to leverage the context specific independence properties inherent in the model via the fixed point equations. Experiments confirm that the variational approximation significantly reduces the time necessary for learning while estimating parameter values that can be used to make reliable predictions.|Jeffrey Johns,Sridhar Mahadevan","57431|GECCO|2005|Combining competent crossover and mutation operators a probabilistic model building approach|This paper presents an approach to combine competent crossover and mutation operators via probabilistic model building. Both operators are based on the probabilistic model building procedure of the extended compact genetic algorithm (eCGA). The model sampling procedure of eCGA, which mimics the behavior of an idealized recombination---where the building blocks (BBs) are exchanged without disruption---is used as the competent crossover operator. On the other hand, a recently proposed BB-wise mutation operator---which uses the BB partition information to perform local search in the BB space---is used as the competent mutation operator. The resulting algorithm, called hybrid extended compact genetic algorithm (heCGA), makes use of the problem decomposition information for () effective recombination of BBs and () effective local search in the BB neighborhood. The proposed approach is tested on different problems that combine the core of three well known problem difficulty dimensions deception, scaling, and noise. The results show that, in the absence of domain knowledge, the hybrid approach is more robust than either single-operator-based approach.|Cl√°udio F. Lima,Kumara Sastry,David E. Goldberg,Fernando G. Lobo","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons","57299|GECCO|2005|An efficient genetic algorithm for predicting protein tertiary structures in the D HP model|Given the amino acid sequence of a protein, predicting its tertiary structure is known as the protein folding problem. This problem has been widely studied under the HP model in which each amino acid is classified, based on its hydrophobicity, as an H (hydrophobic or non-polar) or a P (hydrophilic or polar). Conformation of a protein in the HP model is embedded as a self-avoiding walk in either a two-dimensional or a three-dimensional lattice. The protein folding problem in the HP model is to find a lowest energy conformation. This problem is known to be NP-hard in both two-dimensional and three-dimensional square lattices. In this paper, we present an efficient genetic algorithm for the protein folding problem under the HP model in the two-dimensional square lattice. A special feature of this algorithm is its usage of secondary structures, that the algorithm evolves, as building blocks for the conformation. Experimental results on benchmark sequences show that the algorithm performs very well against existing evolutionary algorithms and Monte Carlo algorithms.|Thang Nguyen Bui,Gnanasekaran Sundarraj","65558|AAAI|2005|Performing Bayesian Inference by Weighted Model Counting|Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman ). Solving such NP-complete tasks by \"compilation to SAT\" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. ) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether \"compilation to model-counting\" could be a practical technique for solving real-world P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.|Tian Sang,Paul Beame,Henry A. Kautz","57277|GECCO|2005|Adaptive isolation model using data clustering for multimodal function optimization|In this paper, we propose a GA model called Adaptive Isolation Model(AIM), for multimodal optimization. It uses a data clustering algorithm to detect clusters in GA population, which identifies the attractors in the fitness landscape. Then, subpopulations which makes-up the clusters are isolated and optimized independently. Meanwhile, the region of the isolated subpopulations in the original landscape are suppressed. The isolation increases comprehensiveness, i.e., the probability of finding weaker attractors, and the overall efficiency of multimodal search. The advantage of the AIM is that it does not require distance between the optima as a presumed parameter, as it is estimated from the variancecovariance matrix of the subpopulation.Further, AIM's behavior and efficiency is equivalent to basic GA in unimodal landscape, in terms of number of evaluation. Therefore, it is applied recursively to all subpopulations until they converge to a suboptima. This makes AIM suitable for locally-multimodal landscapes, which have closely located attractors that are difficult to distinguish in the initial run.The performance of AIM is evaluated in several benchmark problems and compared to iterated hill-climbing methods.|Shin Ando,Jun Sakuma,Shigenobu Kobayashi"],["57295|GECCO|2005|Function choice resiliency and growth in genetic programming|In this paper we examine how the choice of functions in a genetic program (GP) affects the rate of code growth and the development of resilient individuals. We find that functions or combination of functions that produce the most resilient individuals often, but not always, produce the most growth. Finally, the results confirm that there is a high correlation between the resiliency of parent individuals and the fitness of their offspring.|Sireesha Besetti,Terence Soule","65512|AAAI|2005|Geometric Variance Reduction in Markov Chains Application to Value Function and Gradient Estimation|We study a sequential variance reduction technique for Monte Carlo estimation of functionals in Markov Chains. The method is based on designing sequential control variates using successive approximations of the function of interest V. Regular Monte Carlo estimates have a variance of O(N), where N is the number of samples. Here, we obtain a geometric variance reduction O(N) (with   ) up to a threshold that depends on the approximation error V - AV, where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. AV  V), the variance decreases geometrically to zero. An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in policy iteration for Markov Decision Processes. Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity , V of the performance measure V with respect to some parameter  of the transition probabilities. For example, in parametric optimization of the policy, an estimate of the policy gradient is required to perform a gradient optimization method. We show that, using two approximations, the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations.|R√©mi Munos","65922|AAAI|2006|Sample-Efficient Evolutionary Function Approximation for Reinforcement Learning|Reinforcement learning problems are commonly tackled with temporal difference methods, which attempt to estimate the agent's optimal value function. In most real-world problems, learning this value function requires a function approximator, which maps state-action pairs to values via a concise, parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. A recently developed approach called evolutionary function approximation uses evolutionary computation to automate the search for effective representations. While this approach can substantially improve the performance of TD methods, it requires many sample episodes to do so. We present an enhancement to evolutionary function approximation that makes it much more sample-efficient by exploiting the off-policy nature of certain TD methods. Empirical results in a server job scheduling domain demonstrate that the enhanced method can learn better policies than evolution or TD methods alone and can do so in many fewer episodes than standard evolutionary function approximation.|Shimon Whiteson,Peter Stone","65799|AAAI|2006|Value-Function-Based Transfer for Reinforcement Learning Using Structure Mapping|Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.|Yaxin Liu,Peter Stone","80664|VLDB|2006|A Linear Time Algorithm for Optimal Tree Sibling Partitioning and Approximation Algorithms in Natix|Document insertion into a native XML Data Store (XDS) requires to partition the document tree into a number of storage units with limited capacity, such as records on disk pages. As intra partition navigation is much faster than navigation between partitions, minimizing the number of partitions has a beneficial effect on query performance.We present a linear time algorithm to optimally partition an ordered, labeled, weighted tree such that each partition does not exceed a fixed weight limit. Whereas traditionally tree partitioning algorithms only allow child nodes to share a partition with their parent node (i.e. a partition corresponds to a subtree), our algorithm also considers partitions containing several subtrees as long as their roots are adjacent siblings. We call this sibling partitioning.Based on our study of the optimal algorithm, we further introduce two novel, near-optimal heuristics. They are easier to implement, do not need to hold the whole document instance in memory, and require much less runtime than the optimal algorithm.Finally, we provide an experimental study comparing our novel and existing algorithms. One important finding is that compared to partitioning that exclusively considers parent-child partitions, including sibling partitioning as well can decrease the total number of partitions by more than %, and improve query performance by more than a factor of two.|Carl-Christian Kanne,Guido Moerkotte","57640|GECCO|2006|Non-wrapping order crossover an order preserving crossover operator that respects absolute position|In this paper, we introduce a new crossover operator for the permutation representation of a GA. This new operator---Non-Wrapping Order Crossover (NWOX)---is a variation of the well-known Order Crossover (OX) operator. It strongly preserves relative order, as does the original OX, but also respects the absolute positions within the parent permutations. This crossover operator is experimentally compared to several other permutation crossover operators on an NP-Hard problem known as weighted tardiness scheduling with sequence-dependent setups. A GA using this NWOX operator finds new best known solutions for several benchmark problem instances and proves to be superior to the previous best performing metaheuristic for the problem.|Vincent A. Cicirello","57594|GECCO|2006|Smart crossover operator with multiple parents for a Pittsburgh learning classifier system|This paper proposes a new smart crossover operator for a Pittsburgh Learning Classifier System. This operator, unlike other recent LCS approaches of smart recombination, does not learn the structure of the domain, but it merges the rules of N parents (N  ) to generate a new offspring. This merge process uses an heuristic that selects the minimum subset of candidate rules that obtains maximum training accuracy. Moreover the operator also includes a rule pruning scheme to avoid the inclusion of over-specific rules, and to guarantee as much as possible the robust behaviour of the LCS. This operator takes advantage from the fact that each individual in a Pittsburgh LCS is a complete solution, and the system has a global view of the solution space that the proposed rule selection algorithm exploits. We have empirically evaluated this operator using a recent LCS called GAssist. First with the standard LCS benchmark, the  bits multiplexer, and later using  standard real datasets. The results of the experiments over these datasets indicate that the new operator manages to increase the accuracy of the system over the classical crossover in  of the  datasets, and never having a significantly worse performance than the classic operator.|Jaume Bacardit,Natalio Krasnogor","80688|VLDB|2006|Indexing for Function Approximation|Simulation is one of the most powerful tools that scientists have at their disposal for studying and understanding real-world physical phenomena. In order to be realistic, the mathematical models which drive simulations are often very complex and run for a very large number of simulation steps. The required computational resources often make it infeasible to evaluate simulation models exactly at each step, and thus scientists trade accuracy for reduced simulation cost.In this paper, we explore function approximation for a combustion simulation. In particular, we model high-dimensional function approximation (HFA) as a storage and retrieval problem, and we show that HFA defines a novel class of applications for high dimensional index structures. The interesting property of HFA is that it imposes a mixed queryupdate workload on the index which leads to novel tradeoffs between the efficiency of search versus updates. We investigate in detail one specific approach to HFA based on Taylor Series expansions and we analyze tradeoffs in index structure design through a thorough experimental study.|Biswanath Panda,Mirek Riedewald,Stephen B. Pope,Johannes Gehrke,L. Paul Chew","57741|GECCO|2006|A crossover operator for the anonymity problem|Recent dissemination of personal data has created an important optimization problem what is the minimal transformation of a dataset that is needed to guarantee the anonymity of the underlying individuals One natural representation for this problem is a bit-string, which makes a genetic algorithm a logical choice for optimization. Unfortunately, under certain realistic conditions, not all bit combinations will represent valid solutions. This means that in many instances, useful solutions are sparse in the search space. We implement a new crossover operator that preserves valid solutions under this representation. Our results show that this reproductive strategy is more efficient, effective, and robust than previous work. We also investigate how the population size and uniqueness can affect the performance of genetic search on this application.|Monte Lunacek,Darrell Whitley,Indrakshi Ray","65499|AAAI|2005|Samuel Meets Amarel Automating Value Function Approximation Using Global State Space Analysis|Most work on value function approximation adheres to Samuel's original design agents learn a task-specific value function using parameter estimation, where the approximation architecture (e.g, polynomials) is specified by a human designer. This paper proposes a novel framework generalizing Samuel's paradigm using a coordinate-free approach to value function approximation. Agents learn both representations and value functions by constructing geometrically customized task-independent basis functions that form an orthonormal set for the Hilbert space of smooth functions on the underlying state space manifold. The approach rests on a technical result showing that the space of smooth functions on a (compact) Riemanian manifold has a discrete spectrum associated with the Laplace-Beltrami operator. In the discrete setting, spectral analysis of the graph Laplacian yields a set of geometrically customized basis functions for approximating and decomposing value functions. The proposed framework generalizes Samuel's value function approximation paradigm by combining it with a formalization of Saul Amarel's paradigm of representation learning through global state space analysis.|Sridhar Mahadevan"],["16030|IJCAI|2005|Language Learning in Multi-Agent Systems|We present the problem of learning to communicate in decentralized and stochastic environments, analyzing it formally in a decision-theoretic context and illustrating the concept experimentally. Our approach allows agents to converge upon coordinated communication and action over time.|Martin Allen,Claudia V. Goldman,Shlomo Zilberstein","16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins","16233|IJCAI|2005|A Machine Learning Approach to Identification and Resolution of One-Anaphora|We present a machine learning approach to identifying and resolving one-anaphora. In this approach, the system first learns to distinguish different uses of instances of the word one in the second stage, the antecedents of those instances of one that are classified as anaphoric are then determined. We evaluated our approach on written texts drawn from the informative domains of the British National Corpus (BNC), and achieved encouraging results. To our knowledge, this is the first learning-based system for the identification and resolution of one-anaphora.|Hwee Tou Ng,Yu Zhou,Robert Dale,Mary Gardiner","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","65765|AAAI|2006|Detecting Spam Blogs A Machine Learning Approach|Weblogs or blogs are an important new way to publish information, engage in discussions, and form communities on the Internet. The Blogosphere has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from splogs - false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features can be used to detect splogs. We present an evaluation of learned models and their utility to blog search engines systems that employ techniques differing from those of conventional web search engines.|Pranam Kolari,Akshay Java,Tim Finin,Tim Oates,Anupam Joshi","16138|IJCAI|2005|fMRI Analysis via One-class Machine Learning Techniques|We show how one-class compression Neural Networks and one-class SVM can be applied to fMRI data to learn the classification of brain activity associated with a specific motor activity. For comparison purposes, we use two labeled data and see what degree of classification ability is lost compared with the usual two-class SVM.|David R. Hardoon,Larry M. Manevitz","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","65488|AAAI|2005|A Learning-Based Term-Weighting Approach for Information Retrieval|One of the core components in information retrieval (IR) is the document-term-weighting scheme. In this paper, we will propose a novel learning-based term-weighting approach to improve the retrieval performance of vector space model in homogeneous collections. We first introduce a simple learning system to weighting the index terms of documents. Then, we deduce a formal computational approach according to some theories of matrix computation and statistical inference. Our experiments on  collections will show that our approach out-performs classic tfidf weighting, about %-%.|Guangcan Liu,Yong Yu,Xing Zhu","65356|AAAI|2005|Leveraging Language into Learning|I hypothesize that learning a vocabulary to communicate between components of a system is equivalent to general learning. Moreover, I assert that some problems of general learning, such as eliminating bad hypotheses, deepening shallow representations, and generation of near-misses, will become simpler when refactored into communication learning problems.|Jacob Beal","65928|AAAI|2006|Robust Support Vector Machine Training via Convex Outlier Ablation|One of the well known risks of large margin training methods, such as boosting and support vector machines (SVMs), is their sensitivity to outliers. These risks are normally mitigated by using a soft margin criterion, such as hinge loss, to reduce outlier sensitivity. In this paper, we present a more direct approach that explicitly incorporates outlier suppression in the training process. In particular, we show how outlier detection can be encoded in the large margin training principle of support vector machines. By expressing a convex relaxation of the joint training problem as a semide finite program, one can use this approach to robustly train a support vector machine while suppressing outliers. We demonstrate that our approach can yield superior results to the standard soft margin approach in the presence of outliers.|Linli Xu,Koby Crammer,Dale Schuurmans"],["57304|GECCO|2005|Estimating the detector coverage in a negative selection algorithm|This paper proposes a statistical mechanism to analyze the detector coverage in a negative selection algorithm, namely a quantitative measurement of a detector set's capability to detect nonself data. This novel method has the advantage of statistical confidence in the estimation of the actual coverage. Furthermore, unlike the existing analysis works of negative selection, it doesn't depend on specific detector representation and generation algorithm. Not only can it be implemented as a procedure independent from the steps to generate detectors, the experiments in this paper showed that it can also be tightly integrated into the detector generation algorithm to control the number of detectors.|Zhou Ji,Dipankar Dasgupta","57284|GECCO|2005|A model based on ant colony system and rough set theory to feature selection|In this paper we propose a hybrid approach to feature selection based on Ant Colony System algorithm and Rough Set Theory. Rough Set Theory offers the heuristic function to measure the quality of a single subset. We have studied the influence of the setting of the parameters for this problem, in particular for finding reducts. Experimental results show this hybrid approach is a promising method for features selection.|Rafael Bello,Ann Now√©,Yaile Caballero,Yudel G√≥mez,Peter Vrancx","57619|GECCO|2006|Applicability issues of the real-valued negative selection algorithms|The paper examines various applicability issues of the negative selection algorithms (NSA). Recently, concerns were raised on the use of NSAs, especially those using real-valued representation. In this paper, we argued that many reported issues are either due to improper usage of the method or general difficulties which are not specific to negative selection algorithms. On the contrary, the experiments with synthetic data and well-known real-world data show that NSAs have great flexibility to balance between efficiency and robustness, and to accommodate domain-oriented elements in the method, e.g. various distance measures. It is to be noted that all methods are not suitable for all datasets and data representation plays a major role.|Zhou Ji,Dipankar Dasgupta","57375|GECCO|2005|Applying both positive and negative selection to supervised learning for anomaly detection|This paper presents a novel approach of applying both positive selection and negative selection to supervised learning for anomaly detection. It first learns the patterns of the normal class via co-evolutionary genetic algorithm, which is inspired from the positive selection, and then generates synthetic samples of the anomaly class, which is based on the negative selection in the immune system. Two algorithms about synthetic generation of the anomaly class are proposed. One deals with data sets containing a few anomalous samples while the other deals with data sets containing no anomalous samples at all. The experimental results on some benchmark data sets from UCI data set repertory show that the detection rate is improved evidently, accompanied by a slight increase in false alarm rate via introducing novel synthetic samples of the anomaly class. The advantages of our method are the increased ability of classifiers in identifying both previously known and innovative anomalies, and the maximal degradation of overfitting phenomenon.|Xiaoshu Hang,Honghua Dai","57360|GECCO|2005|A statistical learning theory approach of bloat|Code bloat, the excessive increase of code size, is an important issue in Genetic Programming (GP). This paper proposes a theoretical analysis of code bloat in the framework of symbolic regression in GP, from the viewpoint of Statistical Learning Theory, a well grounded mathematical toolbox for Machine Learning. Two kinds of bloat must be distinguished in that context, depending whether the target function lies in the search space or not. Then, important mathematical results are proved using classical results from Statistical Learning. Namely, the Vapnik-Chervonenkis dimension of programs is computed, and further results from Statistical Learning allow to prove that a parsimonious fitness ensures Universal Consistency (the solution minimizing the empirical error does converge to the best possible error when the number of examples goes to infinity). However, it is proved that the standard method consisting in choosing a maximal program size depending on the number of examples might still result in programs of infinitely increasing size with their accuracy a more complicated modification of the fitness is proposed that theoretically avoids unnecessary bloat while nevertheless preserving the Universal Consistency.|Sylvain Gelly,Olivier Teytaud,Nicolas Bredeche,Marc Schoenauer","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","57752|GECCO|2006|Information preserving multi-objective feature selection for unsupervised learning|In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inherently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets.|Ingo Mierswa,Michael Wurst","57541|GECCO|2005|Is negative selection appropriate for anomaly detection|Negative selection algorithms for hamming and real-valued shape-spaces are reviewed. Problems are identified with the use of these shape-spaces, and the negative selection algorithm in general, when applied to anomaly detection. A straightforward self detector classification principle is proposed and its classification performance is compared to a real-valued negative selection algorithm and to a one-class support vector machine. Earlier work suggests that real-value negative selection requires a single class to learn from. The investigations presented in this paper reveal, however, that when applied to anomaly detection, the real-valued negative selection and self detector classification techniques require positive and negative examples to achieve a high classification accuracy. Whereas, one-class SVMs only require examples from a single class.|Thomas Stibor,Philipp H. Mohr,Jonathan Timmis,Claudia Eckert","57567|GECCO|2005|A genetic algorithm approach to the selection of near-optimal subsets from large sets|The problem attempted in this paper is to select a sample from a large set where the sample is required to have a particular average property. The problem can be expressed as an optimisation problem where one selects a subset of r objects from a group of n objects and the objective function is the mismatch between the required average property and that of a proposed sample. We test our method on a real-life problem which arises when we model the assets of a life insurance company in order to understand its risk, solvency andor capital requirements.In this paper we describe a genetic algorithm developed to solve the generic selection task. We demonstrate the algorithm successfully solving our test problem.|P. Whiting,P. W. Poon,J. N. Carter","57625|GECCO|2006|Ensemble selection for evolutionary learning using information theory and prices theorem|This paper presents an information theoretic perspective on design and analysis of evolutionary algorithms. Indicators of solution quality are developed and applied not only to individuals but also to ensembles, thereby ensuring information diversity. Price's Theorem is extended to show how joint indicators can drive reproductive sampling rate of potential parental pairings. Heritability of mutual information is identified as a key issue.|Stuart W. Card,Chilukuri K. Mohan"]]}}