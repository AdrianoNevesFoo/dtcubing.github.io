{"abstract":{"entropy":6.613004611198867,"topics":["data base, data, data systems, data management, database systems, management systems, database, data stream, search engine, large data, relational database, query, xml documents, relational data, query processing, web, xml, base systems, web pages, query optimization","artificial intelligence, natural language, play role, present approach, knowledge base, data model, description logic, logic, belief revision, logic programming, present based, reasoning, conceptual data, present framework, preferences voting, nonmonotonic logic, case-based reasoning, knowledge, situations calculus, argumentation arguments","video coding, sensor network, bayesian network, agents, real world, mobile robots, multi-agent systems, agents coalitions, intelligent systems, wide range, agents environment, network, describe systems, social network, environment, active information, distributed systems, autonomous robots, computer vision, scheme video","machine learning, markov decision, recent years, consider problem, problem, partially observable, learning, present algorithm, markov processes, decision processes, reinforcement learning, solving problem, virtual hashing, algorithm, decision making, search algorithm, solutions problem, constraint problem, planning, planning domains","data base, data systems, data management, data, data stream, management systems, large data, relational data, base systems, problem data, data model, database data, base management, processing data, applications data, stream applications, stream processing, stream systems, relational base, distributed data","query data, xml data, query, query processing, database query, xml documents, widely used, query optimization, systems query, query language, queries data, xml, efficient xml, xml language, queries, queries xml, relational query, queries database, user query, plan query","description logic, logic programming, case-based reasoning, nonmonotonic logic, logic approach, reasoning approach, reasoning, situations calculus, lower bound, logic, logic programs, reasoning systems, programming asp, logic knowledge, modal logic, reasoning knowledge, max-sat solvers, reasoning logic, first-order logic, present programs","natural language, present approach, data model, knowledge base, present model, model, systems model, present, present systems, presented model, normal form, approach, paper approach, functional dependencies, approach data, knowledge, present novel, present data, relational model, mutual dependency","real world, computer games, mobile robots, multi-agent systems, moving objects, artificial intelligence, computer systems, systems complex, task complex, objects image, computer vision, computer, systems developed, systems engineering, human computer, problem environment, virtual environment, information computer, systems environment, state systems","video coding, activity recognition, scheme video, diagnosis systems, objects recognition, paper video, scalable coding, systems resource, task, paper novel, video provides, group agents, problem task, coding efficiency, task classification, present novel, scalable video, novel video, video, problem activity","problem given, planning domains, heuristic search, planning, planning problem, discriminant analysis, heuristic planning, optimal solutions, planning actions, planning systems, actions effects, algorithm optimal, uncertainty actions, values parameters, approach planning, problem optimal, optimal search, planning uncertainty, optimal planning, present planning","recent years, recent work, recent research, constraint satisfaction, recent, past decade, recent shown, nearest neighbor, recent planning, satisfiability sat, recent interest, recent problem, causal effects, variables constraint, problem csp, quantified boolean, constraint values, deals problem, years research, research community"],"ranking":[["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80772|VLDB|2007|Unifying Data and Domain Knowledge Using Virtual Views|The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB  PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.|Lipyeow Lim,Haixun Wang,Min Wang","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80166|VLDB|2002|Optimizing View Queries in ROLEX to Support Navigable Result Trees|An increasing number of applications use XML data published from relational databases. For speed and convenience, such applications routinely cache this XML data locally and access it through standard navigational interfaces such as DOM, sacrificing the consistency and integrity guarantees provided by a DBMS for speed. The ROLEX system is being built to extend the capabilities of relational database systems to deliver fast, consistent and navigable XML views of relational data to an application via a virtual DOM interface. This interface translates navigation operations on a DOM tree into execution-plan actions, allowing a spectrum of possibilities for lazy materialization. The ROLEX query optimizer uses a characterization of the navigation behavior of an application, and optimizes view queries to minimize the expected cost of that navigation. This paper presents the architecture of ROLEX, including its model of query execution and the query optimizer. We demonstrate with a performance study the advantages of the ROLEX approach and the importance of optimizing query execution for navigation.|Philip Bohannon,Sumit Ganguly,Henry F. Korth,P. P. S. Narayan,Pradeep Shenoy","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","65774|AAAI|2006|Towards an Axiom System for Default Logic|Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning.|Gerhard Lakemeyer,Hector J. Levesque","66430|AAAI|2008|Argument Theory Change Applied to Defeasible Logic Programming|In this article we work on certain aspects of the belief change theory in order to make them suitable for argumentation systems. This approach is based on Defeasible Logic Programming as the argumentation formalism from which we ground the definitions. The objective of our proposal is to define an argument revision operator that inserts a new argument into a defeasible logic program in such a way that this argument ends up undefeated after the revision, thus warranting its conclusion. In order to ensure this warrant, the defeasible logic program has to be changed in concordance with a minimal change principle. Finally, we present an algorithm that implements the argument revision operation.|Martín O. Moguillansky,Nicolás D. Rotstein,Marcelo A. Falappa,Alejandro Javier García,Guillermo Ricardo Simari","65043|AAAI|1987|Forward Chaining Logic Programming with the ATMS|Two powerful reasoning tools have recently appeared, logic programming and assumption-based truth maintenance systems (ATMS). An ATMS offers significant advantages to a problem solver assumptions are easily managed and the search for solutions can be carried out in the most general context first and in any order. Logic programming allows us to program a problem solver declaratively--describe what the problem is, rather than describe how to solve the problem. However, we are currently limited when using an ATMS with our problem solvers, because we are forced to describe the problem in terms of a simple language of forward implications. In this paper we present a logic programming language, called FORLOG, that raises the level of programming the ATMS to that of a powerful logic programming language. FORLOG supports the use of \"logical variables\" and both forward and backward reasoning. FORLOG programs are compiled into a data-flow language (similar to the RETE network) that efficiently implements deKleer's consumer architecture. FORLOG has been implemented in Interlisp-D.|Nicholas S. Flann,Thomas G. Dietterich,Dan R. Corpon","65367|AAAI|2005|Practical First-Order Argumentation|There are many frameworks for modelling argumentation in logic. They include a formal representation of individual arguments and techniques for comparing conflicting arguments. A problem with these proposals is that they do not consider arguments for and against first-order formulae. We present a framework for first-order logic argumentation based on argument trees that provide a way of exhaustively collating arguments and counter-arguments. A difficulty with first-order argumentation is that there may be many arguments and counterarguments even with a relatively small knowledgebase. We propose rationalizing the arguments under consideration with the aim of reducing redundancy and highlighting key points.|Philippe Besnard,Anthony Hunter","66519|AAAI|2008|Terminological Reasoning in SHIQ with Ordered Binary Decision Diagrams|We present a new algorithm for reasoning in the description logic SHIQ, which is the most prominent fragment of the Web Ontology Language OWL. The algorithm is based on ordered binary decision diagrams (OBDDs) as a datastructure for storing and operating on large model representations. We thus draw on the success and the proven scalability of OBDD-based systems. To the best of our knowledge, we present the very first agorithm for using OBDDs for reasoning with general Tboxes.|Sebastian Rudolph,Markus Krötzsch,Pascal Hitzler","66300|AAAI|2008|Nonmonotonic Modes of Inference|In this paper we investigate nonmonotonic 'modes of inference'. Our approach uses modal (conditional) logic to establish a uniform framework in which to study nonmonotonic consequence. We consider a particular mode of inference which employs a majority-based account of default reasoning--one which differs from the more familiar preferential accounts--and show how modal logic supplies a framework which facilitates analysis of, and comparison with more traditional formulations of nonmonotonic consequence.|Victor Jauregui","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski"],["65169|AAAI|1993|Quantitative Modeling of Complex Computational Task Environments|There are many formal approaches to specifying how the mental state of an agent entails that it perform particular actions. These approaches put the agent at the center of analysis. For some questions and purposes, it is more realistic and convenient for the center of analysis to be the task environment, domain, or society of which agents will be a part. This paper presents such a task environment-oriented modeling framework that can work hand-in-hand with more agent-centered approaches. Our approach features careful attention to the quantitative computational interrelationships between tasks, to what information is available (and when) to update an agent''s mental state, and to the general structure of the task environment rather than single-instance examples. This framework avoids the methodological problems of relying solely on single-instance examples, and provides concrete, meaningful characterizations with which to state general theories. Task environment models built in our framework can be used for both analysis and simulation to answer questions about how agents should be organized, or the effect of various coordination algorithms on agent behavior. This paper is organized around an example model of cooperative problem solving in a distributed sensor network.|Keith Decker,Victor R. Lesser","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","66590|AAAI|2008|Agent Organized Networks Redux|Individual robots or agents will often need to form coalitions to accomplish shared tasks, e.g., in sensor networks or markets. Furthermore, in most real systems it is infeasible for entities to interact with all peers. The presence of a social network can alleviate this problem by providing a neighborhood system within which entities interact with a reduced number of peers. Previous research has shown that the topology of the underlying social network has a dramatic effect on the quality of coalitions formed and consequently on system performance (Gaston & deslardins a). It has also been shown that it is feasible to develop agents which dynamically alter connections to improve an organization's ability to form coalitions on the network. However those studies have not analysed the network topologies that result from connectivity adaptation strategies. In this paper the resulting network topologies were analysed and it was found that high performance and rapid convergence were attained because scale free networks were being formed. However it was observed that organizational performance is not impacted by limiting the number of links per agent to the total number of skills available within the population. implying that bandwidth was wasted by previous approaches. We used these observations to inform the design of a token based algorithm that attains higher performance using an order of magnitude less messages for both uniform and non-uniform distributions of skills.|Robin Glinton,Katia P. Sycara,Paul Scerri","65428|AAAI|2005|Agent-Organized Networks for Multi-Agent Production and Exchange|As multi-agent systems grow in size and complexity, social networks that govern the interactions among the agents will directly impact system behavior at the individual and collective levels. Examples of such large-scale, networked multi-agent systems include peer-to-peer networks, distributed information retrieval, and agent-based supply chains. One way of dealing with the uncertain and dynamic nature of such environments is to endow agents with the ability to modify the agent social network by autonomously adapting their local connectivity structure. In this paper, we present a framework for agent-organized networks (AONs) in the context of multi-agent production and exchange, and experimentally evaluate the feasibility and efficiency of specific AON strategies. We find that decentralized network adaptation can significantly improve organizational performance. Additionally, we analyze several properties of the resulting network structures and consider their relationship to the observed increase in organizational performance.|Matthew E. Gaston,Marie desJardins","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","66158|AAAI|2007|Interest-Matching Comparisons using CP-nets|The formation of internet-based social networks has revived research on traditional social network models as well as interest-matching, or match-making, systems. In order to automate or augment the process of interest-matching, we describe a method for the comparison of preference orderings represented by CP-nets, which allows one to determine a shared interest level between agents. Empirical results suggest that this distance measure for preference orderings agrees with the intuitive assessment of shared interest levels.|Andrew W. Wicker,Jon Doyle","65588|AAAI|2005|Stable Service Placement on Dynamic Peer-to-Peer Networks A Heuristic for the Distributed Center Problem|The proliferation of wireless networks has underscored the need for systems capable of coping with sporadic network connectivity. The restriction of communication to neighboring hosts makes determining the global state especially difficult, if not impractical. This paper addresses the problem of coordinating the positions of an arbitrary number of services, encapsulated by mobile agents, in a dynamic peer-to-peer network. The agents' collective goal is to minimize the distance between hosts and services, even if the topology is changing constantly. We propose a distributed algorithm to efficiently calculate the stationary distribution of the network. This can be used as a hill climbing heuristic for agents to find near-optimal locations at which to provide services. Finally, we show that the agent-based hill climbing approach is temporally-stable relative to the instantaneous optimum.|Evan Sultanik,William C. Regli","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","65187|AAAI|2004|Intelligent Systems Demonstration The Secure Wireless Agent Testbed SWAT|We will demonstrate the Secure Wireless Agent Testbed (SWAT), a unique facility developed at Drexel University to study integration, networking and information assurance for next-generation wireless mobile agent systems. SWAT is an implemented system that fully integrates ) mobile agents, ) wireless ad hoc multi-hop networks, and ) security. The demonstration will show the functionality of a number of decentralized agent-based applications, including applications for authentication, collaboration, messaging, and remote sensor monitoring. The demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen nodes (PDAs, tablet PCs, and laptops) and hundreds of mobile software agents.|Gustave Anderson,Andrew Burnheimer,Vincent A. Cicirello,David Dorsey,Saturnino Garcia,Moshe Kam,Joseph Kopena,Kris Malfettone,Andrew Mroczkowski,Gaurav Naik,Maxim Peysakhov,William C. Regli,Joshua Shaffer,Evan Sultanik,Kenneth Tsang,Leonardo Urbano,Kyle Usbeck,Jacob Warren","65269|AAAI|2004|Task Allocation via Self-Organizing Swarm Coalitions in Distributed Mobile Sensor Network|This paper presents a task allocation scheme via self-organizing swarm coalitions for distributed mobile sensor network coverage. Our approach uses the concepts of ant behavior to self-regulate the regional distributions of sensors in proportion to that of the moving targets to be tracked in a non-stationary environment. As a result, the adverse effects of task interference between robots are minimized and sensor network coverage is improved. Quantitative comparisons with other tracking strategies such as static sensor placement, potential fields, and auction-based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental changes.|Kian Hsiang Low,Wee Kheng Leow,Marcelo H. Ang Jr."],["65511|AAAI|2005|Error Bounds for Approximate Value Iteration|Approximate Value Iteration (AVI) is an method for solving a Markov Decision Problem by making successive calls to a supervised learning (SL) algorithm. Sequence of value representations Vn are processed iteratively by Vn+  ATVn where T is the Bellman operator and A an approximation operator. Bounds on the error between the performance of the policies induced by the algorithm and the optimal policy are given as a function of weighted Lp-norms (p  ) of the approximation errors. The results extend usual analysis in L-norm, and allow to relate the performance of AVI to the approximation power (usually expressed in Lp-norm, for p   or ) of the SL algorithm. We illustrate the tightness of these bounds on an optimal replacement problem.|Rémi Munos","66436|AAAI|2008|Symbolic Heuristic Search Value Iteration for Factored POMDPs|We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.|Hyeong Seop Sim,Kee-Eung Kim,Jin Hyung Kim,Du-Seong Chang,Myoung-Wan Koo","65199|AAAI|2004|Stochastic Local Search for POMDP Controllers|The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.|Darius Braziunas,Craig Boutilier","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65240|AAAI|2004|Dynamic Programming for Partially Observable Stochastic Games|We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.|Eric A. Hansen,Daniel S. Bernstein,Shlomo Zilberstein","65864|AAAI|2006|Targeting Specific Distributions of Trajectories in MDPs|We define TTD-MDPs, a novel class of Markov decision processes where the traditional goal of an agent is changed from finding an optimal trajectory through a state space to realizing a specified distribution of trajectories through the space. After motivating this formulation, we show how to convert a traditional MDP into a TTD-MDP. We derive an algorithm for finding non-deterministic policies by constructing a trajectory tree that allows us to compute locally-consistent policies. We specify the necessary conditions for solving the problem exactly and present a heuristic algorithm for constructing policies when an exact answer is impossible or impractical. We present empirical results for our algorithm in two domains a synthetic grid world and stories in an interactive drama or game.|David L. Roberts,Mark J. Nelson,Charles Lee Isbell Jr.,Michael Mateas,Michael L. Littman","66008|AAAI|2007|Computing Optimal Subsets|Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered methods for specifying such preferences based on the attribute values of individual elements within the set. Of these, the approach of (Brafman et al. ) appears to be the most general. In this paper, we consider the problem of computing an optimal subset given such a specification. The problem is shown to be NP-hard in the general case, necessitating heuristic search methods. We consider two algorithm classes for this problem direct set construction, and implicit enumeration as solutions to appropriate CSPs. New algorithms are presented in each class and compared empirically against previous results.|Maxim Binshtok,Ronen I. Brafman,Solomon Eyal Shimony,Ajay Mani,Craig Boutilier","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"],["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","79888|VLDB|1977|Auditing Large Scale Data Bases|. Data bases and data base auditing are briefly defined. . The roles of external auditors, internal auditors, and corporate managers in data base auditing are explained. . The general approach taken by the auditor in his audit of a data base is presented. . Consideration is given to the characteristics of data base systems which have particular implications for data base auditing. . Attention is given to the nature of audit software packages. . Data base management systems are discussed in relation to the problems they present for data base auditing generally and specifically for audit software packages several alternative solutions to these problems are put forward. . The impact of distributed systems and the data base administration function on data base auditing is explained. . Several specific problems are listed that are matters of concern to data base auditors. . In the Summary and Conclusions a program of cooperation between data base professional associations and professional auditor associations is urged with respect to specific mutual concerns.|George M. Scott","79809|VLDB|1975|Hierarchical Performance Analysis Models for Data Base Systems|This paper presents a comprehensive set of hierarchically organized synthetic models developed for the performance evaluation of data base management systems. The first set of algebraic models for data base management system itself contains a program behavior model, a retrieval model, a logical data base model, a physical data base model, and a data processing model. These models are intended to clarify logical and physical data base structures and essential operations in the data processing on them. Another set of models for performance analysis contains a macroscopic program behavior model, a storage model, a processor model, a user behavior model, and an interactive model. In this set this paper is particularly concerned with the first  models. The macroscopic program behavior model is based on the discussion of data locality and allows us to estimate the frequency of data page loading expected on a given application program and a data base. The storage model enables us to estimate the traverse time of data page loading. Finally the processor model allows us to evaluate the data retrieval processing time of data manipulation commands of a given data base structure, a data base management system and a set of application programs under the multiprogramming environment. These models are to be applied to the optimization of the application programs or the data base structure in order to obtain a higher data retrieval performance.|Isao Miyamoto","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","80336|VLDB|2003|Tuple Routing Strategies for Distributed Eddies|Many applications that consist of streams of data are inherently distributed. Since input stream rates and other system parameters such as the amount of available computing resources can fluctuate significantly, a stream query plan must be able to adapt to these changes. Routing tuples between operators of a distributed stream query plan is used in several data stream management systems as an adaptive query optimization technique. The routing policy used can have a significant impact on system performance. In this paper, we use a queuing network to model a distributed stream query plan and define performance metrics for response time and system throughput. We also propose and evaluate several practical routing policies for a distributed stream management system. The performance results of these policies are compared using a discrete event simulator. Finally, we study the impact of the routing policy on system throughput and resource allocation when computing resources can be shared between operators.|Feng Tian,David J. DeWitt","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["80274|VLDB|2003|Temporal Slicing in the Evaluation of XML Queries|As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, XQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a XQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.|Dengfeng Gao,Richard T. Snodgrass","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma \u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80236|VLDB|2002|XMark A Benchmark for XML Data Management|While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.|Albrecht Schmidt 0002,Florian Waas,Martin L. Kersten,Michael J. Carey,Ioana Manolescu,Ralph Busse","80412|VLDB|2004|Schema-based Scheduling of Event Processors and Buffer Minimization for Queries on Structured Data Streams|We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach.|Christoph Koch,Stefanie Scherzinger,Nicole Schweikardt,Bernhard Stegmaier","80166|VLDB|2002|Optimizing View Queries in ROLEX to Support Navigable Result Trees|An increasing number of applications use XML data published from relational databases. For speed and convenience, such applications routinely cache this XML data locally and access it through standard navigational interfaces such as DOM, sacrificing the consistency and integrity guarantees provided by a DBMS for speed. The ROLEX system is being built to extend the capabilities of relational database systems to deliver fast, consistent and navigable XML views of relational data to an application via a virtual DOM interface. This interface translates navigation operations on a DOM tree into execution-plan actions, allowing a spectrum of possibilities for lazy materialization. The ROLEX query optimizer uses a characterization of the navigation behavior of an application, and optimizes view queries to minimize the expected cost of that navigation. This paper presents the architecture of ROLEX, including its model of query execution and the query optimizer. We demonstrate with a performance study the advantages of the ROLEX approach and the importance of optimizing query execution for navigation.|Philip Bohannon,Sumit Ganguly,Henry F. Korth,P. P. S. Narayan,Pradeep Shenoy","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80344|VLDB|2003|Efficient Mining of XML Query Patterns for Caching|As XML becomes ubiquitous, the efficient retrieval of XML data becomes critical. Research to improve query response time has been largely concentrated on indexing paths, and optimizing XML queries. An orthogonal approach is to discover frequent XML query patterns and cache their results to improve the performance of XML management systems. In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.|Liang Huai Yang,Mong-Li Lee,Wynne Hsu","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos","80598|VLDB|2005|Statistical Learning Techniques for Costing XML Queries|Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.|Ning Zhang 0002,Peter J. Haas,Vanja Josifovski,Guy M. Lohman,Chun Zhang"],["65598|AAAI|2005|A Theory of Forgetting in Logic Programming|The study of forgetting for reasoning has attracted considerable attention in AI. However, much of the work on forgetting, and other related approaches such as independence, irrelevance and novelty, has been restricted to the classical logics. This paper describes a detailed theoretical investigation of the notion of forgetting in the context of logic programming. We first provide a semantic definition of forgetting under the answer sets for extended logic programs. We then discuss the desirable properties and some motivating examples. An important result of this study is an algorithm for computing the result of forgetting in a logic program. Furthermore, we present a modified version of the algorithm and show that the time complexity of the new algorithm is polynomial with respect to the size of the given logic program if the size of certain rules is fixed. We show how the proposed theory of forgetting can be used to characterize the logic program updates.|Kewen Wang,Abdul Sattar,Kaile Su","65774|AAAI|2006|Towards an Axiom System for Default Logic|Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning.|Gerhard Lakemeyer,Hector J. Levesque","66397|AAAI|2008|Credulous Resolution for Answer Set Programming|The paper presents a calculus based on resolution for credulous reasoning in Answer Set Programming. The new approach allows a top-down and goal directed resolution, in the same spirit as traditional SLD-resolution. The proposed credulous resolution can be used in query-answering with nonground queries and with non-ground, and possibly infinite, programs. Soundness and completeness results for the resolution procedure are proved for large classes of logic programs. The resolution procedure is also extended to handle some traditional syntactic extensions used in Answer Set Programming, such as choice rules and constraints. The paper also describes an initial implementation of a system for credulous reasoning in Answer Set Programming.|Piero A. Bonatti,Enrico Pontelli,Tran Cao Son","65474|AAAI|2005|Only-Knowing Taking It Beyond Autoepistemic Reasoning|The idea of only-knowing a collection of sentences has been previously shown to have a close connection with autoepistemic logic. Here we propose a more general account of only-knowing that captures not only autoepistemic logic but default logic as well. This allows us not only to study the properties of default logic in terms of an underlying model of belief, but also the relationship among different forms of nonmonotonic reasoning, all within a classical monotonic logic characterized semantically in terms of possible worlds.|Gerhard Lakemeyer,Hector J. Levesque","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","66450|AAAI|2008|What Is Answer Set Programming|Answer set programming (ASP) is a form of declarative programming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.|Vladimir Lifschitz","65043|AAAI|1987|Forward Chaining Logic Programming with the ATMS|Two powerful reasoning tools have recently appeared, logic programming and assumption-based truth maintenance systems (ATMS). An ATMS offers significant advantages to a problem solver assumptions are easily managed and the search for solutions can be carried out in the most general context first and in any order. Logic programming allows us to program a problem solver declaratively--describe what the problem is, rather than describe how to solve the problem. However, we are currently limited when using an ATMS with our problem solvers, because we are forced to describe the problem in terms of a simple language of forward implications. In this paper we present a logic programming language, called FORLOG, that raises the level of programming the ATMS to that of a powerful logic programming language. FORLOG supports the use of \"logical variables\" and both forward and backward reasoning. FORLOG programs are compiled into a data-flow language (similar to the RETE network) that efficiently implements deKleer's consumer architecture. FORLOG has been implemented in Interlisp-D.|Nicholas S. Flann,Thomas G. Dietterich,Dan R. Corpon","65170|AAAI|1993|The Paradoxical Success of Fuzzy Logic|Fuzzy logic methods have been used successfully in many real-world applications, but the foundations of fuzzy logic remain under attack. Taken together, these two facts constitute a paradox. A second paradox is that almost all of the successful fuzzy logic applications are embedded controllers, while most of the theoretical papers on fuzzy methods deal with knowledge representation and reasoning. I hope to resolve these paradoxes by identifying which aspects of fuzzy logic render it useful in practice, and which aspects are inessential. My conclusions are based on a mathematical result, on a survey of literature on the use of fuzzy logic in heuristic control and in expert systems, and on practical experience in developing expert systems.|Charles Elkan","66300|AAAI|2008|Nonmonotonic Modes of Inference|In this paper we investigate nonmonotonic 'modes of inference'. Our approach uses modal (conditional) logic to establish a uniform framework in which to study nonmonotonic consequence. We consider a particular mode of inference which employs a majority-based account of default reasoning--one which differs from the more familiar preferential accounts--and show how modal logic supplies a framework which facilitates analysis of, and comparison with more traditional formulations of nonmonotonic consequence.|Victor Jauregui","66012|AAAI|2007|The Modal Logic SF the Default Logic and the Logic Here-and-There|The modal logic SF provides an account for the default logic of Reiter, and several modal nonmonotonic logics of knowledge and belief. In this paper we focus on a fragment of the logic SF concerned with modal formulas called modal defaults, and on sets of modal defaults -- modal default theories. We present characterizations of SF-expansions of modal default theories, and show that strong and uniform equivalence of modal default theories can be expressed in terms of the logical equivalence in the logic SF. We argue that the logic SF can be viewed as the general default logic of nested defaults. We also study special modal default theories called modal programs, and show that this fragment of the logic SF generalizes the logic here-and-there.|Miroslaw Truszczynski"],["65447|AAAI|2005|On Compiling System Models for Faster and More Scalable Diagnosis|Knowledge compilation is one of the more traditional approaches to model-based diagnosis, where a compiled system model is obtained in an off-line phase, and then used to efficiently answer diagnostic queries on-line. The choice of a suitable representation for the compiled model is critical to the success of this approach, and two of the main proposals have been Decomposable Negation Normal Form (DNNF) and Ordered Binary Decision Diagram (OBDD). The contribution of this paper is twofold. First, we show that in the current state of the art, DNNF dominates OBDD in efficiency and scalability for some typical diagnostic tasks. This result is based on a step-by-step comparison of the complexities of diagnostic algorithms for DNNF and OBDD, together with a known succinctness relation between the two representations. Second, we present a tool for model-based diagnosis, which is based on a state-of-the-art DNNF compiler and our implementations of DNNF diagnostic algorithms. We demonstrate the efficiency of this tool against recent results reported on diagnosis using OBDD.|Jinbo Huang,Adnan Darwiche","65828|AAAI|2006|Intuitive linguistic Joint Object Reference in Human-Robot Interaction Human Spatial Reference Systems and Function-Based Categorization for Symbol Grounding|The visionary goal of an easy to use service robot implies intuitive styles of interaction between humans and robots. Such natural interaction can only be achieved if means are found to bridge the gap between the forms of object perception and spatial knowledge maintained by such robots, and the forms of language, used by humans, to communicate such knowledge. Part of bridging this gap consists of allowing user and robot to establish joint reference on objects in the environment - without forcing the user to use unnatural means for object reference. We present an approach to establishing joint object reference which makes use of natural object classification and a computational model of basic intrinsic and relative reference systems. Our object recognition approach assigns natural categories (e.g. \"desk\", \"chair\", \"table\") to new objects based on their functional design. With basic objects within the environment classified, we can then make use of a computational reference model, to process natural projective relations (e.g. \"the briefcase to the left of the chair\"), allowing users to refer to objects which cannot be classified reliably by the recognition system alone.|Reinhard Moratz","65616|AAAI|2005|Hidden Naive Bayes|The conditional independence assumption of naive Bayes essentially ignores attribute dependencies and is often violated. On the other hand, although a Bayesian network can represent arbitrary attribute dependencies, learning an optimal Bayesian network from data is intractable. The main reason is that learning the optimal structure of a Bayesian network is extremely time consuming. Thus, a Bayesian model without structure learning is desirable. In this paper, we propose a novel model, called hidden naive Bayes (HNB). In an HNB, a hidden parent is created for each attribute which combines the influences from all other attributes. We present an approach to creating hidden parents using the average of weighted one-dependence estimators. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. We propose an algorithm for learning HNB based on conditional mutual information. We experimentally test HNB in terms of classification accuracy, using the  UCI data sets recommended by Weka (Witten & Frank ), and compare it to naive Bayes (Langley, Iba, & Thomas ), C. (Quinlan ), SBC (Langley & Sage ), NBTree (Kohavi ), CL-TAN (Friedman, Geiger, & Goldszmidt ), and AODE (Webb, Boughton, & Wang ). The experimental results show that HNB outperforms naive Bayes, C., SBC, NBTree, and CL-TAN, and is competitive with AODE.|Harry Zhang,Liangxiao Jiang,Jiang Su","79880|VLDB|1977|A Consideration on Normal Form of Not-Necessarily-Normalized Relation in the Relational Data Model|In this paper definitions of unnormalized relation, functional dependency on it and Normal Form are presented. The Normal Form plays a key role in our relational data model in which unnormalized relations are admitted as does the Third Normal Form of Codd in the data model of normalized relation. Properties pertaining to Normal Form are discussed emphasizing comparison with NF along with analysis of some typical examples. Similarity and dissimilarity between new functional dependency and Fagin's multivalued dependency are also discussed and presented in the form of proposition.|Akifumi Makinouchi","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80638|VLDB|2006|iDM A Unified and Versatile Data Model for Personal Dataspace Management|Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML . Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace  of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) , , . This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles","66429|AAAI|2008|Transferring Localization Models across Space|Machine learning approaches to indoor WiFi localization involve an offline phase and an online phase. In the offline phase, data are collected from an environment to build a localization model, which will be applied to new data collected in the online phase for location estimation. However, collecting the labeled data across an entire building would be too time consuming. In this paper, we present a novel approach to transferring the learning model trained on data from one area of a building to another. We learn a mapping function between the signal space and the location space by solving an optimization problem based on manifold learning techniques. A low-dimensional manifold is shared between data collected in different areas in an environment as a bridge to propagate the knowledge across the whole environment. With the help of the transferred knowledge, we can significantly reduce the amount of labeled data which are required for building the localization model. We test the effectiveness of our proposed solution in a real indoor WiFi environment.|Sinno Jialin Pan,Dou Shen,Qiang Yang,James T. Kwok","65761|AAAI|2006|Learning Systems of Concepts with an Infinite Relational Model|Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.|Charles Kemp,Joshua B. Tenenbaum,Thomas L. Griffiths,Takeshi Yamada,Naonori Ueda","66440|AAAI|2008|Cross-lingual Propagation for Morphological Analysis|Multilingual parallel text corpora provide a powerful means for propagating linguistic knowledge across languages. We present a model which jointly learns linguistic structure for each language while inducing links between them. Our model supports fully symmetrical knowledge transfer, utilizing any combination of supervised and unsupervised data across language barriers. The proposed non-parametric Bayesian model effectively combines cross-lingual alignment with target language predictions. This architecture is a potent alternative to projection methods which decompose these decisions into two separate stages. We apply this approach to the task of morphological segmentation, where the goal is to separate a word into its individual morphemes. When tested on a parallel corpus of Hebrew and Arabic, our joint bilingual model effectively incorporates all available evidence from both languages, yielding significant performance gains.|Benjamin Snyder,Regina Barzilay","80120|VLDB|1985|NetBook - a Data Model to Support Knowledge Exploration|Knowledge exploration is the activity of finding out what other people have thought about. Normally, people explore knowledge by reading books or articles or by talking to other people. This paper discusses an alternative approach a system whose knowledge is in the form of text fragments plus a query language to help users access appropriate fragments. Drawing primary inspiration from database theory, hypertext systems, knowledge representation, and a study of textual fragments called fragment theory, the paper describes and motivates a data model to support knowledge exploration.|Dennis Shasha"],["65680|AAAI|2006|Building Explainable Artificial Intelligence Systems|As artificial intelligence (AI) systems and behavior models in military simulations become increasingly complex, it has been difficult for users to understand the activities of computer-controlled entities. Prototype explanation systems have been added to simulators, but designers have not heeded the lessons learned from work in explaining expert system behavior. These new explanation systems are not modular and not portable they are tied to a particular AI system. In this paper, we present a modular and generic architecture for explaining the behavior of simulated entities. We describe its application to the Virtual Humans, a simulation designed to teach soft skills such as negotiation and cultural awareness.|Mark G. Core,H. Chad Lane,Michael van Lent,Dave Gomboc,Steve Solomon,Milton Rosenberg","65504|AAAI|2005|TIELT A Testbed for Gaming Environments|Many AI researchers want to test the utility of their systems in complex task environments defined by (e.g., real-time strategy) gaming simulators andor simulators of computer-generated forces. Also, many developers of commercial and military gaming simulators seek behaviors that can be supported by these systems. However, these integrations require great effort. We will demonstrate the late Alpha version of TIELT, a testbed designed to fill these needs.|Matthew Molineaux,David W. Aha","66662|AAAI|2010|Sequential Incremental-Value Auctions|The rapid growth in mobile and wireless communications entails serious problem of security. Formal methods can be used to help building secure mobile computing environment. Tabular expressions have proved to be useful and practical in formulating precise and complete documentation for computer systems. In this paper a framework for specification and runtime enforcement of security policies is proposed basing on the use of tabular expressions. A security policy can be specified with a tabular expression, and checking whether an application adheres to a given policy can be achieved by evaluating the tabular expression with respect to information intercepted at runtime. The advantages of our approach includes () providing precise and readable specification of security policies () developing a general policy enforcement engine rather than one policy enforcement engine for each security policy ()achieving low overheads by simplifying tabular expressions with static information of mobile code.|Xiaoming Zheng,Sven Koenig","65272|AAAI|2004|AI Characters and Directors for Interactive Computer Games|We are creating an environment for investigating the role of advanced AI in interactive, story-based computer games. This environment is based on the Unreal Tournament (UT) game engine and the Soar AI engine. Unreal provides a D virtual environment, while Soar provides a flexible architecture for developing complex AI characters. This paper describes our progress to date, starting with our game, Haunt , which is designed so that complex AI characters will be critical to the success (or failure) of the game. It addresses design issues with constructing a plot for an interactive storytelling environment, creating synthetic characters for that environment, and using a story director agent to tell the story with those characters.|Brian Magerko,John E. Laird,Mazin Assanie,Alex Kerfoot,Devvan Stokes","65865|AAAI|2006|Machine Translation for Manufacturing A Case Study at Ford Motor Company|Machine Translation was one of the first applications of Artificial Intelligence technology that was deployed to solve real-world problems. Since the early s, researchers have been building and utilizing computer systems that can translate from one language to another without extensive human intervention. In the late s, Ford Vehicle Operations began working with Systran Software Inc to adapt and customize their Machine Translation (MT) technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch and Portuguese. The use of Machine Translation (MT) was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. Our MT system has already translated over  million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation (Iwanska & Shapiro ) and natural language processing (Gazdar & Mellish ), can improve the accuracy of Machine Translation in a dynamic environment such as auto manufacturing.|Nestor Rychtyckyj","66370|AAAI|2008|Learning to Connect Language and Perception|To truly understand language, an intelligent system must be able to connect words, phrases, and sentences to its perception of objects and events in the world. Current natural language processing and computer vision systems make extensive use of machine learning to acquire the probabilistic knowledge needed to comprehend linguistic and visual input. However, to date, there has been relatively little work on learning the relationships between the two modalities. In this talk, I will review some of the existing work on learning to connect language and perception, discuss important directions for future research in this area, and argue that the time is now ripe to make a concerted effort to address this important, integrative AI problem.|Raymond J. Mooney","65062|AAAI|1987|KADBASE - A Prototype Expert System-Database Interface for Integrated CAE Environments|Database management systems (DBMSs) are important components of existing integrated computer-aided engineering (CAE) systems. Expert systems (ESs) are being applied to a broad range of engineering problems. However, most of the prototype expert system applications have been restricted to limited amounts of data and have no facility for sophisticated data management. KADBASE is a flexible, knowledge-based interface in which multiple expert systems and multiple databases can communicate as independent, self-descriptive components within an integrated, distributed engineering computing environment.|H. Craig Howard,Daniel R. Rehak","65259|AAAI|2004|An Explainable Artificial Intelligence System for Small-unit Tactical Behavior|As the artificial intelligence (AI) systems in military simulations and computer games become more complex, their actions become increasingly difficult for users to understand. Expert systems for medical diagnosis have addressed this challenge though the addition of explanation generation systems that explain a system's internal processes. This paper describes the AI architecture and associated explanation capability used by Full Spectrum Command, a training system developed for the U.S. Army by commercial game developers and academic researchers.|Michael van Lent,William Fisher,Michael Mancuso","65904|AAAI|2006|Virtual Humans|Imagine a simulated world where the characters you interact with are almost human - they converse with you in English, they understand the world they are in, can reason about what to do, and they exhibit emotions. Some of these characters may be your friends, while others will oppose you. Unlike current video garnes, being successful in this world won't just be a matter of who is quickest on the draw or most adroit at solving puzzles, instead it will be the person who understands the social fabric and cultural context and can use interpersonal skills most effectively. Such a simulation could open up whole new horizons for education, entertainment and simulation. And, given recent advances in AI and graphics, it may not be too far off. Virtual humans are computer-generated characters that can take the part of humans in a variety of limited contexts. These can include acting as role-players in simulations and training systems (Johnson, Rickel & Lester  Swartout et al.  Traum et al.  Johnson, Vilhjlmsson & Marsella ), where they play a variety of parts, such as acting as friendly or hostile forces, or locals in the environment. Other uses for virtual humans include acting as museum guides (Gustafson & Bell ), marketing assistants (Cassell, Bickmore et al. ) or characters in entertainment systems, where the advent of video games such as The Sims  makes clear the growing interest of the computer game industry in virtual humans (see also (Mateas & Stern )).|William R. Swartout","65330|AAAI|2004|Towards Autonomic Computing Adaptive Job Routing and Scheduling|Computer systems are rapidly becoming so complex that maintaining them with human support staffs will be prohibitively expensive and inefficient. In response, visionaries have begun proposing that computer systems be imbued with the ability to configure themselves, diagnose failures, and ultimately repair themselves in response to these failures. However, despite convincing arguments that such a shift would be desirable, as of yet there has been little concrete progress made towards this goal. We view these problems as fundamentally machine learning challenges. Hence, this article presents a new network simulator designed to study the application of machine learning methods from a system-wide perspective. We also introduce learning-based methods for addressing the problems of job routing and scheduling in the networks we simulate. Our experimental results verify that methods using machine learning outperform heuristic and hand-coded approaches on an example network designed to capture many of the complexities that exist in real systems.|Shimon Whiteson,Peter Stone"],["66798|AAAI|2010|Trial-Based Dynamic Programming for Multi-Agent Planning|In this paper, we propose a novel scheme using computing complexity layered scalable video coding (CCLSVC) to optimize the user experience of broadcasting video in the computing capability limited handheld terminals. To address the heterogeneity of computing capability among different handheld devices, we employ hierarchal B reference structure of SVC to divide the frames into multiple computing complexity layers (CC Layers) in server side. The handheld clients simply choose to decode the frames in their corresponding layers in terms of their computation capability to maximize the video PSNR. We have proved that the optimal CC Layers division problem is a precedence constrained scheduling problem, which is an NP-complete problem. And we further propose our fast greedy method to approximately get optimized broadcasting video playback PSNR. The simulation shows that our method is superior to temporal SVC and random frame discarding method.|Feng Wu,Shlomo Zilberstein,Xiaoping Chen","66887|AAAI|2010|Extracting Ontological Selectional Preferences for Non-Pertainym Adjectives from the Google Corpus|In our previous work, we proposed a distributed buffer (DB) scheme to tackle the problem of transporting layered video over erroneous multi-hop wireless networks. In DB scheme, some intermediate nodes are selected as DB nodes, which are used to pre-buffer video packets before the video streaming starts. In this paper, a novel hierarchical queueing model for DB nodes is proposed. The video playback quality including buffer overflow probability and video quality throughput can be computed based on the proposed queueing model. The simulation result matches the queueing performance of video streaming application.|John J. Tanner,Fernando Gomez","66765|AAAI|2010|High-Quality Policies for the Canadian Travelers Problem|JVT scalable video coding (SVC) provides high coding efficiency for progressive video sequences with combined scalability. However, interlaced SVC is only the straightforward extension of H.AVC interlaced coding with the similar FGS coding technique for progressive coding. Based on the particular temporal correlation of interlaced video sequences, this paper presents a novel and efficient FGS coding scheme for interlaced SVC, which is able to achieve higher compression efficiency and further temporal scalability by introducing additional temporal decomposition stage when coding key pictures. The advantages of our proposed interlaced FGS coding scheme are verified by integrating it into JVT-SVC reference software|Patrick Eyerich,Thomas Keller,Malte Helmert","66879|AAAI|2010|Keyword Extraction and Headline Generation Using Novel Word Features|This paper deals with the problem of shot-change detection on H.AVC compressed video. As H.AVC employs several new coding tools, the statistic information of the macroblock is different from the former MPEG video. How to detect shot-changes efficiently on H.AVC compressed video is a challenging work. In this paper, we propose a novel scheme, a macroblock type analysis plus the intra-mode statistical constraint rule, to detect shot-changes for P-frame and B-frame coded video of H.AVC. For detecting shot-changes in I-frame coded video, we propose an intra mode histogram and use the weighted city block distance to measure the similarities among I-frames. Experimental results show that the proposed algorithms achieve satisfactory performance and very fast detection.|Songhua Xu,Shaohui Yang,Francis Lau","66650|AAAI|2010|Integrating Expert Knowledge and Experience|The MPEG- video coding standard introduces a novel concept of sprite or mosaic that is a large image composed of pixels belonging to a video object visible throughout a video segment. The sprite captures spatio-temporal information in a very compact way and makes it possible for efficient object-based video compression. In this paper, we propose a practical approach to generating multiple super-resolution sprites for sprite coding. In order to construct super-resolution sprites and reduce coding cost, we firstly partition a video sequence into multiple independent sprites and group the images covering a similar scene into the same sprite. We then propose efficient and practical algorithms for cumulative global motion estimation and super-resolution sprite construction. Experiments with real video sequences show that the proposed approach outperforms the previous single sprite and multiple sprite techniques.|Ben George Weber","66602|AAAI|2008|Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence AAAI  Chicago Illinois USA July - |In this paper, we propose a novel bit-stream switching scheme for the multiple bit-rate (MBR) video streaming, in which a Wyner-Ziv coded frame is used to overcome the mismatch between the MBR streams when the switching occurs. With the proposed technique, the MBR streams can be independently encoded without losing any coding efficiency. Similar to distributed video coding, the proposed Wyner-Ziv switching scheme also faces the challenge of rate allocation at the server side. To solve this problem, we propose a new correlation model based on the analysis on the reconstructed frames from the streams with different bit rates. Accordingly, the number of transmitted bits can be on-line calculated based on the correlation model without any feedback from the decoder. With the proposed technique, the actually transmitted Wyner-Ziv bits are only few more than the truly requested bits. However, the delay due to the bit requesting process can be avoided|Dieter Fox,Carla P. Gomes","66821|AAAI|2010|Beyond Equilibrium Predicting Human Behavior in Normal-Form Games|In this paper, a novel coding scheme is proposed for switching viewpoint in multiview video streaming, which can switch freely and efficiently between any two adjacent views of the multiview video even if the switching point is not an intra frame. The proposed scheme implements the switch function at the predictive frames and improves the efficiency of the switched frames by limiting the mismatch between the references for the prediction and the reconstruction. Interview prediction, based on a global motion model, is used to reduce the differences between two adjacent views and the global disparity parameters are transmitted. To further improve the switching efficiency, a new rate-distortion optimization model is proposed to give the smart interintra selection when encoding. The H. coding scheme is employed as the platform. Experimental results show that, compared to switching directly, the proposed scheme can improve the coding efficiency up to  dB in PSNR.|James R. Wright,Kevin Leyton-Brown","66785|AAAI|2010|Team Formation with Heterogeneous Agents in Computer Games|In this paper, we propose a novel algorithm to design an optimal non-uniform scalar quantizer for distributed video coding, which aims at achieving a coding rate close to joint conditional entropy of the quantized video frames given the side information. Wyner-Ziv theory on source coding is employed as the basic coding principle and the asymmetric scenario is considered. In this algorithm, a probability distribution model, which considers the influence of the joint distribution of input source and side information to the coding performance, is established and used as the optimality condition firstly. Then, a modified Lloyd Max algorithm is used to design the scalar quantizer to give an optimal quantization for input source before coding. Experimental results show that compared to uniform scalar quantization, proposed algorithm can improve coding performance largely, especially at low bit rate|Robert G. Price,Scott D. Goodwin","66811|AAAI|2010|Multi-Task Active Learning with Output Constraints|Owing to the great computation complexity of the ME (motion estimation) in video coding, a lot of fast ME algorithms have been proposed in literature. Most of them are designed based on a given hypothesis about the character of the videos motion field. But due to the inherent variety of the real-world video, these algorithms are not generally efficient, especially for the variable block size ME in H. , only using one single ME strategy is difficult to get satisfactory result. This paper firstly will give two different fast ME algorithms, which aim at video sequences with different character. An adaptive strategy is proposed to combine the two algorithms together for H. . Experimental result shows that this strategy can keep coding efficiency in all kinds of video sequences, meanwhile the computation complexity is reduced adaptively according to the videos content.|Yi Zhang 0010","66709|AAAI|2010|The Boosting Effect of Exploratory Behaviors|Efficient adaptation to channel bandwidth is broadly required for effective streaming video over the Internet. To address this requirement, a novel seamless switching scheme among scalable video bitstreams is proposed in this paper. It can significantly improve the performance of video streaming over a broad range of bit rates by fully taking advantage of both the high coding efficiency of nonscalable bitstreams and the flexibility of scalable bitstreams, where small channel bandwidth fluctuations are accommodated by the scalability of a single scalable bitstream, whereas large channel bandwidth fluctuations are tolerated by flexible switching between different scalable bitstreams. Two main techniques for switching between video bitstreams are proposed. Firstly, a novel coding scheme is proposed to enable drift-free switching at any frame from the current scalable bitstream to one operated at lower rates without sending any overhead bits. Secondly, a switching-frame coding scheme is proposed to greatly reduce the number of extra bits needed for switching from the current scalable bitstream to one operated at higher rates. Compared with existing approaches, such as switching between nonscalable bitstreams and streaming with a single scalable bitstream, our experimental results clearly show that the proposed scheme brings higher efficiency and more flexibility in video streaming.|Jivko Sinapov,Alexander Stoytchev"],["66087|AAAI|2007|Asymptotically Optimal Encodings of Conformant Planning in QBF|The world is unpredictable, and acting intelligently requires anticipating possible consequences of actions that are taken. Assuming that the actions and the world are deterministic, planning can be represented in the classical propositional logic. Introducing nondeterminism (but not probabilities) or several initial states increases the complexity of the planning problem and requires the use of quantified Boolean formulae (QBF). The currently leading logic-based approaches to conditional planning use explicitly or implicitly a QBF with the prefix . We present formalizations of the planning problem as QBF which have an asymptotically optimal linear size and the optimal number of quantifier alternations in the prefix  and . This is in accordance with the fact that the planning problem (under the restriction to polynomial size plans) is on the second level of the polynomial hierarchy, not on the third.|Jussi Rintanen","66357|AAAI|2008|How Good is Almost Perfect|Heuristic search using algorithms such as A* and IDA* is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A* must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in \"simple\" planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator.|Malte Helmert,Gabriele Röger","65440|AAAI|2005|New Admissible Heuristics for Domain-Independent Planning|Admissible heuristics are critical for effective domain-independent planning when optimal solutions must be guaranteed. Two useful heuristics are the hm heuristics, which generalize the reachability heuristic underlying the planning graph, and pattern database heuristics. These heuristics, however, have serious limitations reachability heuristics capture only the cost of critical paths in a relaxed problem, ignoring the cost of other relevant paths, while PDB heuristics, additive or not, cannot accommodate too many variables in patterns, and methods for automatically selecting patterns that produce good estimates are not known. We introduce two refinements of these heuristics First, the additive hm heuristic which yields an admissible sum of hm heuristics using a partitioning of the set of actions. Second, the constrained PDB heuristic which uses constraints from the original problem to strengthen the lower bounds obtained from abstractions. The new heuristics depend on the way the actions or problem variables are partitioned. We advance methods for automatically deriving additive hm and PDB heuristics from STRIPS encodings. Evaluation shows improvement over existing heuristics in several domains, although, not surprisingly, no heuristic dominates all the others over all domains.|Patrik Haslum,Blai Bonet,Hector Geffner","66443|AAAI|2008|Finding State Similarities for Faster Planning|In many planning applications one can find actions with overlapping effects. If for optimally reaching the goal all that matters is within this overlap, there is no need to consider all these actions -for the task at hand they are equivalent. Using this structure for speed-up has previously been proposed in the context of least commitment planning. Of a similar spirit is the approach for improving best-first search based planning we present here intuitively, given a set of start states, reachable from the initial state, we plan in parallel for all of them, exploiting the similarities between them to gain computational savings. Since the similarity of two states is problem specific, we explicitly infer it by regressing all relevant entities, goal, heuristic function, action preconditions and costs, over the action sequences considered in planning. If the resulting formulae mention only fluents whose values the two states have in common, it suffices to evaluate the formulae in one of them. This leads to computational savings over conventional best-first search.|Christian Fritz","66449|AAAI|2008|Limits and Possibilities of BDDs in State Space Search|The idea of using BDDs for optimal sequential planning is to reduce the memory requirements for the state sets as problem sizes increase. State variables are encoded binary and ordered along their causal graph dependencies. Sets of planning states are represented in form of Boolean functions, and actions are formalized as transition relations. This allows to compute the successor state set, which determines all states reached by applying one action to the states in the input set. Iterating the process (starting with the representation of the initial state) yields a symbolic implementation of breadth-first search. This paper studies the causes for good and bad BDD performance by providing lower and upper bounds for BDD growth in various domains. Besides general applicability to planning benchmarks, our approach covers different cost models it applies to step-optimal propositional planning as well as planning with additive action costs.|Stefan Edelkamp,Peter Kissmann","66021|AAAI|2007|Domain-Independent Construction of Pattern Database Heuristics for Cost-Optimal Planning|Heuristic search is a leading approach to domain-independent planning. For cost-optimal planning, however, existing admissible heuristics are generally too weak to effectively guide the search. Pattern database heuristics (PDBs), which are based on abstractions of the search space, are currently one of the most promising approaches to developing better admissible heuristics. The informedness of PDB heuristics depends crucially on the selection of appropriate abstractions (patterns). Although PDBs have been applied to many search problems, including planning, there are not many insights into how to select good patterns, even manually. What constitutes a good pattern depends on the problem domain, making the task even more difficult for domain-independent planning, where the process needs to be completely automatic and generaL We present a novel way of constructing good patterns automatically from the specification of planning problem instances. We demonstrate that this allows a domain-independent planner to solve planning problems optimally in some very challenging domains, including a STRIPS formulation of the Sokoban puzzle.|Patrik Haslum,Adi Botea,Malte Helmert,Blai Bonet,Sven Koenig","66120|AAAI|2007|Filtering Decomposition and Search Space Reduction for Optimal Sequential Planning|We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.|Stéphane Grandcolas,C. Pain-Barre","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65482|AAAI|2005|Prottle A Probabilistic Temporal Planner|Planning with concurrent durative actions and probabilistic effects, or probabilistic temporal planning, is a relatively new area of research. The challenge is to replicate the success of modern temporal and probabilistic planners with domains that exhibit an interaction between time and uncertainty. We present a general framework for probabilistic temporal planning in which effects, the time at which they occur, and action durations are all probabilistic. This framework includes a search space that is designed for solving probabilistic temporal planning problems via heuristic search, an algorithm that has been tailored to work with it and an effective heuristic based on an extension of the planning graph data structure. Prottle is a planner that implements this framework, and can solve problems expressed in an extension of PDDL.|Iain Little,Douglas Aberdeen,Sylvie Thiébaux","65698|AAAI|2006|Cost-Optimal External Planning|This paper considers strategies for external memory based optimal planning. An external breadth-first search exploration algorithm is devised that is guaranteed to find the costoptimal solution. We contribute a procedure for finding the upper bound on the locality of the search in planning graphs that dictates the number of layers that have to be kept to avoid re-openings. We also discuss an external variant of Enforced Hill Climbing. Using relaxed-plan heuristic without helpful-action pruning we have been able to perform large explorations on metric planning problems, providing better plan lengths than have been reported earlier. A novel approach to plan reconstruction in external setting with linear IO complexity is proposed. We provide external exploration results on some recently proposed planning domains.|Stefan Edelkamp,Shahid Jabbar"],["80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","66878|AAAI|2010|Using Bisimulation for Policy Transfer in MDPs|Much of the work on using Markov Decision Processes (MDPs) in artificial intelligence (AI) focuses on solving a single problem. However, AI agents often exist over a long period of time, during which they may be required to solve several related tasks. This type of scenario has motivated a significant amount of recent research in knowledge transfer methods for MDPs. The idea is to allow an agent to continue to re-use the expertise accumulated while solving past tasks over its lifetime (see Taylor & Stone, , for a comprehensive survey).|Pablo Samuel Castro,Doina Precup","65538|AAAI|2005|SAT-Based versus CSP-Based Constraint Weighting for Satisfiability|Recent research has focused on bridging the gap between the satisfiability (SAT) and constraint satisfaction problem (CSP) formalisms. One approach has been to develop a many-valued SAT formula (MV-SAT) as an intermediate paradigm between SAT and CSP, and then to translate existing highly efficient SAT solvers to the MV-SAT domain. Experimental results have shown this approach can achieve significant improvements in performance compared with the traditional SAT and CSP approaches. In this paper, we follow a different route, developing SAT solvers that can automatically recognise CSP structure hidden in SAT encodings. This allows us to look more closely at how constraint weighting can be implemented in the SAT and CSP domains. Our experimental results show that a SAT-based approach to handle weights, together with CSP-based approach to variable instantiation, is superior to other combinations of SAT and CSP-based approaches. A further experiment on the round robin scheduling problem indicates that this many-valued constraint weighting approach outperforms other state-of-the-art solvers.|Duc Nghia Pham,John Thornton,Abdul Sattar,Abdelraouf Ishtaiwi","65351|AAAI|2005|The Achilles Heel of QBF|In recent years we have seen significant progress in the area of Boolean satisfiability (SAT) solving and its applications. As a new challenge, the community is now moving to investigate whether similar advances can be made in the use of Quantified Boolean Formulas (QBF). QBF provides a natural framework for capturing problem solving and planning in multi-agent settings. However, contrarily to single-agent planning, which can be effectively formulated as SAT, we show that a QBF approach to planning in a multi-agent setting leads to significant unexpected computational difficulties. We identify as a key difficulty of the QBF approach the fact that QBF solvers often end up exploring a much larger search space than the natural search space of the original problem. This is in contrast to the experience with SAT approaches. We also show how one can alleviate these problems by introducing two special QBF formulations and a new QBF solution strategy. We present experiments that show the effectiveness of our approach in terms of a significant improvement in performance compared to earlier work in this area. Our work also provides a general methodology for formulating adversarial scenarios in QBF.|Carlos Ansótegui,Carla P. Gomes,Bart Selman","80687|VLDB|2006|The Making of TPC-DS|For the last decade, the research community and the industry have used TPC-D and its successor TPC-H to evaluate performance of decision support technology. Recognizing a paradigm shift in the industry the Transaction Processing Performance Council has developed a new Decision Support benchmark, TPC-DS, expected to be released this year. From an ease of benchmarking perspective it is similar to past benchmarks. However, it adjusts for new technology and new approaches the industry has embarked on in recent years. This paper describes the main characteristics of TPC-DS, explains why some of the key decisions were made and which performance aspects of decision support system it measures.|Raghunath Othayoth Nambiar,Meikel Poess","66051|AAAI|2007|Understanding Performance Tradeoffs in Algorithms for Solving Oversubscribed Scheduling|In recent years, planning and scheduling research has paid increasing attention to problems that involve resource oversubscription, where cumulative demand for resources outstrips their availability and some subset of goals or tasks must be excluded. Two basic classes of techniques to solve oversubscribed scheduling problems have emerged searching directly in the space of possible schedules and searching in an alternative space of task permutations (by relying on a schedule builder to provide a mapping to schedule space). In some problem contexts, permutation-based search methods have been shown to outperform schedule-space search methods, while in others the opposite has been shown to be the case. We consider two techniques for which this behavior has been observed TaskSwap (TS), a schedule-space repair search procedure, and Squeaky Wheel Optimization (SWO), a permutation-space scheduling procedure. We analyze the circumstances under which one can be expected to dominate the other. Starting from a real-world scheduling problem where SWO has been shown to outperform TS, we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem, where TS has been found to outperform SWO. Experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate.|Laurence A. Kramer,Laura Barbulescu,Stephen F. Smith","80289|VLDB|2003|Continuous K-Nearest Neighbor Queries for Continuously Moving Points with Updates|In recent years there has been an increasing interest in databases of moving objects where the motion and extent of objects are represented as a function of time. The focus of this paper is on the maintenance of continuous K- nearest neighbor (k-NN) queries on moving points when updates are allowed. Updates change the functions describing the motion of the points, causing pending events to change. Events are processed to keep the query result consistent as points move. It is shown that the cost of maintaining a continuous k-NN query result for moving points represented in this way can be significantly reduced with a modest increase in the number of events processed in the presence of updates. This is achieved by introducing a continuous within query to filter the number of objects that must be taken into account when maintaining a continuous k-NN query. This new approach is presented and compared with other recent work. Experimental results are presented showing the utility of this approach.|Glenn S. Iwerks,Hanan Samet,Kenneth P. Smith","80481|VLDB|2005|Database Publication Practices|There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.|Philip A. Bernstein,David J. DeWitt,Andreas Heuer,Zachary G. Ives,Christian S. Jensen,Holger Meyer,M. Tamer \u2013zsu,Richard T. Snodgrass,Kyu-Young Whang,Jennifer Widom","65687|AAAI|2006|Identifying and Generating Easy Sets of Constraints for Clustering|Clustering under constraints is a recent innovation in the artificial intelligence community that has yielded significant practical benefit. However, recent work has shown that for some negative forms of constraints the associated subproblem of just finding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufficient conditions from graph theory to identify a priori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.|Ian Davidson,S. S. Ravi","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram"]]},"title":{"entropy":6.3596784160927395,"topics":["the and, the web, and, the, natural language, reasoning about, semantic web, for and, for web, web services, logic programs, for the, language and, case study, with and, and web, the with, for logic, semantic for, information extraction","data base, system for, for data, data, and data, database system, data model, data management, database, data system, the data, for database, management system, relational database, for and, for, base system, and system, for base, and database","algorithms for, query for, query processing, query optimization, efficient for, solving problem, data streams, search for, for xml, combinatorial auctions, sense disambiguation, artificial intelligence, queries, new for, answering queries, framework for, query xml, efficient, processing queries, optimization for","learning, learning for, reinforcement learning, learning and, bayesian networks, for networks, model for, answer set, using, learning with, planning with, for planning, networks, social networks, activity recognition, planning domains, partially observable, with constraints, matrix factorization, decision processes","for and, semantic web, for web, and, web services, and web, and search, with and, search engine, integrating and, the web, search for, semantic and, web search, web, the and, using web, and application, for services, and data","reasoning about, and reasoning, knowledge and, for reasoning, belief revision, for belief, reasoning, for knowledge, the knowledge, reasoning the, temporal and, human-robot interaction, reasoning with, spatial and, about action, for human-robot, and event, integration and, knowledge, under uncertainty","design for, for constraints, design and, structure for, mechanism for, and constraints, distributed constraints, constraints satisfaction, the design, mechanism design, access data, the conceptual, consistency for, for and, approach design, data structure, constraints, for programs, with constraints, design","interface for, data sources, for schema, index for, approach data, integrated system, schema matching, automatic data, data quality, the schema, schema mapping, for automatic, different data, interface data, user interface, development for, routing for, and schema, database schema, for user","for xml, artificial intelligence, xml documents, materialized views, conference aaai, artificial aaai, intelligence aaai, the skyline, the aaai, xml, indexing xml, and order, for large-scale, for materialized, for intelligence, simple and, simple for, xml and, from xml, views xml","algorithms for, search for, new for, and algorithms, efficient algorithms, for optimal, search, heuristic for, algorithms the, local search, keyword search, heuristic search, and search, approach, approach based, scheme for, new the, search over, approach and, scheduling problem","model for, learning model, partially observable, for agents, model, using model, model and, for environments, planning domains, for inference, for probabilistic, for domains, for analysis, and environments, for generating, representation for, and agents, representation and, probabilistic model, and analysis","for networks, learning with, bayesian networks, bayesian for, for classification, and networks, networks, online for, sensor networks, neural networks, networks using, modeling and, with and, bayesian model, graphical model, learning networks, model networks, modeling for, for with, active learning"],"ranking":[["66195|AAAI|2007|The Virtual Solar-Terrestrial Observatory A Deployed Semantic Web Application Case Study for Scientific Research|The Virtual Solar-Terrestrial Observatory is a production semantic web data framework providing access to observational datasets from fields spanning upper atmospheric terrestrial physics to solar physics. The observatory allows virtual access to a highly distributed and heterogeneous set of data that appears as if all resources are organized, stored and retrievedused in a common way. The end-user community comprises scientists, students, data providers numbering over  out of an estimated community of . We present details on the case study, our technological approach including the semantic web languages, tools and infrastructure deployed, benefits of AI technology to the application, and our present evaluation after the initial nine months of use.|Deborah L. McGuinness,Peter Fox,Luca Cinquini,Patrick West,Jose Garcia,James L. Benedict,Don Middleton","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","66910|AAAI|2010|A General Framework for Representing and Reasoning with Annotated Semantic Web Data|Analyzed the energy consumption disciplinarian of the nodes in WSN, the node's energy attenuation forecast model (EAFM) can be established. A difference-threshold reporting mechanism (DTRM) is used to report the residual energy of nodes. The energy collection mechanism based on EAFM and DTRM can reduce energy data reporting times significantly, improve the efficient of energy data collection, save the node's energy at the same time. The experiments in the platform of telosb nodes show that the predicable rate is between % and %, and this method can extend the node's life by %  .%.|Umberto Straccia,Nuno Lopes 0002,Gergely Lukacsy,Axel Polleres","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","80385|VLDB|2004|Simlarity Search for Web Services|Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem locating desired web services. Traditional keyword search is insufficient in this context the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over  web-service operations that shows the high recall and precision of our algorithms.|Xin Dong,Alon Y. Halevy,Jayant Madhavan,Ema Nemes,Jun Zhang","80268|VLDB|2003|The Semantic Web Semantics for Data on the Web|In our tutorial on Semantic Web (SW) technology, we explain the why, the various technology thrusts and the relationship to database technology. The motivation behind presenting this tutorial is discussed and the framework of the tutorial along with the various component technologies and research areas related to the Semantic Web is presented.|Stefan Decker,Vipul Kashyap","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas Wölfl"],["80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","79798|VLDB|1975|Semantic Integrity in a Relational Data Base System|As a model of some aspect(s) of the real world, the data in a data base must be accurate. In the context of a relational data base system, a facility to allow the expression and enforcement of a set of semantic integrity constraints is discussed. Semantic integrity constraints may describe properties of and relationships between data objects (in a relational data base) that are to hold (state snapshot constraints). Constraints may also place limitations on permissible data base operations (state transition constraints). A second type of semantic integrity information that is important in the context of a relational data base system is the precise description of domains (viewed as sets of atomic data objects), and the specification of their use as underlying domains of columns of relations in a data base. High-level nonprocedural languages to facilitate the expression of these two types of semantic integrity information are introduced and examples are presented.|Michael Hammer,Dennis McLeod","79860|VLDB|1977|A Data Base Design Decision Support System|The task of physical data base design in an DBTG enviornment is examined. Generation of an internal schema which considers questions of storage versus access costs, efficient implementation of data relationships, efficient placement of data within the data base and allocation of primary and secondary storage is shown to be a formidable task. Current data base design aids are reviewed. One aid, a sophisicated mathematical model of DBTG data bases (Gerritsen ) is shown to be a potentially valuable tool. A limitation of this model is that no optimization algorithm other than total enumeration has been found. This paper proposes an implementation of the Gerritsen model. An interactive design tool, based upon the Gerritsen model, is discussed. A Data Base Design Decision Support System (DBD-DSS) for use by the DBA is developed. The objectives and structure of the DBD-DSS are examined. A comprehensive example illustrating both the user interface and the potential benefits of the interactive tool is presented.|Thomas J. Gambino,Rob Gerritsen","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80038|VLDB|1981|Deferring Updates in a Relational Data Base System|Deferred updating in a relational data base is a technique which delays the application of updates until a request is made for the value. In general we do not know, a priori, which of the updated values will be retrieved therefore it is beneficial to defer the access and computation. This technique promotes an \"update-by-need\" policy. The method uses generalization, property inheritance, and procedural attachment for dynamically maintaining the update and query processes. The use of these representational concepts aids in maintaining the structure of the relational model, yet provides opportunities to extend the semantic power of the model.|Stephanie J. Cammarata","80033|VLDB|1981|A Commercial Back-End Data Base System|Back-end Data base machines receive considerable attention owing to their facility for improving performance of installations with heavy data base activity. Many prototype systems have been described in the litterature but few systems are commercially available. This paper presents a commercial back-end data base management machine called MIX (design code), which is developped by our company. The global architecture of this data base machine is described, and the principal issue, discussed here in, applies to its software.|J. P. Armisen,J. Y. Caleca","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber","79831|VLDB|1976|An Enduser Guidance Component for an Intelligent Data Base System|A general application guidance system -GAGS- was developed to convenient enduser component for data base systems. It has the following four main features . Dialogue supported methods dictionary guides the user from a given application problem to the adequate solution method or program. . Dialogue supported data dictionary gives syntactic and semantic information about the data available in data administration systems. . Dialogue supported execution facility offers a system triggered execution of programs found with the help of the methods dictionary in order to process data identified by means of the data dictionary. . Dialogue supported system guidance enables the unexperienced user to work with software systems without being a DP-professional. The basic information structure for the interactive guidance which is the common basic concept of the four facilities listed above, is the information network. It is generated by breaking down the knowledge about the application area in question into information elements (network nodes) which are interconnected (network arcs).|R. Erbe,Georg Walch"],["80418|VLDB|2004|Efficient XML-to-SQL Query Translation Where to Add the Intelligence|We consider the efficiency of queries generated by XML to SQL translation. We first show that published XML-to-SQL query translation algorithms are suboptimal in that they often translate simple path expressions into complex SQL queries even when much simpler equivalent SQL queries exist. There are two logical ways to deal with this problem. One could generate suboptimal SQL queries using a fairly naive translation algorithm, and then attempt to optimize the resulting SQL or one could use a more intelligent translation algorithm with the hopes of generating efficient SQL directly. We show that optimizing the SQL after it is generated is problematic, becoming intractable even in simple scenarios by contrast, designing a translation algorithm that exploits information readily available at translation time is a promising alternative. To support this claim, we present a translation algorithm that exploits translation time information to generate efficient SQL for path expression queries over tree schemas.|Rajasekar Krishnamurthy,Raghav Kaushik,Jeffrey F. Naughton","80267|VLDB|2003|Query Processing for High-Volume XML Message Brokering|XML filtering solutions developed to date have focused on the matching of documents to large numbers of queries but have not addressed the customization of output needed for emerging distributed information infrastructures. Support for such customization can significantly increase the complexity of the filtering process. In this paper, we show how to leverage an efficient, shared path matching engine to extract the specific XML elements needed to generate customized output in an XML Message Broker. We compare three different approaches that differ in the degree to which they exploit the shared path matching engine. We also present techniques to optimize the post-processing of the path matching engine output, and to enable the sharing of such processing across queries. We evaluate these techniques with a detailed performance study of our implementation.|Yanlei Diao,Michael J. Franklin","80712|VLDB|2006|State-Slice New Paradigm of Multi-query Optimization of Window-based Stream Queries|Modern stream applications such as sensor monitoring systems and publishsubscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams. Efficient sharing of computations among multiple continuous queries, especially for the memory- and CPU-intensive window-based operations, is critical. A novel challenge in this scenario is to allow resource sharing among similar queries, even if they employ windows of different lengths. This paper first reviews the existing sharing methods in the literature, and then illustrates the significant performance shortcomings of these methods.This paper then presents a novel paradigm for the sharing of window join queries. Namely we slice window states of a join operator into fine-grained window slices and form a chain of sliced window joins. By using an elaborate pipelining methodology, the number of joins after state slicing is reduced from quadratic to linear. This novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes. Based on the state-slice sharing paradigm, two algorithms are proposed for the chain buildup. One minimizes the memory consumption while the other minimizes the CPU usage. The algorithms are proven to find the optimal chain with respect to memory or CPU usage for a given query workload. We have implemented the slice-share paradigm within the data stream management system CAPE. The experimental results show that our strategy provides the best performance over a diverse range of workload settings among all alternate solutions in the literature.|Song Wang,Elke A. Rundensteiner,Samrat Ganguly,Sudeept Bhatnagar","80714|VLDB|2006|R-SOX Runtime Semantic Query Optimization over XML Streams|Optimizing queries over XML streams has been an important and non-trivial issue with the emergence of complex XML stream applications such as monitoring sensor networks and online transaction processing. Our system, R-SOX, provides a platform for runtime query optimization based on dynamic schema knowledge embedded in the XML streams. Such information provides refined runtime schema knowledge thus dramatically enlarged the opportunity for schema-based query optimizations. In this demonstration, we focus on the following three aspects () annotation of runtime schema knowledge () incremental maintenance of run-time schema knowledge () dynamic semantic query optimization techniques. The overall framework for runtime semantic query optimization, including several classes of dynamic optimization techniques, will be shown in this demonstration.|Song Wang,Hong Su,Ming Li,Mingzhu Wei,Shoushen Yang,Drew Ditto,Elke A. Rundensteiner,Murali Mani","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma \u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80663|VLDB|2006|POPFED Progressive Query Optimization for Federated Queries in DB|Federated queries are regular relational queries accessing data on one or more remote relational or non-relational data sources, possibly combining them with tables stored in the federated DBMS server. Their execution is typically divided between the federated server and the remote data sources. Outdated and incomplete statistics have a bigger impact on federated DBMS than on regular DBMS, as maintenance of federated statistics is unequally more complicated and expensive than the maintenance of the local statistics consequently bad performance commonly occurs for federated queries due to the selection of a suboptimal query plan. To solve this problem we propose a progressive optimization technique for federated queries called POPFED by extending the state of the art for progressive reoptimization for local source queries, POP . POPFED uses (a) an opportunistic, but risk controlled reoptimization technique for federated DBMS, (b) a technique for multiple reoptimizations during federated query processing with a strategy to discover redundant and eliminate partial results, and (c) a mechanism to eagerly procure statistics in a federated environment. In this demonstration we showcase POPFED implemented in a prototype version of WebSphere Information Integrator for DB using the TPC-H benchmark database and its workload. For selected queries of the workload we show unique features including multi-round reoptimizations using both a new graphical reoptimization progress monitor POPMonitor and the DB graphical plan explain tool.|Holger Kache,Wook-Shin Han,Volker Markl,Vijayshankar Raman,Stephan Ewen","80278|VLDB|2003|Mixed Mode XML Query Processing|Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.|Alan Halverson,Josef Burger,Leonidas Galanis,Ameet Kini,Rajasekar Krishnamurthy,Ajith Nagaraja Rao,Feng Tian,Stratis Viglas,Yuan Wang,Jeffrey F. Naughton,David J. DeWitt","80192|VLDB|2002|Efficient Algorithms for Processing XPath Queries|Our experimental analysis of several popular XPath processors reveals a striking fact Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we show how the main ideas of our algorithm can be profitably integrated into existing XPath processors. Finally, we present two fragments of XPath for which linear-time query processing algorithms exist and another fragment with linear-spacequadratic-time query processing.|Georg Gottlob,Christoph Koch,Reinhard Pichler","80790|VLDB|2007|A General Framework for Modeling and Processing Optimization Queries|An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function andor a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that IO optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.|Michael Gibas,Ning Zheng,Hakan Ferhatosmanoglu"],["66622|AAAI|2010|Integrating Sample-Based Planning and Model-Based Reinforcement Learning|This paper introduces an abstract high dependability framework for the implementation of embedded control software with hard real-time constraints. The framework specifies time-triggered sensor readings, atomic component invocations, actuator updates, and pattern switches independent of any implementation platform. In order to leverage model continuity, XML-based description of composite component informal description is required, which supports reuse of components and model information interoperability. By separating the platform-independent from the platform-dependent concerns, Consider a quality process control system in steel industry on a distributed real-time embedded environment, we implement a simplified high dependability design framework to prove the feasibility in the validation and synthesis of embedded control component execution.|Thomas J. Walsh,Sergiu Goschin,Michael L. Littman","66656|AAAI|2010|Learning to Predict Opinion Share in Social Networks|De-noising by the traditional wavelet transform, the result is affected by the choosing of wavelet base. Because the wavelet base is fixed in the traditional wavelet transform, either the smoothness or singularity of the signal canpsilat be fitted quite well. To overcome the limitation, a new self-adaptive lifting scheme based on modulus maximum analysis is presented. Modulus maximum sequence of the large scale wavelet coefficients can locate the point of the signal with big singularity precisely. According to the position of the point with big singularity, proper neighborhood is fixed, and prediction operator can be chosen self-adaptively. In this way, the prediction operator is fitted to the local feature of the signal. The simulation and engineering application showed that the proposed method could overcome the de-noising disadvantage of traditional wavelet transform. It not only can filter noise from original signal effectively but also can hold local characteristics of original signal in the de-noised signals.|Masahiro Kimura,Kazumi Saito,Kouzou Ohara,Hiroshi Motoda","66607|AAAI|2010|Latent Variable Model for Learning in Pairwise Markov Networks|In this paper, an efficient rate-control scheme for H.AVC video encoding is proposed. The redesign of the quantization scheme in H.AVC results in that the relationship between the quantization parameter and the true quantization stepsize is no longer linear. Based on this observation, we propose a new rate-distortion (R-D) model by utilizing the true quantization stepsize and then develop an improved rate-control scheme for the H.AVC encoder based on this new R-D model. In general, the current R-D optimization (RDO) mode-selection scheme in H.AVC test model is difficult for rate control, because rate control usually requires a predetermined set of motion vectors and coding modes to select the quantization parameter, whereas the RDO does in the different order and requires a predetermined quantization parameter to select motion vectors and coding modes. To tackle this problem, we develop a complexity-adjustable rate-control scheme based on the proposed R-D model. Briefly, the proposed scheme is a one-pass process at frame level and a partial two-pass process at macroblock level. Since the number of macroblocks with the two-pass processing can be controlled by an encoder parameter, the fully one-pass implementation is a subset of the proposed algorithm. An additional topic discussed in this paper is about video buffering. Since a hypothetical reference decoder (HRD) has been defined in H.AVC to guarantee that the buffers never overflow or underflow, the more accurate rate-allocation schemes are proposed to satisfy these requirements of HRD.|Saeed Amizadeh,Milos Hauskrecht","66198|AAAI|2007|Mapping and Revising Markov Logic Networks for Transfer Learning|Transfer learning addresses the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers transfer learning with Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch.|Lilyana Mihalkova,Tuyen N. Huynh,Raymond J. Mooney","66893|AAAI|2010|Nonparametric Bayesian Approaches for Reinforcement Learning in Partially Observable Domains|The Domain system's objective is to integrate mainframe capability with local area networking and raster graphics capabilities at a cost appropriate to engineering and graphics applications.|Finale Doshi-Velez","65613|AAAI|2005|Learning Measures of Progress for Planning Domains|We study an approach to learning heuristics for planning domains from example solutions. There has been little work on learning heuristics for the types of domains used in deterministic and stochastic planning competitions. Perhaps one reason for this is the challenge of providing a compact heuristic language that facilitates learning. Here we introduce a new representation for heuristics based on lists of set expressions described using taxonomic syntax. Next, we review the idea of a measure of progress (parmar ), which is any heuristic that is guaranteed to be improvable at every state. We take finding a measure of progress as our learning goal, and describe a simple learning algorithm for this purpose. We evaluate our approach across a range of deterministic and stochastic planning-competition domains. The results show that often greedily following the learned heuristic is highly effective. We also show our heuristic can be combined with learned rule-based policies, producing still stronger results.|Sung Wook Yoon,Alan Fern,Robert Givan","65884|AAAI|2006|Learning Partially Observable Action Schemas|We present an algorithm that derives actions' effects and preconditions in partially observable, relational domains. Our algorithm has two unique features an expressive relational language, and an exact tractable computation. An action-schema language that we present permits learning of preconditions and effects that include implicit objects and unstated relationships between objects. For example, we can learn that replacing a blown fuse turns on all the lights whose switch is set to on. The algorithm maintains and outputs a relational-logical representation of all possible action-schema models after a sequence of executed actions and partial observations. Importantly, our algorithm takes polynomial time in the number of time steps and predicates. Time dependence on other domain parameters varies with the action-schema language. Our experiments show that the relational structure speeds up both learning and generalization, and outperforms propositional learning methods. It also allows establishing apriori-unknown connections between objects (e.g. light bulbs and their switches), and permits learning conditional effects in realistic and complex situations. Our algorithm takes advantage of a DAG structure that can be updated efficiently and preserves compactness of representation.|Dafna Shahaf,Eyal Amir","66549|AAAI|2008|Reinforcement Learning for Vulnerability Assessment in Peer-to-Peer Networks|Proactive assessment of computer-network vulnerability to unknown future attacks is an important but unsolved computer security problem where AI techniques have significant impact potential. In this paper, we investigate the use of reinforcement learning (RL) for proactive security in the context of denial-of-service (DoS) attacks in peer-to-peer (PP) networks. Such a tool would be useful for network administrators and designers to assess and compare the vulnerability of various network configurations and security measures in order to optimize those choices for maximum security. We first discuss the various dimensions of the problem and how to formulate it as RL. Next we introduce compact parametric policy representations for both single attacker and botnets and derive a policy-gradient RL algorithm. We evaluate these algorithms under a variety of network configurations that employ recent fair-use DoS security mechanisms. The results show that nur RL-based approach is able to significantly outperform a number of heuristic strategies in terms of the severity of the attacks discovered. The results also suggest some possible network design lessons for reducing the attack potential of an intelligent attacker.|Scott Dejmal,Alan Fern,Thinh Nguyen","65014|AAAI|1987|Modular Learning in Neural Networks|In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.|Dana H. Ballard","66839|AAAI|2010|Structure Learning for Markov Logic Networks with Many Descriptive Attributes|The multiterabyte Sloan Digital Sky Survey's (SDSS's) catalog data is stored in a commercial relational database management system with SQL query access and a built-in query optimizer. The SDSS catalog archive server adds advanced data mining features to the DBMS to provide fast online access to the data.|Hassan Khosravi,Oliver Schulte,Tong Man,Xiaoyuan Xu,Bahareh Bina"],["80385|VLDB|2004|Simlarity Search for Web Services|Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem locating desired web services. Traditional keyword search is insufficient in this context the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over  web-service operations that shows the high recall and precision of our algorithms.|Xin Dong,Alon Y. Halevy,Jayant Madhavan,Ema Nemes,Jun Zhang","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","80268|VLDB|2003|The Semantic Web Semantics for Data on the Web|In our tutorial on Semantic Web (SW) technology, we explain the why, the various technology thrusts and the relationship to database technology. The motivation behind presenting this tutorial is discussed and the framework of the tutorial along with the various component technologies and research areas related to the Semantic Web is presented.|Stefan Decker,Vipul Kashyap","66261|AAAI|2007|Aggregating User-Centered Rankings to Improve Web Search|This paper is to investigate rank aggregation based on multiple user-centered measures in the context of the web search. We introduce a set of techniques to combine ranking lists in order of user interests termed as a user profile. Moreover, based on the click-history data, a kind of taxonomic hierarchy automatically models the user profile which can include a variety of attributes of user interests. We mainly focus on the topics a user is interested in and the degrees of user interests in these topics. The primary goal of our work is to form a broadly acceptable ranking list, rather than that determined by an individual ranking measure. Experiment results on a real click-history data set show the effectiveness of our aggregation techniques to improve the web search.|Lin Li,Zhenglu Yang,Masaru Kitsuregawa","80263|VLDB|2003|VIPAS Virtual Link Powered Authority Search in the Web|With the exponential growth of the World Wide Web, looking for pages with high quality and relevance in the Web has become an important research field. There have been many keyword-based search engines built for this purpose. However, these search engines usually suffer from the problem that a relevant Web page may not contain the keyword in its page text. Algorithms exploiting the link structure of Web documents, such as HITS, have also been proposed to overcome the problems of traditional search engines. Though these algorithms perform better than keyword-based search engines, they still have some defects. Among others, one major problem is that links in Web pages are only able to reflect the view of the page authors on the topic of those pages but not that of the page readers. In this paper, we propose a new algorithm with the idea of using virtual links which are created according to what the user behaves in browsing the output list of the query result. These virtual links are then employed to identify authoritative resources in the Web. Speci fically, the algorithm, referred to as algorithm VIPAS (standing for virtual link powered authority search), is divided into three phases. The first phase performs basic link analysis. The second phase collects statistics by observing the user behavior in browsing pages listed in the query result, and virtual links are then created according to what observed. In the third phase, these virtual links as well as real ones are taken together to produce an updated list of authoritative pages that will be presented to the user when the query with similar keywords is encountered next time. A Web warehouse is built and the algorithm is integrated into the system. By conducting experiments on the system, we have shown that VIPAS is not only very effective but also very adaptive in providing much more valuable information to users.|Chi-Chun Lin,Ming-Syan Chen","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin"],["66006|AAAI|2007|Spatial Representation and Reasoning for Human-Robot Collaboration|How should a robot represent and reason about spatial information when it needs to collaborate effectively with a human The form of spatial representation that is useful for robot navigation may not be useful in higher-level reasoning or working with humans as a team member. To explore this question, we have extended previous work on how children and robots learn to play hide and seek to a human-robot team covertly approaching a moving target. We used the cognitive modeling system, ACT-R, with an added spatial module to support the robot's spatial reasoning. The robot interacted with a team member through voice, gestures, and movement during the team's covert approach of a moving target. This paper describes the new robotic system and its integration of metric, symbolic, and cognitive layers of spatial representation and reasoning for its individual and team behavior.|William G. Kennedy,Magdalena D. Bugajska,Matthew Marge,William Adams,Benjamin R. Fransen,Dennis Perzanowski,Alan C. Schultz,J. Gregory Trafton","65594|AAAI|2005|Issues in Reasoning about Interaction Networks in Cells Necessity of Event Ordering Knowledge|In this paper we discuss several representation issues that we came across while modelling molecular interactions in cells of living organisms. One of the issues was that the triggering of events inside cells, an important modelling component, are not necessarily immediate, leading to multiple evolution models in the absence of additional information. Second, often an action or a trigger at one level of granularity of representation can be elaborated and refined. We show the problem that existing representation and modelling formalisms have in dealing with the above issues. We then present an action language which builds up on a previous language, and has the ability to express event ordering knowledge. We show that our language is able to adequately address the above-mentioned issues.|Nam Tran,Chitta Baral,Carran Shankland","65526|AAAI|2005|Analysis of Strategic Knowledge in Back of the Envelope Reasoning|Back of the envelope (BotE) reasoning involves generating quantitative answers in situations where exact data and models are unavailable and where available data is often incomplete andor inconsistent. A rough estimate generated quickly is more valuable and useful than a detailed analysis, which might be unnecessary, impractical, or impossible because the situation does not provide enough time, information, or other resources to perform one. Such reasoning is a key component of commonsense reasoning about everyday physical situations. We present an implemented system, BotE-Solver, that can solve about a dozen estimation questions like \"What is the annual cost of healthcare in USA\" from different domains using a library of strategies and the Cyc knowledge base. BotE-Solver is a general-purpose problem solving framework that uses strategies represented as suggestions, and keeps track of problem solving progress in an ANDOR tree. A key contribution of this paper is a knowledge level analysis Newell,  of the strategic knowledge used in BotE reasoning. We present a core collection of seven powerful estimation strategies that provides broad coverage for such problem solving. We hypothesize that this is the complete set of back of the envelope problem solving strategies. We present twofold support for this hypothesis ) an empirical analysis of all problems (n) on Force and Pressure, Rotation and Mechanics, Heat, and Astronomy from Clifford Swartz's \"Back-of-the-Envelope Physics\" Swartz, , and ) an analysis of strategies used by BotE-Solver.|Praveen K. Paritosh,Kenneth D. Forbus","66236|AAAI|2007|Optimal Regression for Reasoning about Knowledge and Actions|We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.|Hans P. van Ditmarsch,Andreas Herzig,Tiago De Lima","65797|AAAI|2006|Reasoning about Discrete Event Sources|We investigate the modelling of workflows, plans, and other event-generating processes as discrete event sources and reason about the possibility of having event sequences ending in undesirable states. In previous research, the problem is shown to be NP-Complete even if the number of events to occur is fixed in advance. In this paper, we consider possible events sequences of indefinite length and show that many interesting cases of such reasoning task are solvable in polynomial time.|Shieu-Hong Lin","65121|AAAI|1987|Default Reasoning through Belief Revision Strategy|The thesis of this paper is that default reasoning can be accomplished rather naturally if an appropriate strategy of belief revision is employed. The idea is based on the premise that new beliefs introduced into a situation change the structure of current beliefs to accomodate the new beliefs as exceptions. It is easy to characterise these exceptions in beliefs if we extend the belief language to include some modal operator and prefix the exceptions with the operator. This serves to make the exceptions syntactically explicit, which can then be processed in a routine way by a default reasoning theorem prover.|Chern H. Seet","65113|AAAI|1987|Hierarchical Reasoning about Inequalities|This paper describes a program called BOUNDER that proves inequalities between functions over finite sets of constraints. Previous inequality algorithms perform well on some subset of the elementary functions, but poorly elsewhere. To overcome this problem, BOUNDER maintains a hierarchy of increasingly complex algorithms. When one fails to resolve an inequality, it tries the next. This strategy resolves more inequalities than any single algorithm. It also performs well on hard problems without wasting time on easy ones. The current hierarchy consists of four algorithms bounds propagation, substitution, derivative inspection, and iterative approximation. Propagation is an extension of interval arithmetic that takes linear time, but ignores constraints between variables and multiple occurrences of variables. The remaining algorithms consider these factors, but require exponential time. Substitution is a new, provably correct, algorithm for utilizing constraints between variables. The final two algorithms analyze constraints between variables. Inspection examines the signs of partial derivatives. Iteration is based on several earlier algorithms from interval arithmetic.|Elisha Sacks","65099|AAAI|1987|Reasoning about Discontinuous Change|Intuitively, discontinuous changes can be seen as very rapid continuous changes. A couple of alternative methods based on this ontology are presented and compared. One, called the approximation method, approximates discontinuous change by continuous function and then calculates a limit. The other, called the direct method, directly creates a chain of hypothetical intermediate states (mythical instants) which a given circuit is supposed to go through during a discontinuous change. Although the direct method may fail to predict certain properties of discontinuity and its applicability is limited, it is more efficient than the approximation method. The direct method has been fully implemented and incorporated into an existing qualitative reasoning program.|Toyoaki Nishida,Shuji Doshita","66460|AAAI|2008|Knowledge-Based Spatial Reasoning for Scene Generation from Text Descriptions|This system translates basic English descriptions of a wide range of objects in a simplistic zoo environment into plausible, three-dimensional, interactive visualizations of their positions, orientations, and dimensions. It combines a semantic network and contextually sensitive knowledge base as representations for explicit and implicit spatial knowledge, respectively. Its linguistic aspects address underspecification, vagueness, uncertainty, and context with respect to intrinsic, extrinsic, and deictic frames of spatial reference. The underlying, commonsense reasoning fomlalism is probability-based geometric fields that are solved through constraint satisfaction. The architecture serves as an extensible test-and-evaluation framework for a multitude of linguistic and artificial-intelligence investigations.|Dan Tappan","65725|AAAI|2006|Bounded Treewidth as a Key to Tractability of Knowledge Representation and Reasoning|Several forms of reasoning in AI - like abduction, closed world reasoning, circumscription, and disjunctive logic programming - are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae or programs is bounded by some constant. Clearly, these theoretical tractability results as such do not immediately yield feasible algorithms. However, we have recently established a new method based on monadic datalog which allowed us to design an efficient algorithm for a related problem in the database area. In this work, we exploit the monadic datalog approach to construct new algorithms for logic-based abduction.|Georg Gottlob,Reinhard Pichler,Fang Wei"],["65980|AAAI|2007|Design of a Mechanism for Promoting Honesty in E-Marketplaces|In this paper, we explore the use of the web as an environment for electronic commerce. In particular, we develop a novel mechanism that creates incentives for honesty in electronic marketplaces where human users are represented by buying and selling agents. In our mechanism, buyers model other buyers and select the most trustworthy ones as their neighbors from which they can ask advice about sellers. In addition, however, sellers model the reputation of buyers. Reputable buyers provide fair ratings of sellers, and are likely to be neighbors of many other buyers. Sellers will provide more attractive products to reputable buyers, in order to build their reputation. We discuss how a marketplace operating with our mechanism leads to better profit both for honest buyers and sellers. With honesty encouraged, our work promotes the acceptance of web-based agent-oriented e-commerce by human users.|Jie Zhang,Robin Cohen","66880|AAAI|2010|Computationally Feasible Automated Mechanism Design General Approach and Case Studies|Hazards ubiquitously exist in combinational circuits, and then should be taken into account for delay testing. This paper analyzes the impact of hazards on small-delay defect (SDD) detection, and presents a new test pattern selection method considering hazards. The concept of arrival time window is introduced and the concept of output deviation is redefined to accurately reflect the pattern capability on SDD detection. A new signal transition probability calculation method is presented to calculate output deviation more practical than that without considering hazards. Patterns from an N-detect test set for transition faults are then selected according to their output deviations. Experimental results show that, for the same pattern count, the patterns selected by the proposed method excite more long paths, and are capable of detecting more small delay defects at the early stage of delay testing compared to the method without considering hazards.|Mingyu Guo,Vincent Conitzer","66244|AAAI|2007|Automated Online Mechanism Design and Prophet Inequalities|Recent work on online auctions for digital goods has explored the role of optimal stopping theory -- particularly secretary problems -- in the design of approximately optimal online mechanisms. This work generally assumes that the size of the market (number of bidders) is known a priori, but that the mechanism designer has no knowledge of the distribution of bid values. However, in many real-world applications (such as online ticket sales), the opposite is true the seller has distributional knowledge of the bid values (e.g., via the history of past transactions in the market), but there is uncertainty about market size. Adopting the perspective of automated mechanism design, introduced by Conitzer and Sandholm, we develop algorithms that compute an optimal, or approximately optimal, online auction mechanism given access to this distributional knowledge. Our main results are twofold. First, we show that when the seller does not know the market size, no constant-approximation to the optimum efficiency or revenue is achievable in the worst case, even under the very strong assumption that bid values are i.i.d. samples from a distribution known to the seller. Second, we show that when the seller has distributional knowledge of the market size as well as the bid values, one can do well in several senses. Perhaps most interestingly, by combining dynamic programming with prophet inequalities (a technique from optimal stopping theory) we are able to design and analyze online mechanisms which are temporally strategyproof (even with respect to arrival and departure times) and approximately efficiency (revenue)-maximizing. In exploring the interplay between automated mechanism design and prophet inequalities, we prove new prophet inequalities motivated by the auction setting.|Mohammad Taghi Hajiaghayi,Robert D. Kleinberg,Tuomas Sandholm","65353|AAAI|2005|Mechanism Design for Single-Value Domains|In \"Single-Value domains\", each agent has the same private value for all desired outcomes. We formalize this notion and give new examples for such domains. including a \"SAT domain\" and a \"single-value combinatorial auctions\" domain. We study two informational models where the set of desired outcomes is public information (the \"known\" case). and where it is private information (the \"unknown\" case). Under the \"known\" assumption, we present several truthful approximation mechanisms. Additionally, we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain (where agent values are either zero or one) to a truthful mechanism, with only a small approximation loss. In contrast, we show that even positive results from the \"unknown single minded combinatorial auctions\" literature fail to extend to the \"unknown\" single-value case. We give a characterization of truthfulness in this case, demonstrating that the difference is subtle and surprising.|Moshe Babaioff,Ron Lavi,Elan Pavlov","66240|AAAI|2007|Data Structures for Generalised Arc Consistency for Extensional Constraints|Extensional (table) constraints are an important tool for attacking combinatorial problems with constraint programming. Recently there has been renewed interest in fast propagation algorithms for these constraints. We describe the use of two alternative data structures for maintaining generalised arc consistency on extensional constraints. The first, the Next-Difference list, is novel and has been developed with this application in mind. The second, the trie, is well known but its use in this context is novel. Empirical analyses demonstrate the efficiency of the resulting approaches, both in GAC-schema, and in the watched-literal table constraint in Minion.|Ian P. Gent,Christopher Jefferson,Ian Miguel,Peter Nightingale","66360|AAAI|2008|Partially Synchronized DEC-MDPs in Dynamic Mechanism Design|In this paper, we combine for the first time the methods of dynamic mechanism design with techniques from decentralized decision making under uncertainty. Consider a multi-agent system with self-interested agents acting in an uncertain environment, each with private actions, states and rewards. There is also a social planner with its own actions, rewards, and states, acting as a coordinator and able to influence the agents via actions (e.g., resource allocations). Agents can only communicate with the center, but may become inaccessible, e.g., when their communication device fails. When accessible to the center, agents can report their local state (and models) and receive recommendations from the center about local policies to follow for the present period and also, should they become inaccessible, until becoming accessible again. Without self-interest, this poses a new problem class which we call partially-synchronized DEC-MDPs, and for which we establish some positive complexity results under reasonable assumptions. Allowing for self-interested agents, we are able to bridge to methods of dynamic mechanism design, aligning incentives so that agents truthfully report local state when accessible and choose to follow the prescribed \"emergency policies\" of the center.|Sven Seuken,Ruggiero Cavallo,David C. Parkes","66647|AAAI|2010|Approximation Algorithms and Mechanism Design for Minimax Approval Voting|This paper presents a multi-state hierarchical approach for facial feature tracking. A hierarchical formulation of statistical shape models is proposed to characterize both global shape constraints of human faces and local structural details of facial components. Gabor wavelets and gray level profiles are integrated for effective and efficient representation of feature points. Furthermore, multi-state local shape models are presented to deal with shape variations of facial components. Meanwhile, face pose estimation helps improve shape constraints for the feature search. Both facial component states and feature point positions are dynamically estimated using a multi-modal tracking approach. Experimental results demonstrate that the proposed method accurately and robustly tracks facial features under different facial expressions and pose variations|Ioannis Caragiannis,Dimitris Kalaitzis,Evangelos Markakis","66095|AAAI|2007|Logic for Automated Mechanism Design - A Progress Report|Over the past half decade, we have been exploring the use of logic in the specification and analysis of computational economic mechanisms. We believe that this approach has the potential to bring the same benefits to the design and analysis of computational economic mechanisms that the use of temporal logics and model checking have brought to the specification and analysis of reactive systems. In this paper, we give a survey of our work. We first discuss the use of cooperation logics such as Alternating-time Temporal Loglc (ATL) for the specification and verification of mechanisms such as social choice procedures. We motivate the approach, and then discuss the work we have done on extensions to ATL to support incomplete information, preferences, and quantification over coalition. We then discuss is the use of ATL-like cooperation logics in the development of social laws.|Michael Wooldridge,Thomas \u2026gotnes,Paul E. Dunne,Wiebe van der Hoek","65964|AAAI|2007|Partial Revelation Automated Mechanism Design|In most mechanism design settings, optimal general-purpose mechanisms are not known. Thus the automated design of mechanisms tailored to specific instances of a decision scenario is an important problem. Existing techniques for automated mechanism design (AMD) require the revelation of full utility information from agents, which can be very difficult in practice. In this work, we study the automated design of mechanisms that only require partial revelation of utilities. Each agent's type space is partitioned into a finite set of partial types, and agents (should) report the partial type within which their full type lies. We provide a set of optimization routines that can be combined to address the trade-offs between the amount of communication, approximation of incentive properties, and objective value achieved by a mechanism. This allows for the automated design of partial revelation mechanisms with worst-case guarantees on incentive properties for any objective function (revenue, social welfare, etc.).|Nathanael Hyafil,Craig Boutilier","66211|AAAI|2007|An Ironing-Based Approach to Adaptive Online Mechanism Design in Single-Valued Domains|Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method (\"ironing\") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.|David C. Parkes,Quang Duong"],["79921|VLDB|1978|EUFID The End User Friendly Interface to Data Management Systems|This paper describes a man-machine interface system, called EUFID, that will permit users of data management systems to communicate with those systems in natural language. At the same time, EUFID will act as a security screen to prevent unauthorized users from having access to particular fields in a data base. Our specific objective is to build a system that will be practical, efficient, and widely usable in existing, real-world applications. Our approach is to model the restricted set of linguistic structures and functions required for each application, rather than the manifold linguistic properties of natural language per se. This allows our system to be powerful enough to efficiently process English queries against specific data bases without attempting to understand forms of English that have little or no function in the contexts of those data bases.|I. Kameny,J. Weiner,M. Crilley,J. Burger,R. Gates,David Brill","79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80608|VLDB|2006|SPIDER a Schema mapPIng DEbuggeR|A schema mapping is a high-level declarative specification of how data structured under one schema, called the source schema, is to be transformed into data structured under a possibly different schema, called the target schema. We demonstrate SPIDER, a prototype tool for debugging schema mappings, where the language for specifying schema mappings is based on a widely adopted formalism. We have built SPIDER on top of a data exchange system, Clio, from IBM Almaden Research Center. At the heart of SPIDER is a data-driven facility for understanding a schema mapping through the display of routes. A route essentially describes the relationship between source and target data with the schema mapping. In this demonstration, we showcase our route engine, where we can display one or all routes starting from either source or target data, as well as the intermediary data and schema elements involved. In addition, we demonstrate \"standard\" debugging features for schema mappings that we have also built, such as computing and exploring routes step-by-step, stopping or pausing the computation with breakpoints, performing \"guided\" computation of routes by taking human input into account, as well as tracking the state of the target instance during the process of computing routes.|Bogdan Alexe,Laura Chiticariu,Wang Chiew Tan","80814|VLDB|2007|Semi-Automatic Schema Integration in Clio|Schema integration is the problem of finding a unified representation, called the integrated schema, from a set of source schemas that are related to each other. The relationships between the source schemas can be represented via correspondences between schema elements or via some other forms of schema mappings such as constraints or views. The integrated schema can be viewed as a means for dealing with the heterogeneity in the source schemas, by providing a standard representation of the data. Schema integration has received much of attention in the research literature , , , ,  and still remains a challenge in practice. Existing approaches require substantial amount of human feedback during the integration process and moreover, the outcome of these approaches is a single integrated schema. In general, however, there can be multiple possible schemas that integrate data in different ways and each may be valuable in a given scenario.|Laura Chiticariu,Mauricio A. Hernández,Phokion G. Kolaitis,Lucian Popa","80023|VLDB|1980|A Dialogue Interface for Data Base Applications|In this paper we describe the interface of the application support system ASS which will be integrated as a top level interface into the distributed DBMS POREL. We introduce the application description language ADL, which can be used for the description of applications and of the inputoutput data of application operators, and we describe the interface which is available for specifying these ADL descriptions. A dialog support is presented for the specification of the input data of an operator. Thus ASS can support a wide range of interactions between the user and a data base system. In addition we investigate the generation of operator occurrences from operator skeletons which provide a flexible connection of application operators to the data base system.|Rudi Studer","79808|VLDB|1975|Automatic Data Base Schema Design and Optimization|The production of an appropriate CODASYL Data Base Task Group (DBTG) Data Description Language (DDL) schema for a given data management application is a significant design problem. This research is devoted to the development of a methodology to automate and optimize the design of DBTG schema structures, using analytic modelling and optimization techniques. Given an implementation independent description of the data management requirements, it is possible to produce a schema configuration which is optimized with respect to logical record access rate, subject to storage and feasibility constraints, within a selected class of schemas. The storageaccess rate trade off is expressable as an integer program, which can be mapped into a network traversal problem with a known dynamic programming solution.|Michael F. Mitoma,Keki B. Irani","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80609|VLDB|2006|Incremental Schema Matching|The goal of schema matching is to identify correspondences between the elements of two schemas. Most schema matching systems calculate and display the entire set of correspondences in a single shot. Invariably, the result presented to the engineer includes many false positives, especially for large schemas. The user is often overwhelmed by all of the edges, annoyed by the false positives, and frustrated at the inability to see second- and third-best choices. We demonstrate a tool that circumvents these problems by doing the matching interactively. The tool suggests candidate matches for a selected schema element and allows convenient navigation between the candidates. The ranking of match candidates is based on lexical similarity, schema structure, element types, and the history of prior matching actions. The technical challenges are to make the match algorithm fast enough for incremental matching in large schemas and to devise a user interface that avoids overwhelming the user. The tool has been integrated with a prototype version of Microsoft BizTalk Mapper, a visual programming tool for generating XML-to-XML mappings.|Philip A. Bernstein,Sergey Melnik,John E. Churchill","80834|VLDB|2007|Self-Organizing Schema Mappings in the GridVine Peer Data Management System|GridVine is a Peer Data Management System based on a decentralized access structure. Built following the principle of data independence, it separates a logical layer -- where data, schemas and mappings are managed -- from a physical layer consisting of a structured Peer-to-Peer network supporting efficient routing of messages and index load-balancing. Our system is totally decentralized, yet it fosters semantic interoperability through pairwise schema mappings and query reformulation. In this demonstration, we present a set of algorithms to automatically organize the network of schema mappings. We concentrate on three key functionalities () the sharing of data, schemas and schema mappings in the network, () the dynamic creation and deprecation of mappings to foster global interoperability, and () the propagation of queries using the mappings. We illustrate these functionalities using bioinformatic schemas and data in a network running on several hundreds of peers simultaneously.|Philippe Cudré-Mauroux,Suchit Agarwal,Adriana Budura,Parisa Haghani,Karl Aberer"],["80418|VLDB|2004|Efficient XML-to-SQL Query Translation Where to Add the Intelligence|We consider the efficiency of queries generated by XML to SQL translation. We first show that published XML-to-SQL query translation algorithms are suboptimal in that they often translate simple path expressions into complex SQL queries even when much simpler equivalent SQL queries exist. There are two logical ways to deal with this problem. One could generate suboptimal SQL queries using a fairly naive translation algorithm, and then attempt to optimize the resulting SQL or one could use a more intelligent translation algorithm with the hopes of generating efficient SQL directly. We show that optimizing the SQL after it is generated is problematic, becoming intractable even in simple scenarios by contrast, designing a translation algorithm that exploits information readily available at translation time is a promising alternative. To support this claim, we present a translation algorithm that exploits translation time information to generate efficient SQL for path expression queries over tree schemas.|Rajasekar Krishnamurthy,Raghav Kaushik,Jeffrey F. Naughton","80311|VLDB|2003|Projecting XML Documents|XQuery is not only useful to query XML in databases, but also to applications that must process XML documents as files or streams. These applications suffer from the limitations of current main-memory XQuery processors which break for rather small documents. In this paper we propose techniques, based on a notion of projection for XML, which can be used to drastically reduce memory requirements in XQuery processors. The main contribution of the paper is a static analysis technique that can identify at compile time which parts of the input document are needed to answer an arbitrary XQuery. We present a loading algorithm that takes the resulting information to build a projected document, which is smaller than the original document, and on which the query yields the same result. We implemented projection in the Galax XQuery processor. Our experiments show that projection reduces memory requirements by a factor of  on average, and is effective for a wide variety of queries. In addition, projection results in some speedup during query evaluation.|Amélie Marian,Jérôme Siméon","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma \u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80599|VLDB|2006|FIX Feature-based Indexing Technique for XML Documents|Indexing large XML databases is crucial for efficient evaluation of XML twig queries. In this paper, we propose a feature-based indexing technique, called FIX, based on spectral graph theory. The basic idea is that for each twig pattern in a collection of XML documents, we calculate a vector of features based on its structural properties. These features are used as keys for the patterns and stored in a B+tree. Given an XPath query, its feature vector is first calculated and looked up in the index. Then a further refinement phase is performed to fetch the final results. We experimentally study the indexing technique over both synthetic and real data sets. Our experiments show that FIX provides great pruning power and could gain an order of magnitude performance improvement for many XPath queries over existing evaluation techniques.|Ning Zhang 0002,M. Tamer \u2013zsu,Ihab F. Ilyas,Ashraf Aboulnaga","66602|AAAI|2008|Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence AAAI  Chicago Illinois USA July - |In this paper, we propose a novel bit-stream switching scheme for the multiple bit-rate (MBR) video streaming, in which a Wyner-Ziv coded frame is used to overcome the mismatch between the MBR streams when the switching occurs. With the proposed technique, the MBR streams can be independently encoded without losing any coding efficiency. Similar to distributed video coding, the proposed Wyner-Ziv switching scheme also faces the challenge of rate allocation at the server side. To solve this problem, we propose a new correlation model based on the analysis on the reconstructed frames from the streams with different bit rates. Accordingly, the number of transmitted bits can be on-line calculated based on the correlation model without any feedback from the decoder. With the proposed technique, the actually transmitted Wyner-Ziv bits are only few more than the truly requested bits. However, the delay due to the bit requesting process can be avoided|Dieter Fox,Carla P. Gomes","66646|AAAI|2010|Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence AAAI  Atlanta Georgia USA July - |Numerous speech representations have been reported to be useful in speaker recognition. However, there is much less agreement on which speech representation provides a perfect representation of speaker-specific information conveyed in a speech signal. Unlike previous work, we propose an alternative approach to speaker modeling by the simultaneous use of different speech representations in an optimal way. Inspired by our previous empirical studies, we present a soft competition scheme on different speech representations to exploit different speech representations in encoding speaker-specific information. On the basis of this soft competition scheme, we present a parametric statistical model, generalized Gaussian mixture model (GGMM), to characterize a speaker identity based on different speech representations. Moreover, we develop an expectation-maximization algorithm for parameter estimation in the GGMM. The proposed speaker modeling approach has been applied to text-independent speaker recognition and comparative results on the KING speech corpus demonstrate its effectiveness.|Maria Fox,David Poole","80769|VLDB|2007|Structured Materialized Views for XML Queries|The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad  prototype and we present a performance analysis.|Andrei Arion,Véronique Benzaken,Ioana Manolescu,Yannis Papakonstantinou","80429|VLDB|2004|Indexing Temporal XML Documents|Different models have been proposed recently for representing temporal data, tracking historical information, and recovering the state of the document as of any given time, in XML documents. We address the problem of indexing temporal XML documents. In particular we show that by indexing continuous paths, i.e. paths that are valid continuously during a certain interval in a temporal XML graph, we can dramatically increase query performance. We describe in detail the indexing scheme, denoted TempIndex, and compare its performance against both a system based on a nontemporal path index, and one based on DOM.|Alberto O. Mendelzon,Flavio Rizzolo,Alejandro A. Vaisman"],["66869|AAAI|2010|EWLS A New Local Search for Minimum Vertex Cover|This paper presents a general approach to simulate heterogeneity of car-following behaviors in freeway traffic. This is achieved by proposing a new car-following model called general individual based car-following model(GIBM) which takes into account the characteristics of the individual driver. The model parameters are expressed as functions with regard to ldquoaggressiveness levelrdquo of drivers. The numerical simulation of the perturbation propagation in dense traffic flow by using the GIBM shows that small perturbations could develop into stable moving jams, growing moving jam, and decaying moving jams randomly, due to the heterogeneity of individual car-following behaviors in traffic flow. The result of the simulation is consistent with the empirical observation. The GIBM appears promising for application to microscopic simulations of traffic dynamics including randomness. It presents a general approach to simulate the heterogeneity of car-following behaviors efficiently and simply.|Shaowei Cai,Kaile Su,Qingliang Chen","66094|AAAI|2007|Synthesis of Constraint-Based Local Search Algorithms from High-Level Models|The gap in automation between MIPSAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location.|Pascal Van Hentenryck,Laurent D. Michel","65581|AAAI|2005|Domain-Dependent Parameter Selection of Search-based Algorithms Compatible with User Performance Criteria|Search-based algorithms, like planners, schedulers and satisfiability solvers, are notorious for having numerous parameters with a wide choice of values that can affect their performance drastically. As a result, the users of these algorithms, who may not be search experts, spend a significant time in tuning the values of the parameters to get acceptable performance on their particular problem domains. In this paper, we present a learning-based approach for automatic tuning of search-based algorithms to help such users. The benefit of our methodology is that it handles diverse parameter types, performs effectively for a broad range of systematic as well as non-systematic search based solvers (the selected parameters could make the algorithms solve up to % problems while the bad parameters would lead to none being solved), incorporates user-specified performance criteria () and is easy to implement. Moreover, the selected parameter will satisfy  in the first try or the ranked candidates can be used along with  to minimize the number of times the parameter settings need to he adjusted until a problem is solved.|Biplav Srivastava,Anupam Mediratta","65618|AAAI|2005|Simultaneous Heuristic Search for Conjunctive Subgoals|We study the problem of building effective heuristics for achieving cunjunctive goals from heuristics for individual goals. We consider a straightforward method for building conjunctive heuristics that smoothly trades off between previous common methods. In addition to first explicitly formulating the problem of designing conjunctive heuristics. our major contribution is the discovery that this straightforward method substantially outperforms previously used methods across a wide range of domains. Based on a single positive real parameter k, our heuristic measure sums the individual heuristic values for the subgoal conjuncts, each raised to the k'th power. Varying k allows loose approximation and combination of the previous min, max. and sum approaches, while mitigating some of the weaknesses in those approaches. Our empirical work shows that for many benchmark planning domains there exist fixed parameter values that perform well-- we give evidence that these values can be found automatically by training. Our method, applied to top-level conjunctive goals, shows dramatic improvements over the heuristic used in the FF planner across a wide range of planning competition benchmarks. Also, our heuristic, without computing landmarks, consistently improves upon the success ratio of a recently published landmark-based planner FF-L.|Lin Zhu,Robert Givan","65398|AAAI|2005|The Max Armed Bandit A New Model of Exploration Applied to Search Heuristic Selection|The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit--the Max K-Armed Bandit--in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperfonns the other approaches.|Vincent A. Cicirello,Stephen F. Smith","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80708|VLDB|2006|Similarity Search A Matching Based Approach|Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks ) it leaves many partial similarities uncovered ) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper.The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved, which is especially useful for information retrieval from multiple systems. We can also apply the proposed algorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that ) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities ) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.|Anthony K. H. Tung,Rui Zhang 0003,Nick Koudas,Beng Chin Ooi","65414|AAAI|2005|Cost-Algebraic Heuristic Search|Heuristic search is used to efficiently solve the single-node shortest path problem in weighted graphs. In practice, however, one is not only interested in finding a short path, but an optimal path, according to a certain cost notion. We propose an algebraic formalism that captures many cost notions, like typical Quality of Service attributes. We thus generalize A*, the popular heuristic search algorithm. for solving optimal-path problem. The paper provides an answer to a fundamental question for AI search, namely to which general notion of cost, heuristic search algorithms can be applied. We proof correctness of the algorithms and provide experimental results that validate the feasibility of the approach.|Stefan Edelkamp,Shahid Jabbar,Alberto Lluch-Lafuente","65939|AAAI|2006|A Breadth-First Approach to Memory-Efficient Graph Search|Recent work shows that the memory requirements of A* and related graph-search algorithms can be reduced substantially by only storing nodes that are on or near the search frontier, using special techniques to prevent node regeneration, and recovering the solution path by a divide-and-conquer technique. When this approach is used to solve graph-search problems with unit edge costs, we have shown that a breadth-first search strategy can be more memory-efficient than a best-first strategy. We provide an overview of our work using this approach, which we call breadth-first heuristic search.|Rong Zhou,Eric A. Hansen","80391|VLDB|2004|High Performance Index Build Algorithms for Intranet Search Engines|There has been a substantial amount of research on high-performance algorithms for constructing an inverted text index. However, constructing the inverted index in a intranet search engine is only the final step in a more complicated index build process. Among other things, this process requires an analysis of all the data being indexed to compute measures like PageRank. The time to perform this global analysis step is significant compared to the time to construct the inverted index, yet it has not received much attention in the research literature. In this paper, we describe how the use of slightly outdated information from global analysis and a fast index construction algorithm based on radix sorting can be combined in a novel way to significantly speed up the index build process without sacrificing search quality.|Marcus Fontoura,Eugene J. Shekita,Jason Y. Zien,Sridhar Rajagopalan,Andreas Neumann"],["66038|AAAI|2007|Probabilistic Community Discovery Using Hierarchical Latent Gaussian Mixture Model|Complex networks exist in a wide array of diverse domains, ranging from biology, sociology, and computer science. These real-world networks, while disparate in nature, often comprise of a set of loose clusters(a.k.a communities), whose members are better connected to each other than to the rest of the network. Discovering such inherent community structures can lead to deeper understanding about the networks and therefore has raised increasing interests among researchers from various disciplines. This paper describes GWN-LDA (Generic weighted network-Latent Dirichlet Allocation) model, a hierarchical Bayesian model derived from the widely-received LDA model, for discovering probabilistic community profiles in social networks. In this model, communities are modeled as latent variables and defined as distributions over the social actor space. In addition, each social actor belongs to every community with different probability. This paper also proposes two different network encoding approaches and explores the impact of these two approaches to the community discovery performance. This model is evaluated on two research collaborative networks CiteSeer and NanoSCI. The experimental results demonstrate that this approach is promising for discovering community structures in large-scale networks.|Haizheng Zhang,C. Lee Giles,Henry C. Foley,John Yen","66622|AAAI|2010|Integrating Sample-Based Planning and Model-Based Reinforcement Learning|This paper introduces an abstract high dependability framework for the implementation of embedded control software with hard real-time constraints. The framework specifies time-triggered sensor readings, atomic component invocations, actuator updates, and pattern switches independent of any implementation platform. In order to leverage model continuity, XML-based description of composite component informal description is required, which supports reuse of components and model information interoperability. By separating the platform-independent from the platform-dependent concerns, Consider a quality process control system in steel industry on a distributed real-time embedded environment, we implement a simplified high dependability design framework to prove the feasibility in the validation and synthesis of embedded control component execution.|Thomas J. Walsh,Sergiu Goschin,Michael L. Littman","65587|AAAI|2005|Observation-based Model for BDI-Agents|We present a new computational model of BDI-agents, called the observation-based BDI-model. The key point of this BDI-model is to express agents' beliefs, desires and intentions as a set of runs (computing paths), which is exactly a system in the interpreted system model, a well-known agent model due to Halpern and his colleagues. Our BDI-model is computationally grounded in that we are able to associate the BDI-agent model with a computer program, and formulas, involving agents' beliefs, desires (goals) and intentions, can be understood as properties of program computations. We present a sound and complete proof system with respect to our BDI-model and explore how symbolic model checking techniques can be applied to model checking BDI-agents. In order to make our BDI-model more flexible and practically realistic, we generalize it so that agents can have multiple sources of beliefs, goals and intentions.|Kaile Su,Abdul Sattar,Kewen Wang,Xiangyu Luo,Guido Governatori,Vineet Padmanabhan","79869|VLDB|1977|A Simulation Model for Performance Analysis of Large Shared Data Bases|This paper describes a data base simulator, or rather, a family of simulators, that is being developed at the University of Stockholm. The purpose of this discrete event simulation model is to provide a tool for  investigating the performance of data base oriented information systems (existing and planned)  increasing the knowledge of i) design of data base oriented information systems, ii) design of DBMS (or setting parameters when generating a DBMS). A typical target system would be a heavily loaded multiprogrammed on-line system with a very large data base. One of the guidelines for design of the simulator has been user orientation. The preparation and execution of the model is done interactively and on a problem-oriented leve , e.g. the analyst describes the data base with DDLDML instructions in a DBTG-like language. It is possible to model hierarchical as well as network-based data bases.|Christer Hulten,Lars Söderlund","65320|AAAI|2004|Encoding Probabilistic Causal Model in Probabilistic Action Language|Pearl's probabilistic causal model has been used in many domains to reason about causality. Pearl's treatment of actions is very diffewnt from the way actions are represented explicitly in action languages. In this paper we show how to encode Pearl's probabilistic causal model in the action language PAL thus relating this two distinct approaches to reasoning about actions.|Nam Tran,Chitta Baral","65381|AAAI|2005|An Inference Model for Semantic Entailment in Natural Language|Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.|Rodrigo de Salvo Braz,Roxana Girju,Vasin Punyakanok,Dan Roth,Mark Sammons","65785|AAAI|2006|Performing Incremental Bayesian Inference by Dynamic Model Counting|The ability to update the structure of a Bayesian network when new data becomes available is crucial for building adaptive systems. Recent work by Sang, Beame, and Kautz (AAAI ) demonstrates that the well-known Davis-Putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in Bayesian networks with high density and width. In this paper, we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple runs on a series of problems with similar structure. This allows us to perform Bayesian inference incrementally as the structure of the network changes. Experimental results show that our approach yields significant improvements over the previous model counting approaches on multiple challenging Bayesian network instances.|Wei Li 0002,Peter van Beek,Pascal Poupart","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir","65558|AAAI|2005|Performing Bayesian Inference by Weighted Model Counting|Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman ). Solving such NP-complete tasks by \"compilation to SAT\" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. ) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether \"compilation to model-counting\" could be a practical technique for solving real-world P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.|Tian Sang,Paul Beame,Henry A. Kautz","80333|VLDB|2003|A Platform Based on the Multi-dimensional Data Model for Analysis of Bio-Molecular Structures|A platform called AnMol for supporting analytical applications over structural data of large biomolecules is described. The term \"biomolecular structure\" has various connotations and different representations. AnMol reduces these representations into graph structures. Each of these graphs are then stored as one or more vectors in a database. Vectors encapsulate structural features of these graphs. Structural queries like similarity and substructure are transformed into spatial constructs like distance and containment within regions. Query results are based on inexact matches. A refinement mechanism is supported for increasing accuracy of the results. Design and implementation issues of AnMol including schema structure and performance results are discussed in this paper.|Srinath Srinivasa,Sujit Kumar"],["65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","65741|AAAI|2006|Identifiability in Causal Bayesian Networks A Sound and Complete Algorithm|This paper addresses the problem of identifying causal effects from nonexperimental data in a causal Bayesian network, i.e., a directed acyclic graph that represents causal relationships. The identifiability question asks whether it is possible to compute the probability of some set of (effect) variables given intervention on another set of (intervention) variables, in the presence of non-observable (i.e., hidden or latent) variables. It is well known that the answer to the question depends on the structure of the causal Bayesian network, the set of observable variables, the set of effect variables, and the set of intervention variables. Our work is based on the work of Tian, Pearl, Huang, and Valtorta (Tian & Pearl a b  Huang & Valtorta a) and extends it. We show that the identify algorithm that Tian and Pearl define and prove sound for semi-Markovian models can be transfered to general causal graphs and is not only sound, but also complete. This result effectively solves the identifiability question for causal Bayesian networks that Pearl posed in  (Pearl ), by providing a sound and complete algorithm for identifiability.|Yimin Huang,Marco Valtorta","66607|AAAI|2010|Latent Variable Model for Learning in Pairwise Markov Networks|In this paper, an efficient rate-control scheme for H.AVC video encoding is proposed. The redesign of the quantization scheme in H.AVC results in that the relationship between the quantization parameter and the true quantization stepsize is no longer linear. Based on this observation, we propose a new rate-distortion (R-D) model by utilizing the true quantization stepsize and then develop an improved rate-control scheme for the H.AVC encoder based on this new R-D model. In general, the current R-D optimization (RDO) mode-selection scheme in H.AVC test model is difficult for rate control, because rate control usually requires a predetermined set of motion vectors and coding modes to select the quantization parameter, whereas the RDO does in the different order and requires a predetermined quantization parameter to select motion vectors and coding modes. To tackle this problem, we develop a complexity-adjustable rate-control scheme based on the proposed R-D model. Briefly, the proposed scheme is a one-pass process at frame level and a partial two-pass process at macroblock level. Since the number of macroblocks with the two-pass processing can be controlled by an encoder parameter, the fully one-pass implementation is a subset of the proposed algorithm. An additional topic discussed in this paper is about video buffering. Since a hypothetical reference decoder (HRD) has been defined in H.AVC to guarantee that the buffers never overflow or underflow, the more accurate rate-allocation schemes are proposed to satisfy these requirements of HRD.|Saeed Amizadeh,Milos Hauskrecht","65591|AAAI|2005|Approximate Inference of Bayesian Networks through Edge Deletion|In this paper, we introduce two new algorithms for approximate inference of Bayesian networks that use edge deletion techniques. The first reduces a network to its maximal weight spanning tree using the Kullback-Leibler information divergence as edge weights, and then runs Pearl's algorithm on the resulting tree for linear-time inference. The second algorithm deletes edges from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound, thus placing a polynomial time bound on inference. When tested for efficiency, these two algorithms perform up to , times faster than exact techniques. See www.cis.ksu.edujasresearch.html for more information.|Julie Thornton","80389|VLDB|2004|Model-Driven Data Acquisition in Sensor Networks|Declarative queries are proving to be an attractive paradigm for ineracting with networks of wireless sensors. The metaphor that \"the sensornet is a database\" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.|Amol Deshpande,Carlos Guestrin,Samuel Madden,Joseph M. Hellerstein,Wei Hong","66181|AAAI|2007|Adaptive Traitor Tracing with Bayesian Networks|The practical success of broadcast encryption hinges on the ability to () revoke the access of compromised keys and () determine which keys have been compromised. In this work we focus on the latter, the so-called traitor tracing problem. We present an adaptive tracing algorithm that selects forensic tests according to the information gain criteria. The results of the tests refine an explicit, Bayesian model of our beliefs that certain keys are compromised. In choosing tests based on this criteria, we significantly reduce the number of tests, as compared to the state-of-the-art techniques, required to identify compromised keys. As part of the work we developed an efficient, distributable inference algorithm that is suitable for our application and also give an efficient heuristic for choosing the optimal test.|Philip Zigoris,Hongxia Jin","66281|AAAI|2008|A General Framework for Generating Multivariate Explanations in Bayesian Networks|Many existing explanation methods in Bayesian networks, such as Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE), generate complete assignments for target variables. A priori, the set of target variables is often large, but only a few of them may be most relevant in explaining given evidence. Generating explanations with all the target variables is hence not always desirable. This paper addresses the problem by proposing a new framework called Most Relevant Explanation (MRE), which aims to automatically identify the most relevant target variables. We will also discuss in detail a specific instance of the framework that uses generalized Bayes factor as the relevance measure. Finally we will propose an approximate algorithm based on Reversible Jump MCMC and simulated annealing to solve MRE. Empirical results show that the new approach typically finds much more concise explanations than existing methods.|Changhe Yuan,Tsai-Ching Lu","66097|AAAI|2007|Macroscopic Models of Clique Tree Growth for Bayesian Networks|In clique tree clustering, inference consists of propagation in a clique tree compiled from a Bayesian network. In this paper, we develop an analytical approach to characterizing clique tree growth as a function of increasing Bayesian network connectedness, specifically (i) the expected number of moral edges in their moral graphs or (ii) the ratio of the number of non-root nodes to the number of root nodes. In experiments, we systematically increase the connectivity of bipartite Bayesian networks, and find that clique tree size growth is well-approximated by Gompertz growth curves. This research improves the understanding of the scaling behavior of clique tree clustering, provides a foundation for benchmarking and developing improved BN inference algorithms, and presents an aid for analytical trade-off studies of tree clustering using growth curves.|Ole J. Mengshoel","65014|AAAI|1987|Modular Learning in Neural Networks|In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.|Dana H. Ballard","66671|AAAI|2010|GTPA A Generative Model For Online Mentor-Apprentice Networks|The D standard k -  model coupled with the volume of fluid (VOF) method of water-air two-phase flow is established in long-distance diversion tunnel with free flow. The method of wall functions which considers the roughness of the walls is utilized near the walls, and the method of pressure-implicit with splitting of operators (PISO) is applied to solve the unsteady water-air two-phase flow field. The results show that water level in the tunnel increase with filling time and tend to arrive at a steady state. The dynamic analysis results of water-air two-phase flow of typical section in water-filling process show that when the inlet flow rate is  ms, the flow can pass through the tunnel smoothly. The different water levels in the drainage and emptying tunnel of a reservoir are calculated. The calculation results agree well with the experimental ones.|Muhammad Aurangzeb Ahmad,David Huffaker,Jing Wang,Jeffrey William Treem,Marshall Scott Poole,Jaideep Srivastava"]]}}