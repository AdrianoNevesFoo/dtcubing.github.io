{"abstract":{"entropy":7.015724071054453,"topics":["recent years, neural network, constraint satisfaction, cartesian genetic, play role, markov decision, wide range, programming cartesian, sensor network, artificial intelligence, petri nets, agents, markov processes, decision processes, graph edge, vector machine, distributed system, grid computing, support vector, spanning tree","presents novel, presents based, speech noise, ultra wideband, speech enhancement, zero-correlation zone, sequence zone, access control, paper, ultra uwb, motion estimation, particle swarm, ldpc decoder, low-density parity-check, paper based, proposes, space-time block, proposes novel, paper scheme, orthogonal division","speech recognition, search engine, natural language, knowledge base, xml data, data stream, describes speech, web search, materialized views, web sites, game players, acoustic speech, question answering, feature extraction, xml document, describes system, widely used, query database, query processing, information retrieval","genetic algorithm, evolutionary algorithm, genetic programming, algorithm, presents algorithm, search algorithm, optimization problem, algorithm problem, evolutionary computation, algorithm optimization, consider problem, presents approach, algorithm solving, presents learning, presents model, solving problem, optimization, problem, learning classifier, heuristic search","problem graph, quantified boolean, algorithm graph, graph edge, spanning tree, spanning vertex, widely used, graph genetic, graph vertices, powerful tool, combinatorial problem, solving problem, minimum spanning, graph vertex, shortest path, bayesian classification, graph, cellular automata, undirected vertex, intelligent tutoring","neural network, markov decision, wide range, sensor network, problem network, network, markov processes, decision processes, bayesian network, consider problem, markov mdps, decision mdps, processes mdps, web services, mobile devices, data management, management system, mobile network, wireless network, partially observable","speech noise, speech enhancement, paper based, presents based, ldpc decoder, control system, based, system based, paper algorithm, paper presented, estimation noise, algorithm based, paper, wavelet transform, reduction noise, decoding algorithm, paper system, reduction technique, algorithm proposed, color simulator","adaptive algorithm, estimation distribution, based adaptive, design system, presents design, presents adaptive, proposes based, space-time block, proposes novel, design, proposes, proposes algorithm, adaptive system, adaptive, timing synchronization, paper adaptive, block code, space-time code, presents stereo, proposes system","natural language, search engine, knowledge base, xml document, web search, web sites, information retrieval, web, paper knowledge, knowledge, semantic web, information integration, represent data, web engine, information, web information, search retrieval, similarity measures, presents web, engine retrieval","speech recognition, describes system, widely used, describes speech, describes, acoustic speech, automatic speech, language generation, novel approach, modeling speech, system translation, speech system, automatic asr, generation applications, model recognition, hardware verification, robust speech, spontaneous speech, technique speech, describes approach","learning classifier, classifier xcs, based theory, presents structure, crossover operator, situations calculus, learning xcs, partially observable, synthesis nonautonomous, simd synthesis, algorithm structure, necessary conditions, necessary sufficient, structure protein, calculus actions, description logic, system lcs, sufficient conditions, learning lcs, problem discrete-time","search algorithm, evolutionary search, heuristic search, ant colony, fitness functions, search, testing test, local search, algorithm applied, genetic fitness, algorithm fitness, heuristic algorithm, algorithm local, functions, different fitness, solutions fitness, software evolution, search problem, algorithm test, fitness landscape"],"ranking":[["43062|IEICE Transations|2006|Time Complexity Analysis of the Legal Firing Sequence Problem of Petri Nets with Inhibitor Arcs|Petri nets with inhibitor arcs are referred to as inhibitor-arc Petri nets. It is shown that modeling capability of inhibitor-arc Petri nets is equivalent to that of Turing machines. The subject of this paper is the legal firing sequence problem (INLFS) for inhibitor-arc Petri nets given an inhibitor-arc Petri net IN, an initial marking M and a firing count vector X, find a firing sequence  such that its firing starts from M and each transition t appears in  exactly X(t) times as prescribed by X. The paper is the first step of research for time complexity analysis and designing algorithms of INLFS, one of the most fundamental problems for inhibitor-arc Petri nets having more modeling capability than ordinary Peri nets. The recognition version of INLFS, denoted as RINLFS, means a decision problem, asking a \"yes\" or \"no\" answer on the existence of a solution  to INLFS. The main results are the following () and (). () Proving (-) and (-) when the underlying Petri net of IN is an unweighted state machine (-) INLFS can be solved in pseudo-polynomial (O(X)) time for IN of non-adjacent type having only one special place called a rivet (-) RINLFS is NP-hard for IN with at least three rivets () Proving that RINLFS for IN whose underlying Petri net is unweighted and forward conflict-free is NP-hard. Heuristic algorithms for solving INLFS are going to be proposed in separate papers.|Satoshi Taoka,Toshimasa Watanabe","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","42977|IEICE Transations|2005|Analysis on the Parameters of the Evolving Artificial Agents in Sequential Bargaining Game|Over the past few years, a considerable number of studies have been conducted on modeling the bargaining game using artificial agents on within-model interaction. However, very few attempts have been made at study on the interaction and co-evolutionary process among heterogeneous artificial agents. Therefore, we present two kinds of artificial agents, based on genetic algorithm (GA) and reinforcement learning (RL), which play a game on between-model interaction. We investigate their co-evolutionary processes and analyze their parameters using the analysis of variance.|Seokcheol Chang,Joung-Il Yun,Ju-Sang Lee,Sang-Uk Lee,Nitaigour-Premchand Mahalik,Byung-Ha Ahn","16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","16267|IJCAI|2005|Conditional Planning in the Discrete Belief Space|Probabilistic planning with observability restrictions, as formalized for example as partially observable Markov decision processes (POMDP), has a wide range of applications, but it is computationally extremely difficult. For POMDPs, the most general decision problems about existence of policies satisfying certain properties are undecidable. We consider a computationally easier form of planning that ignores exact probabilities, and give an algorithm for a class of planning problems with partial observability. We show that the basic backup step in the algorithm is NP-complete. Then we proceed to give an algorithm for the backup step, and demonstrate how it can be used as a basis of an efficient algorithm for constructing plans.|Jussi Rintanen","65597|AAAI|2005|CORMS AI Decision Support System for Monitoring US Maritime Environment|Rule based reasoning and case based reasoning have emerged as two important and complementary reasoning methodologies in artificial intelligence (AI). This paper describes the approach for the development of CORMS AI, a decision support system which employs rule-based and case-based reasoning to assist NOAA's Center for Operational Oceanographic Products and Services watch standing personnel in monitoring the quality of marine environmental data and information. CORMS AI has been in operation since July . The system accurately and reliably identifies suspect data and network disruptions, and has decreased the amount of time it takes to identify and troubleshoot sensor, network, and server failures. CORMS AI has proven to be robust, extendable, and cost effective. It is estimated that CORMS AI will save government over one million dollars per year when its full range of quality control monitoring capabilities is implemented.|Haleh Vafaie,Carl Cecere","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65422|AAAI|2005|Efficient Maximization in Solving POMDPs|We present a simple, yet effective improvement to the dynamic programming algorithm for solving partially observable Markov decision processes. The technique targets the vector pruning operation during the maximization step, a key source of complexity in POMDP algorithms. We identify two types of structures in the belief space and exploit them to reduce significantly the number of constraints in the linear programs used for pruning. The benefits of the new technique are evaluated both analytically and experimentally, showing that it can lead to significant performance improvement. The results open up new research opportunities to enhance the performance and scalability of several POMDP algorithms.|Zhengzhu Feng,Shlomo Zilberstein","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aur√©lie Beynier,Abdel-Illah Mouaddib"],["43211|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Construction Using a Cyclic Hadamard Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone (hereinafter binary ZCA sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCA sequence set by using a cyclic difference set and a collection of mutually orthogonal complementary sets.|Takafumi Hayashi","43038|IEICE Transations|2006|Simulation of Interference Effects from MB-OFDM and DS-UWB to a QPSK Digital Transmission System|This paper reports on a study of the interference effects from  types of ultra wideband (UWB) sources on a QPSK transmission system by simulation. The culprit UWB sources were multi-band orthogonal frequency-division multiple-access (MB-OFDM) and direct-sequence UWB (DS-UWB), which were modeled on the proposal specifications in the IEEE ..a to standardize high-speed wireless personal area networks. Average bit error rate (BER) degradation of the victim system was evaluated under in-band interference from the UWB signals. The proposed modified equivalent baseband system was employed in the simulation in order to reduce the simulation costs. Interference effects from the UWB sources were also examined under a Rayleigh fading channel.|Atsushi Tomiki,Idnin Pasya,Takehiko Kobayashi","42665|IEICE Transations|2005|Low-Complexity Viterbi Equalizer for MBOK DS-UWB Systems|This paper presents a low-complexity equalization for M-ary biorthogonal keying based direct sequence ultra wideband (MBOK DS-UWB) systems. We focus on a Viterbi equalizer, which is based on maximum likelihood sequence estimation (MLSE). To reduce the computational complexity of MLSE-based equalizer, we use two strategies. One is the use of delayed-decision feedback sequence estimation (DDFSE), which is a hybrid estimation between MLSE and decision feedback estimation (DFE). And the other is the truncation of state transition in MLSE by considering MBOK pulse mapping. The reduced complexity sequence estimation is named as reduced state (RS)-DDFSE. By the use of RS-DDFSE, the complexity of Viterbi equalizer for MBOK DS-UWB is significantly reduced, by comparison with that of MLSE. The performance of RS-DDFSE based equalizer is evaluated on multipath fading channel models provided by IEEE..a. An analysis on trellis diagram of RS-DDFSE and simulation results show that the impact on error rate performance generated by the complexity lower is slight.|Kenichi Takizawa,Ryuji Kohno","43279|IEICE Transations|2006|Families of Sequence Pairs with Zero Correlation Zone|A family of sequences with zero correlation zone, which is shortly called a ZCZ set, can provide CDMA system without co-channel interference nor influence of multipath. This paper presents two types of ZCZ sets of non-binary sequence pairs, which achieve the upper bound of family size for length and zero correlation zone. One, which is produced by use of a perfect complementary pair and an orthogonal code, can change zero correlation zone, while the upper bound is kept. The other, which is generated by use of a newly defined orthogonal pair and an orthogonal code, can offer such CDMA system as a binary ZCZ set seems to be used.|Shinya Matsufuji","43408|IEICE Transations|2006|PSWF-Based Direct-Sequence UWB Transmission Using Orthogonal Ternary Code Sets|An enhanced Ultra Wideband (UWB) signaling scheme that employs PSWF (Prolate Spheroidal Wave Functions)-based orthogonal chip pulses and ternary complementary code sets is proposed for Direct-Sequence (DS) UWB systems. Every information bit of each user is modulated and transmitted over a set of parallel sequences of PSWF-based orthogonal chip pulses and are further assigned to a ternary complementary code set with additional zero padding if necessary. Moreover, the ternary complementary code sets are generated to be mutually orthogonal and assigned to any pair of multiple users. Hence, the mitigation of multipath interference as well as multiple user interference (MUI) can be expected. Furthermore, the ternary code length can be greatly shortened by taking advantage of pulse and code orthogonality. Thus, the proposed transmission scheme is especially suitable for high data rate DS-UWB systems that offer very high flexibility.|Chihong Cho,Honggang Zhang,Masao Nakagawa","42722|IEICE Transations|2005|Bounds on Aperiodic Autocorrelation and Crosscorrelation of Binary LCZZCZ Sequences|In order to eliminate the co-channel and multi-path interference of quasi-synchronous code division multiple access (QS-CDMA) systems, spreading sequences with low or zero correlation zone (LCZ or ZCZ) can be used. The significance of LCZZCZ to QS-CDMA systems is that, even there are relative delays between the transmitted spreading sequences due to the inaccurate access synchronization and the multipath propagation, the orthogonality (or quasi-orthogonality) between the transmitted signals can still be maintained, as long as the relative delay does not exceed certain limit. In this paper, several lower bounds on the aperiodic autocorrelation and crosscorrelation of binary LCZZCZ sequence set with respect to the family size, sequence length and the aperiodic low or zero correlation zone, are derived. The results show that the new bounds are tighter than previous bounds for the LCZZCZ sequences.|Daiyuan Peng,Pingzhi Fan,Naoki Suehiro","43367|IEICE Transations|2006|VLSI Design of a Fully-Parallel High-Throughput Decoder for Turbo Gallager Codes|The most powerful channel coding schemes, namely those based on turbo codes and low-density parity-check (LDPC) Gallager codes, have in common the principle of iterative decoding. However, the relative coding structures and decoding algorithms are substantially different. This paper presents a -bit, rate- soft decision decoder for a new class of codes known as Turbo Gallager Codes. These codes are turbo codes with properly chosen component convolutional codes such that they can be successfully decoded by means of the decoding algorithm used for LDPC codes, i.e., the belief propagation algorithm working on the code Tanner graph. These coding schemes are important in practical terms for two reasons (i) they can be encoded as classical turbo codes, giving a solution to the encoding problem of LDPC codes (ii) they can also be decoded in a fully parallel manner, partially overcoming the routing congestion bottleneck of parallel decoder VLSI implementations thanks to the locality of the interconnections. The implemented decoder can support up to  Gbits data rate and performs up to  decoding iterations ensuring both high throughput and good coding gain. In order to evaluate the performance and the gate complexity of the decoder VLSI architecture, it has been synthesized in a . m standard-cell CMOS technology.|Luca Fanucci,Pasquale Ciao,Giulio Colavolpe","43089|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Constructed from an M-Sequence|The present paper introduces an improved construction of a class of binary sequences having a zero-correlation zone (hereafter binary ZCZ sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCZ sequence set from an arbitrary M-sequence. The previously reported sequence construction of binary ZCZ sequence sets from an M-sequence can generate a single series of binary ZCZ sequence sets from an M-sequence. The present paper proposes an improved sequence construction that can generate more than one series of binary ZCZ sequence sets from an M-sequence.|Takafumi Hayashi","43185|IEICE Transations|2006|On Optimal Construction of Two Classes of ZCZ Codes|This paper presents constructions of two kinds of sets of sequences with a zero correlation zone, called ZCZ code, which can reach the upper bound of the member size of the sequence set. One is a ZCZ code which can be constructed by a unitary matrix and a perfect sequence. Especially, a ternary perfect sequence with elements  and zero can be used to construct the proposed ZCZ code. The other is a ZCZ code of pairs of ternary sequences and binary sequences which can be constructed by an orthogonal matrix that includes a Hadamard matrix and an orthogonal sequence pair. As a special case, an orthogonal sequence pair, which consists of a ternary sequence and a binary sequence, can be used to construct the proposed ZCZ code. These codes can provide CDMA systems without co-channel interference.|Takafumi Hayashi,Shinya Matsufuji","43181|IEICE Transations|2006|Impact of Timing Jitter on DS-UWB and Hybrid DS-Multiband-UWB Systems with Rake Reception over Multipath Environment|In this paper, the impact of timing jitter in direct sequence ultra wideband (DS-UWB) system is investigated over multipath fading channel. Also, a novel hybrid direct sequence multiband UWB (DS-MB-UWB) system is proposed to mitigate the impact of timing jitter. We analyze and compare the system performance for conventional DS-UWB and hybrid DS-MB-UWB with Rake receiver in the presence of timing jitter over additive white Gaussian noise (AWGN) and multipath channel. Theoretical framework is developed to calculate the amount of average energy captured in the multipath profiles and symbol error rate (SER) considering the presence of timing jitter. It is found that DS-MB-UWB system, which employs multiple sub-bands is more jitter-robust than conventional DS-UWB systems. Besides, timing jitter is found to have different impacts on DS-UWB and DS-MB-UWB systems corresponding to different parameters such as number of sub-bands employed, pulse shape, center frequency, bandwidth, number of combined paths in Rake receiver and channel power delay profile (PDP). These different impacts are analyzed and discussed in the paper.|Chin-Sean Sum,Shigenobu Sasaki,Hisakazu Kikuchi"],["43003|IEICE Transations|2005|Processing Aggregate Queries with Materialized Views in Data Warehouse Environment|Materialized views, which are derived from base relations and stored in the database, offer opportunities for significant performance gain in query evaluation by providing quick access to the pre-computed data. A materialized view can be utilized in evaluating a query if it has pre-computed result of some part of the query plan. Although many approaches to utilizing materialized views in evaluating a query have been proposed, there exist several restrictions in selecting such views. This paper proposes new ways of utilizing materialized views in answering an aggregate query. Views including relations that are not referred to in the given query are utilized. Attributes missing from a view can be recovered under certain conditions. We identify the conditions where a view may be used in evaluating a query and present the algorithm to search for the most efficient query among the equivalent ones. We also report on a simulation based on the TPC-H and GRID databases. Simulation results show that our approach provides impressive performance improvements to the data warehousing environment where aggregate views are often pre-computed and materialized.|Jae-young Chang,Han-joon Kim","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","42355|IEICE Transations|2005|Improving Keyword Recognition of Spoken Queries by Combining Multiple Speech Recognizers Outputs for Speech-driven WEB Retrieval Task|This paper presents speech-driven Web retrieval models which accept spoken search topics (queries) in the NTCIR- Web retrieval task. The major focus of this paper is on improving speech recognition accuracy of spoken queries and then improving retrieval accuracy in speechdriven Web retrieval. We experimentally evaluated the techniques of combining outputs of multiple LVCSR models in recognition of spoken queries. As model combination techniques, we compared the SVM learning technique with conventional voting schemes such as ROVER. In addition, for investigating the effects on the retrieval performance in vocabulary size of the language model, we prepared two kinds of language models the one's vocabulary size was ,, the other's one was ,. Then, we evaluated the differences in the recognition rates of the spoken queries and the retrieval performance. We showed that the techniques of multiple LVCSR model combination could achieve improvement both in speech recognition and retrieval accuracies in speech-driven text retrieval. Comparing with the retrieval accuracies when an LM with a ,, vocabulary size is used in an LVCSR system, we found that the larger the vocabulary size is, the better the retrieval accuracy is.|Masahiko Matsushita,Hiromitsu Nishizaki,Takehito Utsuro,Seiichi Nakagawa","43591|IEICE Transations|2006|An Efficient Schema-Based Technique for Querying XML Data|As data integration over the Web has become an increasing demand, there is a growing desire to use XML as a standard format for data exchange. For sharing their grammars efficiently, most of the XML documents in use are associated with a document structure description, such as DTD or XML schema. However, the document structure information is not utilized efficiently in previously proposed techniques of XML query processing. In this paper, we present a novel technique that reduces the disk IO complexity of XML query processing. We design a schema-based numbering scheme called SPAR that incorporates both structure information and tag names extracted from DTD or XML schema. Based on SPAR, we develop a mechanism called VirtualJoin that significantly reduces disk IO workload for processing XML queries. As shown by experiments, VirtualJoin outperforms many prior techniques.|Dao Dinh Kha,Masatoshi Yoshikawa","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","42345|IEICE Transations|2005|Recent Progress in Corpus-Based Spontaneous Speech Recognition|This paper overviews recent progress in the development of corpus-based spontaneous speech recognition technology. Although speech is in almost any situation spontaneous, recognition of spontaneous speech is an area which has only recently emerged in the field of automatic speech recognition. Broadening the application of speech recognition depends crucially on raising recognition performance for spontaneous speech. For this purpose, it is necessary to build large spontaneous speech corpora for constructing acoustic and language models. This paper focuses on various achievements of a Japanese -year national project \"Spontaneous Speech Corpus and Processing Technology\" that has recently been completed. Because of various spontaneous-speech specific phenomena, such as filled pauses, repairs, hesitations, repetitions and disfluencies, recognition of spontaneous speech requires various new techniques. These new techniques include flexible acoustic modeling, sentence boundary detection, pronunciation modeling, acoustic as well as language model adaptation, and automatic summarization. Particularly automatic summarization including indexing, a process which extracts important and reliable parts of the automatic transcription, is expected to play an important role in building various speech archives, speech-based information retrieval systems, and human-computer dialogue systems.|Sadaoki Furui","43660|IEICE Transations|2006|A New Question Answering System for Chinese Restricted Domain|In this paper, we propose the construction of a web-based Question Answering (QA) system for restricted domain, which combines three resource information databases for the retrieval mechanism, including a Question&Answer database, a special domain documents database and the web resource retrieved by Google search engine. We describe a new retrieval technique of integrating a probabilistic technique based on OkapiBM and a semantic analysis which based on the ontology of HowNet knowledge base and a special domain HowNet created for the restricted domain. Furthermore, we provide a method of question expansion by computing word semantic similarity. The system is first developed for a middle-size domain of sightseeing information. The experiments proved the efficiency of our method for restricted domain and it is feasible to transfer to other domains expediently using the proposed method.|Haiqing Hu,Peilin Jiang,Fuji Ren,Shingo Kuroiwa","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57609|GECCO|2006|An agent-based algorithm for generalized graph colorings|This paper presents an algorithm for solving a number of generalized graph coloring problems. Specifically, it gives an agent-based algorithm for the Bandwidth Coloring problem. Using a standard method for preprocessing the input, the same algorithm can also be used to solve the Multicoloring and Bandwidth Multicoloring problems. In the algorithm a number of agents, called ants, each of which colors a portion of the graph, collaborate to obtain a coloring of the entire graph. This coloring is then further improved by a local optimization algorithm. Experimental results on a set of benchmark graphs for these generalized coloring problems show that this algorithm performs very well compared to other heuristic approaches.|Thang Nguyen Bui,ThanhVu H. Nguyen","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","57466|GECCO|2005|Evolutionary change in developmental timing|This paper presents a mutation-based evolutionary algorithm that evolves genotypic genes for regulating developmental timing of phenotypic values. The genotype sequentially generates a given number of entire phenotypes and then finishes its life at each generation. Each genotypic gene represents a cycle time of changing probability to determine its corresponding phenotypic value in a life span of the genotype. This cycle time can be considered to be a sort of information on developmental timing. Furthermore, the algorithm has a learning mechanism for genotypic genes representing a long cycle time to change the probability more adaptively than those representing a short cycle time. Therefore, it can be expected that the algorithm brings different evolution speed to each phenotypic value. The experimental results show that the algorithm can identify building blocks of uniformly-scaled problems sequentially and also that a population size required for solving the problems is quite small but the number of function evaluations required is sub-exponential scale-up with the problem size.|Kei Ohnishi,Kaori Yoshida","57297|GECCO|2005|Solving geometric TSP with ants|This paper presents an ant-based approach for solving the Traveling Salesman Problem (TSP). Novel concepts of this algorithm that distinguish it from the other heuristics are the inclusion of a preprocessing stage and the use of a modified version of an ant-based approach with local optimization in multi stages. Experimental results show that this algorithm outperforms ACS  and is comparable to MMAS  for Euclidean TSP instances. Of the  instances of Euclidean TSP from TSPLIB  that were tested, this algorithm found the optimal solution for  instances. For the remaining instances, this algorithm returned solutions that were within .% of the optimum.|Thang Nguyen Bui,Mufit Colpan","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57347|GECCO|2005|A new evolutionary method for time series forecasting|This paper presents a new method --- the Time-delay Added Evolutionary Forecasting (TAEF) method --- for time series prediction which performs an evolutionary search of the minimum necessary number of dimensions embedded in the problem for determining the characteristic phase space of the time series. The method proposed is inspired in F. Takens theorem and consists of an intelligent hybrid model composed of an artificial neural network (ANN) combined with a modified genetic algorithm (GA). Initially, the TAEF method finds the most fitted predictor model for representing the series and then performs a behavioral statistical test in order to adjust time phase distortions.|Tiago A. E. Ferreira,Germano C. Vasconcelos,Paulo J. L. Adeodato","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa"],["42731|IEICE Transations|2005|A Linear Time Algorithm for Bi-Connectivity Augmentation of Graphs with Upper Bounds on Vertex-Degree Increase|The -vertex-connectivity augmentation problem of a graph with degree constraints, VCA-DC, is defined as follows \"Given an undirected graph G  (V, E) and an upper bound a(vG)  Z+   on vertex-degree increase for each v  V, find a smallest set E' of edges such that (V, E  E') has at least two internally-disjoint paths between any pair of vertices in V and such that vertex-degree increase of each v  V by the addition of E' to G is at most a(vG), where Z+ is the set of nonnegative integers.\" In this paper we show that checking the existence of a feasible solution and finding an optimum solution to VCA-DC can be done in O(V + E) time.|Takanori Fukuoka,Toshiya Mashima,Satoshi Taoka,Toshimasa Watanabe","57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","43079|IEICE Transations|2006|A Minimum Feedback Vertex Set in the Trivalent Cayley Graph|In this paper, we study the feedback vertex set problem for trivalent Cayley graphs, and construct a minimum feedback vertex set in trivalent Cayley graphs using the result on cube-connected cycles and the Cayley graph representation of trivalent Cayley graphs.|Yuuki Tanaka,Yukio Shibata","43729|IEICE Transations|2006|Increasing the Edge-Connectivity by Contracting a Vertex Subset in Graphs|Let G  (V,E) be an edge weighted graph with n vertices and m edges. For a given integer p with  p n, we call a set X  V of p vertices a p-maximizer if X has a property that the edge-connectivity of the graph obtained by contracting X into a single vertex is no less than that of the graph obtained by contracting any other subset of p vertices. In this paper, we first show that there always exists an ordering v,v,...,vn of vertices in V such that, for each i  ,,...,n - , set v,v,...,vi is an i-maximizer. We give an O(mn + nlog n) time algorithm for finding such an ordering and then show an application to the source location problem.|Hiroshi Nagamochi","43205|IEICE Transations|2006|Maximum-Cover Source-Location Problems|Given a graph G  (V,E), a set of vertices S  V covers   V if the edge connectivity between S and  is at least a given number k. Vertices in S are called sources. The source location problem is a problem of finding a minimum-size source set covering all vertices of a given graph. This paper presents a new variation of the problem, called maximum-cover source-location problem, which finds a source set S with a given size p, maximizing the sum of the weight of vertices covered by S. It presents an O(np + m + n log n)-time algorithm for k  , where n  V and m  E. Especially it runs linear time if G is connected. This algorithm uses a subroutine for finding a subtree with the maximum weight among p-leaf trees of a given vertex-weighted tree. For the problem we give a greedy-based linear-time algorithm, which is an extension of the linear-time algorithm for finding a longest path of a given tree presented by E. W. Dijkstra around . Moreover, we show some polynomial solvable cases, e.g., a given graph is a tree or (k - )-edge-connected, and NP-hard cases, e.g., a vertex-cost function is given or G is a digraph.|Kenya Sugihara,Hiro Ito","42587|IEICE Transations|2005|A Self-Stabilizing Approximation Algorithm for the Distributed Minimum -Domination|Self-stabilization is a theoretical framework of nonmasking fault-tolerant distributed algorithms. In this paper, we investigate a self-stabilizing distributed approximation for the minimum k-dominating set (KDS) problem in general networks. The minimum KDS problem is a generalization of the well-known dominating set problem in graph theory. For a graph G  (V, E), a set Dk  V is a KDS of G if and only if each vertex not in Dk is adjacent to at least k vertices in Dk. The approximation ratio of our algorithm is k( + (k-+)), where  is the maximum degree of G, in the networks of which the minimum degree is more than or equal to k.|Sayaka Kamei,Hirotsugu Kakugawa","57610|GECCO|2006|An ant-based algorithm for finding degree-constrained minimum spanning tree|A spanning tree of a graph such that each vertex in the tree has degree at most d is called a degree-constrained spanning tree. The problem of finding the degree-constrained spanning tree of minimum cost in an edge weighted graphis well known to be NP-hard. In this paper we give an Ant-Based algorithm for finding low cost degree-constrained spanning trees. Ants are used to identify a set of candidate edges from which a degree-constrained spanning tree can be constructed. Extensive experimental results show that the algorithm performs very well against other algorithms on a set of  problem instances.|Thang Nguyen Bui,Catherine M. Zrncic","43435|IEICE Transations|2006|A Polynomial Time Algorithm for Obtaining a Minimum Vertex Ranking Spanning Tree in Outerplanar Graphs|The minimum vertex ranking spanning tree problem is to find a spanning tree of G whose vertex ranking is minimum. This problem is NP-hard and no polynomial time algorithm for solving it is known for non-trivial classes of graphs other than the class of interval graphs. This paper proposes a polynomial time algorithm for solving the minimum vertex ranking spanning tree problem on outerplanar graphs.|Shin-ichi Nakayama,Shigeru Masuyama","43421|IEICE Transations|2006|Bi-Connectivity Augmentation for Specified Vertices of a Graph with Upper Bounds on Vertex-Degree Increase|The -vertex-connectivity augmentation problem for a specified set of vertices of a graph with degree constraints, VCA-SV-DC, is defined as follows \"Given an undirected graph G  (V,E), a specified set of vertices S  V with S   and a function g  V  Z+  , find a smallest set E' of edges such that (V,E  E') has at least two internally-disjoint paths between any pair of vertices in S and such that vertex-degree increase of each v  V by the addition of E' to G is at most g(v), where Z+ is the set of nonnegative integers.\" This paper shows a linear time algorithm for VCA-SV-DC.|Toshiya Mashima,Takanori Fukuoka,Satoshi Taoka,Toshimasa Watanabe","57390|GECCO|2005|The blob code is competitive with edge-sets in genetic algorithms for the minimum routing cost spanning tree problem|Among the many codings of spanning trees for evolutionary search are those based on bijections between Prfer strings---strings of n- vertex labels---and spanning trees on the labeled vertices. One of these bijections, called the Blob Code, showed promise as an evolutionary coding, but EAs that use it to represent spanning trees have not performed well. Here, a genetic algorithm that represents spanning trees via the Blob Code is faster than, and returns results competitive with those of, a GA that encodes spanning trees as edge-sets on Euclidean instances of the minimum routing cost spanning tree problem. On instances whose edge weights have been chosen at random, the Blob-coded GA maintains its time advantage, but its results are inferior to those of the edge-set-coded GA, and both GAs are hard pressed to keep up with a simple stochastic hill-climber on all the test instances.|Bryant A. Julstrom"],["16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","16190|IJCAI|2005|An MCMC Approach to Solving Hybrid Factored MDPs|Hybrid approximate linear programming (HALP) has recently emerged as a promising framework for solving large factored Markov decision processes (MDPs) with discrete and continuous state and action variables. Our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. We analyze this problem and propose a novelMarkov chainMonte Carlo (MCMC) method for finding the most violated constraint of a relaxed HALP. This method does not require the discretization of continuous variables, searches the space of constraints intelligently based on the structure of factored MDPs, and its space complexity is linear in the number of variables. We test the method on a set of large control problems and demonstrate improvements over alternative approaches.|Branislav Kveton,Milos Hauskrecht","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","16267|IJCAI|2005|Conditional Planning in the Discrete Belief Space|Probabilistic planning with observability restrictions, as formalized for example as partially observable Markov decision processes (POMDP), has a wide range of applications, but it is computationally extremely difficult. For POMDPs, the most general decision problems about existence of policies satisfying certain properties are undecidable. We consider a computationally easier form of planning that ignores exact probabilities, and give an algorithm for a class of planning problems with partial observability. We show that the basic backup step in the algorithm is NP-complete. Then we proceed to give an algorithm for the backup step, and demonstrate how it can be used as a basis of an efficient algorithm for constructing plans.|Jussi Rintanen","43600|IEICE Transations|2006|Design of a Mobile Application Framework with Context Sensitivities|New service concepts involving mobile devices with a diverse range of embedded sensors are emerging that share contexts supporting communication on a wireless network infrastructure. To promote these services in mobile devices, we propose a method that can efficiently detect a context provider by partitioning the location, time, speed, and discovery sensitivities.|Hyung-Min Yoon,WooShik Kang,Oh-Young Kwon,Seong-Hun Jeong,Bum-Seok Kang,Tack-Don Han","65523|AAAI|2005|Markov Decision Processes for Control of a Sensor Network-based Health Monitoring System|Optimal use of energy is a primary concern in fielddeployable sensor networks. Artificial intelligence algorithms offer the capability to improve the performance or sensor networks in dynamic environments by minimizing energy utilization while not compromising overall performance. However, they have been used only to a limited extent in sensor networks primarily due to their expensive computing requirements. We describe the use of Markov decision processes for the adaptive control of sensor sampling rates in a sensor network used for human health monitoring. The MDP controller is designed to gather optimal information about the patient's health while guaranteeing a minimum lifetime of the system. At every control step, the MDP controller varies the frequency at which the data is collected according to the criticality of the patient's health at that time. We present a stochastic model that is used to generate the optimal policy offline. In cases where a model of the observed process is not available a-priori. we descrihe a Q-learning technique to learn the control policy, by using a pre-existing master controller. Simulation results that illustrate the performance of the controller are presented.|Anand Panangadan,Syed Muhammad Ali,Ashit Talukder","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong"],["43234|IEICE Transations|2006|Speech Noise Reduction System Based on Frequency Domain ALE Using Windowed Modified DFT Pair|The speech noise reduction system based on the frequency domain adaptive line enhancer using a windowed modified DFT (MDFT) pair is presented. The adaptive line enhancer (ALE) is effective for extracting sinusoidal signals blurred by a broadband noise. In addition, it utilizes only one microphone. Therefore, it is suitable for the realization of speech noise reduction in portable electronic devices. In the ALE, an input signal is generated by delaying a desired signal using the decorrelation parameter, which makes the noise in the input signal decorrelated with that in the desired one. In the present paper, we propose to set decorrelation parameters in the frequency domain and adjust them to optimal values according to the relationship between speech and noise. Such frequency domain decorrelation parameters enable the reduction of the computational complexity of the proposed system. Also, we introduce the window function into MDFT for suppressing spectral leakage. The performance of the proposed noise reduction system is examined through computer simulations.|Isao Nakanishi,Yuudai Nagata,Takenori Asakura,Yoshio Itoh,Yutaka Fukui","42748|IEICE Transations|2005|Performance Improvement for Distributed Active Noise Control Systems Based on Simultaneous Equations Method|For multiple-channel active noise control (ANC) systems, distributed systems consisting of more than one controller are useful. In this paper, we propose a performance improvement algorithm for the distributed multiple-channel ANC system based on the simultaneous equations method. In the proposed algorithm, no estimation of error paths is required. This algorithm can provide good performance in canceling primary noises with auto-cross-correlations and achieve stable noise reduction under a change of the error paths.|Mitsuji Muneyasu,Ken'ichi Kagawa,Kensaku Fujii,Takao Hinamoto","43118|IEICE Transations|2006|A Noise Reduction System for Wideband and Sinusoidal Noise Based on Adaptive Line Enhancer and Inverse Filter|A noise reduction technique to reduce wideband and sinusoidal noise in a noisy speech is proposed. In an actual environment, background noise includes not only wideband noise but also sinusoidal noise, such as ventilation fan and engine noise. In this paper, we propose a new noise reduction system which uses two types of adaptive line enhancers (ALE) and a noise estimation filter (NEF). First, the two ALEs are used to estimate speech components. The first ALE is used to reduce sinusoidal noise superposed on speech and wideband noise, while the second ALE is used to reduce wideband noise superposed on speech. However, since the quality of the speech enhanced by two ALEs is not good enough due to the difficulty in estimating unvoiced sound using the two ALEs, the NEF is used to improve on noise reduction capability. The NEF accurately estimates the background noise from the signal occupied by noise components, which is obtained by subtracting the speech enhanced by two ALEs from noisy speech. The enhanced speech is obtained by subtracting the estimated noise from noisy speech. Furthermore, the noise reduction system with feedback path is proposed to improve further the quality of enhanced speech.|Naoto Sasaoka,Keisuke Sumi,Yoshio Itoh,Kensaku Fujii,Arata Kawamura","43444|IEICE Transations|2006|A Microcalcification Detection Using Adaptive Contrast Enhancement on Wavelet Transform and Neural Network|Microcalcification detection is an important part of early breast cancer detection. In this paper, we propose a microcalcification detection algorithm using adaptive contrast enhancement in a mammography CAD (computer-aided diagnosis) system. The proposed microcalcification detection algorithm includes two parts. One is adaptive contrast enhancement in which the enhancement filtering parameters are determined based on noise characteristics of the mammogram. The other is a multi-stage microcalcification detection. The results show that the proposed microcalcification detection algorithm is much more robust against fluctuating noisy environments.|Ho Kyung Kang,Yong Man Ro,Sung-Min Kim","43405|IEICE Transations|2006|A Two-Stage Method for Single-Channel Speech Enhancement|A time domain (TD) speech enhancement technique to improve SNR in noise-contaminated speech is proposed. Additional supplementary scheme is applied to estimate the degree of noise of noisy speech. This is estimated from a function, which is previously prepared as the function of the parameter of the degree of noise. The function is obtained by least square (LS) method using the given degree of noise and the estimated parameter of the degree of noise. This parameter is obtained from the autocorrelation function (ACF) on frame-by-frame basis. This estimator almost accurately estimates the degree of noise and it is useful to reduce noise. The proposed method is based on two-stage processing. In the first stage, subtraction in time domain (STD), which is equivalent to ordinary spectral subtraction (SS), is carried out. In the result, the noise is reduced to a certain level. Further reduction of noise and by-product noise residual is carried out in the second stage, where blind source separation (BSS) technique is applied in time domain. Because the method is a single-channel speech enhancement, the other signal is generated by taking the noise characteristics into consideration in order to apply BSS. The generated signal plays a very important role in BSS. This paper presents an adaptive algorithm for separating sources in convolutive mixtures modeled by finite impulse response (FIR) filters. The coefficients of the FIR filter are estimated from the decorrelation of two mixtures. Here we are recovering only one signal of interest, in particular the voice of primary speaker free from interfering noises. In the experiment, the different levels of noise are added to the clean speech signal and the improvement of SNR at each stage is investigated. The noise types considered initially in this study consist of the synthesized white and color noise with SNR set from  to  dB. The proposed method is also tested with other real-world noises. The results show that the satisfactory SNR improvement is attained in the two-stage processing.|Mohammad E. Hamid,Takeshi Fukabayashi","42874|IEICE Transations|2005|An ICA-Domain Shrinkage Based Poisson-Noise Reduction Algorithm and Its Application to Penumbral Imaging|Penumbral imaging is a technique which exploits the fact that spatial information can be recovered from the shadow or penumbra that an unknown source casts through a simple large circular aperture. Since the technique is based on linear deconvolution, it is sensitive to noise. In this paper, a two-step method is proposed for decoding penumbral images first, a noise-reduction algorithm based on ICA-domain (independent component analysis-domain) shrinkage is applied to smooth the given noise second, the conventional linear deconvolution follows. The simulation results show that the reconstructed image is dramatically improved in comparison to that without the noise-removing filters, and the proposed method is successfully applied to real experimental X-ray imaging.|Xian-Hua Han,Zensho Nakao,Yen-Wei Chen,Ryosuke Kodama","42375|IEICE Transations|2005|Improving the Performance of the Minimum Statistics Noise Estimator for Single Channel Speech Enhancement|This paper proposes an algorithm to improve the performance of the noise power spectrum estimation using the minimum statistics (MS). The minimum statistics noise estimator (MSNE) that is most efficient for speech enhancement often underestimates noise power when the signal characteristics changes abruptly. The proposed algorithm improves the accuracy of noise estimation by removing harmonic components of the speech signal. Simulation results verify that the performance of the proposed algorithm is better than that of the conventional algorithm in terms of the segmental SNR (SegSNR) and the spectral distance (SD).|Seung-Kyun Ryu,Hong-Goo Kang,Sung-Kyo Jung,Dae Hee Youn","42472|IEICE Transations|2005|A Noise Reduction Method Based on Linear Prediction with Variable Step-Size|A noise reduction technique that uses the linear prediction to remove noise components in speech signals has been proposed previously. The noise reduction works well for additive white noise signals, because the coefficients of the linear predictor converge such that the prediction error becomes white. In this method, the linear predictor is updated by a gradient-based algorithm with a fixed step-size. However, the optimal value of the step-size changes with the values of the prediction coefficients. In this paper, we propose a noise reduction system using the linear predictor with a variable step-size. The optimal value of the step-size depends also on the variance of the white noise, however the variance is unknown. We therefore introduce a speechnon-speech detector, and estimate the variance in non-speech segments where the observed signal includes only noise components. The simulation results show that the noise reduction capability of the proposed system is better than that of the conventional one with a fixed step-size.|Arata Kawamura,Youji Iiguni,Yoshio Itoh","42717|IEICE Transations|2005|Speech Enhancement by Spectral Subtraction Based on Subspace Decomposition|This paper presents a novel algorithm for spectral subtraction (SS). The method is derived from a relation between the spectrum obtained by the discrete Fourier transform (DFT) and that by a subspace decomposition method. By using the relation, it is shown that a noise reduction algorithm based on subspace decomposition is led to an SS method in which noise components in an observed signal are eliminated by subtracting variance of noise process in the frequency domain. Moreover, it is shown that the method can significantly reduce computational complexity in comparison with the method based on the standard subspace decomposition. In a similar manner to the conventional SS methods, our method also exploits the variance of noise process estimated from a preceding segment where speech is absent, whereas the noise is present. In order to more reliably detect such non-speech segments, a novel robust voice activity detector (VAD) is then proposed. The VAD utilizes the spread of eigenvalues of an autocorrelation matrix corresponding to the observed signal. Simulation results show that the proposed method yields an improved enhancement quality in comparison with the conventional SS based schemes.|Takahiro Murakami,Tetsuya Hoya,Yoshihisa Ishida","42696|IEICE Transations|2005|Spectrum Estimation by Noise-Compensated Data Extrapolation|High-resolution spectrum estimation techniques have been extensively studied in recent publications. Knowledge of the noise variance is vital for spectrum estimation from noise-corrupted observations. This paper presents the use of noise compensation and data extrapolation for spectrum estimation. We assume that the observed data sequence can be represented by a set of autoregressive parameters. A recently proposed iterative algorithm is then used for noise variance estimation while autoregressive parameters are used for data extrapolation. We also present analytical results to show the exponential decay characteristics of the extrapolated samples and the frequency domain smoothing effect of data extrapolation. Some statistical results are also derived. The proposed noise-compensated data extrapolation approach is applied to both the autoregressive and FFT-based spectrum estimation methods. Finally, simulation results show the superiority of the method in terms of bias reduction and resolution improvement for sinusoids buried in noise.|Jonah Gamba,Tetsuya Shimamura"],["42420|IEICE Transations|2005|Globally Guaranteed Robustness Adaptive Fuzzy Control with Application on Highly Uncertain Robot Manipulators|This study proposes a novel adaptive fuzzy control methodology to remove disadvantages of traditional fuzzy approximation based control. Meanwhile, the highly uncertain robot manipulator is taken as an application with either guaranteed robust tracking performances or asymptotic stability in a global sense. First, the design concept, namely, feedforward fuzzy approximation based control, is introduced for a simple uncertain system. Here the desired commands are utilized as the inputs of the Takagi-Sugeno (T-S) fuzzy system to closely compensate the unknown feedforward term required during steady state. Different to traditional works, the assumption on bounded fuzzy approximation error is not needed, while this scheme allows easier implementation architecture. Next, the concept is extended to controlling manipulators and achieves global robust tracking performances. Note that a linear matrix inequality (LMI) technique is applied and provides an easier gain design. Finally, numerical simulations are carried out on a two-link robot to illustrate the expected performances.|Chian-Song Chiu","42533|IEICE Transations|2005|A Frequency Domain Nonlinearity for Stereo Echo Cancellation|This letter proposes a novel nonlinear distortion for the unique identification of receiving room impulses in stereo acoustic echo cancellation when applying the frequency-domain adaptive filtering technique. This nonlinear distortion is effective in reducing the coherence between the two incoming audio channels and its influence on audio quality is inaudible.|Ming Wu,Zhibin Lin,Xiaojun Qiu","42511|IEICE Transations|2005|On the Use of Wavelet Packets in Ultra Wideband Pulse Shape Modulation Systems|This paper proposes wavelet packets for use in ultra wideband communications. The pulse shapes that are generated are quasi orthogonal and have almost identical time duration. After normalization, an M-ary signaling set can be constructed allowing higher data rate. Finally, the performance of such a system when multipath propagation occurs is investigated by computer simulations. In order to combat multipath fading, a Rake receiver using coherent channel estimation is designed. This channel estimation is carried out using adaptive algorithms such as least-mean square (LMS), normalized least-mean square (NLMS), or recursive least square (RLS) algorithms which adapt the received signal given a reference signal.|St√©phane Ciolino,Mohammad Ghavami,Hamid Aghvami","42459|IEICE Transations|2005|Near-Field Sound-Source Localization Based on a Signed Binary Code|This paper proposes near-field sound-source localization based on crosscorrelation of a signed binary code. The signed binary code eliminates multibit signal processing for simpler implementation. Explicit formulae with near-field assumption are derived for a two microphone scenario and extended to a three microphone case with front-rear discrimination. Adaptive threshold for enabling and disabling source localization is developed for robustness in noisy environment. The proposed sound-source localization algorithm is implemented on a fixed-point DSP. Evaluation results in a robot scenario demonstrate that near-field assumption and front-rear discrimination provides almost % improvement in DOA estimation. A correct detection rate of % is obtained by a robot in a home environment.|Miki Sato,Akihiko Sugiyama,Osamu Hoshuyama,Nobuyuki Yamashita,Yoshihiro Fujita","43204|IEICE Transations|2006|Robust Beamforming of Microphone Array Using Adaptive Filtering Technique|In ASR (Automatic Speech Recognition) applications, one of the most important issues in the real-time beamforming of microphone arrays is the inability to capture the whole acoustic dynamics via a finite-length of data and a finite number of array elements. For example, the reflected source signal impinging from the side-lobe direction presents a coherent interference, and the non-minimal phase channel dynamics may require an infinite amount of data in order to achieve perfect equalization (or inversion). All these factors appear as uncertainties or un-modeled dynamics in the receiving signals. Traditional adaptive algorithms such as NLMS that do not consider these errors will result in performance deterioration. In this paper, a time domain beamformer using H filtering approach is proposed to adjust the beamforming parameters. Furthermore, this work also proposes a frequency domain approach called SPFDBB (Soft Penalty Frequency Domain Block Beamformer) using H filtering approach that can reduce computational efforts and provide a purified data to the ASR application. Experimental results show that the adaptive H filtering method is robust to the modeling errors and suppresses much more noise interference than that in the NLMS based method. Consequently, the correct rate of ASR is also enhanced.|Jwu-Sheng Hu,Wei-Han Liu,Chieh-Cheng Cheng","43562|IEICE Transations|2006|Novel Block Motion Estimation Based on Adaptive Search Patterns|An improved algorithm for fast motion estimation based on the block matching algorithm (BMA) is presented for use in a block-based video coding system. To achieve enhanced motion estimation performance, we propose an adaptive search pattern length for each iteration for the current macro block (MB). In addition, search points that must be checked are determined by means of directional information from the error surface, thus reducing intermediate searches. The proposed algorithm is tested with several sequences and excellent performance is verified.|Byung-Gyu Kim,Seon-Tae Kim,Seok-Kyu Song,Pyeong-Soo Mah","43070|IEICE Transations|2006|Projection Based Adaptive Window Size Selection for Efficient Motion Estimation in HAVC|This paper introduces a block based motion estimation algorithm based on projection with adaptive window size selection. The blocks cannot match well if their corresponding D projection does not match well, with this as foundation D block matching problem is translated to a simpler D matching, which eliminates majority of potential pixel participation. This projection method is combined with adaptive window size selection in which, appropriate search window for each block is determined on the basis of motion vectors and prediction errors obtained for the previous block, which makes this novel method several times faster than exhaustive search with negligible performance degradation. Encoding QCIF size video by the proposed method results in reduction of computational complexity of motion estimation by roughly % and over all encoding by %, while maintaining imagevideo quality.|Anand Paul,Jhing-Fa Wang,Jia-Ching Wang,An-Chao Tsai,Jang-Ting Chen","43171|IEICE Transations|2006|A Software Definable Architecture for Adaptive Space Diversity at Handsets in MC-CDMA Systems|Software-Defined Radio (SDR) represents a major paradigm shift in the design of radios, allowing a large fraction of the functionality to be implemented through programmable signal processing devices, enabling the radio to change its operating parameters to accommodate new air interface, features and capabilities. However, the actual realization of innovative and software-reconfigurable receiver diversity at mobile handsets in intermediate frequency band to provide wide-ranging benefits, including more effective filtered result, less cost of the mixed channel access, improved capacity, better link reliability, and reduced power consumption, has been slowed down largely due to an absence of effective architecture reducing the complexity of adaptive combining algorithms. This paper proposes a novel reconfigurable architecture for adaptive space diversity at handsets in MC-CDMA (multicode code-division multiple-access) systems. The key to which is the development of a valid and effective alternative to the time-consuming multiplication operation and despreading acquisition. A software definable algorithm can become a multiplier-free architecture if it can restrict the weight factors to power-of-two values and repetitive gradient search procedure to contain shift operations and predicate functions. The results of numerical simulation and experimentation confirm the expectation that the constrained approach should perform comparably to, but not better than the traditional diversity algorithm. That is, the feasibility of SDR depends on its trading some performance for reduced computational complexity, improved area efficiency and less power consumption.|K. Robert Lai,Yuan-Lung Chang","42578|IEICE Transations|2005|Spatio-Temporal Equalization for Space-Time Block Coded Transmission over Frequency Selective Fading Channel with Co-channel Interference|In this paper, we propose a spatio-temporal equalizer for the space-time block coded transmission over the frequency selective fading channels with the presence of co-channel interference (CCI). The proposed equalizer, based on the tapped delay line adaptive array (TDLAA), performs signal equalization and CCI suppression simultaneously using the minimum mean square error (MMSE) method. It is to show that our scheme outperforms the previous two-stage combined adaptive antenna and delayed decision feedback sequence estimator (DDFSE) approach. We also show that performance can be further improved if the synchronization between the preceding and delayed paths is achieved.|Xuan Nam Tran,Tetsuki Taniguchi,Yoshio Karasawa","42963|IEICE Transations|2005|Reconfigurable Adaptive FEC System Based on Reed-Solomon Code with Interleaving|This paper proposes a reconfigurable adaptive FEC system based on Reed-Solomon (RS) code with interleaving. In adaptive FEC schemes, error correction capability t is changed dynamically according to the communication channel condition. For given error correction capability t, we can implement an optimal RS decoder composed of minimum hardware units for each t. If the hardware units of the RS decoder can be reduced for any given error correction capability t, we can embed as large deinterleaver as possible into the RS decoder for each t. Reconfiguring the RS decoder embedded with the expanded deinterleaver dynamically for each error correction capability t allows us to decode larger interleaved codes which are more robust error correction codes to burst errors. In a reliable transport protocol, experimental results show that our system achieves up to % lower packet error rate and .% higher data transmission throughput compared to the adaptive FEC scheme on a conventional fixed hardware system. In an unreliable transport protocol, our system achieves up to % better bit error performance with higher code rate compared to the adaptive FEC scheme on a conventional fixed hardware system.|Kazunori Shimizu,Nozomu Togawa,Takeshi Ikenaga,Satoshi Goto"],["16174|IJCAI|2005|Automatic Hypertext Keyphrase Detection|This paper describes initial experiments in applying knowledge derived from hypertext structure to domain-specific automatic keyphrase extraction. It is found that hyperlink information can improve the effectiveness of automatic keyphrase extraction by %. However, the primary goal of this project is to apply similar techniques to information retrieval tasks such as web searching. These initial results show promise for the applicability of these techniques to more far-reaching tasks.|Daniel Kelleher,Saturnino Luz","42355|IEICE Transations|2005|Improving Keyword Recognition of Spoken Queries by Combining Multiple Speech Recognizers Outputs for Speech-driven WEB Retrieval Task|This paper presents speech-driven Web retrieval models which accept spoken search topics (queries) in the NTCIR- Web retrieval task. The major focus of this paper is on improving speech recognition accuracy of spoken queries and then improving retrieval accuracy in speechdriven Web retrieval. We experimentally evaluated the techniques of combining outputs of multiple LVCSR models in recognition of spoken queries. As model combination techniques, we compared the SVM learning technique with conventional voting schemes such as ROVER. In addition, for investigating the effects on the retrieval performance in vocabulary size of the language model, we prepared two kinds of language models the one's vocabulary size was ,, the other's one was ,. Then, we evaluated the differences in the recognition rates of the spoken queries and the retrieval performance. We showed that the techniques of multiple LVCSR model combination could achieve improvement both in speech recognition and retrieval accuracies in speech-driven text retrieval. Comparing with the retrieval accuracies when an LM with a ,, vocabulary size is used in an LVCSR system, we found that the larger the vocabulary size is, the better the retrieval accuracy is.|Masahiko Matsushita,Hiromitsu Nishizaki,Takehito Utsuro,Seiichi Nakagawa","65827|AAAI|2006|Corpus-based and Knowledge-based Measures of Text Semantic Similarity|This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to % error rate reduction with respect to the traditional vector-based similarity metric.|Rada Mihalcea,Courtney Corley,Carlo Strapparava","65397|AAAI|2005|Scheduling Engineering Works for the MTR Corporation in Hong Kong|This paper describes a Hong Kong MTR Corporation subway project to enhance and extend the current Web-based Engineering Works and Traffic Information Management System (ETMS) with an intelligent \"AI Engine.\" The challenge is to be able to fully and accurately encapsulate all the necessary domain and operation knowledge on subway engineering works and to be able to apply this knowledge in an efficient manner for both validation as well as scheduling. Since engineering works can only be performed a few hours each night, it is crucially important that the \"AI Engine\" maximizes the number of jobs done while ensuring operational safety and resource availability. Previously, all constraintresource checking and scheduling decisions were made manually. The new AI approach streamlines the entire planning, scheduling and rescheduling process and extends the ETMS with intelligent abilities to () automatically detect potential conflicts as work requests are entered, () check all approved work schedules for any conflicts before execution, () generate weekly operational schedules, () repair schedules after changes and () generate quarterly schedules for planning. The AI Engine uses a rule representation combined with heuristic search and a genetic algorithm for scheduling. An iterative repair algorithm was used for dynamic rescheduling.|Andy Hon Wai Chun,Dennis Wai Ming Yeung,Garbbie Pui Shan Lam,Daniel Lai,Richard Keefe,Jerome Lam,Helena Chan","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","43660|IEICE Transations|2006|A New Question Answering System for Chinese Restricted Domain|In this paper, we propose the construction of a web-based Question Answering (QA) system for restricted domain, which combines three resource information databases for the retrieval mechanism, including a Question&Answer database, a special domain documents database and the web resource retrieved by Google search engine. We describe a new retrieval technique of integrating a probabilistic technique based on OkapiBM and a semantic analysis which based on the ontology of HowNet knowledge base and a special domain HowNet created for the restricted domain. Furthermore, we provide a method of question expansion by computing word semantic similarity. The system is first developed for a middle-size domain of sightseeing information. The experiments proved the efficiency of our method for restricted domain and it is feasible to transfer to other domains expediently using the proposed method.|Haiqing Hu,Peilin Jiang,Fuji Ren,Shingo Kuroiwa","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","65807|AAAI|2006|Bookmark Hierarchies and Collaborative Recommendation|GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely. The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web. GiveALink does not use tags, content, or links in the submitted Web pages. Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users. In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network. Search results can be personalized using the bookmarks submitted by a user. We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.|Benjamin Markines,Lubomira Stoilova,Filippo Menczer","65378|AAAI|2005|Genre Classification of Web Documents|Retrieving relevant documents over the Web is an overwhelming task when search engines return thousands of Web documents. Sifting through these documents is time-consuming and sometimes leads to an unsuccessful search. One problem is that most search engines rely on matching a query to documents based solely on topical keywords. However, many users of search engines have a particular genre in mind for the desired documents. The genre of a document concerns aspects of the document such as the style or readability, presentation layout, and meta-content such as words in the title or the existence of graphs or photos. By including genre in Web searches, we hypothesize that Web document retrieval could greatly improve accuracy by better matching documents to the user's information needs. Before implementing a search engine capable of discriminating on both genre and topic, a feasibility analysis of genre classification is needed. Our previous research achieved % classification accuracy across ten genres, whereas similar research range between  and % accuracy. However, the ten genres used in our research were mostly distinct and only exemplar Web documents (consisting of only one genre) were chosen. This paper discusses our current work which involves an in-depth analysis of maintaining high accuracy rates among genres that are very similar.|Elizabeth Sugar Boese,Adele E. Howe"],["43531|IEICE Transations|2006|Verification of Speech Recognition Results Incorporating In-domain Confidence and Discourse Coherence Measures|Conventional confidence measures for assessing the reliability of ASR (automatic speech recognition) output are typically derived from \"low-level\" information which is obtained during speech recognition decoding. In contrast to these approaches, we propose a novel utterance verification framework which incorporates \"high-level\" knowledge sources. Specifically, we investigate two application-independent measures in-domain confidence, the degree of match between the input utterance and the application domain of the back-end system, and discourse coherence, the consistency between consecutive utterances in a dialogue session. A joint confidence score is generated by combining these two measures with an orthodox measure based on GPP (generalized posterior probability). The proposed framework was evaluated on an utterance verification task for spontaneous dialogue performed via a (EnglishJapanese) speech-to-speech translation system. Incorporating the two proposed measures significantly improved utterance verification accuracy compared to using GPP alone, realizing reductions in CER (confidence error-rate) of .% and .% for the English and Japanese sides, respectively. When negligible ASR errors (that do not affect translation) were ignored, further improvement was achieved for the English side, realizing a reduction in CER of up to .% compared to the GPP case.|Ian R. Lane,Tatsuya Kawahara","43612|IEICE Transations|2006|A Hybrid HMMBN Acoustic Model Utilizing Pentaphone-Context Dependency|The most widely used acoustic unit in current automatic speech recognition systems is the triphone, which includes the immediate preceding and following phonetic contexts. Although triphones have proved to be an efficient choice, it is believed that they are insufficient in capturing all of the coarticulation effects. A wider phonetic context seems to be more appropriate, but often suffers from the data sparsity problem and memory constraints. Therefore, an efficient modeling of wider contexts needs to be addressed to achieve a realistic application for an automatic speech recognition system. This paper presents a new method of modeling pentaphone-context units using the hybrid HMMBN acoustic modeling framework. Rather than modeling pentaphones explicitly, in this approach the probabilistic dependencies between the triphone context unit and the second precedingfollowing contexts are incorporated into the triphone state output distributions by means of the BN. The advantages of this approach are that we are able to extend the modeled phonetic context within the triphone framework, and we can use a standard decoding system by assuming the next precedingfollowing context variables hidden during the recognition. To handle the increased parameter number, tying using knowledge-based phoneme classes and a data-driven clustering method is applied. The evaluation experiments indicate that the proposed model outperforms the standard HMM based triphone model, achieving a --% relative word error rate (WER) reduction.|Sakriani Sakti,Konstantin Markov,Satoshi Nakamura","42925|IEICE Transations|2005|Speech Synthesis with Various Emotional Expressions and Speaking Styles by Style Interpolation and Morphing|This paper describes an approach to generating speech with emotional expressivity and speaking style variability. The approach is based on a speaking style and emotional expression modeling technique for HMM-based speech synthesis. We first model several representative styles, each of which is a speaking style andor an emotional expression, in an HMM-based speech synthesis framework. Then, to generate synthetic speech with an intermediate style from representative ones, we synthesize speech from a model obtained by interpolating representative style models using a model interpolation technique. We assess the style interpolation technique with subjective evaluation tests using four representative styles, i.e., neutral, joyful, sad, and rough in read speech and synthesized speech from models obtained by interpolating models for all combinations of two styles. The results show that speech synthesized from the interpolated model has a style in between the two representative ones. Moreover, we can control the degree of expressivity for speaking styles or emotions in synthesized speech by changing the interpolation ratio in interpolation between neutral and other representative styles. We also show that we can achieve style morphing in speech synthesis, namely, changing style smoothly from one representative style to another by gradually changing the interpolation ratio.|Makoto Tachibana,Junichi Yamagishi,Takashi Masuko,Takao Kobayashi","43558|IEICE Transations|2006|Acoustic Model Adaptation Using First-Order Linear Prediction for Reverberant Speech|This paper describes a hands-free speech recognition technique based on acoustic model adaptation to reverberant speech. In hands-free speech recognition, the recognition accuracy is degraded by reverberation, since each segment of speech is affected by the reflection energy of the preceding segment. To compensate for the reflection signal we introduce a frame-by-frame adaptation method adding the reflection signal to the means of the acoustic model. The reflection signal is approximated by a first-order linear prediction from the observation signal at the preceding frame, and the linear prediction coefficient is estimated with a maximum likelihood method by using the EM algorithm, which maximizes the likelihood of the adaptation data. Its effectiveness is confirmed by word recognition experiments on reverberant speech.|Tetsuya Takiguchi,Masafumi Nishimura,Yasuo Ariki","43571|IEICE Transations|2006|ATR Parallel Decoding Based Speech Recognition System Robust to Noise and Speaking Styles|In this paper, we describe a parallel decoding-based ASR system developed of ATR that is robust to noise type, SNR and speaking style. It is difficult to recognize speech affected by various factors, especially when an ASR system contains only a single acoustic model. One solution is to employ multiple acoustic models, one model for each different condition. Even though the robustness of each acoustic model is limited, the whole ASR system can handle various conditions appropriately. In our system, there are two recognition sub-systems which use different features such as MFCC and Differential MFCC (DMFCC). Each sub-system has several acoustic models depending on SNR, speaker gender and speaking style, and during recognition each acoustic model is adapted by fast noise adaptation. From each sub-system, one hypothesis is selected based on posterior probability. The final recognition result is obtained by combining the best hypotheses from the two sub-systems. On the AURORA-J task used widely for the evaluation of noise robustness, our system achieved higher recognition performance than a system which contains only a single model. Also, our system was tested using normal and hyper-articulated speech contaminated by several background noises, and exhibited high robustness to noise and speaking styles.|Shigeki Matsuda,Takatoshi Jitsuhiro,Konstantin Markov,Satoshi Nakamura","42345|IEICE Transations|2005|Recent Progress in Corpus-Based Spontaneous Speech Recognition|This paper overviews recent progress in the development of corpus-based spontaneous speech recognition technology. Although speech is in almost any situation spontaneous, recognition of spontaneous speech is an area which has only recently emerged in the field of automatic speech recognition. Broadening the application of speech recognition depends crucially on raising recognition performance for spontaneous speech. For this purpose, it is necessary to build large spontaneous speech corpora for constructing acoustic and language models. This paper focuses on various achievements of a Japanese -year national project \"Spontaneous Speech Corpus and Processing Technology\" that has recently been completed. Because of various spontaneous-speech specific phenomena, such as filled pauses, repairs, hesitations, repetitions and disfluencies, recognition of spontaneous speech requires various new techniques. These new techniques include flexible acoustic modeling, sentence boundary detection, pronunciation modeling, acoustic as well as language model adaptation, and automatic summarization. Particularly automatic summarization including indexing, a process which extracts important and reliable parts of the automatic transcription, is expected to play an important role in building various speech archives, speech-based information retrieval systems, and human-computer dialogue systems.|Sadaoki Furui","43467|IEICE Transations|2006|Noise Reduction in Time Domain Using Referential Reconstruction|We present a novel approach for single-channel noise reduction of speech signals contaminated by additive noise. In this approach, the system requires speech samples to be uttered in advance by the same speaker as that of the input signal. Speech samples used in this method must have enough phonetic variety to reconstruct the input signal. In the proposed method, which we refer to as referential reconstruction, we have used a small database created from examples of speech, which will be called reference signals. Referential reconstruction uses an example-based approach, in which the objective is to find the candidate speech frame which is the most similar to the clean input frame without noise, although the input frame is contaminated with noise. When candidate frames are found, they become final outputs without any special processing. In order to find the candidate frames, a correlation coefficient is used as a similarity measure. Through automatic speech recognition experiments, the proposed method was shown to be effective, particularly for low-SNR speech signals corrupted with white noise or noise in high-frequency bands. Since the direct implementation of this method requires infeasible computational cost for searching through reference signals, a coarse-to-fine strategy is introduced in this paper.|Takehiro Ihara,Takayuki Nagai,Kazuhiko Ozeki,Akira Kurematsu","42347|IEICE Transations|2005|Multiple Regression of Log Spectra for In-Car Speech Recognition Using Multiple Distributed Microphones|This paper describes a new multi-channel method of noisy speech recognition, which estimates the log spectrum of speech at a closetalking microphone based on the multiple regression of the log spectra (MRLS) of noisy signals captured by distributed microphones. The advantages of the proposed method are as follows ) The method does not require a sensitive geometric layout, calibration of the sensors nor additional pre-processing for tracking the speech source ) System works in very small computation amounts and ) Regression weights can be statistically optimized over the given training data. Once the optimal regression weights are obtained by regression learning, they can be utilized to generate the estimated log spectrum in the recognition phase, where the speech of close-talking is no longer required. The performance of the proposed method is illustrated by speech recognition of real in-car dialogue data. In comparison to the nearest distant microphone and multi-microphone adaptive beamformer, the proposed approach obtains relative word error rate (WER) reductions of .% and .%, respectively.|Weifeng Li,Tetsuya Shinde,Hiroshi Fujimura,Chiyomi Miyajima,Takanori Nishino,Katunobu Itou,Kazuya Takeda,Fumitada Itakura","42451|IEICE Transations|2005|Harmonicity Based Dereverberation for Improving Automatic Speech Recognition Performance and Speech Intelligibility|A speech signal captured by a distant microphone is generally smeared by reverberation, which severely degrades both the speech intelligibility and Automatic Speech Recognition (ASR) performance. Previously, we proposed a single-microphone dereverberation method, named \"Harmonicity based dEReverBeration (HERB).\" HERB estimates the inverse filter for an unknown room transfer function by utilizing an essential feature of speech, namely harmonic structure. In previous studies, improvements in speech intelligibility was shown solely with spectrograms, and improvements in ASR performance were simply confirmed by matched condition acoustic model. In this paper, we undertook a further investigation of HERB's potential as regards to the above two factors. First, we examined speech intelligibility by means of objective indices. As a result, we found that HERB is capable of improving the speech intelligibility to approximately that of clean speech. Second, since HERB alone could not improve the ASR performance sufficiently, we further analyzed the HERB mechanism with a view to achieving further improvements. Taking the analysis results into account, we proposed an appropriate ASR configuration and conducted experiments. Experimental results confirmed that, if HERB is used with an ASR adaptation scheme such as MLLR and a multicondition acoustic model, it is very effective for improving ASR performance even in unknown severely reverberant environments.|Keisuke Kinoshita,Tomohiro Nakatani,Masato Miyoshi","43509|IEICE Transations|2006|Robust Recognition of Fast Speech|This letter describes a robust speech recognition system for recognizing fast speech by stretching the length of the utterance in the cepstrum domain. The degree of stretching for an utterance is determined by its rate of speech (ROS), which is based on a maximum likelihood (ML) criterion. The proposed method was evaluated on -digits mobile phone numbers. The results of the simulation show that the overall error rate was reduced by .% when the proposed method was employed.|Ki-Seung Lee"],["57615|GECCO|2006|Hyper-ellipsoidal conditions in XCS rotation linear approximation and solution structure|The learning classifier system XCS is an iterative rule-learning system that evolves rule structures based on gradient-based prediction and rule quality estimates. Besides classification and reinforcement learning tasks, XCS was applied as an effective function approximator. Hereby, XCS learns space partitions to enable a maximally accurate and general function approximation. Recently, the function approximation approach was improved by replacing () hyperrectangular conditions with hyper-ellipsoids and () iterative linear approximation with the recursive least squares method. This paper combines the two approaches assessing the usefulness of each. The evolutionary process is further improved by changing the mutation operator implementing an angular mutation that rotates ellipsoidal structures explicitly. Both enhancements improve XCS performance in various non-linear functions. We also analyze the evolving ellipsoidal structures confirming that XCS stretches and rotates the evolving ellipsoids according to the shape of the underlying function. The results confirm that improvements in both the evolutionary approach and the gradient approach can result in significantly better performance.|Martin V. Butz,Pier Luca Lanzi,Stewart W. Wilson","57747|GECCO|2006|Multi-step environment learning classifier systems applied to hyper-heuristics|Heuristic Algorithms (HA) are very widely used to tackle practical problems in operations research. They are simple, easy to understand and inspire confidence. Many of these HAs are good for some problem instances while very poor for other cases. While Meta-Heuristics try to find which is the best heuristic andor parameters to apply for a given problem instance Hyper-Heuristics (HH) try to combine several heuristics in the same solution searching process, switching among them whenever the circumstances vary. Besides, instead to solve a single problem instance it tries to find a general algorithm to apply to whole families of problems. HH use evolutionary methods to search for such a problem-solving algorithm and, once produced, to apply it to any new problem instance desired. Learning Classifier Systems (LCS), and in particular XCS, represents an elegant and simple way to try to fabricate such a composite algorithm. This represents a different kind of problem to those already studied by the LCS community. Previous work, using single step environments, already showed the usefulness of the approach. This paper goes further and studies the novel use of multi-step environments for HH and an alternate way to consider states to see if chains of actions can be learnt. A non-trivial, NP-hard family of problems, the Bin Packing one, is used as benchmark for the procedure. Results of the approach are very encouraging, showing outperformance over all HAs used individually and over previously reported work by the authors, including non-LCS (a GA based approach used for the same BP set of problems) and LCS (using single step environments).|Javier G. Mar√≠n-Bl√°zquez,Sonia Schulenburg","57340|GECCO|2005|XCS with eligibility traces|The development of the XCS Learning Classifier System has produced a robust and stable implementation that performs competitively in direct-reward environments. Although investigations in delayed-reward (i.e. multi-step) environments have shown promise, XCS still struggles to efficiently find optimal solutions in environments with long action-chains. This paper highlights the strong relation of XCS to reinforcement learning and identifies some of the major differences. This makes it possible to add Eligibility Traces to XCS, a method taken from reinforcement learning to update the prediction of the whole action-chain on each step, which should cause prediction update to be faster and more accurate. However, it is shown that the discrete nature of the condition representation of a classifier and the operation of the genetic algorithm cause traces to propagate back incorrect prediction values and in some cases results in a decrease of system performance. As a result further investigation of the existing approach to generalisation is proposed.|Jan Drugowitsch,Alwyn Barry","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llor√†,David E. Goldberg","57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","57594|GECCO|2006|Smart crossover operator with multiple parents for a Pittsburgh learning classifier system|This paper proposes a new smart crossover operator for a Pittsburgh Learning Classifier System. This operator, unlike other recent LCS approaches of smart recombination, does not learn the structure of the domain, but it merges the rules of N parents (N  ) to generate a new offspring. This merge process uses an heuristic that selects the minimum subset of candidate rules that obtains maximum training accuracy. Moreover the operator also includes a rule pruning scheme to avoid the inclusion of over-specific rules, and to guarantee as much as possible the robust behaviour of the LCS. This operator takes advantage from the fact that each individual in a Pittsburgh LCS is a complete solution, and the system has a global view of the solution space that the proposed rule selection algorithm exploits. We have empirically evaluated this operator using a recent LCS called GAssist. First with the standard LCS benchmark, the  bits multiplexer, and later using  standard real datasets. The results of the experiments over these datasets indicate that the new operator manages to increase the accuracy of the system over the classical crossover in  of the  datasets, and never having a significantly worse performance than the classic operator.|Jaume Bacardit,Natalio Krasnogor","57300|GECCO|2005|Kernel-based ellipsoidal conditions in the real-valued XCS classifier system|Many learning classifier system (LCS) implementations are restricted to the binary problem realm. Recently, the XCS classifier system was enhanced to be able to handle real-valued inputs among others. In the real-valued enhancement, XCSF applies as a function approximation system that partitions the input space in hyperrectangular subspaces specified in the classifiers. This paper changes the classifier conditions to hyperspheres and hyperellipsoids and investigates the consequent performance impact. It is shown that the modifications yield improved performance in continuous functions. Even in discontinuous functions with parallel boundaries, XCS's performance does not degrade. Thus, for the real-valued problem domain, ellipsoidal condition structures can improve XCS's performance. From a more general perspective, this paper shows that XCS is readily applicable in diverse problem domains. To apply the system even more successfully, suitable kernel-based bases need to be found and used as classifier conditions. XCS distributes the available structures over the problem space evolving more specialized structures in more complex problem subspaces.|Martin V. Butz","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llor√†,Kumara Sastry","57616|GECCO|2006|Studying XCSBOA learning in Boolean functions structure encoding and random Boolean functions|Recently, studies with the XCS classifier system on Boolean functions have shown that in certain types of functions simple crossover operators can lead to disruption and, consequently, a more effective recombination mechanism is required. Simple crossover operators were replaced by recombination based on estimation of distribution algorithms (EDAs). The combination showed that XCS with such a statistics-based crossover operator can solve challenging hierarchical functions more efficiently. This study elaborates the gained competence further investigating the coding scheme for the EDA component (BOA in our case) of XCS as well as performance in randomly generated Boolean function problems. Results in hierarchical Boolean functions show that the originally used -bit coding scheme induces a certain learning bias that stresses additional diversity in the evolving XCS population. A -bit coding scheme as well as a restricted -bit coding scheme confirm the suspected bias. The alternative encodings decrease the unnecessary bias towards specificity and increase performance robustness. The paper concludes with a discussion on the challenges ahead for XCS in Boolean function problems as well as on the implications of the obtained results for real-valued and multiple-valued classification problems, multi-step problems, and function approximation problems.|Martin V. Butz,Martin Pelikan","57553|GECCO|2005|Hyper-heuristics and classifier systems for solving D-regular cutting stock problems|This paper presents a method for combining concepts of Hyper-heuristics and Learning Classifier Systems for solving D Cutting Stock Problems. The idea behind Hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. In this paper, the Hyper-heuristic is formed using a XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The XCS evolves a behavior model which determines the possible actions (selection and placement heuristics) for given states of the problem. When tested with a collection of different problems, the method finds very competitive results for most of the cases. The testebed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Mar√≠n,E. J. Flores-√?lvarez,Peter Ross"],["57439|GECCO|2005|Measuring mobility and the performance of global search algorithms|The global search properties of heuristic search algorithms are not well understood. In this paper, we introduce a new metric, mobility, that quantifies the dispersion of local optima visited during a search. This allows us to explore two questions How disperse are the local optima visited during a search How does mobility relate to algorithm performance We compare local search with two evolutionary algorithms, CHC and CMA-ES, on a set of non-separable, non-symmetric, multi-modal test functions. Given our mobility metric, we show that algorithms visiting more disperse local optima tend to be better optimizers.|Monte Lunacek,L. Darrell Whitley,James N. Knight","57542|GECCO|2005|On the impact of objective function transformations on evolutionary and black-box algorithms|Different fitness functions describe different problems. Hence, certain fitness transformations can lead to easier problems although they are still a model of the considered problem. In this paper, the class of neutral transformations for a simple rank-based evolutionary algorithm (EA) is described completely, i.e., the class of functions that transfers easy problems for this EA in easy ones and difficult problems in difficult ones. Moreover, the class of neutral transformations for this population-based EA is equal to the black-box neutral transformations. Hence, it is a proper superset of the corresponding class for an EA based on fitness-proportional selection, but it is a proper subset of the class for random search. Furthermore, the minimal and maximal classes of neutral transformations are investigated in detail.|Tobias Storch","57706|GECCO|2006|On the local performance of simulated annealing and the  evolutionary algorithm|Simulated annealing and the (+) EA, a simple evolutionary algorithm, are both general randomized search heuristics that optimize any objective function with probability converging to . But they use very different techniques to achieve this global convergence. The (+) EA applies global mutations than can reach any point in the search space in one step together with an elitist selection mechanism. Simulated annealing restricts its search to a neighborhood but employs a randomized selection scheme where the probability for accepting a move to a new point in the search space depends on the difference in function values as well as on the current time step. Otherwise, the two algorithms are equal. It is known that the different philosophies of search implemented in the two heuristics can lead to exponential performance gaps between the two algorithms with respect to the expected optimization time. Even for very restricted classes of objective functions where the differences in function values between neighboring points are strictly limited the performance differences can be huge. Here, a more local point of view is taken. Considering obstacles in the fitness landscapes it is proven that the local performance of the two algorithms is remarkably similar in spite of their different search behaviors.|Thomas Jansen,Ingo Wegener","57376|GECCO|2005|An empirical study of the robustness of two module clustering fitness functions|Two of the attractions of search-based software engineering (SBSE) derive from the nature of the fitness functions used to guide the search. These have proved to be highly robust (for a variety of different search algorithms) and have yielded insight into the nature of the search space itself, shedding light upon the software engineering problem in hand.This paper aims to exploit these two benefits of SBSE in the context of search based module clustering. The paper presents empirical results which compare the robustness of two fitness functions used for software module clustering one (MQ) used exclusively for module clustering. The other is EVM, a clustering fitness function previously applied to time series and gene expression data.The results show that both metrics are relatively robust in the presence of noise, with EVM being the more robust of the two. The results may also yield some interesting insights into the nature of software graphs.|Mark Harman,Stephen Swift,Kiarash Mahdavi","57690|GECCO|2006|ALPS the age-layered population structure for reducing the problem of premature convergence|To reduce the problem of premature convergence we define a new method for measuring an individual's age and propose the Age-Layered Population Structure (ALPS). This new measure of age measures how long the genetic material has been evolving in the population offspring start with an age of  plus the age of their oldest parent instead of starting with an age of  as with traditional measures of age. ALPS differs from a typical evolutionary algorithm (EA) by segregating individuals into different age-layers by their age and by regularly introducing new, randomly generated individuals in the youngest layer. The introduction of randomly generated individuals at regular intervals results in an EA that is never completely converged and is always exploring new parts of the fitness landscape. By using age to restrict competition and breeding, younger individuals are able to develop without being dominated by older ones. Analysis of the search behavior of ALPS finds that the offspring of individuals that are randomly generated mid-way through a run are able to move the population out of mediocre local-optima to better parts of the fitness landscape. In comparison against a traditional EA, a multi-start EA and two other EAs with diversity maintenance schemes we find that ALPS produces significantly better designs with a higher reliability than the other EAs.|Gregory Hornby","57322|GECCO|2005|Directional self-learning of genetic algorithm|In order to overcome the low convergence speed and prematurity of classical genetic algorithm, an improved method named directional self-learning of genetic algorithm (DSLGA) is proposed in this paper. Through the self-learning operator directional information was introduced in local search process. The search direction was guided by the false derivative of the function fitness. Using the four operators among the individuals, the best solution was updated continuously. In experiments, DSLGA was tested on  unconstrained benchmark problems, and the results were compared with the algorithms presented recently. It showed that DSLGA performs much better than the other algorithms both in the quality of the solutions and in the computational complexity.|Lin Cong,Yuheng Sha,Licheng Jiao,Fang Liu","57494|GECCO|2005|EA models and population fixed-points versus mutation rates for functions of unitation|Using a dynamic systems model for the Simple Genetic Algorithm due to Vose, we analyze the fixed point behavior of the model without crossover applied to functions of unitation. Unitation functions are simplified fitness functions that reduce the search space into a smaller number of equivalence classes. This reduction allows easier computation of fixed points. We also create a dynamic systems model from a simple nondecreasing EA like the (+) EA and variants, then analyze this models on unitation classes.|J. Neal Richter,John Paxton,Alden H. Wright","57357|GECCO|2005|On favoring positive correlations between form and quality of candidate solutions via the emergence of genomic self-similarity|A key property for the effectiveness of stochastic search techniques, including evolutionary algorithms, is the existence of a positive correlation between the form and the quality of candidate solutions. In this paper, we show that when the ordering of genomic symbols in a genetic algorithm is completely independent of the fitness function and therefore free to evolve along the candidate solutions it encodes, the resulting genomes self-organize into self-similar structures that favor this key stochastic search property.|Ivan I. Garibay,Annie S. Wu,Ozlem O. Garibay","57573|GECCO|2005|On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization|Estimation of distribution algorithms construct an explicit model of the problem to be solved, and then use this model to guide the search for good solutions. For an important class of fitness functions, namely those with k-bounded epistasis, it is possible to construct a complete explicit representation of the fitness function by sampling the fitness function. A very natural model of the problem to be solved is the Boltzmann distribution of the fitness function, which is an exponential of the fitness normalized to a probability distribution. As the exponentiation factor (inverse temperature) of the Boltzmann distribution is increased, probability is increasingly concentrated on the set of optimal points. We show that for fitness functions of k-bounded epistasis that satisfy an additional property called the running intersection property, an explicit computable exact factorization of the Boltzmann distribution with an arbitrary exponentiation factor can be constructed. This factorization allows the Boltzmann distribution to be efficiently sampled, which leads to an algorithm which finds the optimum with high probability.|Alden H. Wright,Sandeep Pulavarty","57825|GECCO|2006|On the analysis of the  memetic algorithm|Memetic algorithms are evolutionary algorithms incorporating local search to increase exploitation. This hybridization has been fruitful in countless applications. However, theory on memetic algorithms is still in its infancy.Here, we introduce a simple memetic algorithm, the (+) Memetic Algorithm (+(MA)), working with a population size of  and no crossover. We compare it with the well-known (+) EA and randomized local search and show that these algorithms can outperform each other drastically.On problems like, e.g., long path problems it is essential to limit the duration of local search. We investigate the (+) MA with a fixed maximal local search duration and define a class of fitness functions where a small variation of the local search duration has a large impact on the performance of the (+) MA.All results are proved rigorously without assumptions.|Dirk Sudholt"]]},"title":{"entropy":6.1818932257929,"topics":["neural networks, for networks, its application, the web, and its, and for, efficient for, and application, for data, control for, least squares, networks, for mobile, for system, sensor networks, for application, planning with, history branch, semantic web, control system","speech recognition, learning for, model for, reinforcement learning, method for, and for, for speech, support machines, support vector, and speech, vector machines, using and, based model, for recognition, blind separation, using model, for using, based and, recognition and, feature extraction","genetic algorithm, algorithm for, for the, the problem, genetic for, algorithm the, the, the and, genetic programming, for problem, using genetic, evolutionary for, evolutionary algorithm, and algorithm, particle swarm, and for, using algorithm, for optimization, and genetic, evolutionary","for system, method for, for with, adaptive for, design for, estimation for, with, motion estimation, adaptive filter, for image, design and, estimation and, system with, system using, decoding codes, and system, maximum likelihood, design system, noise reduction, and for","and its, for data, and application, its application, for application, least squares, and data, and for, with application, data stream, for stream, and quality, data, load for, control its, the application, the effects, data system, for its, multi-agent system","the web, efficient for, control for, control system, for web, the semantic, semantic web, efficient and, for robot, and web, semantic for, web search, for management, large scale, management system, for system, web services, from data, and control, extracting from","method for, support machines, support vector, vector machines, support for, for vector, method using, for classification, method and, nearest neighbor, for machines, machines using, and vector, nearest search, using vector, for interconnect, interconnect capacitance, support system, capacitance dummy, normal form","speech recognition, for speech, using and, for recognition, and speech, recognition and, and for, for using, recognition using, speech enhancement, speech using, natural language, system using, speech based, using pattern, and its, using, and system, on-line for, for enhancement","and for, and its, job shop, for testing, for sequence, hybrid for, sequence zone, for scheduling, for tree, for sets, the sequence, spanning tree, job scheduling, shop scheduling, evolutionary testing, and mutation, and sets, for software, zero-correlation zone, decision processes","algorithm for, genetic algorithm, algorithm the, evolutionary for, evolutionary algorithm, for optimization, and algorithm, particle swarm, using algorithm, optimization algorithm, algorithm networks, efficient algorithm, particle optimization, evolutionary optimization, multi-objective optimization, swarm optimization, genetic selection, algorithm with, for multi-objective, multi-objective evolutionary","estimation for, estimation and, for detection, estimation algorithm, adaptive estimation, for circuits, noise reduction, maximum likelihood, low power, ultra wideband, for noise, noise based, for signal, estimation with, based estimation, linear for, test for, test scan, reduction for, detection system","for with, for coding, adaptive for, with, with and, adaptive with, video coding, wavelet transform, error for, algorithm with, with source, system with, for source, based coding, coding and, image coding, compression based, using wavelet, for real-time, with using"],"ranking":[["16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","42391|IEICE Transations|2005|New Criteria of Selective Orthogonal Matrix Least-Squares Method for Macromodeling Multiport Networks Characterized by Sampled Data|This paper presents the selective orthogonal matrix least-squares (SOM-LS) method for representing a multiport network characterized by sampled data with the rational matrix, improving the previous works ,  and providing new criteria. Recently, it is needed in a circuit design to evaluate physical effects of interconnects and package, and the evaluation is done by numerical electromagnetic analysis or measurement by network analyzer. Here, the SOM-LS method with the criteria will play an important role for generating the macromodels of interconnects and package in circuit simulation level. The accuracy of the macromodels is predictable and controllable, that is, the SOM-LS method fits the rational matrix to the sampled data, selecting the dominant poles of the rational matrix. In examples, simple PCB models are analyzed, where the rational matrices are described by Verilog-A, and some simulations are carried out on a commercial circuit simulator.|Yuichi Tanji,Masaya Suzuki,Takayuki Watanabe,Hideki Asai","42766|IEICE Transations|2005|An Application of Uplink Common Channel to Packet Relay in CDMA Radio Access Networks|Multihop techniques in CDMA radio access networks, enable dead-spot mobile stations, which cannot communicate with base stations directly, to send data to them via other mobile stations. In this paper, we propose a mechanism for establishing connections and relaying packets between mobile stations. In this mechanism, the mobile stations are connected to one another and relay packets through a random access channel, which is an uplink common channel. In addition, our mechanism satisfies the requirements for applying multihop techniques to third generation radio access networks. Moreover, we also discuss our evaluation of the performance of the mechanism through computer simulations. The results we obtained reveal that it is capable of reducing dead-spot mobile stations and improving throughput with only limited modifications to conventional systems. Furthermore, we propose an adaptive transmission power control to enhance our mechanism and also evaluate this method through computer simulations.|Satoshi Okada,Ryoichi Shinkuma,Tatsuro Takahashi","42720|IEICE Transations|2005|Optimal Call Admission Control for Voice Traffic in Cellular Mobile Communication Networks|We propose a new call admission control (CAC) scheme for voice calls in cellular mobile communication networks. It is assumed that the rejection of a hand-off call is less desirable than that of a new call, for a hand-off call loss would cause a severe mental pain to a user. We consider the pains of rejecting new and hand-off calls as different costs. The key idea of our CAC is to restrict the admission of new calls in order to minimize the total expected costs per unit time over the long term. An optimal policy is derived from a semi-Markov decision process in which the intervals between successive decision epochs are exponentially distributed. Based on this optimal policy, we calculate the steady state probability for the number of established voice connections in a cell. We then evaluate the probability of blocking new calls and the probability of forced termination of hand-off calls. In the numerical experiments, it is found that the forced termination probability of hand-off calls is reduced significantly by our CAC scheme at the slight expense of the blocking probability of new calls and the channel utilization. Comparison with the static guard channel scheme is made.|Minoru Ohmikawa,Hideaki Takagi,Sang-Yong Kim","43652|IEICE Transations|2006|Branch Aggregation Multicast BAM An Energy Efficient and Highly Compatible Multicast Protocol for Wireless Sensor Networks|In this paper, we propose a multicast protocol, called BAM (Branch Aggregation Multicast), for wireless sensor networks. The main contribution of BAM is a reduction in the radio communication load, which is a key determinant of sensor energy consumption. BAM does not use any control packets such as joinleave messages and does not manage multicast groups. BAM is highly compatible with existing wireless sensor protocols, such as routing protocols, MAC protocols, and other kinds of energy efficient protocols. BAM implementation is quite simple and BAM works on various networks even if some sensors are not BAM-capable. BAM is composed of two aggregation techniques. One is single hop aggregation (S-BAM) and the other is multiple paths aggregation (M-BAM). S-BAM aggregates radio transmission within a single hop and enables single transmission to multiple intended receivers. M-BAM aggregates multiple paths into fewer ones and limits the range of radio transmission. S-BAM is designed to reduce redundant communication at every branch while M-BAM is designed to reduce the number of branches. SM-BAM, the combination of S-BAM and M-BAM, can reduce the radio communication load thus enabling energy efficient multicast communication. We evaluate BAM in three ways, qualitative evaluation by theoretical analysis, quantitative evaluation through computer simulations, and experiments using Cross-Bow's MICA. Our results show that BAM is a very energy efficient multicast protocol that well supports wireless sensor networks.|Akihito Okura,Takeshi Ihara,Akira Miura","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","43395|IEICE Transations|2006|A Method of Simple Adaptive Control for Nonlinear Systems Using Neural Networks|This paper presents a method of simple adaptive control (SAC) using neural networks for a class of nonlinear systems with bounded-input bounded-output (BIBO) and bounded nonlinearity. The control input is given by the sum of the output of the simple adaptive controller and the output of the neural network. The neural network is used to compensate for the nonlinearity of the plant dynamics that is not taken into consideration in the usual SAC. The role of the neural network is to construct a linearized model by minimizing the output error caused by nonlinearities in the control systems. Furthermore, convergence and stability analysis of the proposed method is performed. Finally, the effectiveness of the proposed method is confirmed through computer simulation.|Muhammad Yasser,Agus Trisanto,Jianming Lu,Takashi Yahagi","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings","43481|IEICE Transations|2006|Distributed Hierarchical Multicast Tree Algorithms for Application Layer Mesh Networks|This paper proposes a set of novel distributed algorithms on m-D mesh overlay configurations for short delay and low resource consumption application layer multicast. In contrast to previous approaches, our application layer multicast adopts two-layer tree architecture and the novelty and contribution are () cluster formation algorithm assigns the closest group members into the same cluster that greatly decreases the multicast delay and resource consumption caused by the message transmission among the members with long distances () optimal core selection algorithm seeks the cluster member who has the minimum sum of static delay distances to other cluster members as the optimal cores (i.e. cluster cores) that guarantees the short multicast delay () weighted shortest path tree generation algorithm constructs a shortest path tree rooted at the optimal core for each cluster. The shortest path tree utilizes the minimum sum of links that are on the shortest paths among the cluster members and () distributed multicast routing algorithm directs the multicast messages to be efficiently distributed along the two-layer multicast architecture in parallel without a global control. The extended simulation results indicate that the application layer multicast constructed by our algorithms is efficient in terms of short multicast delay and low network resource consumption as compared with other well-known existing multicast solutions.|Weijia Jia,Wanqing Tu,Jie Wu","42662|IEICE Transations|2005|History-Based Auxiliary Mobility Management Strategy for Hierarchical Mobile IPv Networks|The reduction of the signaling load associated with IP mobility management is one of the significant challenges to IP mobility support protocols. Hierarchical Mobile IPv (HMIPv) aims to reduce the number of the signaling messages in the backbone networks, and improve handoff performance by reducing handoff latency. However, this does not imply any change to the periodic binding update (BU) to the home agent (HA) and the correspondent node (CN), and now a mobile node (MN) additionally should send it to the mobility anchor point (MAP). Moreover, the MAP should tunnel the received packets to be routed to the MN. These facts mean that the reduction of the BU messages in the backbone networks can be achieved at the expense of the increase in the signaling bandwidth consumption within a MAP domain. On the other hand, it is observed that an MN may habitually stay for a relatively long time or spend on using much Internet in a specific cell (hereafter, home cell) covering its home, office or laboratory, etc. Thus, considering the preceding facts and observation, HMIPv may not be favorable especially during a home cell residence time in terms of signaling bandwidth consumption. To overcome these drawbacks of HMIPv, we propose a history-based auxiliary mobility management strategy (H-HMIPv) to enable an MN to selectively switch its mobility management protocols according to whether it is currently in its home cell or not in HMIPv networks. The operation of H-HMIPv is almost the same as that of HMIPv except either when an MN entersleaves its home cell or while it stays in its home cell. Once an MN knows using its history that it enters its home cell, it behaves as if it operates in Mobile IPv (MIPv), not in HMIPv, until it leaves its home cell No periodic BU messages to the MAP and no packet tunneling occur during the MN's home cell residence time. The numerical results indicate that compared with HMIPv, H-HMIPv has apparent potential to reduce the signaling bandwidth consumption and the MAP blocking probability.|Ki-Sik Kong,Sung-Ju Roh,Chong-Sun Hwang"],["42342|IEICE Transations|2005|A Data-Driven Model Parameter Compensation Method for Noise-Robust Speech Recognition|A data-driven approach that compensates the HMM parameters for the noisy speech recognition is proposed. Instead of assuming some statistical approximations as in the conventional methods such as the PMC, the various statistical information necessary for the HMM parameter adaptation is directly estimated by using the Baum-Welch algorithm. The proposed method has shown improved results compared with the PMC for the noisy speech recognition.|Yongjoo Chung","43068|IEICE Transations|2006|Detection of Overlapping Speech in Meetings Using Support Vector Machines and Support Vector Regression|In this paper, a method of detecting overlapping speech segments in meetings is proposed. It is known that the eigenvalue distribution of the spatial correlation matrix calculated from a multiple microphone input reflects information on the number and relative power of sound sources. However, in a reverberant sound field, the feature of the number of sources in the eigenvalue distribution is degraded by the room reverberation. In the Support Vector Machines approach, the eigenvalue distribution is classified into two classes (overlapping speech segments and single speech segments). In the Support Vector Regression approach, the relative power of sound sources is estimated by using the eigenvalue distribution, and overlapping speech segments are detected based on the estimated relative power. The salient feature of this approach is that the sensitivity of detecting overlapping speech segments can be controlled simply by changing the threshold value of the relative power. The proposed method was evaluated using recorded data of an actual meeting.|Kiyoshi Yamamoto,Futoshi Asano,Takeshi Yamada,Nobuhiko Kitawaki","42830|IEICE Transations|2005|Robust Speech Recognition Using Discrete-Mixture HMMs|This paper introduces new methods of robust speech recognition using discrete-mixture HMMs (DMHMMs). The aim of this work is to develop robust speech recognition for adverse conditions that contain both stationary and non-stationary noise. In particular, we focus on the issue of impulsive noise, which is a major problem in practical speech recognition system. In this paper, two strategies were utilized to solve the problem. In the first strategy, adverse conditions are represented by an acoustic model. In this case, a large amount of training data and accurate acoustic models are required to present a variety of acoustic environments. This strategy is suitable for recognition in stationary or slow-varying noise conditions. The second is based on the idea that the corrupted frames are treated to reduce the adverse effect by compensation method. Since impulsive noise has a wide variety of features and its modeling is difficult, the second strategy is employed. In order to achieve those strategies, we propose two methods. Those methods are based on DMHMM framework which is one type of discrete HMM (DHMM). First, an estimation method of DMHMM parameters based on MAP is proposed aiming to improve trainability. The second is a method of compensating the observation probabilities of DMHMMs by threshold to reduce adverse effect of outlier values. Observation probabilities of impulsive noise tend to be much smaller than those of normal speech. The motivation in this approach is that flooring the observation probability reduces the adverse effect caused by impulsive noise. Experimental evaluations on Japanese LVCSR for read newspaper speech showed that the proposed method achieved the average error rate reduction of .% in impulsive noise conditions. Also the experimental results in adverse conditions that contain both stationary and impulsive noises showed that the proposed method achieved the average error rate reduction of .%.|Tetsuo Kosaka,Masaharu Katoh,Masaki Kohda","42348|IEICE Transations|2005|Applying Sparse KPCA for Feature Extraction in Speech Recognition|This paper presents an analysis of the applicability of Sparse Kernel Principal Component Analysis (SKPCA) for feature extraction in speech recognition, as well as, a proposed approach to make the SKPCA technique realizable for a large amount of training data, which is an usual context in speech recognition systems. Although the KPCA (Kernel Principal Component Analysis) has proved to be an efficient technique for being applied to speech recognition, it has the disadvantage of requiring training data reduction, when its amount is excessively large. This data reduction is important to avoid computational unfeasibility andor an extremely high computational burden related to the feature representation step of the training and the test data evaluations. The standard approach to perform this data reduction is to randomly choose frames from the original data set, which does not necessarily provide a good statistical representation of the original data set. In order to solve this problem a likelihood related re-estimation procedure was applied to the KPCA framework, thus creating the SKPCA, which nevertheless is not realizable for large training databases. The proposed approach consists in clustering the training data and applying to these clusters a SKPCA like data reduction technique generating the reduced data clusters. These reduced data clusters are merged and reduced in a recursive procedure until just one cluster is obtained, making the SKPCA approach realizable for a large amount of training data. The experimental results show the efficiency of SKPCA technique with the proposed approach over the KPCA with the standard sparse solution using randomly chosen frames and the standard feature extraction techniques.|Amaro Lima,Heiga Zen,Yoshihiko Nankaku,Keiichi Tokuda,Tadashi Kitamura,Fernando Gil Resende","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","43149|IEICE Transations|2006|Adaptive Morse Code Recognition Using Support Vector Machines for Persons with Physical Disabilities|In this paper, Morse code is selected as a communication adaptive device for persons whose hand coordination and dexterity are impaired by such ailments as amyotrophic lateral sclerosis, multiple sclerosis, muscular dystrophy, and other severe handicaps. Morse code is composed of a series of dots, dashes, and space intervals, and each element is transmitted by sending a signal for a defined length of time. A suitable adaptive automatic recognition method is needed for persons with disabilities due to their difficulty in maintaining a stable typing rate. To overcome this problem, the proposed method combines the support vector machines method with a variable degree variable step size LMS algorithm. The method is divided into five stages tone recognition, space recognition, training process, adaptive processing, and character recognition. Statistical analyses demonstrated that the proposed method elicited a better recognition rate in comparison to alternative methods from the literature.|Cheng-Hong Yang,Li-Yeh Chuang,Cheng-Huei Yang,Ching-Hsing Luo","42822|IEICE Transations|2005|Alaryngeal Speech Enhancement Using Pattern Recognition Techniques|An alaryngeal speech enhancement system is proposed to improve the intelligibility and quality of speech signals generated by an artificial larynx transducer (ALT). Proposed system identifies the voiced segments of alaryngeal speech signal, by using pattern recognition methods, and replaces these by their equivalent voiced segments of normal speech. Evaluation results show that proposed system provides a fairly good improvement of the quality and intelligibility of ALT generated speech.|Gualberto Aguilar-Torres,Mariko Nakano-Miyatake,H√©ctor M. P√©rez Meana","42328|IEICE Transations|2005|Speech Recognition Using Finger Tapping Timings|Behavioral synchronization between speech and finger tapping provides a novel approach to improving speech recognition accuracy. We combine a sequence of finger tapping timings recorded alongside an utterance using two distinct methods in the first method, HMM state transition probabilities at the word boundaries are controlled by the timing of the finger tapping in the second, the probability (relative frequency) of the finger tapping is used as a 'feature' and combined with MFCC in a HMM recognition system. We evaluate these methods through connected digit recognition under different noise conditions (AURORA-J). Leveraging the synchrony between speech and finger tapping provides a % relative improvement in connected digit recognition experiments.|Hiromitsu Ban,Chiyomi Miyajima,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","43520|IEICE Transations|2006|Robust Speech Recognition by Using Compensated Acoustic Scores|This paper proposes a new compensation method of acoustic scores in the Viterbi search for robust speech recognition. This method introduces noise models to represent a wide variety of noises and realizes robust decoding together with conventional techniques of subtraction and adaptation. This method uses likelihoods of noise models in two ways. One is to calculate a confidence factor for each input frame by comparing likelihoods of speech models and noise models. Then the weight of the acoustic score for a noisy frame is reduced according to the value of the confidence factor for compensation. The other is to use the likelihood of noise model as an alternative that of a silence model when given noisy input. Since a lower confidence factor compresses acoustic scores, the decoder rather relies on language scores and keeps more hypotheses within a fixed search depth for a noisy frame. An experiment using commentary transcriptions of a broadcast sports program (MLB Major League Baseball) showed that the proposed method obtained a .% relative word error reduction. The method also reduced the relative error rate of key words by .%, and this is expected lead to an improvement metadata extraction accuracy.|Shoei Sato,Kazuo Onoe,Akio Kobayashi,Toru Imai","43415|IEICE Transations|2006|Support Vector Machines Based Generalized Predictive Control of Chaotic Systems|This work presents an application of the previously proposed Support Vector Machines Based Generalized Predictive Control (SVM-Based GPC) method  to the problem of controlling chaotic dynamics with small parameter perturbations. The Generalized Predictive Control (GPC) method, which is included in the class of Model Predictive Control, necessitates an accurate model of the plant that plays very crucial role in the control loop. On the other hand, chaotic systems exhibit very complex behavior peculiar to them and thus it is considerably difficult task to get their accurate model in the whole phase space. In this work, the Support Vector Machines (SVMs) regression algorithm is used to obtain an acceptable model of a chaotic system to be controlled. SVM-Based GPC exploits some advantages of the SVM approach and utilizes the obtained model in the GPC structure. Simulation results on several chaotic systems indicate that the SVM-Based GPC scheme provides an excellent performance with respect to local stabilization of the target (an originally unstable equilibrium point). Furthermore, it somewhat performs targeting, the task of steering the chaotic system towards the target by applying relatively small parameter perturbations. It considerably reduces the waiting time until the system, starting from random initial conditions, enters the local control region, a small neighborhood of the chosen target. Moreover, SVM-Based GPC maintains its performance in the case that the measured output is corrupted by an additive Gaussian noise.|Serdar Iplikci"],["42394|IEICE Transations|2005|Solving Facility Layout Problem Using an Improved Genetic Algorithm|The facility layout problem is one of the most fundamental quadratic assignment problems in operations research. In this paper, we present an improved genetic algorithm for solving the facility layout problem. In our computational model, we propose several improvements to the basic genetic procedures including conditional crossover and mutation. The performance of the proposed method is evaluated on some benchmark problems. Computational results showed that the improved genetic algorithm is capable of producing high-quality solutions.|Rong Long Wang,Kozo Okazaki","57269|GECCO|2005|Inexact pattern matching using genetic algorithm|A Genetic Algorithm for graphical pattern matching based on angle matching had been proposed. It has proven quite effective in matching simple patterns. However, the algorithm needs some modifications to enhance its accuracy on pattern matching when there are some differences between two patterns in terms of numbers of nodes, shapes and rotations. This paper presents the modifications, such as the introduction of node exemption, inexact matching between straight lines and curves in the patterns, and consideration of rotational degrees of the patterns. Each angle is also given with a weight to indicate the significant degree of the angle. A multi-objective function is used to reflect the similarity between two patterns. The experiments designed to evaluate the algorithm have shown very promising results. It is highly accurate on patterns matching with dissimilarities in shapes, numbers of nodes and rotational degrees.|Surapong Auwatanamongkol","57314|GECCO|2005|MDGA motif discovery using a genetic algorithm|Computationally identifying transcription factor binding sites in the promoter regions of genes is an important problem in computational biology and has been under intensive research for a decade. To predict the binding site locations efficiently, many algorithms that incorporate either approximate or heuristic techniques have been developed. However, the prediction accuracy is not satisfactory and binding site prediction thus remains a challenging problem. In this paper, we develop an approach that can be used to predict binding site motifs using a genetic algorithm. Based on the generic framework of a genetic algorithm, the approach explores the search space of all possible starting locations of the binding site motifs in different target sequences with a population that undergoes evolution. Individuals in the population compete to participate in the crossovers and mutations occur with a certain probability. Initial experiments demonstrated that our approach could achieve high prediction accuracy in a small amount of computation time. A promising advantage of our approach is the fact that the computation time does not explicitly depend on the length of target sequences and hence may not increase significantly when the target sequences become very long.|Dongsheng Che,Yinglei Song,Khaled Rasheed","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","43356|IEICE Transations|2006|Solving the Graph Planarization Problem Using an Improved Genetic Algorithm|An improved genetic algorithm for solving the graph planarization problem is presented. The improved genetic algorithm which is designed to embed a graph on a plane, performs crossover and mutation conditionally instead of probability. The improved genetic algorithm is verified by a large number of simulation runs and compared with other algorithms. The experimental results show that the improved genetic algorithm performs remarkably well and outperforms its competitors.|Rong Long Wang,Kozo Okazaki","57432|GECCO|2005|Primer design for multiplex PCR using a genetic algorithm|Multiplex Polymerase Chain Reaction (PCR) experiments are used for amplifying several segments of the target DNA simultaneously and thereby to conserve template DNA, reduce the experimental time, and minimize the experimental expense. The success of the experiment is dependent on primer design. However, this can be a dreary task as there are many constrains such as melting temperatures, primer length, GC content and complementarity that need to be optimized to obtain a good PCR product. Motivated by the lack of primer design tools for multiplex PCR genotypic assay, we propose a multiplex PCR primer design tool using a genetic algorithm, which is a stochastic approach based on the concept of biological evolution, biological genetics and genetic operations on chromosomes, to find an optimal selection of primer pairs for multiplex PCR experiments. The presented experimental results indicate that the proposed algorithm is capable of finding a series of primer pairs that obeies the design properties in the same tube.|Feng-Mao Lin,Hsien-Da Huang,Hsi-Yuan Huang,Jorng-Tzong Horng","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["42806|IEICE Transations|2005|The Design of Diagnosis System in Maglev Train|The diagnosis system of Maglev Train is one of most important parts, which can obtain kinds of status messages of electric and electronic devices in vehicle to ensure the whole train safety. In this paper, diagnosis system structure and diagnosis method are analyzed and discussed in detail. The disadvantages of diagnosis system are described. In virtue of the theory of ADS, some basic ideas of ADS are applied in new diagnosis system. The structure, component parts and diagnosis method of new diagnosis system are proposed, designed and discussed in detail. The analysis results show that new diagnosis not only embodies some ADS' ideas but also better meets the demands of Maglev Train Diagnosis System.|Zhigang Liu","43102|IEICE Transations|2006|Synchronization Verification in System-Level Design with ILP Solvers|Concurrency is one of the most important issues in system-level design. Interleaving among parallel processes can cause an extremely large number of different behaviors, making design and verification difficult tasks. In this work, we propose a synchronization verification method for system-level designs described in the SpecC language. Instead of modeling the design with timed FSMs and using a model checker for timed automata (such as UPPAAL or KRONOS), we formulate the timing constraints with equalitiesinequalities that can be solved by integer linear programming (ILP) tools. Verification is conducted in two steps. First, similar to other software model checkers, we compute the reachability of an error state in the absence of timing constraints. Then, if a path to an error state exists, its feasibility is checked by using the ILP solver to evaluate the timing constraints along the path. This approach can drastically increase the sizes of the designs that can be verified. Abstraction and abstraction refinement techniques based on the Counterexample-Guided Abstraction Refinement (CEGAR) paradigm are applied.|Thanyapat Sakunkonchak,Satoshi Komatsu,Masahiro Fujita","43118|IEICE Transations|2006|A Noise Reduction System for Wideband and Sinusoidal Noise Based on Adaptive Line Enhancer and Inverse Filter|A noise reduction technique to reduce wideband and sinusoidal noise in a noisy speech is proposed. In an actual environment, background noise includes not only wideband noise but also sinusoidal noise, such as ventilation fan and engine noise. In this paper, we propose a new noise reduction system which uses two types of adaptive line enhancers (ALE) and a noise estimation filter (NEF). First, the two ALEs are used to estimate speech components. The first ALE is used to reduce sinusoidal noise superposed on speech and wideband noise, while the second ALE is used to reduce wideband noise superposed on speech. However, since the quality of the speech enhanced by two ALEs is not good enough due to the difficulty in estimating unvoiced sound using the two ALEs, the NEF is used to improve on noise reduction capability. The NEF accurately estimates the background noise from the signal occupied by noise components, which is obtained by subtracting the speech enhanced by two ALEs from noisy speech. The enhanced speech is obtained by subtracting the estimated noise from noisy speech. Furthermore, the noise reduction system with feedback path is proposed to improve further the quality of enhanced speech.|Naoto Sasaoka,Keisuke Sumi,Yoshio Itoh,Kensaku Fujii,Arata Kawamura","43406|IEICE Transations|2006|Accurate Channel Estimation Method for Frequency Domain Equalization on cdma High Rate Packet Data System|In order to improve the forward link capacity of cdma HRPD (High Rate Packet Data) or CDMA xEV-DO, it is significant to overcome multi-path interference. This paper focuses on FDE (Frequency Domain Equalization) with MMSE (Minimum Mean Square Error) criterion. On top of that, backward compatibility with HRPD should be maintained, in other words common channels such as the pilot channel should not be changed. Thus, the PN (Pseudo Noise) spread pilot block without CP (Cyclic Prefix) signals has to be dealt with for FDE. However, this will cause the conventional channel estimation accuracy to deteriorate. In order to improve the estimation accuracy of the conventional method, this paper presents a MRC (Maximal Ratio Combining) spectrum estimator, IPI (Inter-Path Interference) canceller, and path searcher. The results obtained from computer simulations reveal that the proposed method can improve the PER (Packet Error Rate) performance significantly. If compared with Rake combiner and TDE (Time Domain Equalization) with NLMS (Normalized Least Mean Square) scheme, the maximum data rates at a fixed PER of % can be increased by  to  times and . to . times, respectively.|Noriaki Miyazaki,Toshinori Suzuki,Shuichi Matsumoto","57518|GECCO|2005|Design of air pump system using bond graph and genetic programming method|This paper introduces a redesign method for an air pump system using bond graphs and genetic programming to maximize outflow subject to a constraint specifying maximum power consumption. The redesign process can alter the topological connections among components and can introduce additional components. The air pump system is a mixed-domain system that includes electromagnetic, mechanical and pneumatic elements. Bond graphs are domain independent, allow free composition, and are efficient for classification and analysis of models. Genetic programming is well recognized as a powerful tool for open-ended search. The combination of these two powerful methods, BGGP, was applied for redesign of an air pump system.|Kisung Seo,Erik D. Goodman,Ronald C. Rosenberg","42380|IEICE Transations|2005|A Noise Reduction Method for Non-stationary Noise Based on Noise Reconstruction System with ALE|A noise reduction technique to reduce background noise in noisy speech is proposed. We have proposed the noise reduction method which uses a noise reconstruction system. However, since a residual speech signal is included in the input signal of a noise reconstruction filter (NRF) used for reconstructing the background noise, the long time average value of error signal for estimating the background noise is needed not to estimate the speech signal. Therefore, the ability of tracking the non-stationary noise is decreased. In order to solve this problem, we propose the noise reconstruction system with adaptive line enhancer (ALE). Since ALE works to obtain the signal occupied by noise components, the input signal of the NRF includes only a few speech components. Therefore, we can give the high tracking ability to NRF.|Naoto Sasaoka,Yoshio Itoh,Kensaku Fujii","42961|IEICE Transations|2005|Design of Realtime -D Sound Processing System|An interactive -D sound processing system and its implementation is described, which is to provide virtual auditory environments to listeners. While conventional -D sound processing systems require high performance workstations or large DSP arrays, the proposed system is reduced in hardware size for practical applications. The proposed system is implemented using a prevailing IBM-compatible PC and a single DSP. Since the organization of the proposed system is independent of implementation details such as operation precision and number of audio tracks, the proposed system can be ported to various hardware entities. In addition, an easy-to-use user interface is also implemented on PC software for realtime input of -D sound movement. Owing to these features, the presented system is valuable as a prototype for various implementation of -D sound processing systems, while the current implementation is useful as a -D sound content production system.|Kosuke Tsujino,Kazuhiko Furuya,Wataru Kobayashi,Tomonori Izumi,Takao Onoye,Yukihiro Nakamura","42555|IEICE Transations|2005|A Simple Estimation of the Rotation Parameter for the -Axes Stabilization System|The recent sight system requires high stabilization functions for the longer range of observation and the higher kill probability. To this end, it is necessary to compensate rotational disturbances which are not stabilized with the conventional -axes stabilization system. This paper proposes a simple method on the rotational motion estimation for the stabilization of the sight system.|Dong-Noh Kim,Ki-Hong Kim,Tae-Yeon Jung,Duk-Gyoo Kim","43133|IEICE Transations|2006|The AMS Extension to System Level Design Language - SpecC|Recently, system level design languages (SLDLs), which can describe both hardware and software aspects of the design, are receiving attentions. Analog mixed-signal (AMS) extensions to SLDLs enable current discrete-oriented SLDLs to describe and simulate not only digital systems but also digital-analog mixed-signal systems. In this paper, we present our work on the AMS extension to one of the system level design language---SpecC. The extended language supports designer to describe all the analog, digital and software aspects in a universal language.|Yu Liu,Satoshi Komatsu,Masahiro Fujita","42627|IEICE Transations|2005|Design of Ogg Vorbis Decoder System for Embedded Platform|This paper describes a design of Ogg Vorbis decoder for embedded platform. Since Ogg Vorbis decoding process incurs high computational complexity, a trivial software-based implementation requires high operation frequency. Thus in our design specific hardware modules are devised for functional blocks, which have higher computational complexity than other blocks in Ogg Vorbis decoding process. Based on computational cost analysis of whole decoding process, IMDCT (Inverse Modified Discrete Cosine Transform) and residue decoding process are detected as the computation-intensive functional blocks. As a result of hardware implementation, % improvement in CPU load is achieved by specific hardware modules for IMDCT and residue decoding process.|Atsushi Kosaka,Hiroyuki Okuhata,Takao Onoye,Isao Shirakawa"],["65868|AAAI|2006|Expressive Commerce and Its Application to Sourcing|Sourcing professionals buy several trillion dollars worth of goods and services yearly. We introduced a new paradigm called expressive commerce and applied it to sourcing. It combines the advantages of highly expressive human negotiation with the advantages of electronic reverse auctions. The idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auctions, and are algorithmically cleared. This creates a Pareto efficiency improvement in the allocation (a win-win between the buyer and the sellers) but the market clearing problem is a highly complex combinatorial optimization problem. We developed the world's fastest tree search algorithms for solving it. We have hosted $ billion of sourcing using the technology, and created $. billion of hard-dollar savings. The suppliers also benefited by being able to express production efficiencies and creativity, and through exposure problem removal. Supply networks were redesigned, with quantitative understanding of the tradeoffs, and implemented in weeks instead of months.|Tuomas Sandholm","43383|IEICE Transations|2006|Sequence Set with Three Zero Correlation Zones and Its Application in MC-CDMA System|Sequence set with Three Zero Correlation Zones (T-ZCZ) can efficiently mitigate the Multiple Access Interference (MAI) and Inter Symbol Interference (ISI) caused by multi-path in CDMA system. In this paper, an algorithm for generating T-ZCZ sequence set is presented. Moreover, in order to study the restrictions among the parameters such as the length of the sequence, the number of the sequences and the length of the T-ZCZ etc., the corresponding theoretical bound is investigated and proved. Additionally, the performance of T-ZCZ sequence in MC-CDMA system is evaluated to confirm the capability of interference cancellation as well as system capacity improvement.|Chao Zhang,Xiaoming Tao,Shigeki Yamada,Mitsutoshi Hatori","42867|IEICE Transations|2005|Extension of Hidden Markov Models for Multiple Candidates and Its Application to Gesture Recognition|We propose a modified Hidden Markov Model (HMM) with a view to improve gesture recognition using a moving camera. The conventional HMM is formulated so as to deal with only one feature candidate per frame. However, for a mobile robot, the background and the lighting conditions are always changing, and the feature extraction problem becomes difficult. It is almost impossible to extract a reliable feature vector under such conditions. In this paper, we define a new gesture recognition framework in which multiple candidates of feature vectors are generated with confidence measures and the HMM is extended to deal with these multiple feature vectors. Experimental results comparing the proposed system with feature vectors based on DCT and the method of selecting only one candidate feature point verifies the effectiveness of the proposed technique.|Yosuke Sato,Tetsuji Ogawa,Tetsunori Kobayashi","42891|IEICE Transations|2005|Output Phase Optimization for AND-OR-EXOR PLAs with Decoders and Its Application to Design of Adders|This paper presents a design method for three-level programmable logic arrays (PLAs), which have input decoders and two-input EXOR gates at the outputs. The PLA realizes an EXOR of two sum-of-products expressions (EX-SOP) for multiple-valued input two-valued output functions. We developed an output phase optimization method for EX-SOPs where some outputs of the function are minimized in the complemented form and presented techniques to minimize EX-SOPs for adders by using an extension of Dubrova-Miller-Muzio's AOXMIN algorithm. The proposed algorithm produces solutions with a half products of AOXMIN-like algorithm in  times shorter time for large adders with two-valued inputs. We also proved that an n-bit adder with two-valued inputs requires at most   n -  + n -  products in an EX-SOP while it is known that a sum-of-products expression (SOP) requires   n - n -  products.|Debatosh Debnath,Tsutomu Sasao","42408|IEICE Transations|2005|Robust Subspace Analysis and Its Application in Microphone Array for Speech Enhancement|A robust microphone array for speech enhancement and noise suppression is studied in this paper. To overcome target signal cancellation problem of conventional beamformer caused by array imperfections or reverberation effects of acoustic enclosure, the proposed microphone array adopts an arbitrary model of channel transfer function (TF) relating microphone and speech source. Since the estimation of channel TF itself is often intractable, herein, transfer function ratio (TFR) is estimated instead and used to form a suboptimal beamformer. A robust TFR estimation method is proposed based on signal subspace analysis technique against stationary or slowly varying noise. Experiments using simulated signal and actual signal recorded in a real room illustrate that the proposed method has high performance in adverse environment.|Zhu Liang Yu,Meng Hwa Er","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80698|VLDB|2006|Next Generation Data Management in Enterprise Application Platforms|As a leading provider of applications and application infrastructure software, SAP has been always interested in the entire spectrum of enterprise data management from transactional to analytical, structured and unstructured, as well as high-volatility event data streams. The underlying architecture for enterprise applications has fundamentally changed in the last decade, with the adoption of service-oriented architectures representing the latest shift. However, DBMS architecture has not evolved sufficiently to meet the challenges that these new application characteristics pose. As a result, at SAP we have been rethinking the way enterprise applications manage their data. In this talk, we will present some key aspects of this rethinking. We will start with a description of the shift in application architecture and the challenges that this shift poses on data management. We will then describe the failings of a single overarching DBMS architecture against these needs, and then describe some examples of usage-specific data management in enterprise application platforms. In particular we will focus on our approach to managing analytical, transactional and master data. We will present some results that describe how, with a combination of better utilization of main-memory based data management techniques, addressing the needs of the next generation application infrastructure and advances in the underlying computing and storage infrastructure, we can do significantly more efficient data management.|Vishal Sikka","57812|GECCO|2006|The complete-basis-functions parameterization in ES and its application to laser pulse shaping|This paper presents a new parameterization method for the Evolution Strategies (ES) field, and its application to a challenging real-life high-dimensional Physics optimization problem, namely Femtosecond Laser Pulse Shaping. The so-called Complete-Basis-Functions Parameterization method (CBFP), to be introduced here for the first time, is developed for tackling efficiently the given laser optimization task, but nevertheless is a general method that can be used for learning any n-variables functions. The emphasis is on dimensionality reduction of the search space and the speeding-up of the convergence process respectively. This is achieved by learning the target function by using complete-basis functions as building blocks in an evolutionary search. The method is shown to boost the learning process of the given laser problem, and to yield highly satisfying results.|Ofer M. Shir,Christian Siedschlag,Thomas B√§ck,Marc J. J. Vrakking","42709|IEICE Transations|2005|RF-CMOS Implementation of UWB Transceivers and Its Application to Video Transmission|This paper shows activities of the ultra wideband (UWB) research and development consortium organized by the National Institute of Information and Communications Technology (NICT). Fully CMOS monolithic microwave integrate circuits (MMICs) are designed and fabricated both for the multiband OFDM and the impulse radio. UWB transceivers are constructed with the MMICs as their front-end devices and evaluated by some measurements such as time domain waveform, spectrum, error vector magnitude, and so on. To show the application capabilities of the UWB transceivers, two kinds of video transmission system are constructed and demonstrated.|Akifumi Kasamatsu,Akio Tanaka,Hiroshi Kodama,Satoru Tanoi,Yasuhiro Kaizaki,Juichi Nakada,Masami Hagio,Yoshiaki Kuraishi,Keren Li,Hitoshi Utagawa,Toshiaki Matsui,Ryuji Kohno","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o"],["80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","43017|IEICE Transations|2005|Image Collector II A System to Gather a Large Number of Images from the Web|We propose a system that enables us to gather hundreds of images related to one set of keywords provided by a user from the World Wide Web. The system is called Image Collector II. The Image Collector, which we proposed previously, can gather only one or two hundreds of images. We propose the two following improvements on our previous system in terms of the number of gathered images and their precision () We extract some words appearing with high frequency from all HTML files in which output images are embedded in an initial image gathering, and using them as keywords, we carry out a second image gathering. Through this process, we can obtain hundreds of images for one set of keywords. () The more images we gather, the more the precision of gathered images decreases. To improve the precision, we introduce word vectors of HTML files embedding images into the image selecting process in addition to image feature vectors.|Keiji Yanai","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|C√©cile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","16244|IJCAI|2005|Building the Semantic Web Tower from RDF Straw|A same-syntax extension of RDF to first-order logic results in a collapse of the model theory due to logical paradoxes resulting from diagonalization. RDF is thus the wrong material for building the Semantic Web tower.|Peter F. Patel-Schneider","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["42801|IEICE Transations|2005|Composite Support Vector Machines with Extended Discriminative Features for Accurate Face Detection|This paper describes a pattern classifier for detecting frontal-view faces via learning a decision boundary. The proposed classifier consists of two major parts for improving classification accuracy the implicit modeling of both the face and the near-face classes resulting in an extended discriminative feature set, and the subsequent composite Support Vector Machines (SVMs) for speeding up the classification. For the extended discriminative feature set, Principal Component Analysis (PCA) or Independent Component Analysis (ICA) is performed for the face and near-face classes separately. The projections and distances to the two different subspaces are complementary, which significantly enhances classification accuracy of SVM. Multiple nonlinear SVMs are trained for the local facial feature spaces considering the general multi-modal characteristic of the face space. Each component SVM has a simpler boundary than that of a single SVM for the whole face space. The most appropriate component SVM is selected by a gating mechanism based on clustering. The classification by utilizing one of the multiple SVMs guarantees good generalization performance and speeds up face detection. The proposed classifier is finally implemented to work in real-time by cascading a boosting based face detector.|Tae-Kyun Kim,Josef Kittler","65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","43068|IEICE Transations|2006|Detection of Overlapping Speech in Meetings Using Support Vector Machines and Support Vector Regression|In this paper, a method of detecting overlapping speech segments in meetings is proposed. It is known that the eigenvalue distribution of the spatial correlation matrix calculated from a multiple microphone input reflects information on the number and relative power of sound sources. However, in a reverberant sound field, the feature of the number of sources in the eigenvalue distribution is degraded by the room reverberation. In the Support Vector Machines approach, the eigenvalue distribution is classified into two classes (overlapping speech segments and single speech segments). In the Support Vector Regression approach, the relative power of sound sources is estimated by using the eigenvalue distribution, and overlapping speech segments are detected based on the estimated relative power. The salient feature of this approach is that the sensitivity of detecting overlapping speech segments can be controlled simply by changing the threshold value of the relative power. The proposed method was evaluated using recorded data of an actual meeting.|Kiyoshi Yamamoto,Futoshi Asano,Takeshi Yamada,Nobuhiko Kitawaki","57436|GECCO|2005|Combating user fatigue in iGAs partial ordering support vector machines and synthetic fitness|One of the daunting challenges of interactive genetic algorithms (iGAs)---genetic algorithms in which fitness measure of a solution is provided by a human rather than by a fitness function, model, or computation---is user fatigue which leads to sub-optimal solutions. This paper proposes a method to combat user fatigue by augmenting user evaluations with a synthetic fitness function. The proposed method combines partial ordering concepts, notion of non-domination from multiobjective optimization, and support vector machines to synthesize a fitness model based on user evaluation. The proposed method is used in an iGA on a simple test problem and the results demonstrate that the method actively combats user fatigue by requiring -- times less user evaluation when compared to a simple iGA.|Xavier Llor√†,Kumara Sastry,David E. Goldberg,Abhimanyu Gupta,Lalitha Lakshmi","57480|GECCO|2005|Evolutionary strategies for multi-scale radial basis function kernels in support vector machines|In support vector machines (SVM), the kernel functions which compute dot product in feature space significantly affect the performance of classifiers. Each kernel function is suitable for some tasks. A universal kernel is not possible, and the kernel must be chosen for the tasks under consideration by hand. In order to obtain a flexible kernel function, a family of radial basis function (RBF) kernels is proposed. Multi-scale RBF kernels are combined by including weights. Then, the evolutionary strategies are used to adjust these weights and the widths of the RBF kernels. The proposed kernel is proved to be a Mercer's kernel. The experimental results show that the use of multi-scale RBF kernels result in better performance than that of a single Gaussian RBF on benchmarks.|Tanasanee Phienthrakul,Boonserm Kijsirikul","65867|AAAI|2006|Closest Pairs Data Selection for Support Vector Machines|This paper presents data selection procedures for support vector machines (SVM). The purpose of data selection is to reduce the dataset by eliminating as many non support vectors (non-SVs) as possible. Based on the fact that support vectors (SVs) are those vectors close to the decision boundary, data selection keeps only the closest pair vectors of opposite classes. The selected dataset will replace the full dataset as the training component for any standard SVM algorithm.|Chaofan Sun","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","43149|IEICE Transations|2006|Adaptive Morse Code Recognition Using Support Vector Machines for Persons with Physical Disabilities|In this paper, Morse code is selected as a communication adaptive device for persons whose hand coordination and dexterity are impaired by such ailments as amyotrophic lateral sclerosis, multiple sclerosis, muscular dystrophy, and other severe handicaps. Morse code is composed of a series of dots, dashes, and space intervals, and each element is transmitted by sending a signal for a defined length of time. A suitable adaptive automatic recognition method is needed for persons with disabilities due to their difficulty in maintaining a stable typing rate. To overcome this problem, the proposed method combines the support vector machines method with a variable degree variable step size LMS algorithm. The method is divided into five stages tone recognition, space recognition, training process, adaptive processing, and character recognition. Statistical analyses demonstrated that the proposed method elicited a better recognition rate in comparison to alternative methods from the literature.|Cheng-Hong Yang,Li-Yeh Chuang,Cheng-Huei Yang,Ching-Hsing Luo","43415|IEICE Transations|2006|Support Vector Machines Based Generalized Predictive Control of Chaotic Systems|This work presents an application of the previously proposed Support Vector Machines Based Generalized Predictive Control (SVM-Based GPC) method  to the problem of controlling chaotic dynamics with small parameter perturbations. The Generalized Predictive Control (GPC) method, which is included in the class of Model Predictive Control, necessitates an accurate model of the plant that plays very crucial role in the control loop. On the other hand, chaotic systems exhibit very complex behavior peculiar to them and thus it is considerably difficult task to get their accurate model in the whole phase space. In this work, the Support Vector Machines (SVMs) regression algorithm is used to obtain an acceptable model of a chaotic system to be controlled. SVM-Based GPC exploits some advantages of the SVM approach and utilizes the obtained model in the GPC structure. Simulation results on several chaotic systems indicate that the SVM-Based GPC scheme provides an excellent performance with respect to local stabilization of the target (an originally unstable equilibrium point). Furthermore, it somewhat performs targeting, the task of steering the chaotic system towards the target by applying relatively small parameter perturbations. It considerably reduces the waiting time until the system, starting from random initial conditions, enters the local control region, a small neighborhood of the chosen target. Moreover, SVM-Based GPC maintains its performance in the case that the measured output is corrupted by an additive Gaussian noise.|Serdar Iplikci","43085|IEICE Transations|2006|An Efficient Method for Simplifying Decision Functions of Support Vector Machines|A novel method to simplify decision functions of support vector machines (SVMs) is proposed in this paper. In our method, a decision function is determined first in a usual way by using all training samples. Next those support vectors which contribute less to the decision function are excluded from the training samples. Finally a new decision function is obtained by using the remaining samples. Experimental results show that the proposed method can effectively simplify decision functions of SVMs without reducing the generalization capability.|Jun Guo,Norikazu Takahashi,Tetsuo Nishi"],["42830|IEICE Transations|2005|Robust Speech Recognition Using Discrete-Mixture HMMs|This paper introduces new methods of robust speech recognition using discrete-mixture HMMs (DMHMMs). The aim of this work is to develop robust speech recognition for adverse conditions that contain both stationary and non-stationary noise. In particular, we focus on the issue of impulsive noise, which is a major problem in practical speech recognition system. In this paper, two strategies were utilized to solve the problem. In the first strategy, adverse conditions are represented by an acoustic model. In this case, a large amount of training data and accurate acoustic models are required to present a variety of acoustic environments. This strategy is suitable for recognition in stationary or slow-varying noise conditions. The second is based on the idea that the corrupted frames are treated to reduce the adverse effect by compensation method. Since impulsive noise has a wide variety of features and its modeling is difficult, the second strategy is employed. In order to achieve those strategies, we propose two methods. Those methods are based on DMHMM framework which is one type of discrete HMM (DHMM). First, an estimation method of DMHMM parameters based on MAP is proposed aiming to improve trainability. The second is a method of compensating the observation probabilities of DMHMMs by threshold to reduce adverse effect of outlier values. Observation probabilities of impulsive noise tend to be much smaller than those of normal speech. The motivation in this approach is that flooring the observation probability reduces the adverse effect caused by impulsive noise. Experimental evaluations on Japanese LVCSR for read newspaper speech showed that the proposed method achieved the average error rate reduction of .% in impulsive noise conditions. Also the experimental results in adverse conditions that contain both stationary and impulsive noises showed that the proposed method achieved the average error rate reduction of .%.|Tetsuo Kosaka,Masaharu Katoh,Masaki Kohda","42400|IEICE Transations|2005|An Objective Method for Evaluating Speech Translation System Using a Second Language Learners Corpus|In this paper we propose an objective method for assessing the capability of a speech translation system. It automates the translation paired comparison method, which gives a simple, easy to understand TOEIC score proposed by Sugaya et al., to succinctly evaluate a speech translation system. To avoid the expensive evaluation cost of the original method where large manual effort is required, the new objective method automates the procedure by employing an objective metric such as BLEU and DP-based measure. The evaluation results obtained by the proposed method are similar to those of the original method. Also, the proposed method is used to evaluate the usefulness of a speech translation system. It is then found that our speech translation system is useful in general, even to users with higher TOEIC score than the system's.|Keiji Yasuda,Fumiaki Sugaya,Toshiyuki Takezawa,Gen-ichiro Kikui,Seiichi Yamamoto,Masuzo Yanagida","42822|IEICE Transations|2005|Alaryngeal Speech Enhancement Using Pattern Recognition Techniques|An alaryngeal speech enhancement system is proposed to improve the intelligibility and quality of speech signals generated by an artificial larynx transducer (ALT). Proposed system identifies the voiced segments of alaryngeal speech signal, by using pattern recognition methods, and replaces these by their equivalent voiced segments of normal speech. Evaluation results show that proposed system provides a fairly good improvement of the quality and intelligibility of ALT generated speech.|Gualberto Aguilar-Torres,Mariko Nakano-Miyatake,H√©ctor M. P√©rez Meana","42347|IEICE Transations|2005|Multiple Regression of Log Spectra for In-Car Speech Recognition Using Multiple Distributed Microphones|This paper describes a new multi-channel method of noisy speech recognition, which estimates the log spectrum of speech at a closetalking microphone based on the multiple regression of the log spectra (MRLS) of noisy signals captured by distributed microphones. The advantages of the proposed method are as follows ) The method does not require a sensitive geometric layout, calibration of the sensors nor additional pre-processing for tracking the speech source ) System works in very small computation amounts and ) Regression weights can be statistically optimized over the given training data. Once the optimal regression weights are obtained by regression learning, they can be utilized to generate the estimated log spectrum in the recognition phase, where the speech of close-talking is no longer required. The performance of the proposed method is illustrated by speech recognition of real in-car dialogue data. In comparison to the nearest distant microphone and multi-microphone adaptive beamformer, the proposed approach obtains relative word error rate (WER) reductions of .% and .%, respectively.|Weifeng Li,Tetsuya Shinde,Hiroshi Fujimura,Chiyomi Miyajima,Takanori Nishino,Katunobu Itou,Kazuya Takeda,Fumitada Itakura","42328|IEICE Transations|2005|Speech Recognition Using Finger Tapping Timings|Behavioral synchronization between speech and finger tapping provides a novel approach to improving speech recognition accuracy. We combine a sequence of finger tapping timings recorded alongside an utterance using two distinct methods in the first method, HMM state transition probabilities at the word boundaries are controlled by the timing of the finger tapping in the second, the probability (relative frequency) of the finger tapping is used as a 'feature' and combined with MFCC in a HMM recognition system. We evaluate these methods through connected digit recognition under different noise conditions (AURORA-J). Leveraging the synchrony between speech and finger tapping provides a % relative improvement in connected digit recognition experiments.|Hiromitsu Ban,Chiyomi Miyajima,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","42946|IEICE Transations|2005|Speech Quality Enhancement Using Wavelet Reconstruction Filters|The present paper describes a quality enhancement of band-limited speech signals. In regular telephone communication, the quality of the received speech signal is degraded by band limitation. We propose an effective but simple scheme for obtaining narrowband speech signals in which the frequency components are estimated from band limited signals. The proposed method utilizes aliasing components generated by wavelet reconstruction filters in the inverse discrete wavelet transform. The results of enhancement have been verified by applying this method to speech samples via telephone lines to obtain a noticeable improvement in speech quality.|Seiji Hayashi,Masahiro Suguimoto","43520|IEICE Transations|2006|Robust Speech Recognition by Using Compensated Acoustic Scores|This paper proposes a new compensation method of acoustic scores in the Viterbi search for robust speech recognition. This method introduces noise models to represent a wide variety of noises and realizes robust decoding together with conventional techniques of subtraction and adaptation. This method uses likelihoods of noise models in two ways. One is to calculate a confidence factor for each input frame by comparing likelihoods of speech models and noise models. Then the weight of the acoustic score for a noisy frame is reduced according to the value of the confidence factor for compensation. The other is to use the likelihood of noise model as an alternative that of a silence model when given noisy input. Since a lower confidence factor compresses acoustic scores, the decoder rather relies on language scores and keeps more hypotheses within a fixed search depth for a noisy frame. An experiment using commentary transcriptions of a broadcast sports program (MLB Major League Baseball) showed that the proposed method obtained a .% relative word error reduction. The method also reduced the relative error rate of key words by .%, and this is expected lead to an improvement metadata extraction accuracy.|Shoei Sato,Kazuo Onoe,Akio Kobayashi,Toru Imai","42715|IEICE Transations|2005|Adaptive Nonlinear Regression Using Multiple Distributed Microphones for In-Car Speech Recognition|In this paper, we address issues in improving hands-free speech recognition performance in different car environments using multiple spatially distributed microphones. In the previous work, we proposed the multiple linear regression of the log spectra (MRLS) for estimating the log spectra of speech at a close-talking microphone. In this paper, the concept is extended to nonlinear regressions. Regressions in the cepstrum domain are also investigated. An effective algorithm is developed to adapt the regression weights automatically to different noise environments. Compared to the nearest distant microphone and adaptive beamformer (Generalized Sidelobe Canceller), the proposed adaptive nonlinear regression approach shows an advantage in the average relative word error rate (WER) reductions of .% and .%, respectively, for isolated word recognition under  real car environments.|Weifeng Li,Chiyomi Miyajima,Takanori Nishino,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","42363|IEICE Transations|2005|An Unsupervised Speaker Adaptation Method for Lecture-Style Spontaneous Speech Recognition Using Multiple Recognition Systems|This paper describes an accurate unsupervised speaker adaptation method for lecture style spontaneous speech recognition using multiple LVCSR systems. In an unsupervised speaker adaptation framework, the improvement of recognition performance by adapting acoustic models remarkably depends on the accuracy of labels such as phonemes and syllables. Therefore, extraction of the adaptation data guided by confidence measure is effective for unsupervised adaptation. In this paper, we looked for the high confidence portions based on the agreement between two LVCSR systems, adapted acoustic models using the portions attached with high accurate labels, and then improved the recognition accuracy. We applied our method to the Corpus of Spontaneous Japanese (CSJ) and the method improved the recognition rate by about .% in comparison with a traditional method.|Seiichi Nakagawa,Tomohiro Watanabe,Hiromitsu Nishizaki,Takehito Utsuro","43509|IEICE Transations|2006|Robust Recognition of Fast Speech|This letter describes a robust speech recognition system for recognizing fast speech by stretching the length of the utterance in the cepstrum domain. The degree of stretching for an utterance is determined by its rate of speech (ROS), which is based on a maximum likelihood (ML) criterion. The proposed method was evaluated on -digits mobile phone numbers. The results of the simulation show that the overall error rate was reduced by .% when the proposed method was employed.|Ki-Seung Lee"],["43385|IEICE Transations|2006|Evolutionary Computing of Petri Net Structure for Cyclic Job Shop Scheduling|This paper considers Cyclic Job-Shop Scheduling Problems (CJSSP) extended from the Job-Shop Scheduling Problem (JSSP). We propose an evolutionary computing method to solve the problem approximately by generating the Petri net structure for scheduling. The crossover proposed in this paper employs structural analysis of Petri net model, that is, the crossover improves the cycle time by breaking the bottle-neck circuit obtained by solving a linear programming problem. Experimental evaluation shows the effectiveness of our approach.|Morikazu Nakamura,Koji Hachiman,Hiroki Tohme,Takeo Okazaki,Shiro Tamaki","43211|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Construction Using a Cyclic Hadamard Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone (hereinafter binary ZCA sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCA sequence set by using a cyclic difference set and a collection of mutually orthogonal complementary sets.|Takafumi Hayashi","43164|IEICE Transations|2006|Ternary Sequence Set Having Periodic and Aperiodic Zero-Correlation Zone|A new class of ternary sequence with a zero-correlation zone is introduced. The proposed sequence sets have a zero-correlation zone for both periodic and aperiodic correlation functions. The proposed sequences can be constructed from a pair of Hadamard matrices of size n  n and a Hadamard matrix of size n  n. The constructed sequence set consists of nn ternary sequences, and the length of each sequence is n(m+)(n + ) for a non-negative integer m. The zero-correlation zone of the proposed sequences is  nm-, where  is the phase shift. The sequence member size of the proposed sequence set is equal to nn+ times that of the theoretical upper bound of the member size of a sequence set with a zero-correlation zone.|Takafumi Hayashi","57842|GECCO|2006|Pairwise sequence comparison for fitness evaluation in evolutionary structural software testing|Evolutionary algorithms are among the metaheuristic search methods that have been applied to the structural test data generation problem. Fitness evaluation methods play an important role in the performance of evolutionary algorithms and various methods have been devised for this problem. In this paper, we propose a new fitness evaluation method based on pairwise sequence comparison also used in bioinformatics. Our preliminary study shows that this method is easy to implement and produces promising results.|H. Turgut Uyar,A. Sima Etaner-Uyar,A. Emre Harmanci","43279|IEICE Transations|2006|Families of Sequence Pairs with Zero Correlation Zone|A family of sequences with zero correlation zone, which is shortly called a ZCZ set, can provide CDMA system without co-channel interference nor influence of multipath. This paper presents two types of ZCZ sets of non-binary sequence pairs, which achieve the upper bound of family size for length and zero correlation zone. One, which is produced by use of a perfect complementary pair and an orthogonal code, can change zero correlation zone, while the upper bound is kept. The other, which is generated by use of a newly defined orthogonal pair and an orthogonal code, can offer such CDMA system as a binary ZCZ set seems to be used.|Shinya Matsufuji","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","42514|IEICE Transations|2005|A New Class of Polyphase Sequence Sets with Optimal Zero-Correlation Zones|This paper proposes a new class of polyphase ZCZ (zero-correlation zone) sequence sets which satisfy a mathematical upper bound. The proposed ZCZ sequence sets are obtained from DFT matrices and unitary matrices. In addition, this paper discusses the cross-correlation property between different ZCZ sequence sets which belong to the proposed class.|Hideyuki Torii,Makoto Nakamura,Naoki Suehiro","57723|GECCO|2006|Combining simplex with niche-based evolutionary computation for job-shop scheduling|We propose a hybrid algorithm (called ALPINE) between Genetic Algorithm and Dantzig's Simplex method to approximate optimal solutions for the Flexible Job-Shop Problem. Locally, Simplex is extended for the JSP linear program to reduce the number of infeasible solutions while solution quality is improved with an operation order search. Globally, a niche-based evolutionary strategy is employed to gain parallelization while solution diversity is maintained in two ways composite dispatching rule-based population initialization and memory-based machine assignment. Performance results on benchmark problems show that ALPINE outperforms existing hybrid techniques with a new global optima found for the x Flexible Job Shop Problem.|Syhlin Kuah,Joc Cing Tay","57346|GECCO|2005|A comparison of messy GA and permutation based GA for job shop scheduling|This paper presents the results of a fair comparison between a messy GA and a permutation based simple GA as applied to a job shop scheduling system. An examination is made at a macro level in terms of performance and quality of schedules achieved and conclusions are drawn as to the superiority of messy GA or otherwise.|Pio Fenton,Paul Walsh","43089|IEICE Transations|2006|Binary Zero-Correlation Zone Sequence Set Constructed from an M-Sequence|The present paper introduces an improved construction of a class of binary sequences having a zero-correlation zone (hereafter binary ZCZ sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCZ sequence set from an arbitrary M-sequence. The previously reported sequence construction of binary ZCZ sequence sets from an M-sequence can generate a single series of binary ZCZ sequence sets from an M-sequence. The present paper proposes an improved sequence construction that can generate more than one series of binary ZCZ sequence sets from an M-sequence.|Takafumi Hayashi"],["57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","43119|IEICE Transations|2006|Particle Swarm Optimization Algorithm for Energy-Efficient Cluster-Based Sensor Networks|In order to reduce the traffic load and improve the system's lifetime, a cluster-based routing protocol has attracted more attention. In cluster-based sensor networks, energy can be conserved by combining redundant data from nearby sensors into cluster head nodes before forwarding the data to the destination. The lifespan of the whole network can also be expanded by the clustering of sensor nodes and through data aggregation. In this paper, we propose a cluster-based routing protocol which uses the location information of sensors to assist in network clustering. Our protocol partitions the entire network into several clusters by a particle swarm optimization (PSO) clustering algorithm. In each cluster, a cluster head is selected to deal with data aggregation or compression of nearby sensor nodes. For this clustering technique, the correct selection of the number of clusters is challenging and important. To cope with this issue, an energy dissipation model is used in our protocol to automatically estimate the optimal number of clusters. Several variations of PSO-clustering algorithm are proposed to improve the performance of our protocol. Simulation results show that the performance of our protocol is better than other protocols.|Tzay-Farn Shih","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","43209|IEICE Transations|2006|Objective Function Adjustment Algorithm for Combinatorial Optimization Problems|An improved algorithm of Guided Local Search called objective function adjustment algorithm is proposed for combinatorial optimization problems. The performance of Guided Local Search is improved by objective function adjustment algorithm using multipliers which can be adjusted during the search process. Moreover, the idea of Tabu Search is introduced into the objective function adjustment algorithm to further improve the performance. The simulation results based on some TSPLIB benchmark problems showed that the objective function adjustment algorithm could find better solutions than Local Search, Guided Local Search and Tabu Search.|Hiroki Tamura,Zongmei Zhang,Zheng Tang,Masahiro Ishii","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","57298|GECCO|2005|Map-labelling with a multi-objective evolutionary algorithm|We present a multi-objective evolutionary algorithm approach to the map-labelling problem. Map-labelling involves placing labels for sites onto a map such that the result is easy to read and usable for navigation. However, map-users vary in their priorities and capabilities for example, sight-impaired users need to maximise font-size, whereas other users may be willing to accept smaller labels in exchange for increased clarity of bindings of labels to sites. With a multi-objective approach, we evolve a range of labellings from which users can select according to their particular circumstances. We present results from labelling two maps, including a difficult, dense map of Newcastle County in Delaware, which clearly illustrate the advantages of the multi-objective approach.|Lucas Bradstreet,Luigi Barone,R. Lyndon While","57416|GECCO|2005|A co-evolutionary hybrid algorithm for multi-objective optimization of gene regulatory network models|In this paper, the parameters of a genetic network for rice flowering time control have been estimated using a multi-objective genetic algorithm approach. We have modified the recently introduced concept of fuzzy dominance to hybridize the well-known Nelder Mead Simplex algorithm for better exploitation with a multi-objective genetic algorithm. A co-evolutionary approach is proposed to adapt the fuzzy dominance parameters. Additional changes to the previous approach have also been incorporated here for faster convergence, including elitism. Our results suggest that this hybrid algorithm performs significantly better than NSGA-II, a standard algorithm for multi-objective optimization.|Praveen Koduru,Sanjoy Das,Stephen Welch,Judith L. Roe,Zenaida P. Lopez-Dee","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["80676|VLDB|2006|Adaptive Density Estimation|This demonstration illustrates the APDF tree an adaptive tree that supports the effective and effcient computation of continuous density information. The APDF tree allocates more partition points in non-linear areas of the density function and fewer points in linear areas of the density function. This yields not only a bounded, but a tight control of the error. The demonstration explains the core steps of the computation of the APDF tree (split, kernel additions, tree optimization, kernel additions, unsplit) and demos the implementation for different datasets.|Arturas Mazeika,Michael H. B√∂hlen,Andrej Taliun","43118|IEICE Transations|2006|A Noise Reduction System for Wideband and Sinusoidal Noise Based on Adaptive Line Enhancer and Inverse Filter|A noise reduction technique to reduce wideband and sinusoidal noise in a noisy speech is proposed. In an actual environment, background noise includes not only wideband noise but also sinusoidal noise, such as ventilation fan and engine noise. In this paper, we propose a new noise reduction system which uses two types of adaptive line enhancers (ALE) and a noise estimation filter (NEF). First, the two ALEs are used to estimate speech components. The first ALE is used to reduce sinusoidal noise superposed on speech and wideband noise, while the second ALE is used to reduce wideband noise superposed on speech. However, since the quality of the speech enhanced by two ALEs is not good enough due to the difficulty in estimating unvoiced sound using the two ALEs, the NEF is used to improve on noise reduction capability. The NEF accurately estimates the background noise from the signal occupied by noise components, which is obtained by subtracting the speech enhanced by two ALEs from noisy speech. The enhanced speech is obtained by subtracting the estimated noise from noisy speech. Furthermore, the noise reduction system with feedback path is proposed to improve further the quality of enhanced speech.|Naoto Sasaoka,Keisuke Sumi,Yoshio Itoh,Kensaku Fujii,Arata Kawamura","42416|IEICE Transations|2005|An Adaptive Receiver for Power-Line Communications with the Estimation of Instantaneous Noise Power|The noise on power-lines is non-stationary, while the instantaneous noise power in different frequency bands are dependent. Under such noise environments, the instantaneous noise power in a frequency band can be estimated by observing the noise in other frequency bands. In this paper, we propose a receiver structure which uses the estimated instantaneous noise power in the decoding process and show its superiority in BER performance to conventional systems.|Yuichi Hirayama,Hiraku Okada,Takaya Yamazato,Masaaki Katayama","42604|IEICE Transations|2005|Maximum Likelihood Estimation of Trellis Encoder and Modulator Transition Utilizing HMM for Adaptive Channel Coding and Modulation Technique|In order to achieve adaptive channel coding and adaptive modulation, the main causes of degradation to system performance are the decoder selection error and modulator estimation error. The utilization of supplementary information, in an estimation system utilizing channel estimation results, blind modulation estimation, and blind encoder estimation using several decoders information and encoder transitions have been considered to overcome these two problems. There are however many issues in these methods, such as the channel estimation difference between transmitter and receiver, computational complexity and the assumption of perfect Channel State Information (CSI). Our proposal, on the other hand, decreases decoder and demodulator selection error using a Hidden-Markov Model (HMM). In order to estimate the switching patterns of the encoder and modulator, our proposed system selects the maximum likelihood encoder and modulator transition patterns using both encoder and modulator transition probability based on the HMM obtained by CSI and also Decoder and Demodulator Selection Error probabilities. Therefore, the decoder and demodulation results can be achieved efficiently without any restraint on the pattern of switching encoder and modulation.|Kentaro Ikemoto,Ryuji Kohno","43110|IEICE Transations|2006|Noise Spectrum Estimation with Entropy-Based VAD in Non-stationary Environments|This study presents a fast adaptive algorithm for noise estimation in non-stationary environments. To make noise estimation adapt quickly to non-stationary noise environments, a robust entropy-based voice activity detection (VAD) is thus required. It is well-known that the entropy-based measure defined in spectral domain is very insensitive to the changing level of nose. To exploit the specific nature of straight lines existing on speech-only spectrogram, the proposed spectrum entropy measurement improved from spectrum entropy proposed by Shen et al. is further presented and is named band-splitting spectrum entropy (BSE). Consequently, the proposed recursive noise estimator including BSE-based VAD can update noise power spectrum accurately even if the noise-level quickly changes.|Bing-Fei Wu,Kun-Ching Wang","80650|VLDB|2006|Link Spam Detection Based on Mass Estimation|Link spamming intends to mislead search engines and trigger an artificially high link-based ranking of specific target web pages. This paper introduces the concept of spam mass, a measure of the impact of link spamming on a page's ranking. We discuss how to estimate spam mass and how the estimates can help identifying pages that benefit significantly from link spamming. In our experiments on the host-level Yahoo web graph we use spam mass estimates to successfully identify tens of thousands of instances of heavyweight link spamming.|Zolt√°n Gy√∂ngyi,Pavel Berkhin,Hector Garcia-Molina,Jan O. Pedersen","43425|IEICE Transations|2006|Robust Talker Direction Estimation Based on Weighted CSP Analysis and Maximum Likelihood Estimation|This paper describes a new talker direction estimation method for front-end processing to capture distant-talking speech by using a microphone array. The proposed method consists of two algorithms One is a TDOA (Time Delay Of Arrival) estimation algorithm based on a weighted CSP (Cross-power Spectrum Phase) analysis with an average speech spectrum and CSP coefficient subtraction. The other is a talker direction estimation algorithm based on ML (Maximum Likelihood) estimation in a time sequence of the estimated TDOAs. To evaluate the effectiveness of the proposed method, talker direction estimation experiments were carried out in an actual office room. The results confirmed that the talker direction estimation performance of the proposed method is superior to that of the conventional methods in both diffused- and directional-noise environments.|Yuki Denda,Takanobu Nishiura,Yoichi Yamashita","43323|IEICE Transations|2006|An Adaptive Manipulator Controller Based on Force and Parameter Estimation|Consideration of manipulator dynamics and external disturbances in robot control system design can enhance the stability and performance properties of the whole system. In this paper, we present an approach to solve the control problem when the inertia parameters of robot are unknown, and at the same time robot is subjected to external force disturbances. This approach is based on simultaneous estimation of force signal and inertia parameters and utilizing them in the control law. The update laws and the control law are derived based on a single time-varying Lyapunov function, so that the global convergence of the tracking error is ensured. A theorem with a detailed proof is presented to guarantee the global uniform asymptotic stability of the whole system. Some simulations are made for a number of external forces to illustrate the effectiveness of the proposed approach.|Mohammad Danesh,Farid Sheikholeslam,Mehdi Keshmiri","43725|IEICE Transations|2006|Adaptive Noise Estimation Using Least-Squares Line in Wavelet Packet Transform Domain|In this letter, we suggest a noise estimation method which can be applied for speech enhancement in various noise environments. The proposed method consists of the following two main processes to analyze and estimate efficiently the noise from the noisy speech. First, a least-squares line is used, which is obtained by applying coefficient magnitudes in node with a uniform wavelet packet transform to a least squares method. Next, a differential forgetting factor and a correlation coefficient per subband are applied, where each subband consists of several nodes with the uniform wavelet packet transform. In particular, this approach has the ability to update noise estimation by using the estimated noise at the previous frame only instead of employing the statistical information of long past frames and explicit nonspeech frames detection consisted of noise signals. In objective assessments, we observed that the performance of the proposed method was better than that of the compared methods. Furthermore, our method showed a reliable result even at low SNR.|Sung-il Jung,Younghun Kwon,Sung-il Yang","42696|IEICE Transations|2005|Spectrum Estimation by Noise-Compensated Data Extrapolation|High-resolution spectrum estimation techniques have been extensively studied in recent publications. Knowledge of the noise variance is vital for spectrum estimation from noise-corrupted observations. This paper presents the use of noise compensation and data extrapolation for spectrum estimation. We assume that the observed data sequence can be represented by a set of autoregressive parameters. A recently proposed iterative algorithm is then used for noise variance estimation while autoregressive parameters are used for data extrapolation. We also present analytical results to show the exponential decay characteristics of the extrapolated samples and the frequency domain smoothing effect of data extrapolation. Some statistical results are also derived. The proposed noise-compensated data extrapolation approach is applied to both the autoregressive and FFT-based spectrum estimation methods. Finally, simulation results show the superiority of the method in terms of bias reduction and resolution improvement for sinusoids buried in noise.|Jonah Gamba,Tetsuya Shimamura"],["42414|IEICE Transations|2005|The Adaptive Distributed Source Coding of Multi-View Images in Camera Sensor Networks|We show that distributed source coding of multi-view images in camera sensor networks (CSNs) using adaptive modules can come close to the Slepian-Wolf bound. In a systematic scenario with limited node abilities, work by Slepian and Wolf suggest that it is possible to encode statistically dependent signals in a distributed manner to the same rate as with a system where the signals are jointly encoded. We considered three nodes (PN, CN and CNs), which are statistically depended. Different distributed architecture solutions are proposed based on a parent node and child node framework. A PN sends the whole image whereas a CNsCN only partially, using an adaptive coding based on adaptive module-operation at a rate close to theoretical bound - H(CNsPN)H(CNPN,CNs). CNs sends sub-sampled image and encodes the rest of image, however CN encodes all image. In other words, the proposed scheme allows independent encoding and jointly decoding of views. Experimental results show performance close to the information-theoretic limit. Furthermore, good performance of the proposed architecture with adaptive scheme shows significant improvement over previous work.|Mehrdad Panahpour Tehrani,Toshiaki Fujii,Masayuki Tanimoto","42401|IEICE Transations|2005|Optimal Quantization Noise Allocation and Coding Gain in Transform Coding with Two-Dimensional Morphological Haar Wavelet|This paper analytically formulates both the optimal quantization noise allocation ratio and the coding gain of the two-dimensional morphological Haar wavelet transform. The two-dimensional morphological Haar wavelet transform has been proposed as a nonlinear wavelet transform. It has been anticipated for application to nonlinear transform coding. To utilize a transformation to transform coding, both the optimal quantization noise allocation ratio and the coding gain of the transformation should be derived beforehand regardless of whether the transformation is linear or nonlinear. The derivation is crucial for progress of nonlinear transform image coding with nonlinear wavelet because the two-dimensional morphological Haar wavelet is the most basic nonlinear wavelet. We derive both the optimal quantization noise allocation ratio and the coding gain of the two-dimensional morphological Haar wavelet transform by introducing appropriate approximations to handle the cumbersome nonlinear operator included in the transformation. Numerical experiments confirmed the validity of formulations.|Yasunari Yokota,Xiaoyong Tan","43010|IEICE Transations|2005|Variable Frame Skipping Scheme Based on Estimated Quality of Non-coded Frames at Decoder for Real-Time Video Coding|This paper proposes a block-based video encoder employing variable frame skipping (VFS) to improve the video quality in low bit rate channel. The basic idea of VFS mechanism is to decide and skip a suitable, non-fixed number of frames in temporal domain to reduce bit usage. The saved bits can be allocated to enhance the spatial quality of video. In literature, several methods of frame skipping decision have been proposed, but most of them only consider the similarities between neighboring coded frames as the decision criteria. Our proposed method takes into account the reconstruction of the skipped frames using motion-compensated frame interpolation at decoder. The proposed VFS models the reconstructed objective quality of the skipped frame and, therefore, can provide a fast estimate to the frame skipping at encoder. The proposed VFS can determine the suitable frame skipping in real time and provide the encoded video with better spatial-temporal bit allocation.|Tien-Ying Kuo","42607|IEICE Transations|2005|Switching Wavelet Transform for ROI Image Coding|In region-of-interest (ROI) image coding based on wavelet transforms, the tap length of the wavelet filter as well as energy compaction characteristics affect the quality of the restored image. This paper presents a wavelet transform comprised of two wavelet filter sets with different tap lengths. The wavelet filter is switched to the shorter-length set to code a ROI of an image and to the longer-length one for the remaining region, the region of non-interest (RONI). ROI coding examples demonstrate that this switching wavelet transform provides better quality levels than fixed transforms under the same total bits the quality of the recovered ROI is improved in the lossy coding of both regions while that of the full image is improved in the lossless coding of the ROI.|Shinji Fukuma,Toshihiko Tanaka,Masahiko Nawate","43332|IEICE Transations|2006|A Hardware Implementation of a Content-Based Motion Estimation Algorithm for Real-Time MPEG- Video Coding|Power efficiency and real-time processing capability are two major issues in today's mobile video applications. We proposed a novel Motion Estimation (ME) engine for power-efficient real-time MPEG- video coding based on our previously proposed content-based ME algorithm , . By adopting Full Search (FS) and Three Step Search (TSS) alternatively according to the nature of video contents, this algorithm keeps the visual quality very close to that of FS with only % of its computational power. We designed a flexible Block Matching (BM) Unit with -PE SIMD data path so that the adaptive ME can be performed at a much lower clock frequency and hardware cost as compared with previous FS based work. To reduce the energy cost caused by excessive external memory access, on-chip SRAM is also utilized and optimized for parallel processing in the BM Unit. The ME engine is fabricated with TSMC . m technology. When processing QCIF ( fps) video, the estimated power is . mW. MHz (supply voltage . V). It is believed to be a favorable contribution to the video encoder LSI design for mobile applications.|Shen Li,Takeshi Ikenaga,Hideki Takeda,Masataka Matsui,Satoshi Goto","57713|GECCO|2006|Classifier prediction based on tile coding|This paper introduces XCSF extended with tile coding prediction each classifier implements a tile coding approximator the genetic algorithm is used to adapt both classifier conditions (i.e., to partition the problem) and the parameters of each approximator thus XCSF evolves an ensemble of tile coding approximators instead of the typical monolithic approximator used in reinforcement learning. The paper reports a comparison between (i) XCSF with tile coding prediction and (ii) plain tile coding. The results show that XCSF with tile coding always reaches optimal performance, it usually learns as fast as the best parametrized tile coding, and it can be faster than the typical tile coding setting. In addition, the analysis of the evolved tile coding ensembles shows that XCSF actually adapts local approximators following what is currently considered the best strategy to adapt the tile coding parameters in a given problem.|Pier Luca Lanzi,Daniele Loiacono,Stewart W. Wilson,David E. Goldberg","43271|IEICE Transations|2006|Subband Adaptive Array for Space-Time Block Coding|Diversity transmission using space-time block coding (STBC) shows a degraded performance in frequency selective fading (FSF) channel. In this paper, assuming the CSI is unknown at both transmitter and receiver while a pilot signal is available during the training period, we propose a MIMO transmission scheme using STBC by adopting subband adaptive array (SBAA) processing. The receive signal is converted into the frequency-domain and adaptive processing is done at each subband. A novel construction of SBAA is introduced to process received signal based on STBC. Simulation results demonstrate that the proposed scheme has a better performance compare to conventional STBC, and has a better performance and less computational load compare to STBC-TDLAA.|Nordin Bin Ramli,Xuan Nam Tran,Tetsuki Taniguchi,Yoshio Karasawa","43634|IEICE Transations|2006|A Gradient Based Predictive Coding for Lossless Image Compression|Natural, continuous tone images have a very important property of high correlation of adjacent pixels. Images which we wish to compress are usually non-stationary and can be reasonably modeled as smooth and textured areas separated by edges. This property has been successfully exploited in LOCO-I and CALIC by applying gradient based predictive coding as a major de-correlation tool. However, they only examine the horizontal and vertical gradients, and assume the local edge can only occur in these two directions. Their over-simplified assumptions hurt the robustness of the prediction in higher complex areas. In this paper, we propose an accurate gradient selective prediction (AGSP) algorithm which is designed to perform robustly around any type of image texture. Our method measures local texture information by comparison and selection of normalized scalar representation of the gradients in four directions. An adaptive predictor is formed based on the local gradient information and immediate causal pixels. Local texture properties are also exploited in the context modeling of the prediction error. The results we obtained on a test set of several standard images are encouraging. On the average, our method achieves a compression ratio significantly better than CALIC without noticeably increasing of computational complexity.|Haijiang Tang,Sei-ichiro Kamata","43384|IEICE Transations|2006|Polyphase Downsampling Based Multiple Description Coding Applied to H Video Coding|This paper presents a video coding method that improves error resilient functionality of H. with good coding efficiency. The method is based on PD (polyphase downsampling) multiple description coding. The only changes to H. are inserting PD before the DCT process and having new data partitioning NAL units. A coded slice is sent on  data partitioning NAL units. A header NAL unit contains motion vectors and block modes. Each of the other two NAL units contains a description generated by PD multiple description coding. The experimental results on all  of the test sequences of JVT SVC show that the proposed method gives . to  dB enhancement over the existing H. FMO checker board mode with motion vector based error-concealment.|Jie Jia,Hae-Kwang Kim","42502|IEICE Transations|2005|Source Coding Algorithms Using the Randomness of a Past Sequence|We propose source coding algorithms that use the randomness of a past sequence. The proposed algorithms solve the problems of multi-terminal source coding, rate-distortion source coding, and source coding with partial side information at the decoder. We analyze the encoding rate and the decoding error rate in terms of almost-sure convergence.|Jun Muramatsu"]]}}