{"abstract":{"entropy":5.113532106685635,"topics":["data stream, emerging queries, query language, processing data, processing stream, data model, applications data, applications queries, data streaming, queries data, queries, tracking, querying, framework, problem, novel, efficient, repositories, present, data","recent years, data mining, tools, made, number, important, text, major, repositories, xml, applications, data, database","optimization query, query, database query, database, execution, relational, search, web, evaluation, used, xml, systems","management data, database data, management systems, xml data, systems data, views, systems, paper, xpath, computing, semantic, challenge, data, key, large, information, based, present, applications, queries","repositories","data stream, processing data, processing stream, data streaming","made, number, repositories","recent years, tools","database, relational","search, web, execution","xml, present, xpath","views, computing, semantic"],"ranking":[["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","80503|VLDB|2005|Sketching Streams Through the Net Distributed Approximate Query Tracking|Emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Effective solutions have to be simultaneously spacetime efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and spacetime-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.|Graham Cormode,Minos N. Garofalakis","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80497|VLDB|2005|An Efficient SQL-based RDF Querying Scheme|Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings ) They are difficult to integrate with SQL queries used in database applications, and ) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDFMATCH to query RDF data. The results of RDFMATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDFMATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDFMATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using  million RDF triples from UniProt protein and annotation data.|Eugene Inseok Chong,Souripriya Das,George Eadon,Jagannathan Srinivasan","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80473|VLDB|2005|On k-Anonymity and the Curse of Dimensionality|In recent years, the wide availability of personal data has made the problem of privacy preserving data mining an important one. A number of methods have recently been proposed for privacy preserving data mining of multidimensional data records. One of the methods for privacy preserving data mining is that of anonymization, in which a record is released only if it is indistinguishable from k other entities in the data. We note that methods such as k-anonymity are highly dependent upon spatial locality in order to effectively implement the technique in a statistically robust way. In high dimensional space the data becomes sparse, and the concept of spatial locality is no longer easy to define from an application point of view. In this paper, we view the k-anonymization problem from the perspective of inference attacks over all possible combinations of attributes. We show that when the data contains a large number of attributes which may be considered quasi-identifiers, it becomes difficult to anonymize the data without an unacceptably high amount of information loss. This is because an exponential number of combinations of dimensions can be used to make precise inference attacks, even when individual attributes are partially specified within a range. We provide an analysis of the effect of dimensionality on k-anonymity methods. We conclude that when a data set contains a large number of attributes which are open to inference attacks, we are faced with a choice of either completely suppressing most of the data or losing the desired level of anonymity. Thus, this paper shows that the curse of high dimensionality also applies to the problem of privacy preserving data mining.|Charu C. Aggarwal","80516|VLDB|2005|Parameter Free Bursty Events Detection in Text Streams|Text classification is a major data mining task. An advanced text classification technique is known as partially supervised text classification, which can build a text classifier using a small set of positive examples only. This leads to our curiosity whether it is possible to find a set of features that can be used to describe the positive examples. Therefore, users do not even need to specify a set of positive examples. As the first step, in this paper, we formalize it as a new problem, called hot bursty events detection, to detect bursty events from a text stream which is a sequence of chronologically ordered documents. Here, a bursty event is a set of bursty features, and is considered as a potential category to build a text classifier. It is important to know that the hot bursty events detection problem, we study in this paper, is different from TDT (topic detection and tracking) which attempts to cluster documents as events using clustering techniques. In other words, our focus is on detecting a set of bursty features for a bursty event. In this paper, we propose a new novel parameter free probabilistic approach, called feature-pivot clustering. Our main technique is to fully utilize the time information to determine a set of bursty features which may occur in differenttime windows. We detect bursty events based on the feature distributions. There is no need to tune or estimate any parameters. We conduct experiments using real life data, a major English newspaper in Hong Kong, and show that the parameter free feature-pivot clustering approach can detect the bursty events with a high success rate.|Gabriel Pui Cheong Fung,Jeffrey Xu Yu,Philip S. Yu,Hongjun Lu","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80571|VLDB|2005|CMS-ToPSS Efficient Dissemination of RSS Documents|Recent years have seen a rise in the number of unconventional publishing tools on the Internet. Tools such as wikis, blogs, discussion forums, and web-based content management systems have experienced tremendous rise in popularity and use primarily because they provide something traditional tools do not easy of use for non computer-oriented users and they are based on the idea of \"collaboration.\" It is estimated, by pewinternet.org, that  million people in the US read blogs (which represents % of the estimated  million US Internet users) while  million people have said that they have created blogs.|Milenko Petrovic,Haifeng Liu,Hans-Arno Jacobsen","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80506|VLDB|2005|Bridging the Gap between OLAP and SQL|In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N , ranking , , spreadsheet computations , grouping sets , data cube , and moving sums  in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the clientserver architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.|Jens-Peter Dittrich,Donald Kossmann,Alexander Kreutz","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80482|VLDB|2005|MINERVA Collaborative PP Search|This paper proposes the live demonstration of a prototype of MINERVA, a novel PP Web search engine. The search engine is layered on top of a DHT-based overlay network that connects an a-priori unlimited number of peers, each of which maintains a personal local database and a local search facility. Each peer posts a small amount of metadata to a physically distributed directory that is used to efficiently select promising peers from across the peer population that can best locally execute a query. The proposed demonstration serves as a proof of concept for PP Web search by deploying the project on standard notebook PCs and also invites everybody to join the network by instantly installing a small piece of software from a USB memory stick.|Matthias Bender,Sebastian Michel,Peter Triantafillou,Gerhard Weikum,Christian Zimmer","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli"],["80548|VLDB|2005|Hubble An Advanced Dynamic Folder Technology for XML|A significant amount of information is stored in computer systems today, but people are struggling to manage their documents such that the information is easily found. XML is a de-facto standard for content publishing and data exchange. The proliferation of XML documents has created new challenges and opportunities for managing document collections. Existing technologies for automatically organizing document collections are either imprecise or based on only simple criteria. Since XML documents are self describing, it is now possible to automatically categorize XML documents precisely, according to their content. With the availability of the standard XML query languages, e.g. XQuery, much more powerful folder technologies are now feasible. To address this new challenge and exploit this new opportunity, this paper proposes a new and powerful dynamic folder mechanism, called Hubble. Hubble fully exploits the rich data model and semantic information embedded in the XML documents to build folder hierarchies dynamically and to categorize XML collections precisely. Besides supporting basic folder operations, Hubble also provides advanced features such as multi-path navigation and folder traversal across multiple document collections. Our performance study shows that Hubble is both efficient and scalable. Thus, it is an ideal technology for automating the process of organizing and categorizing XML documents.|Ning Li,Joshua Hui,Hui-I Hsiao,Kevin S. Beyer","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80507|VLDB|2005|iMeMex Escapes from the Personal Information Jungle|Modern computer work stations provide thousands of applications that store data in  . files on the file system of the underlying OS. To handle these files data processing logic is reinvented inside each application. This results in a jungle of data processing solutions and a jungle of data and file formats. For a user, it is extremely hard to manage information in this jungle. Most of all it is impossible to use data distributed among different files and formats for combined queries, e.g., join and union operations. To solve the problems arising from file based data management, we present a software system called iMeMex as a unified solution to personal information management and integration. iMeMex is designed to integrate seamlessly into existing operating systems like Windows, Linux and Mac OS X. Our system enables existing applications to gradually dispose file based storage. By using iMeMex modern operating systems are enabled to make use of sophisticated DBMS, IR and data integration technologies. The seamless integration of iMeMex into existing operating systems enables new applications that provide concepts of data storage and analysis unseen before.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles,Donald Kossmann,Lukas Blunschi","80556|VLDB|2005|The Integrated Microbial Genomes IMG System A Case Study in Biological Data Management|Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.|Victor M. Markowitz,Frank Korzeniewski,Krishna Palaniappan,Ernest Szeto,Natalia Ivanova,Nikos Kyrpides","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Ravi Vijay","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum","80510|VLDB|2005|Query Translation from XPath to SQL in the Presence of Recursive DTDs|The interaction between recursion in XPATH and recursion in DTDS makes it challenging to answer XPATH queries on XML data that is stored in an RDBMS via schema-based shredding. We present a new approach to translating XPATH queries into SQL queries with a simple least fixpoint (LFP) operator, which is already supported by most commercial RDBMS. The approach is based on our algorithm for rewriting XPATH queries into regular XPATH expressions, which are capable of capturing both DTD recursion and XPATH queries in a uniform framework. Furthermore, we provide an algorithm for translating regular XPATH queries to SQL queries with LFP, and optimization techniques for minimizing the use of the LFP operator. The novelty of our approach consists in its capability to answer a large class of XPATH queries by means of only low-end RDBMS features already available in most RDBMS. Our experimental results verify the effectiveness of our techniques.|Wenfei Fan,Jeffrey Xu Yu,Hongjun Lu,Jianhua Lu,Rajeev Rastogi"],["80543|VLDB|2005|Approximate Joins Concepts and Techniques|The quality of the data residing in information repositories and databases gets degraded due to a multitude of reasons. Such reasons include typing mistakes during insertion (e.g., character transpositions), lack of standards for recording database fields (e.g., addresses), and various errors introduced by poor database design (e.g., missing integrity constraints). Data of poor quality can result in significant impediments to popular business practices sending products or bills to incorrect addresses, inability to locate customer records during service calls, inability to correlate customers across multiple services, etc.|Nick Koudas,Divesh Srivastava","80477|VLDB|2005|Structure and Content Scoring for XML|XML repositories are usually queried both on structure and content. Due to structural heterogeneity of XML, queries are often interpreted approximately and their answers are returned ranked by scores. Computing answer scores in XML is an active area of research that oscillates between pure content scoring such as the well-known tf*idf and taking structure into account. However, none of the existing proposals fully accounts for structure and combines it with content to score query answers. We propose novel XML scoring methods that are inspired by tf*idf and that account for both structure and content while considering query relaxations. Twig scoring, accounts for the most structure and content and is thus used as our reference method. Path scoring is an approximation that loosens correlations between query nodes hence reducing the amount of time required to manipulate scores during top-k query processing. We propose efficient data structures in order to speed up ranked query processing. We run extensive experiments that validate our scoring methods and that show that path scoring provides very high precision while improving score computation time.|Sihem Amer-Yahia,Nick Koudas,Am√©lie Marian,Divesh Srivastava,David Toman","80580|VLDB|2005|Client Assignment in Content Dissemination Networks for Dynamic Data|Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of data item, coherence served by each repository and a set of client, data item, coherence requests, we address the following problem How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximizedIn this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.|Shetal Shah,Krithi Ramamritham,Chinya V. Ravishankar","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum"],["80568|VLDB|2005|Streaming Pattern Discovery in Multiple Time-Series|In this paper, we introduce SPIRIT (Streaming Pattern dIscoveRy in multIple Time-series). Given n numerical data streams, all of whose values we observe at each time tick t, SPIRIT can incrementally find correlations and hidden variables, which summarise the key trends in the entire stream collection. It can do this quickly, with no buffering of stream values and without comparing pairs of streams. Moreover, it is any-time, single pass, and it dynamically detects changes. The discovered trends can also be used to immediately spot potential anomalies, to do efficient forecasting and, more generally, to dramatically simplify further data processing. Our experimental evaluation and case studies show that SPIRIT can incrementally capture correlations and discover trends, efficiently and effectively.|Spiros Papadimitriou,Jimeng Sun,Christos Faloutsos","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","80544|VLDB|2005|StreamGlobe Processing and Sharing Data Streams in Grid-Based PP Infrastructures|Data stream processing is currently gaining importance due to the developments in novel application areas like e-science, e-health, and e-business (considering RFID, for example). Focusing on e-science, it can be observed that scientific experiments and observations in many fields, e. g., in physics and astronomy, create huge volumes of data which have to be interchanged and processed. With experimental and observational data coming in particular from sensors, online simulations, etc., the data has an inherently streaming nature. Furthermore, continuing advances will result in even higher data volumes, rendering storing all of the delivered data prior to processing increasingly impractical. Hence, in such e-science scenarios, processing and sharing of data streams will play a decisive role. It will enable new possibilities for researchers, since they will be able to subscribe to interesting data streams of other scientists without having to set up their own devices or experiments. This results in much better utilization of expensive equipment such as telescopes, satellites, etc. Further, processing and sharing data streams on-the-fly in the network helps to reduce network traffic and to avoid network congestion. Thus, even huge streams of data can be handled efficiently by removing unnecessary parts early on, e. g., by early filtering and aggregation, and by sharing previously generated data streams and processing results.|Richard Kuntschke,Bernhard Stegmaier,Alfons Kemper,Angelika Reiser","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","80539|VLDB|2005|One-Pass Wavelet Synopses for Maximum-Error Metrics|We study the problem of computing wavelet-based synopses for massive data sets in static and streaming environments. A compact representation of a data set is obtained after a thresholding process is applied on the coefficients of its wavelet decomposition. Existing polynomial-time thresholding schemes that minimize maximum error metrics are disadvantaged by impracticable time and space complexities and are not applicable in a data stream context. This is a cardinal issue, as the problem at hand in its most practically interesting form involves the time-efficient approximation of huge amounts of data, potentially in a streaming environment. In this paper we fill this gap by developing efficient and practicable wavelet thresholding algorithms for maximum-error metrics, for both a static and a streaming case. Our algorithms achieve near-optimal accuracy and superior runtime performance, as our experiments show, under frugal space requirements in both contexts.|Panagiotis Karras,Nikos Mamoulis","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum"],["80543|VLDB|2005|Approximate Joins Concepts and Techniques|The quality of the data residing in information repositories and databases gets degraded due to a multitude of reasons. Such reasons include typing mistakes during insertion (e.g., character transpositions), lack of standards for recording database fields (e.g., addresses), and various errors introduced by poor database design (e.g., missing integrity constraints). Data of poor quality can result in significant impediments to popular business practices sending products or bills to incorrect addresses, inability to locate customer records during service calls, inability to correlate customers across multiple services, etc.|Nick Koudas,Divesh Srivastava","80534|VLDB|2005|BATON A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.|H. V. Jagadish,Beng Chin Ooi,Quang Hieu Vu","80563|VLDB|2005|Answering Imprecise Queries over Web Databases|The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding \"instant gratification\". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.|Ullas Nambiar,Subbarao Kambhampati","80473|VLDB|2005|On k-Anonymity and the Curse of Dimensionality|In recent years, the wide availability of personal data has made the problem of privacy preserving data mining an important one. A number of methods have recently been proposed for privacy preserving data mining of multidimensional data records. One of the methods for privacy preserving data mining is that of anonymization, in which a record is released only if it is indistinguishable from k other entities in the data. We note that methods such as k-anonymity are highly dependent upon spatial locality in order to effectively implement the technique in a statistically robust way. In high dimensional space the data becomes sparse, and the concept of spatial locality is no longer easy to define from an application point of view. In this paper, we view the k-anonymization problem from the perspective of inference attacks over all possible combinations of attributes. We show that when the data contains a large number of attributes which may be considered quasi-identifiers, it becomes difficult to anonymize the data without an unacceptably high amount of information loss. This is because an exponential number of combinations of dimensions can be used to make precise inference attacks, even when individual attributes are partially specified within a range. We provide an analysis of the effect of dimensionality on k-anonymity methods. We conclude that when a data set contains a large number of attributes which are open to inference attacks, we are faced with a choice of either completely suppressing most of the data or losing the desired level of anonymity. Thus, this paper shows that the curse of high dimensionality also applies to the problem of privacy preserving data mining.|Charu C. Aggarwal","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","80532|VLDB|2005|Personalized Systems Models and Methods from an IR and DB Perspective|In today's knowledge-driven society, information abundance and personal electronic device ubiquity have made it difficult for users to find the right information at the right time and at the right level of detail. To solve this problem, researchers have developed systems that adapt their behavior to the goals, tasks, interests, and other characteristics of their users. Based on models that capture important user characteristics, these personalized systems maintain their users' profiles and take them into account to customize the content generated or its presentation to the different individuals.|Yannis E. Ioannidis,Georgia Koutrika","80477|VLDB|2005|Structure and Content Scoring for XML|XML repositories are usually queried both on structure and content. Due to structural heterogeneity of XML, queries are often interpreted approximately and their answers are returned ranked by scores. Computing answer scores in XML is an active area of research that oscillates between pure content scoring such as the well-known tf*idf and taking structure into account. However, none of the existing proposals fully accounts for structure and combines it with content to score query answers. We propose novel XML scoring methods that are inspired by tf*idf and that account for both structure and content while considering query relaxations. Twig scoring, accounts for the most structure and content and is thus used as our reference method. Path scoring is an approximation that loosens correlations between query nodes hence reducing the amount of time required to manipulate scores during top-k query processing. We propose efficient data structures in order to speed up ranked query processing. We run extensive experiments that validate our scoring methods and that show that path scoring provides very high precision while improving score computation time.|Sihem Amer-Yahia,Nick Koudas,Am√©lie Marian,Divesh Srivastava,David Toman","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80580|VLDB|2005|Client Assignment in Content Dissemination Networks for Dynamic Data|Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of data item, coherence served by each repository and a set of client, data item, coherence requests, we address the following problem How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximizedIn this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.|Shetal Shah,Krithi Ramamritham,Chinya V. Ravishankar","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum"],["80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","80501|VLDB|2005|Prediction Cubes|In this paper, we introduce a new family of tools for exploratory data analysis, called prediction cubes. As in standard OLAP data cubes, each cell in a prediction cube contains a value that summarizes the data belonging to that cell, and the granularity of cells can be changed via operations such as roll-up and drill-down. In contrast to data cubes, in which each cell value is computed by an aggregate function, e.g., SUM or AVG, each cell value in a prediction cube summarizes a predictive model trained on the data corresponding to that cell, and characterizes its decision behavior or predictiveness. In this paper, we propose and motivate prediction cubes, and show that they can be efficiently computed by exploiting the idea of model decomposition.|Bee-Chung Chen,Lei Chen 0003,Yi Lin,Raghu Ramakrishnan","80473|VLDB|2005|On k-Anonymity and the Curse of Dimensionality|In recent years, the wide availability of personal data has made the problem of privacy preserving data mining an important one. A number of methods have recently been proposed for privacy preserving data mining of multidimensional data records. One of the methods for privacy preserving data mining is that of anonymization, in which a record is released only if it is indistinguishable from k other entities in the data. We note that methods such as k-anonymity are highly dependent upon spatial locality in order to effectively implement the technique in a statistically robust way. In high dimensional space the data becomes sparse, and the concept of spatial locality is no longer easy to define from an application point of view. In this paper, we view the k-anonymization problem from the perspective of inference attacks over all possible combinations of attributes. We show that when the data contains a large number of attributes which may be considered quasi-identifiers, it becomes difficult to anonymize the data without an unacceptably high amount of information loss. This is because an exponential number of combinations of dimensions can be used to make precise inference attacks, even when individual attributes are partially specified within a range. We provide an analysis of the effect of dimensionality on k-anonymity methods. We conclude that when a data set contains a large number of attributes which are open to inference attacks, we are faced with a choice of either completely suppressing most of the data or losing the desired level of anonymity. Thus, this paper shows that the curse of high dimensionality also applies to the problem of privacy preserving data mining.|Charu C. Aggarwal","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80545|VLDB|2005|FiST Scalable XML Document Filtering by Sequencing Twig Patterns|In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pub-sub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Pr&uumlfer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.|Joonho Kwon,Praveen Rao,Bongki Moon,Sukho Lee","80571|VLDB|2005|CMS-ToPSS Efficient Dissemination of RSS Documents|Recent years have seen a rise in the number of unconventional publishing tools on the Internet. Tools such as wikis, blogs, discussion forums, and web-based content management systems have experienced tremendous rise in popularity and use primarily because they provide something traditional tools do not easy of use for non computer-oriented users and they are based on the idea of \"collaboration.\" It is estimated, by pewinternet.org, that  million people in the US read blogs (which represents % of the estimated  million US Internet users) while  million people have said that they have created blogs.|Milenko Petrovic,Haifeng Liu,Hans-Arno Jacobsen","80540|VLDB|2005|PrediCalc A Logical Spreadsheet Management System|Computerized spreadsheets are a great success. They are often touted in newspapers and magazine articles as the first \"killer app\" for personal computers. Over the years, they have proven their worth time and again. Today, they are used for managing enterprises of all sorts - from one-person projects to multi-institutional conglomerates.|Michael Kassoff,Lee-Ming Zen,Ankit Garg,Michael R. Genesereth","80481|VLDB|2005|Database Publication Practices|There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.|Philip A. Bernstein,David J. DeWitt,Andreas Heuer,Zachary G. Ives,Christian S. Jensen,Holger Meyer,M. Tamer √\u2013zsu,Richard T. Snodgrass,Kyu-Young Whang,Jennifer Widom","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80506|VLDB|2005|Bridging the Gap between OLAP and SQL|In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N , ranking , , spreadsheet computations , grouping sets , data cube , and moving sums  in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the clientserver architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.|Jens-Peter Dittrich,Donald Kossmann,Alexander Kreutz","80554|VLDB|2005|Query Caching and View Selection for XML Databases|In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath queryview answerability, which allows us to reduce tree operations to string operations for matching a queryview pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.|Bhushan Mandhani,Dan Suciu","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80511|VLDB|2005|Optimizing Refresh of a Set of Materialized Views|In many data warehousing environments, it is common to have materialized views (MVs) at different levels of aggregation of one or more dimensions. The extreme case of this is relational OLAP environments, where, for performance reasons, nearly all levels of aggregation across all dimensions may be computed and stored in MVs. Furthermore, base tables and MVs are usually partitioned for ease and speed of maintenance. In these scenarios, updates to the base table are done using Bulk or Partition operations like add, exchange, truncate and drop partition. If changes to base tables can be tracked at the partition level, join dependencies. functional dependencies and query rewrite can be used to optimize refresh of an individual MV. The refresh optimizer, in the presence of partitioned tables and MVs, may recognize dependencies between base table and the MV partitions leading to the generation of very efficient refresh expressions. Additionally, in the presence of multiple MVs, the refresh subsytem can come up with an optimal refresh schedule such that MVs can be refreshed using query rewrite against previously refreshed MVs. This makes the database server more manageable and user friendly since a single function call can optimally refresh all the MVs in the system.|Nathan Folkert,Abhinav Gupta,Andrew Witkowski,Sankar Subramanian,Srikanth Bellamkonda,Shrikanth Shankar,Tolga Bozkaya,Lei Sheng","80479|VLDB|2005|Designing Information-Preserving Mapping Schemes for XML|An XML-to-relational mapping scheme consists of a procedure for shredding documents into relational databases, a procedure for publishing databases back as documents, and a set of constraints the databases must satisfy. In previous work, we defined two notions of information preservation for mapping schemes losslessness, which guarantees that any document can be reconstructed from its corresponding database and validation, which requires every legal database to correspond to a valid document. We also described one information-preserving mapping scheme, called Edge++, and showed that, under reasonable assumptions, losslessness and validation are both undecidable. This leads to the question we study in this paper how to design mapping schemes that are information-preserving. We propose to do it by starting with a scheme known to be information-preserving and applying to it equivalence-preserving transformations written in weakly recursive ILOG. We study an instance of this framework, the LILO algorithm, and show that it provides significant performance improvements over Edge++ and introduces constraints that are efficiently enforced in practice.|Denilson Barbosa,Juliana Freire,Alberto O. Mendelzon"],["80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80482|VLDB|2005|MINERVA Collaborative PP Search|This paper proposes the live demonstration of a prototype of MINERVA, a novel PP Web search engine. The search engine is layered on top of a DHT-based overlay network that connects an a-priori unlimited number of peers, each of which maintains a personal local database and a local search facility. Each peer posts a small amount of metadata to a physically distributed directory that is used to efficiently select promising peers from across the peer population that can best locally execute a query. The proposed demonstration serves as a proof of concept for PP Web search by deploying the project on standard notebook PCs and also invites everybody to join the network by instantly installing a small piece of software from a USB memory stick.|Matthias Bender,Sebastian Michel,Peter Triantafillou,Gerhard Weikum,Christian Zimmer","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80526|VLDB|2005|Link Spam Alliances|Link spam is used to increase the ranking of certain target web pages by misleading the connectivity-based ranking algorithms in search engines. In this paper we study how web pages can be interconnected in a spam farm in order to optimize rankings. We also study alliances, that is, interconnections of spam farms. Our results identify the optimal structures and quantify the potential gains. In particular, we show that alliances can be synergistic and improve the rankings of all participants. We believe that the insights we gain will be useful in identifying and combating link spam.|Zolt√°n Gy√∂ngyi,Hector Garcia-Molina","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","80538|VLDB|2005|Bidirectional Expansion For Keyword Search on Graph Databases|Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the \"best\" answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree.In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search.|Varun Kacholia,Shashank Pandit,Soumen Chakrabarti,S. Sudarshan,Rushi Desai,Hrishikesh Karambelkar","80514|VLDB|2005|Discovering Large Dense Subgraphs in Massive Graphs|We present a new algorithm for finding large, dense subgraphs in massive graphs. Our algorithm is based on a recursive application of fingerprinting via shingles, and is extremely efficient, capable of handling graphs with tens of billions of edges on a single machine with modest resources.We apply our algorithm to characterize the large, dense subgraphs of a graph showing connections between hosts on the World Wide Web this graph contains over M hosts and B edges, gathered from .B web pages. We measure the distribution of these dense subgraphs and their evolution over time. We show that more than half of these hosts participate in some dense subgraph found by the analysis. There are several hundred giant dense subgraphs of at least ten thousand hosts two thousand dense subgraphs at least a thousand hosts and almost K dense subgraphs of at least a hundred hosts.Upon examination, many of the dense subgraphs output by our algorithm are link spam, i.e., websites that attempt to manipulate search engine rankings through aggressive interlinking to simulate popular content. We therefore propose dense subgraph extraction as a useful primitive for spam detection, and discuss its incorporation into the workflow of web search engines.|David Gibson,Ravi Kumar,Andrew Tomkins","80567|VLDB|2005|Shuffling a Stacked Deck The Case for Partially Randomized Ranking of Search Engine Results|In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits andor in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively \"shut out,\" and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.|Sandeep Pandey,Sourashis Roy,Christopher Olston,Junghoo Cho,Soumen Chakrabarti"],["80480|VLDB|2005|Benefits of Path Summaries in an XML Query Optimizer Supporting Multiple Access Methods|We compare several optimization strategies implemented in an XML query evaluation system. The strategies incorporate the use of path summaries into the query optimizer, and rely on heuristics that exploit data statistics.We present experimental results that demonstrate a wide range of performance improvements for the different strategies supported. In addition, we compare the speedups obtained using path summaries with those reported for index-based methods. The comparison shows that low-cost path summaries combined with optimization strategies achieve essentially the same benefits as more expensive index structures.|Attila Barta,Mariano P. Consens,Alberto O. Mendelzon","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80562|VLDB|2005|Tree-Pattern Queries on a Lightweight XML Processor|Popular XML languages, like XPath, use \"tree-pattern\" queries to select nodes based on their structural characteristics. While many processing methods have already been proposed for such queries, none of them has found its way to any of the existing \"lightweight\" XML engines (i.e. engines without optimization modules). The main reason is the lack of a systematic comparison of query methods under a common storage model. In this work, we aim to fill this gap and answer two important questions what the relative similarities and important differences among the tree-pattern query methods are, and if there is a prominent method among them in terms of effectiveness and robustness that an XML processor should support. For the first question, we propose a novel classification of the methods according to their matching process. We then describe a common storage model and demonstrate that the access pattern of each class conforms or can be adapted to conform to this model. Finally, we perform an experimental evaluation to compare their relative performance. Based on the evaluation results, we conclude that the family of holistic processing methods, which provides performance guarantees, is the most robust alternative for such an environment.|Mirella Moura Moro,Zografoula Vagena,Vassilis J. Tsotras","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80545|VLDB|2005|FiST Scalable XML Document Filtering by Sequencing Twig Patterns|In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pub-sub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Pr&uumlfer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.|Joonho Kwon,Praveen Rao,Bongki Moon,Sukho Lee","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","80510|VLDB|2005|Query Translation from XPath to SQL in the Presence of Recursive DTDs|The interaction between recursion in XPATH and recursion in DTDS makes it challenging to answer XPATH queries on XML data that is stored in an RDBMS via schema-based shredding. We present a new approach to translating XPATH queries into SQL queries with a simple least fixpoint (LFP) operator, which is already supported by most commercial RDBMS. The approach is based on our algorithm for rewriting XPATH queries into regular XPATH expressions, which are capable of capturing both DTD recursion and XPATH queries in a uniform framework. Furthermore, we provide an algorithm for translating regular XPATH queries to SQL queries with LFP, and optimization techniques for minimizing the use of the LFP operator. The novelty of our approach consists in its capability to answer a large class of XPATH queries by means of only low-end RDBMS features already available in most RDBMS. Our experimental results verify the effectiveness of our techniques.|Wenfei Fan,Jeffrey Xu Yu,Hongjun Lu,Jianhua Lu,Rajeev Rastogi","80550|VLDB|2005|CXHist  An On-line Classification-Based Histogram for XML String Selectivity Estimation|Query optimization in IBM's System RX, the first truly relational-XML hybrid data management system, requires accurate selectivity estimation of path-value pairs, i.e., the number of nodes in the XML tree reachable by a given path with the given text value. Previous techniques have been inadequate, because they have focused mainly on the tag-labeled paths (tree structure) of the XML data. For most real XML data, the number of distinct string values at the leaf nodes is orders of magnitude larger than the set of distinct rooted tag paths. Hence, the real challenge lies in accurate selectivity estimation of the string predicates on the leaf values reachable via a given path.In this paper, we present CXHist, a novel workload-aware histogram technique that provides accurate selectivity estimation on a broad class of XML string-based queries. CXHist builds a histogram in an on-line manner by grouping queries into buckets using their true selectivity obtained from query feedback. The set of queries associated with each bucket is summarized into feature distributions. These feature distributions mimic a Bayesian classifier that is used to route a query to its associated bucket during selectivity estimation. We show how CXHist can be used for two general types of path, string queries exact match queries and substring match queries. Experiments using a prototype show that CXHist provides accurate selectivity estimation for both exact match queries and substring match queries.|Lipyeow Lim,Min Wang,Jeffrey Scott Vitter"],["80572|VLDB|2005|Large Scale Data Warehouses on Grid Oracle Database g and HP ProLiant Systems|Grid computing has the potential to drastically change enterprise computing as we know it today. The main concept of grid computing is viewing computing as a utility. It should not matter where data resides, or what computer processes a task. This concept has been applied successfully to academic research. It also has many advantages for commercial data warehouse applications such as virtualization, flexible provisioning, reduced cost due to commodity hardware, high availability and high scale-out. In this paper we show how a large-scale, high-performing and scalable grid-based data warehouse can be implemented using commodity hardware (industry-standard x-based). Oracle Database g and the Linux operating system. We further demonstrate this architecture in a recently published TPC-H benchmark.|Meikel Poess,Raghunath Othayoth Nambiar","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","80591|VLDB|2005|Checking for k-Anonymity Violation by Views|When a private relational table is published using views, secrecy or privacy may be violated. This paper uses a formally-defined notion of k-anonymity to measure disclosure by views, where k  is a positive integer. Intuitively, violation of k-anonymity occurs when a particular attribute value of an entity can be determined to be among less than k possibilities by using the views together with the schema information of the private table. The paper shows that, in general, whether a set of views violates k-anonymity is a computationally hard problem. Subcases are identified and their computational complexities discussed. Especially interesting are those subcases that yield polynomial checking algorithms (in the number of tuples in the views). The paper also provides an efficient conservative algorithm that checks for necessary conditions for k-anonymity violation.|Chao Yao,Xiaoyang Sean Wang,Sushil Jajodia","80502|VLDB|2005|Stack-based Algorithms for Pattern Matching on DAGs|Existing work for query processing over graph data models often relies on pre-computing the transitive closure or path indexes. In this paper, we propose a family of stack-based algorithms to handle path, twig, and dag pattern queries for directed acyclic graphs (DAGs) in particular. Our algorithms do not precompute the transitive closure nor path indexes for a given graph, however they achieve an optimal runtime complexity quadratic in the average size of the query variable bindings. We prove the soundness and completeness of our algorithms and present the experimental results.|Li Chen,Amarnath Gupta,M. Erdem Kurul","80554|VLDB|2005|Query Caching and View Selection for XML Databases|In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath queryview answerability, which allows us to reduce tree operations to string operations for matching a queryview pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.|Bhushan Mandhani,Dan Suciu","80546|VLDB|2005|View Matching for Outer-Join Views|Prior work on computing queries from materialized views has focused on views defined by expressions consisting of selection, projection, and inner joins, with an optional aggregation on top (SPJG views). This paper provides the first view matching algorithm for views that may also contain outer joins (SPOJG views). The algorithm relies on a normal form for SPOJ expressions and does not use bottom-up syntactic matching of expressions. It handles any combination of inner and outer joins, deals correctly with SQL bag semantics and exploits not-null constraints, uniqueness constraints and foreign key constraints.|Per-√\u2026ke Larson,Jingren Zhou","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80539|VLDB|2005|One-Pass Wavelet Synopses for Maximum-Error Metrics|We study the problem of computing wavelet-based synopses for massive data sets in static and streaming environments. A compact representation of a data set is obtained after a thresholding process is applied on the coefficients of its wavelet decomposition. Existing polynomial-time thresholding schemes that minimize maximum error metrics are disadvantaged by impracticable time and space complexities and are not applicable in a data stream context. This is a cardinal issue, as the problem at hand in its most practically interesting form involves the time-efficient approximation of huge amounts of data, potentially in a streaming environment. In this paper we fill this gap by developing efficient and practicable wavelet thresholding algorithms for maximum-error metrics, for both a static and a streaming case. Our algorithms achieve near-optimal accuracy and superior runtime performance, as our experiments show, under frugal space requirements in both contexts.|Panagiotis Karras,Nikos Mamoulis","80594|VLDB|2005|Efficient Computation of the Skyline Cube|Skyline has been proposed as an important operator for multi-criteria decision making, data mining and visualization, and user-preference queries. In this paper. we consider the problem of efficiently computing a SKYCUBE, which consists of skylines of all possible non-empty subsets of a given set of dimensions. While existing skyline computation algorithms can be immediately extended to computing each skyline query independently, such \"shared-nothing\" algorithms are inefficient. We develop several computation sharing strategies based on effectively identifying the computation dependencies among multiple related skyline queries. Based on these sharing strategies, two novel algorithms, Bottom-Up and Top-Down algorithms, are proposed to compute SKYCUBE efficiently. Finally, our extensive performance evaluations confirm the effectiveness of the sharing strategies. It is shown that new algorithms significantly outperform the na&iumlve ones.|Yidong Yuan,Xuemin Lin,Qing Liu,Wei Wang 0011,Jeffrey Xu Yu,Qing Zhang"]]},"title":{"entropy":4.505184845582654,"topics":["for database, database system, database, query for, and system, queries database, views, xquery, over, from, querying, semantic, xml, efficient, the","management system, for web, the web, system for, for search, the, queries, with, distributed, relational, application, processing","data stream, and data, and for, efficient and, and, for efficient, and stream, networks, algorithms","for xml, for data, for, pattern, matching, mapping, indexing, schema","system, views","over, xquery, querying","the, search","web, application","data stream","networks, algorithms","mapping, schema","data"],"ranking":[["80518|VLDB|2005|ConQuer A System for Efficient Querying Over Inconsistent Databases|Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.|Ariel Fuxman,Diego Fuxman,Ren√©e J. Miller","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80491|VLDB|2005|Flexible Database Generators|Evaluation and applicability of many database techniques, ranging from access methods, histograms, and optimization strategies to data normalization and mining, crucially depend on their ability to cope with varying data distributions in a robust way. However, comprehensive real data is often hard to come by, and there is no flexible data generation framework capable of modelling varying rich data distributions. This has led individual researchers to develop their own ad-hoc data generators for specific tasks. As a consequence, the resulting data distributions and query workloads are often hard to reproduce, analyze, and modify, thus preventing their wider usage. In this paper we present a flexible, easy to use, and scalable framework for database generation. We then discuss how to map several proposed synthetic distributions to our framework and report preliminary results.|Nicolas Bruno,Surajit Chaudhuri","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80519|VLDB|2005|Database Change Notifications Primitives for Efficient Database Query Result Caching|Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier , . The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.|C√©sar A. Galindo-Legaria,Torsten Grabs,Christian Kleinerman,Florian Waas","80481|VLDB|2005|Database Publication Practices|There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.|Philip A. Bernstein,David J. DeWitt,Andreas Heuer,Zachary G. Ives,Christian S. Jensen,Holger Meyer,M. Tamer √\u2013zsu,Richard T. Snodgrass,Kyu-Young Whang,Jennifer Widom"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","80563|VLDB|2005|Answering Imprecise Queries over Web Databases|The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding \"instant gratification\". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.|Ullas Nambiar,Subbarao Kambhampati","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","80556|VLDB|2005|The Integrated Microbial Genomes IMG System A Case Study in Biological Data Management|Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.|Victor M. Markowitz,Frank Korzeniewski,Krishna Palaniappan,Ernest Szeto,Natalia Ivanova,Nikos Kyrpides","80490|VLDB|2005|PSYCHO A Prototype System for Pattern Management|Patterns represent in a compact and rich in semantics way huge quantity of heterogeneous data. Due to their characteristics, specific systems are required for pattern management, in order to model and manipulate patterns, with a possibly user-defined structure, in an efficient and effective way. In this demonstration we present PSYCHO, a pattern based management system prototype. PSYCHO allows the user to (i) use standard pattern types or define new ones (ii) generate or import patterns, represented according to existing standards (iii) manipulate possibly heterogeneous patterns under an integrated environment.|Barbara Catania,Anna Maddalena,Maurizio Mazza","80540|VLDB|2005|PrediCalc A Logical Spreadsheet Management System|Computerized spreadsheets are a great success. They are often touted in newspapers and magazine articles as the first \"killer app\" for personal computers. Over the years, they have proven their worth time and again. Today, they are used for managing enterprises of all sorts - from one-person projects to multi-institutional conglomerates.|Michael Kassoff,Lee-Ming Zen,Ankit Garg,Michael R. Genesereth"],["80494|VLDB|2005|Adaptive Stream Filters for Entity-based Queries with Non-Value Tolerance|We study the problem of applying adaptive filters for approximate query processing in a distributed stream environment. We propose filter bound assignment protocols with the objective of reducing communication cost. Most previous works focus on value-based queries (e.g., average) with numerical error tolerance. In this paper, we cover entity-based queries (e.g., nearest neighbor) with non-value-based error tolerance. We investigate different non-value-based error tolerance definitions and discuss how they are applied to two classes of entity-based queries non-rank-based and rank-based queries. Extensive experiments show that our protocols achieve significant savings in both communication overhead and server computation.|Reynold Cheng,Ben Kao,Sunil Prabhakar,Alan Kwan,Yi-Cheng Tu","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80493|VLDB|2005|Loadstar Load Shedding in Data Stream Mining|In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.|Yun Chi,Haixun Wang,Philip S. Yu","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","80580|VLDB|2005|Client Assignment in Content Dissemination Networks for Dynamic Data|Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of data item, coherence served by each repository and a set of client, data item, coherence requests, we address the following problem How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximizedIn this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.|Shetal Shah,Krithi Ramamritham,Chinya V. Ravishankar","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim","80594|VLDB|2005|Efficient Computation of the Skyline Cube|Skyline has been proposed as an important operator for multi-criteria decision making, data mining and visualization, and user-preference queries. In this paper. we consider the problem of efficiently computing a SKYCUBE, which consists of skylines of all possible non-empty subsets of a given set of dimensions. While existing skyline computation algorithms can be immediately extended to computing each skyline query independently, such \"shared-nothing\" algorithms are inefficient. We develop several computation sharing strategies based on effectively identifying the computation dependencies among multiple related skyline queries. Based on these sharing strategies, two novel algorithms, Bottom-Up and Top-Down algorithms, are proposed to compute SKYCUBE efficiently. Finally, our extensive performance evaluations confirm the effectiveness of the sharing strategies. It is shown that new algorithms significantly outperform the na&iumlve ones.|Yidong Yuan,Xuemin Lin,Qing Liu,Wei Wang 0011,Jeffrey Xu Yu,Qing Zhang"],["80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","80502|VLDB|2005|Stack-based Algorithms for Pattern Matching on DAGs|Existing work for query processing over graph data models often relies on pre-computing the transitive closure or path indexes. In this paper, we propose a family of stack-based algorithms to handle path, twig, and dag pattern queries for directed acyclic graphs (DAGs) in particular. Our algorithms do not precompute the transitive closure nor path indexes for a given graph, however they achieve an optimal runtime complexity quadratic in the average size of the query variable bindings. We prove the soundness and completeness of our algorithms and present the experimental results.|Li Chen,Amarnath Gupta,M. Erdem Kurul","80486|VLDB|2005|Information Preserving XML Schema Embedding|A fundamental concern of information integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source document(s) is preserved. In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be defined from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also provide efficient heuristic algorithms to find candidate embeddings, along with experimental results to evaluate and compare the algorithms. These yield the first systematic and effective approach to finding information preserving XML mappings.|Philip Bohannon,Wenfei Fan,Michael Flaster,P. P. S. Narayan","80578|VLDB|2005|Tuning Schema Matching Software using Synthetic Scenarios|Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user must then tune the system select the right component to be executed and correctly adjust their numerous \"knobs\" (e.g., thresholds, formula coefficients). Tuning is skill- and time-intensive, but (as we show) without it the matching accuracy is significantly inferior.We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of matching components. While the tuning process is completely automatic, eTuner can also exploit user assistance (whenever available) to further improve the tuning quality. We employed eTuner to tune four recently developed matching systems on several real-world domains. eTuner produced tuned matching systems that achieve higher accuracy than using the systems with currently possible tuning methods, at virtually no cost to the domain user.|Mayssam Sayyadian,Yoonkyong Lee,AnHai Doan,Arnon Rosenthal","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","80553|VLDB|2005|From Region Encoding To Extended Dewey On Efficient Processing of XML Twig Pattern Matching|Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.|Jiaheng Lu,Tok Wang Ling,Chee Yong Chan,Ting Chen","80488|VLDB|2005|On Map-Matching Vehicle Tracking Data|Vehicle tracking data is an essential \"raw\" material for a broad range of applications such as traffic management and control, routing, and navigation. An important issue with this data is its accuracy. The method of sampling vehicular movement using GPS is affected by two error sources and consequently produces inaccurate trajectory data. To become useful, the data has to be related to the underlying road network by means of map matching algorithms. We present three such algorithms that consider especially the trajectory nature of the data rather than simply the current position as in the typical map-matching case. An incremental algorithm is proposed that matches consecutive portions of the trajectory to the road network, effectively trading accuracy for speed of computation. In contrast, the two global algorithms compare the entire trajectory to candidate paths in the road network. The algorithms are evaluated in terms of (i) their running time and (ii) the quality of their matching result. Two novel quality measures utilizing the Fr&eacutechet distance are introduced and subsequently used in an experimental evaluation to assess the quality of matching real tracking data to a road network.|Sotiris Brakatsoulas,Dieter Pfoser,Randall Salas,Carola Wenk","80479|VLDB|2005|Designing Information-Preserving Mapping Schemes for XML|An XML-to-relational mapping scheme consists of a procedure for shredding documents into relational databases, a procedure for publishing databases back as documents, and a set of constraints the databases must satisfy. In previous work, we defined two notions of information preservation for mapping schemes losslessness, which guarantees that any document can be reconstructed from its corresponding database and validation, which requires every legal database to correspond to a valid document. We also described one information-preserving mapping scheme, called Edge++, and showed that, under reasonable assumptions, losslessness and validation are both undecidable. This leads to the question we study in this paper how to design mapping schemes that are information-preserving. We propose to do it by starting with a scheme known to be information-preserving and applying to it equivalence-preserving transformations written in weakly recursive ILOG. We study an instance of this framework, the LILO algorithm, and show that it provides significant performance improvements over Edge++ and introduces constraints that are efficiently enforced in practice.|Denilson Barbosa,Juliana Freire,Alberto O. Mendelzon","80478|VLDB|2005|Approximate Matching of Hierarchical Data Using pq-Grams|When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.|Nikolaus Augsten,Michael H. B√∂hlen,Johann Gamper"],["80570|VLDB|2005|Catching the Best Views of Skyline A Semantic Approach Based on Decisive Subspaces|The skyline operator is important for multi-criteria decision making applications. Although many recent studies developed efficient methods to compute skyline objects in a specific space, the fundamental problem on the semantics of skylines remains open Why and in which subspaces is (or is not) an object in the skyline Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces How can we effectively analyze the subspace skylines Can we efficiently compute skylines in various subspacesIn this paper, we investigate the semantics of skylines, propose the subspace skyline analysis, and extend the full-space skyline computation to subspace skyline computation. We introduce a novel notion of skyline group which essentially is a group of objects that are coincidentally in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drilldown analysis is introduced. We also develop an efficient algorithm, Skyey, to compute the set of skyline groups and, for each subspace, the set of objects that are in the subspace skyline. A performance study is reported to evaluate our approach.|Jian Pei,Wen Jin,Martin Ester,Yufei Tao","80505|VLDB|2005|Answering Queries from Statistics and Probabilistic Views|Systems integrating dozens of databases, in the scientific domain or in a large corporation, need to cope with a wide variety of imprecisions, such as different representations of the same object in different sources imperfect and noisy schema alignments contradictory information across sources constraint violations or insufficient evidence to answer a given query. If standard query semantics were applied to such data, all but the most trivial queries will return an empty answer.|Nilesh N. Dalvi,Dan Suciu","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80591|VLDB|2005|Checking for k-Anonymity Violation by Views|When a private relational table is published using views, secrecy or privacy may be violated. This paper uses a formally-defined notion of k-anonymity to measure disclosure by views, where k  is a positive integer. Intuitively, violation of k-anonymity occurs when a particular attribute value of an entity can be determined to be among less than k possibilities by using the views together with the schema information of the private table. The paper shows that, in general, whether a set of views violates k-anonymity is a computationally hard problem. Subcases are identified and their computational complexities discussed. Especially interesting are those subcases that yield polynomial checking algorithms (in the number of tuples in the views). The paper also provides an efficient conservative algorithm that checks for necessary conditions for k-anonymity violation.|Chao Yao,Xiaoyang Sean Wang,Sushil Jajodia","80546|VLDB|2005|View Matching for Outer-Join Views|Prior work on computing queries from materialized views has focused on views defined by expressions consisting of selection, projection, and inner joins, with an optional aggregation on top (SPJG views). This paper provides the first view matching algorithm for views that may also contain outer joins (SPOJG views). The algorithm relies on a normal form for SPOJ expressions and does not use bottom-up syntactic matching of expressions. It handles any combination of inner and outer joins, deals correctly with SQL bag semantics and exploits not-null constraints, uniqueness constraints and foreign key constraints.|Per-√\u2026ke Larson,Jingren Zhou","80490|VLDB|2005|PSYCHO A Prototype System for Pattern Management|Patterns represent in a compact and rich in semantics way huge quantity of heterogeneous data. Due to their characteristics, specific systems are required for pattern management, in order to model and manipulate patterns, with a possibly user-defined structure, in an efficient and effective way. In this demonstration we present PSYCHO, a pattern based management system prototype. PSYCHO allows the user to (i) use standard pattern types or define new ones (ii) generate or import patterns, represented according to existing standards (iii) manipulate possibly heterogeneous patterns under an integrated environment.|Barbara Catania,Anna Maddalena,Maurizio Mazza","80540|VLDB|2005|PrediCalc A Logical Spreadsheet Management System|Computerized spreadsheets are a great success. They are often touted in newspapers and magazine articles as the first \"killer app\" for personal computers. Over the years, they have proven their worth time and again. Today, they are used for managing enterprises of all sorts - from one-person projects to multi-institutional conglomerates.|Michael Kassoff,Lee-Ming Zen,Ankit Garg,Michael R. Genesereth","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80511|VLDB|2005|Optimizing Refresh of a Set of Materialized Views|In many data warehousing environments, it is common to have materialized views (MVs) at different levels of aggregation of one or more dimensions. The extreme case of this is relational OLAP environments, where, for performance reasons, nearly all levels of aggregation across all dimensions may be computed and stored in MVs. Furthermore, base tables and MVs are usually partitioned for ease and speed of maintenance. In these scenarios, updates to the base table are done using Bulk or Partition operations like add, exchange, truncate and drop partition. If changes to base tables can be tracked at the partition level, join dependencies. functional dependencies and query rewrite can be used to optimize refresh of an individual MV. The refresh optimizer, in the presence of partitioned tables and MVs, may recognize dependencies between base table and the MV partitions leading to the generation of very efficient refresh expressions. Additionally, in the presence of multiple MVs, the refresh subsytem can come up with an optimal refresh schedule such that MVs can be refreshed using query rewrite against previously refreshed MVs. This makes the database server more manageable and user friendly since a single function call can optimally refresh all the MVs in the system.|Nathan Folkert,Abhinav Gupta,Andrew Witkowski,Sankar Subramanian,Srikanth Bellamkonda,Shrikanth Shankar,Tolga Bozkaya,Lei Sheng"],["80518|VLDB|2005|ConQuer A System for Efficient Querying Over Inconsistent Databases|Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.|Ariel Fuxman,Diego Fuxman,Ren√©e J. Miller","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80573|VLDB|2005|Parallel Querying with Non-Dedicated Computers|We present DITN, a new method of parallel querying based on dynamic outsourcing of join processing tasks to non-dedicated, heterogeneous computers. In DITN, partitioning is not the means of parallelism. Data layout decisions are taken outside the scope of the DBMS, and handled within the storage software query processors see a \"Data In The Network\" image. This allows gradual scaleout as the workload grows, by using non-dedicated computers.A typical operator in a parallel query plan is Exchange . We argue that Exchange is unsuitable for non-dedicated machines because it poorly addresses node heterogeneity, and is vulnerable to failures or load spikes during query execution. DITN uses an alternate intra-fragment parallelism where each node executes an independent select-project-join-aggregate-group by block, with no tuple exchange between nodes. This method cleanly handles heterogeneous nodes, and well adapts during execution to node failures or load spikes.Initial experiments suggest that DITN performs competitively with a traditional configuration of dedicated machines and well-partitioned data for up to  processors at least. At the same time, DITN gives significant flexibility in terms of gradual scaleout and handling of heterogeneity, load bursts, and failures.|Vijayshankar Raman,Wei Han,Inderpal Narang","80563|VLDB|2005|Answering Imprecise Queries over Web Databases|The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding \"instant gratification\". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.|Ullas Nambiar,Subbarao Kambhampati","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80497|VLDB|2005|An Efficient SQL-based RDF Querying Scheme|Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings ) They are difficult to integrate with SQL queries used in database applications, and ) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDFMATCH to query RDF data. The results of RDFMATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDFMATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDFMATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using  million RDF triples from UniProt protein and annotation data.|Eugene Inseok Chong,Souripriya Das,George Eadon,Jagannathan Srinivasan","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","80489|VLDB|2005|OLAP Over Uncertain and Imprecise Data|We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.|Douglas Burdick,Prasad Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80487|VLDB|2005|Pathfinder XQuery - The Relational Way|Relational query processors are probably the best understood (as well as the best engineered) query engines available today. Although carefully tuned to process instances of the relational model (tables of tuples), these processors can also provide a foundation for the evaluation of \"alien\" (non-relational) query languages if a relational encoding of the alien data model and its associated query language is given, the RDBMS may act like a special-purpose processor for the new language.|Peter A. Boncz,Torsten Grust,Maurice van Keulen,Stefan Manegold,Jan Rittinger,Jens Teubner"],["80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","80585|VLDB|2005|An Efficient and Versatile Query Engine for TopX Search|This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in ) the need to consider scores for XML elements while aggregating them at the document level, ) the combination of vague content conditions with XML path conditions, ) the need to relax query conditions if too few results satisfy all conditions, and ) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.|Martin Theobald,Ralf Schenkel,Gerhard Weikum","80565|VLDB|2005|Why Search Engines are Used Increasingly to Offload Queries from Databases|The development of future search engine technology is no longer limited to free text. Rather, the aim is to build core indexing services that focus on extreme performance and scalability for retrieval and analysis across structured and unstructured data sources alike. In addition, binary query evaluation is being replaced with advanced frameworks that provide both fuzzy matching and ranking schemes, to separate value from noise. As another trend, analytical applications are being enabled by the computation of contextual concept relationships across billions of documentsrecords on-the-fly.Based on these developments in search engine technology, a set of new information retrieval infrastructure patterns are appearing. the mirroring of DB content into a search engine in order to improve query capacity and user experience,. the use of search engine technology as the default access pattern to both structured and unstructured data in applications such as CRM and storage and document management, and. a paradigm shift is predicted in business intelligence.The presentation will review key trends from search engine development and relate these to concrete user scenarios.|Bj√∏rn Olstad","80482|VLDB|2005|MINERVA Collaborative PP Search|This paper proposes the live demonstration of a prototype of MINERVA, a novel PP Web search engine. The search engine is layered on top of a DHT-based overlay network that connects an a-priori unlimited number of peers, each of which maintains a personal local database and a local search facility. Each peer posts a small amount of metadata to a physically distributed directory that is used to efficiently select promising peers from across the peer population that can best locally execute a query. The proposed demonstration serves as a proof of concept for PP Web search by deploying the project on standard notebook PCs and also invites everybody to join the network by instantly installing a small piece of software from a USB memory stick.|Matthias Bender,Sebastian Michel,Peter Triantafillou,Gerhard Weikum,Christian Zimmer","80542|VLDB|2005|Database-Inspired Search|\"WQL A Query Language for the WWW\", published in , presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. WQL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether WQL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.|David Konopnicki,Oded Shmueli","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","80538|VLDB|2005|Bidirectional Expansion For Keyword Search on Graph Databases|Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the \"best\" answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree.In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search.|Varun Kacholia,Shashank Pandit,Soumen Chakrabarti,S. Sudarshan,Rushi Desai,Hrishikesh Karambelkar","80567|VLDB|2005|Shuffling a Stacked Deck The Case for Partially Randomized Ranking of Search Engine Results|In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits andor in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively \"shut out,\" and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.|Sandeep Pandey,Sourashis Roy,Christopher Olston,Junghoo Cho,Soumen Chakrabarti"],["80597|VLDB|2005|Light-weight Domain-based Form Assistant Querying Web Databases On the Fly|The Web has been rapidly \"deepened\" by myriad searchable databases online, where data are hidden behind query forms. Helping users query alternative \"deep Web\" sources in the same domain (e.g., Books, Airfares) is an important task with broad applications. As a core component of those applications, dynamic query translation (i.e., translating a user's query across dynamically selected sources) has not been extensively explored. While existing works focus on isolated subproblems (e.g., schema matching, query rewriting) to study, we target at building a complete query translator and thus face new challenges ) To complete the translator, we need to solve the predicate mapping problem (i.e., map a source predicate to target predicates), which is largely unexplored by existing works ) To satisfy our application requirements, we need to design a customizable system architecture to assemble various components addressing respective subproblems (i.e., schema matching, predicate mapping, query rewriting). Tackling these challenges, we develop a light-weight domain-based form assistant, which can generally handle alternative sources in the same domain and is easily customizable to new domains. Our experiment shows the effectiveness of our form assistant in translating queries for real Web sources.|Zhen Zhang,Bin He,Kevin Chen-Chuan Chang","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","80563|VLDB|2005|Answering Imprecise Queries over Web Databases|The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding \"instant gratification\". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.|Ullas Nambiar,Subbarao Kambhampati","80530|VLDB|2005|WISE-Integrator A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web|We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.|Hai He,Weiyi Meng,Clement T. Yu,Zonghuan Wu","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80528|VLDB|2005|Parallel Execution of Test Runs for Database Application Systems|In a recent paper , it was shown how tests for database application systems can be executed efficiently. The challenge was to control the state of the database during testing and to order the test runs in such a way that expensive reset operations that bring the database into the right state need to be executed as seldom as possible. This work extends that work so that test runs can be executed in parallel. The goal is to achieve linear speed-up andor exploit the available resources as well as possible. This problem is challenging because parallel testing can involve interference between the execution of concurrent test runs.|Florian Haftmann,Donald Kossmann,Eric Lo","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Ravi Vijay","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi"],["80494|VLDB|2005|Adaptive Stream Filters for Entity-based Queries with Non-Value Tolerance|We study the problem of applying adaptive filters for approximate query processing in a distributed stream environment. We propose filter bound assignment protocols with the objective of reducing communication cost. Most previous works focus on value-based queries (e.g., average) with numerical error tolerance. In this paper, we cover entity-based queries (e.g., nearest neighbor) with non-value-based error tolerance. We investigate different non-value-based error tolerance definitions and discuss how they are applied to two classes of entity-based queries non-rank-based and rank-based queries. Extensive experiments show that our protocols achieve significant savings in both communication overhead and server computation.|Reynold Cheng,Ben Kao,Sunil Prabhakar,Alan Kwan,Yi-Cheng Tu","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80493|VLDB|2005|Loadstar Load Shedding in Data Stream Mining|In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.|Yun Chi,Haixun Wang,Philip S. Yu","80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim","80488|VLDB|2005|On Map-Matching Vehicle Tracking Data|Vehicle tracking data is an essential \"raw\" material for a broad range of applications such as traffic management and control, routing, and navigation. An important issue with this data is its accuracy. The method of sampling vehicular movement using GPS is affected by two error sources and consequently produces inaccurate trajectory data. To become useful, the data has to be related to the underlying road network by means of map matching algorithms. We present three such algorithms that consider especially the trajectory nature of the data rather than simply the current position as in the typical map-matching case. An incremental algorithm is proposed that matches consecutive portions of the trajectory to the road network, effectively trading accuracy for speed of computation. In contrast, the two global algorithms compare the entire trajectory to candidate paths in the road network. The algorithms are evaluated in terms of (i) their running time and (ii) the quality of their matching result. Two novel quality measures utilizing the Fr&eacutechet distance are introduced and subsequently used in an experimental evaluation to assess the quality of matching real tracking data to a road network.|Sotiris Brakatsoulas,Dieter Pfoser,Randall Salas,Carola Wenk","80489|VLDB|2005|OLAP Over Uncertain and Imprecise Data|We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.|Douglas Burdick,Prasad Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan"],["80534|VLDB|2005|BATON A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.|H. V. Jagadish,Beng Chin Ooi,Quang Hieu Vu","80502|VLDB|2005|Stack-based Algorithms for Pattern Matching on DAGs|Existing work for query processing over graph data models often relies on pre-computing the transitive closure or path indexes. In this paper, we propose a family of stack-based algorithms to handle path, twig, and dag pattern queries for directed acyclic graphs (DAGs) in particular. Our algorithms do not precompute the transitive closure nor path indexes for a given graph, however they achieve an optimal runtime complexity quadratic in the average size of the query variable bindings. We prove the soundness and completeness of our algorithms and present the experimental results.|Li Chen,Amarnath Gupta,M. Erdem Kurul","80522|VLDB|2005|Space Efficiency in Synopsis Construction Algorithms|Histograms and Wavelet synopses have been found to be useful in query optimization, approximate query answering and mining. Over the last few years several good synopsis algorithms have been proposed. These have mostly focused on the running time of the synopsis constructions, optimum or approximate, vis-a-vis their quality. However the space complexity of synopsis construction algorithms has not been investigated as thoroughly. Many of the optimum synopsis construction algorithms (as well as few of the approximate ones) are expensive in space. In this paper, we propose a general technique that reduces space complexity. We show that the notion of \"working space\" proposed in these contexts is redundant. We believe that our algorithm also generalizes to a broader range of dynamic programs beyond synopsis construction. Our modifications can be easily adapted to existing algorithms. We demonstrate the performance benefits through experiments on real-life and synthetic data.|Sudipto Guha","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","80580|VLDB|2005|Client Assignment in Content Dissemination Networks for Dynamic Data|Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of data item, coherence served by each repository and a set of client, data item, coherence requests, we address the following problem How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximizedIn this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.|Shetal Shah,Krithi Ramamritham,Chinya V. Ravishankar","80560|VLDB|2005|KLEE A Framework for Distributed Top-k Query Algorithms|This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.|Sebastian Michel,Peter Triantafillou,Gerhard Weikum","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","80470|VLDB|2005|Semantic Overlay Networks|In a handful of years only, Peer-to-Peer (PP) systems have become an integral part of the Internet. After a few key successes related to music-sharing (e.g., Napster or Gnutella), they rapidly developed and are nowadays firmly established in various contexts, ranging from large-scale content distribution (Bit Torrent) to Internet telephony(Skype) or networking platforms (JXTA). The main idea behind PP is to leverage on the power of end-computers Instead of relying on central components (e.g., servers), services are powered by decentralized overlay architectures where end-computers connect to each other dynamically.|Karl Aberer,Philippe Cudr√©-Mauroux","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","80512|VLDB|2005|Semantic Adaptation of Schema Mappings when Schemas Evolve|Schemas evolve over time to accommodate the changes in the information they represent. Such evolution causes invalidation of various artifacts depending on the schemas, such as schema mappings. In a heterogenous environment, where cooperation among data sources depends essentially upon them, schema mappings must be adapted to reflect schema evolution. In this study, we explore the mapping composition approach for addressing this mapping adaptation problem. We study the semantics of mapping composition in the context of mapping adaptation and compare our approach with the incremental approach of Velegrakis et al . We show that our method is superior in terms of capturing the semantics of both the original mappings and the evolution. We design and implement a mapping adaptation system based on mapping composition as well as additional mapping pruning techniques that significantly speed up the adaptation. We conduct comprehensive experimental analysis and show that the composition approach is practical in various evolution scenarios. The mapping language that we consider is a nested relational extension of the second-order dependencies of Fagin et al . Our work can also be seen as an implementation of the mapping composition operator of the model management framework.|Cong Yu,Lucian Popa","80475|VLDB|2005|Interactive Schema Translation with Instance-Level Mappings|We demonstrate a prototype that translates schemas from a source metamodel (e.g., OO, relational, XML) to a target metamodel. The prototype is integrated with Microsoft Visual Studio  to generate relational schemas from an object-oriented design. It has four novel features. First, it produces instance mappings to round-trip the data between the source schema and the generated target schema. It compiles the instance mappings into SQL views to reassemble the objects stored in relational tables. Second, it offers interactive editing, i.e., incremental modifications of the source schema yield incremental modifications of the target schema. Third, it incorporates a novel mechanism for mapping inheritance hierarchies to relations, which supports all known strategies and their combinations. Fourth, it is integrated with a commercial product featuring a high-quality user interface. The schema translation process is driven by high-level rules that eliminate constructs that are absent from the target metamodel.|Philip A. Bernstein,Sergey Melnik,Peter Mork","80486|VLDB|2005|Information Preserving XML Schema Embedding|A fundamental concern of information integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source document(s) is preserved. In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be defined from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also provide efficient heuristic algorithms to find candidate embeddings, along with experimental results to evaluate and compare the algorithms. These yield the first systematic and effective approach to finding information preserving XML mappings.|Philip Bohannon,Wenfei Fan,Michael Flaster,P. P. S. Narayan","80578|VLDB|2005|Tuning Schema Matching Software using Synthetic Scenarios|Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user must then tune the system select the right component to be executed and correctly adjust their numerous \"knobs\" (e.g., thresholds, formula coefficients). Tuning is skill- and time-intensive, but (as we show) without it the matching accuracy is significantly inferior.We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of matching components. While the tuning process is completely automatic, eTuner can also exploit user assistance (whenever available) to further improve the tuning quality. We employed eTuner to tune four recently developed matching systems on several real-world domains. eTuner produced tuned matching systems that achieve higher accuracy than using the systems with currently possible tuning methods, at virtually no cost to the domain user.|Mayssam Sayyadian,Yoonkyong Lee,AnHai Doan,Arnon Rosenthal","80479|VLDB|2005|Designing Information-Preserving Mapping Schemes for XML|An XML-to-relational mapping scheme consists of a procedure for shredding documents into relational databases, a procedure for publishing databases back as documents, and a set of constraints the databases must satisfy. In previous work, we defined two notions of information preservation for mapping schemes losslessness, which guarantees that any document can be reconstructed from its corresponding database and validation, which requires every legal database to correspond to a valid document. We also described one information-preserving mapping scheme, called Edge++, and showed that, under reasonable assumptions, losslessness and validation are both undecidable. This leads to the question we study in this paper how to design mapping schemes that are information-preserving. We propose to do it by starting with a scheme known to be information-preserving and applying to it equivalence-preserving transformations written in weakly recursive ILOG. We study an instance of this framework, the LILO algorithm, and show that it provides significant performance improvements over Edge++ and introduces constraints that are efficiently enforced in practice.|Denilson Barbosa,Juliana Freire,Alberto O. Mendelzon"],["80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","80493|VLDB|2005|Loadstar Load Shedding in Data Stream Mining|In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.|Yun Chi,Haixun Wang,Philip S. Yu","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80488|VLDB|2005|On Map-Matching Vehicle Tracking Data|Vehicle tracking data is an essential \"raw\" material for a broad range of applications such as traffic management and control, routing, and navigation. An important issue with this data is its accuracy. The method of sampling vehicular movement using GPS is affected by two error sources and consequently produces inaccurate trajectory data. To become useful, the data has to be related to the underlying road network by means of map matching algorithms. We present three such algorithms that consider especially the trajectory nature of the data rather than simply the current position as in the typical map-matching case. An incremental algorithm is proposed that matches consecutive portions of the trajectory to the road network, effectively trading accuracy for speed of computation. In contrast, the two global algorithms compare the entire trajectory to candidate paths in the road network. The algorithms are evaluated in terms of (i) their running time and (ii) the quality of their matching result. Two novel quality measures utilizing the Fr&eacutechet distance are introduced and subsequently used in an experimental evaluation to assess the quality of matching real tracking data to a road network.|Sotiris Brakatsoulas,Dieter Pfoser,Randall Salas,Carola Wenk","80489|VLDB|2005|OLAP Over Uncertain and Imprecise Data|We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.|Douglas Burdick,Prasad Deshpande,T. S. Jayram,Raghu Ramakrishnan,Shivakumar Vaithyanathan","80478|VLDB|2005|Approximate Matching of Hierarchical Data Using pq-Grams|When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.|Nikolaus Augsten,Michael H. B√∂hlen,Johann Gamper"]]}}