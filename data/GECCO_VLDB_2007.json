{"abstract":{"entropy":6.314633341481664,"topics":["artificial immune, immune systems, detection problem, analysis critical, learning systems, process analysis, autonomous selection, artificial systems, analysis applications, performance research, systems control, systems based, use design, research design, systems problem, model systems, artificial, systems, design, performance","learning classifier, real world, support vector, data similar, increasing number, data spatial, present novel, work present, present framework, work, problem data, techniques, effort, task, years, structure, matching, complex, dna, present","data, mining web, data management, data integration, recently management, data web, data systems, queries distributed, sql query, management information, data mining, query, management integration, large data, query data, query function, important data, database, schema data, information","evolutionary algorithm, genetic algorithm, algorithm, particle swarm, genetic programming, optimization problem, optimization algorithm, hierarchical bayesian, algorithm problem, present algorithm, genetic problem, describe algorithm, estimation distribution, swarm optimization, particle optimization, use algorithm, differential evolution, problem, multiobjective optimization, multi-objective optimization","artificial systems, systems based, systems control, based, distributed, increasingly, adaptive, agents, present, domain, crucial, autonomous, describe, use, computer, algorithm","performance research, research design, design, performance, development, research, study, cell, important, explore, processes, level, quality, evolutionary","real world, problem, applications, structure, matching, objective, tool, code, range, optimisation, datasets, prediction, number","learning classifier, support vector, data spatial, services, find, problem","sql query, database, query, testing, processing, different, nature, relational, generating, xml, distributed, number","data, data systems, important data, data information, requirements data, information, views, coevolutionary, scientific, access, present, based, current, useful, fundamental, sensitive","evolutionary algorithm, hierarchical bayesian, estimation distribution, hierarchical algorithm, distribution algorithm, hybrid algorithm, estimation algorithm, bayesian algorithm, hybrid evolutionary, robust, parameters, probabilistic, typical, diversity, benefit","genetic algorithm, genetic gas, algorithm gas, paper algorithm, differential evolution, present approach, algorithm search, genetic approach, analyze algorithm, novel approach, model algorithm, novel algorithm, evolution algorithm, paper, approach model, paper model, paper genetic, approach, analyze, model"],"ranking":[["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","58226|GECCO|2007|A biologically inspired solution for an evolved simulated agent|Biologically inspired designs can improve the design of artificial agents. In this paper we explain and explore the role of directional light sensors from an Evolutionary Robotics perspective using a dynamical systems approach. It was found that by using directionally specific sensors in the agent, there was a simplification of the neural controller employed. This simplification helped not only with the analysis of this type of controller but also improved the behavioural performance of the agents, thereby showing a good example of the ecological balance principle.|Edgar Bermudez Contreras","58163|GECCO|2007|Artificial ecosystems for creative discovery|This paper discusses the concept of an artificial ecosystem for use in machine-assisted creative discovery. Properties and processes from natural ecosystems are abstracted and applied to the design of creative systems, in a similar way that evolutionary computing methods use the metaphor of Darwinian evolution to solve problems in search and optimisation. The paper examines some appropriate mechanisms and metaphors when applying artificial ecosystems to problems in creative design. General properties and processes of evolutionary artificial ecosystems are presented as a basis for developing individual systems that automate the discovery of novelty without explicit teleological goals. The adaptation of species to fit their environment drives the creative solutions, so the role of the designer shifts to the design of environments. This allows a variety of creative solutions to emerge in simulation without the need for explicit or human-evaluated fitness measures, such as those used in interactive evolution. Two example creative ecosystems are described to highlight the effectiveness of the method presented.|Jon McCormack","57892|GECCO|2007|Dynamical blueprints exploiting levels of system-environment interaction|Developmental systems typically produce a phenotype through a generative process whose outcome depends on feedback from the environment. In most artificial developmental systems, this feedback occurs in one way The environment affects the development process, but the development process does not necessarily affect the environment. Here we explore a condition where both the developing system and the environment affect each other on a similar timescale, thus resulting in system-environment dynamical interaction. Using a model inspired by termite nest construction, we demonstrate how evolution can exploit this system-environment dynamics to generate adaptive and self-repairing structure more efficiently than a purely reactive developmental system. Finally, we offer a metric to quantify the level of interaction and distinguish between reactive and interactive developmental systems.|Nicolás S. Estévez,Hod Lipson","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","58073|GECCO|2007|Using genetic algorithms for naval subsystem damage assessment and design improvements|Some auxiliary systems of next generation naval ships will utilize distributed automatic control. Such distributed control systems will use interconnected sensors, actuators, controllers and networking components to diagnose and reconfigure the auxiliary systems. Testing these systems will be difficult with traditional methods of fault analysis due to the interconnected and automatic nature of these subsystems. We have designed a suite of genetic algorithms to find interesting and hidden damage scenarios in a testbed of a naval subsystem. Given this knowledge, we use a genetic algorithm to improve upon the design of this subsystem.|Christopher McCubbin,David Scheidt,Oliver Bandte,Steven Marshall,Iavor Trifonov","58017|GECCO|2007|Procreating V-detectors for nonself recognition an application to anomaly detection in power systems|The artificial immune system approach for self-nonself discrimination and its application to anomaly detection problems in engineering is showing great promise. A seminal contribution in this area is the V-detectors algorithm that can very effectively cover the nonself region of the feature space with a set of detectors. The detector set can be used to detect anomalous inputs. In this paper, a multistage approach to create an effective set of V-detectors is considered. The first stage of the algorithm generates an initial set of V-detectors. In subsequent stage, new detectors are grown from existing ones, by means of a mechanism called procreation. Procreating detectors can more effectively fill hard-to-reach interstices in the nonself region, resulting in better coverage. The effectiveness of the algorithm is first illustrated by applying it to a well-known fractal, the Koch curve. The algorithm is then applied to the problem of detecting anomalous behavior in power distribution systems, and can be of much use for maintenance-related decision-making in electrical utility companies.|Min Gui,Sanjoy Das,Anil Pahwa","80760|VLDB|2007|Self-Tuning Database Systems A Decade of Progress|In this paper we discuss advances in self-tuning database systems over the past decade, based on our experience in the AutoAdmin project at Microsoft Research. This paper primarily focuses on the problem of automated physical database design. We also highlight other areas where research on self-tuning database technology has made significant progress. We conclude with our thoughts on opportunities and open issues.|Surajit Chaudhuri,Vivek R. Narasayya","58124|GECCO|2007|Stochastic training of a biologically plausible spino-neuromuscular system model|A primary goal of evolutionary robotics is to create systems that are as robust and adaptive as the human body. Moving toward this goal often involves training control systems that processes sensory information in a way similar to humans. Artificial neural networks have been an increasingly popular option for thisbecause they consist of processing units that approximate thesynaptic activity of biological signal processing units, i.e. neurons. In this paper we train a nonlinear recurrent spino-neuromuscular system model(SNMS) comparing the performance of genetic algorithms (GA)s, particle swarm optimizers (PSO)s, and GAPSO hybrids. This model includes several key features of the SNMS that have previously been modeled individually but have not been combined into a single model as is done here. The results show that each algorithm produces fit solutions and generates fundamental biological behaviors that are not directly trained for such as tonic tension behaviors and tricepsactivation patterns.|Stanley Phillips Gotshall,Terence Soule"],["80827|VLDB|2007|Detecting Attribute Dependencies from Query Feedback|Real-world datasets exhibit a complex dependency structure among the data attributes. Learning this structure is a key task in automatic statistics configuration for query optimizers, as well as in data mining, metadata discovery, and system management. In this paper, we provide a new method for discovering dependent attribute pairs based on query feedback. Our approach avoids the problem of searching through a combinatorially large space of candidate attribute pairs, automatically focusing system resources on those pairs of demonstrable interest to users. Unlike previous methods, our technique combines all of the pertinent feedback for a specified pair of attributes in a principled and robust manner, while being simple and fast enough to be incorporated into current commercial products. The method is similar in spirit to the CORDS algorithm, which proactively collects frequencies of data values and computes a chi-squared statistic from the resulting contingency table. In the reactive query-feedback setting, many entries of the contingency table are missing, and a key contribution of this paper is a variant of classical chi-squared theory that handles this situation. Because we typically discover a large number of dependent attribute pairs, we provide novel methods for ranking the pairs based on degree of dependency. Such ranking information, e.g., enables a database system to avoid exceeding the space budget for the system catalog by storing only the currently most important multivariate statistics. Experiments indicate that our dependency rankings are stable even in the presence of relatively few feedback records.|Peter J. Haas,Fabian Hueske,Volker Markl","80784|VLDB|2007|Peer-to-Peer Similarity Search in Metric Spaces|This paper addresses the efficient processing of similarity queries in metric spaces, where data is horizontally distributed across a PP network. The proposed approach does not rely on arbitrary data movement, hence each peer joining the network autonomously stores its own data. We present SIMPEER, a novel framework that dynamically clusters peer data, in order to build distributed routing information at super-peer level. SIMPEER allows the evaluation of range and nearest neighbor queries in a distributed manner that reduces communication cost, network latency, bandwidth consumption and computational overhead at each individual peer. SIMPEER utilizes a set of distributed statistics and guarantees that all similar objects to the query are retrieved, without necessarily flooding the network during query processing. The statistics are employed for estimating an adequate query radius for k-nearest neighbor queries, and transform the query to a range query. Our experimental evaluation employs both real-world and synthetic data collections, and our results show that SIMPEER performs efficiently, even in the case of high degree of distribution.|Christos Doulkeridis,Akrivi Vlachou,Yannis Kotidis,Michalis Vazirgiannis","80788|VLDB|2007|CADS Continuous Authentication on Data Streams|We study processing and authentication of long-running queries on outsourced data streams. In this scenario, a data owner (DO) constantly transmits its data to a service provider (SP), together with additional authentication information. Clients register continuous range queries to the SP. Whenever the data change, the SP must update the results of all affected queries and inform the clients accordingly. The clients can verify the correctness of the results using the authentication information provided by the DO. Compared to conventional databases, stream environments pose new challenges such as the need for fast structure updating, support for continuous query processing and authentication, and provision for temporal completeness. Specifically, in addition to the correctness of individual results, the client must be able to verify that there are no missing results in between updates. We face these challenges through several contributions. Since there is no previous work, we first present a technique, called REF, that achieves correctness and temporal completeness but incurs false transmissions, i.e., the SP has to inform clients whenever there is a data update, even if their results are not affected. Then, we propose CADS, which minimizes the processing and transmission overhead through an elaborate indexing scheme and a virtual caching mechanism. Finally, we extend CADS to the case where multiple owners outsource their data to the same SP. The SP integrates all data in a single authentication process, independently of the number of DOs.|Stavros Papadopoulos,Yin Yang,Dimitris Papadias","80739|VLDB|2007|VGRAM Improving Performance of Approximate Queries on String Collections Using Variable-Length Grams|Many applications need to solve the following problem of approximate string matching from a collection of strings, how to find those similar to a given string, or the strings in another (possibly the same) collection of strings Many algorithms are developed using fixed-length grams, which are substrings of a string used as signatures to identify similar strings. In this paper we develop a novel technique, called VGRAM, to improve the performance of these algorithms. Its main idea is to judiciously choose high-quality grams of variable lengths from a collection of strings to support queries on the collection. We give a full specification of this technique, including how to select high-quality grams from the collection, how to generate variable-length grams for a string based on the preselected grams, and what is the relationship between the similarity of the gram sets of two strings and their edit distance. A primary advantage of the technique is that it can be adopted by a plethora of approximate string algorithms without the need to modify them substantially. We present our extensive experiments on real data sets to evaluate the technique, and show the significant performance improvements on three existing algorithms.|Chen Li,Bin Wang,Xiaochun Yang","80836|VLDB|2007|Seeking Stable Clusters in the Blogosphere|The popularity of blogs has been increasing dramatically over the last couple of years. As topics evolve in the blogosphere, keywords align together and form the heart of various stories. Intuitively we expect that in certain contexts, when there is a lot of discussion on a specific topic or event, a set of keywords will be correlated the keywords in the set will frequently appear together (pair-wise or in conjunction) forming a cluster. Note that such keyword clusters are temporal (associated with specific time periods) and transient. As topics recede, associated keyword clusters dissolve, because their keywords no longer appear frequently together. In this paper, we formalize this intuition and present efficient algorithms to identify keyword clusters in large collections of blog posts for specific temporal intervals. We then formalize problems related to the temporal properties of such clusters. In particular, we present efficient algorithms to identify clusters that persist over time. Given the vast amounts of data involved, we present algorithms that are fast (can efficiently process millions of blogs with multiple millions of posts) and take special care to make them efficiently realizable in secondary storage. Although we instantiate our techniques in the context of blogs, our methodology is generic enough to apply equally well to any temporally ordered text source. We present the results of an experimental study using both real and synthetic data sets, demonstrating the efficiency of our algorithms, both in terms of performance and in terms of the quality of the keyword clusters and associated temporal properties we identify.|Nilesh Bansal,Fei Chiang,Nick Koudas,Frank Wm. Tompa","80766|VLDB|2007|Ranked Subsequence Matching in Time-Series Databases|Existing work on similar sequence matching has focused on either whole matching or range subsequence matching. In this paper, we present novel methods for ranked subsequence matching under time warping, which finds top-k subsequences most similar to a query sequence from data sequences. To the best of our knowledge, this is the first and most sophisticated subsequence matching solution mentioned in the literature. Specifically, we first provide a new notion of the minimum-distance matching-window pair (MDMWP) and formally define the mdmwp-distance, a lower bound between a data subsequence and a query sequence. The mdmwp-distance can be computed prior to accessing the actual subsequence. Based on the mdmwp-distance, we then develop a ranked subsequence matching algorithm to prune unnecessary subsequence accesses. Next, to reduce random disk IOs and bad buffer utilization, we develop a method of deferred group subsequence retrieval. We then derive another lower bound, the window-group distance, that can be used to effectively prune unnecessary subsequence accesses during deferred group-subsequence retrieval. Through extensive experiments with many data sets, we showcase the superiority of the proposed methods.|Wook-Shin Han,Jinsoo Lee,Yang-Sae Moon,Haifeng Jiang","80764|VLDB|2007|Querying Complex Structured Databases|Correctly generating a structured query (e.g., an XQuery or a SQL query) requires the user to have a full understanding of the database schema, which can be a daunting task. Alternative query models have been proposed to give users the ability to query the database without schema knowledge. Those models, including simple keyword search and labeled keyword search, aim to extract meaningful data fragments that match the structure-free query conditions (e.g., keywords) based on various matching semantics. Typically, the matching semantics are content-based they are defined on data node inter-relationships and incur significant query evaluation cost. Our first contribution is a novel matching semantics based on analyzing the database schema. We show that query models employing a schema-based matching semantics can reduce query evaluation cost significantly while maintaining or even improving result quality. The adoption of schema-based matching semantics does not change the nature of those query models they are still schema-ignorant, i.e., users express no schema knowledge (except the labels in labeled keyword search) in the query. While those models work well for some queries on some databases, they often encounter problems when applied to complex queries on databases with complex schemas. Our second contribution is a novel query model that incorporates partial schema knowledge through the use of schema summary. This new summary-aware query model, called Meaningful Summary Query (MSQ), seamlessly integrates summary-based structural conditions and structure-free conditions, and enables ordinary users to query complex databases. We design algorithms for evaluating MSQ queries, and demonstrate that MSQ queries can produce better results against complex databases when compared with previous approaches, and that they can be efficiently evaluated.|Cong Yu,H. V. Jagadish","80841|VLDB|2007|K-Anonymization as Spatial Indexing Toward Scalable and Incremental Anonymization|In this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. We use this observation to leverage over  years of work on database indexing to provide efficient and dynamic anonymization techniques. Experiments with our implementation show that the R-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. Finally, we show that the anonymizations generated by the R-tree approach do not sacrifice quality in their search for efficiency in fact, by several previously proposed quality metrics, the compact partitioning properties of R-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms.|Tochukwu Iwuchukwu,Jeffrey F. Naughton","80798|VLDB|2007|Modeling and Querying Vague Spatial Objects Using Shapelets|Research in modeling and querying spatial data has primarily focused on traditional \"crisp\" spatial objects with exact location and spatial extent. More recent work, however, has begun to address the need for spatial data types describing spatial phenomena that cannot be modeled by objects having sharp boundaries. Other work has focused on point objects whose location is not precisely known and is typically described using a probability distribution. In this paper, we present a new technique for modeling and querying vague spatial objects. Using shapelets, an image decomposition technique developed in astronomy, as base data type, we introduce a comprehensive set of low-level operations that provide building blocks for versatile high-level operations on vague spatial objects. In addition, we describe an implementation of this data model as an extension to PostgreSQL, including an indexing technique for shapelet objects. Unlike existing techniques for modeling and querying vague or fuzzy data, our approach is optimized for localized, smoothly varying spatial objects, and as such is more suitable for many real-world datasets.|Daniel Zinn,Jim Bosch,Michael Gertz","58225|GECCO|2007|Nonlinearity linkage detection for financial time series analysis|Standard detection algorithms for nonlinearity linkage fail when applied to typical problems in the analysis of financial time-series data. We explain how this failure arises when standard algorithms are applied navely, how linkage detection needs to be applied directly to the observed data samples, and how this raises problems that are not addressedby current techniques. We extend the existing DSMDGA linkage detection algorithm and present a new system that can determine the required nonlinearity linkage in observed data samples for financial time series. The new system has been evaluated on synthetic datasets and experimental results are provided. The sensitivity of the system to changes in both the problem and the algorithm parameters has also been explored and we discuss the results. We present evidence of the success of the new system and identify areas for further work.|Theodore Chiotis,Christopher D. Clack"],["58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80809|VLDB|2007|A Bayesian Method for Guessing the Extreme Values in a Data Set|For a large number of data management problems, it would be very useful to be able to obtain a few samples from a data set, and to use the samples to guess the largest (or smallest) value in the entire data set. Minmax online aggregation, top-k query processing, outlier detection, and distance join are just a few possible applications. This paper details a statistically rigorous, Bayesian approach to attacking this problem. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.|Mingxi Wu,Chris Jermaine","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80772|VLDB|2007|Unifying Data and Domain Knowledge Using Virtual Views|The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB  PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.|Lipyeow Lim,Haixun Wang,Min Wang","80803|VLDB|2007|Request Window an Approach to Improve Throughput of RDBMS-based Data Integration System by Utilizing Data Sharing Across Concurrent Distributed Queries|This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently issue data requests and fetch data over the network. This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a .x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.|Rubao Lee,Minghong Zhou,Huaming Liao","80846|VLDB|2007|iTrails Pay-as-you-go Information Integration in Dataspaces|Dataspace management has been recently identified as a new agenda for information management ,  and information integration . In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration , as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.|Marcos Antonio Vaz Salles,Jens-Peter Dittrich,Shant Kirakos Karakashian,Olivier René Girard,Lukas Blunschi","80747|VLDB|2007|Model Management and Schema Mappings Theory and Practice|We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.|Philip A. Bernstein,Howard Ho","80859|VLDB|2007|Query language support for incomplete information in the MayBMS system|MayBMS , , ,  is a data management system for incomplete information developed at Saarland University. Its main features are a simple and compact representation system for incomplete information and a language called I-SQL with explicit operations for handling uncertainty. MayBMS is currently an extension of PostgreSQL and manages both complete and incomplete data and evaluates I-SQL queries.|Lyublena Antova,Christoph Koch,Dan Olteanu"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58180|GECCO|2007|Alternative techniques to solve hard multi-objective optimization problems|In this paper, we propose the combination of different optimization techniques in order to solve \"hard\" two- and three-objective optimization problems at a relatively low computational cost. First, we use the -constraint method in order to obtain a few points over (or very near of) the true Pareto front, and then we use an approach based on rough sets to spread these solutions, so that the entire Pareto front can be covered. The constrained single-objective optimizer required by the -constraint method, is the cultured differential evolution, which is an efficient approach for approximating the global optimum of a problem with a low number of fitness function evaluations. The proposed approach is validated using several difficult multi-objective test problems, and our results are compared with respect to a multi-objective evolutionary algorithm representative of the state-of-the-art in the area the NSGA-II.|Ricardo Landa Becerra,Carlos A. Coello Coello,Alfredo García Hernández-Díaz,Rafael Caballero,Julián Molina Luque","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Suárez,Manuel Valenzuela-Rendón,Hugo Terashima-Marín,Eduardo Uresti-Charre","58190|GECCO|2007|Multi-objective hybrid PSO using -fuzzy dominance|This paper describes a PSO-Nelder Mead Simplex hybrid multi-objective optimization algorithm based on a numerical metric called  -fuzzy dominance. Within each iteration of this approach, in addition to the position and velocity update of each particle using PSO, the k-means algorithm is applied to divide the population into smaller sized clusters. The Nelder-Mead simplex algorithm is used separately within each cluster for added local search. The proposed algorithm is shown to perform better than MOPSO on several test problems as well as for the optimization of a genetic model for flowering time control in Arabidopsis. Adding the local search achieves faster convergence, an important feature in computationally intensive optimization of gene networks.|Praveen Koduru,Sanjoy Das,Stephen Welch","58170|GECCO|2007|Parameter cross-validation and early-stopping in univariate marginal distribution algorithm|In this paper, a cross-validation and early-stopping algorithm is devised for parameter updating in the Univariate Marginal Distribution Algorithm (UMDA) to reduce overftting. Our hypothesis is that the well-known problem of diversity loss in UMDA is a consequence of overfitting during the parameter estimation step at each generation. It is tested by experiments on two different optimization problems.|Hao Wu,Jonathan L. Shapiro","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","58243|GECCO|2007|Analyzing probabilistic models in hierarchical BOA on traps and spin glasses|The hierarchical Bayesian optimization algorithm (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling probabilistic models of promising solutions. This paper analyzes probabilistic models in hBOA on two common test problems concatenated traps and D Ising spin glasses with periodic boundary conditions. We argue that although Bayesian networks with local structures can encode complex probability distributions, analyzing these models in hBOA is relatively straightforward and the results of such analyses may provide practitioners with useful information about their problems. The results show that the probabilistic models in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in subsequent iterations of BOA, and creating adequate probabilistic models by hand is not straightforward even with complete knowledge of the optimization problem.|Mark Hauschild,Martin Pelikan,Cláudio F. Lima,Kumara Sastry","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith"],["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","58226|GECCO|2007|A biologically inspired solution for an evolved simulated agent|Biologically inspired designs can improve the design of artificial agents. In this paper we explain and explore the role of directional light sensors from an Evolutionary Robotics perspective using a dynamical systems approach. It was found that by using directionally specific sensors in the agent, there was a simplification of the neural controller employed. This simplification helped not only with the analysis of this type of controller but also improved the behavioural performance of the agents, thereby showing a good example of the ecological balance principle.|Edgar Bermudez Contreras","57965|GECCO|2007|Effects of passengers arrival distribution to double-deck elevator group supervisory control systems using genetic network programming|The Elevator Group Supervisory Control Systems (EGSCS) are the control systems that systematically manage three or more elevators in order to efficiently transport the passengers in buildings. Double-deck elevators, where two cages are connected with each other, are expected to be the next generation elevator systems. Meanwhile, Destination Floor Guidance Systems (DFGS) are also expected in Double-Deck Elevator Systems (DDES). With these, the passengers could be served at two consecutive floors and could input their destinations at elevator halls instead of conventional systems without DFGS. Such systems become more complex than the traditional systems and require new control methods Genetic Network Programming (GNP), a graph-based evolutionary method, has been applied to EGSCS and its advantages are shown in some previous papers. GNP can obtain the strategy of a new hall call assignment to the optimal elevator because it performs crossover and mutation operations to judgment nodes and processing nodes. In studies so far, the passenger's arrival has been assumed to take Exponential distribution for many years. In this paper, we have applied Erlang distribution and Binomial distribution in order to study how the passenger's arrival distribution affects EGSCS. We have found that the passenger's arrival distribution has great influence on EGSCS. It has been also clarified that GNP makes good performances under different conditions.|Lu Yu,Jin Zhou,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu,Sandor Markon","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","58073|GECCO|2007|Using genetic algorithms for naval subsystem damage assessment and design improvements|Some auxiliary systems of next generation naval ships will utilize distributed automatic control. Such distributed control systems will use interconnected sensors, actuators, controllers and networking components to diagnose and reconfigure the auxiliary systems. Testing these systems will be difficult with traditional methods of fault analysis due to the interconnected and automatic nature of these subsystems. We have designed a suite of genetic algorithms to find interesting and hidden damage scenarios in a testbed of a naval subsystem. Given this knowledge, we use a genetic algorithm to improve upon the design of this subsystem.|Christopher McCubbin,David Scheidt,Oliver Bandte,Steven Marshall,Iavor Trifonov","57939|GECCO|2007|Peer-to-peer evolutionary algorithms with adaptive autonomous selection|In this paper we describe and evaluate a fully distributed PP evolutionary algorithm (EA) with adaptive autonomous selection. Autonomous selection means that decisions regarding survival and reproduction are taken by the individuals themselves independently, without any central control.This allows for a fully distributed EA, where not only reproduction (crossover and mutation) but also selection is performed at local level. An unwanted consequence of adding and removing individuals in a non-synchronized manner is that the population size gets out of control too. This problem is resolved by addingan adaptation mechanism allowing individuals to regulate their own selection pressure. The key tothis is a gossiping algorithm that enables individuals to maintain estimates on the size andthe fitness of the population. The algorithm is experimentally evaluated on a test problem to show the viability of the idea and to gain insight into the run-time dynamics of such an algorithm. The results convincingly demonstrate the feasibility of a fully decentralized EA in which the population size can be kept stable.|W. R. M. U. K. Wickramasinghe,Maarten van Steen,A. E. Eiben","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","58209|GECCO|2007|Evolving distributed agents for managing air traffic|Air traffic management offers an intriguing real world challenge to designing large scale distributed systems using evolutionary computation. The ability to evolve effective air traffic flow strategies depends not only on evolving good local strategies, but also on ensuring that those local strategies result in good global solutions. While traditional, direct evolutionary strategies can be highly effective in certain combinatorial domains, they are not well-suited to complex air traffic flow problems because of the large interdependencies among the local subsystems. In this paper, we propose an evolutionary agent-based solution to the air traffic flow problem. In this approach, we evolve agents both to learn the right local flow strategies to alleviate congestion in their immediate surroundings, and to prevent the creation of congestion \"downstream\" from their local areas. The agent-based approach leads to better and more fault-tolerant solutions. To validate this approach, we use FACET, an air traffic simulator developed at NASA and used extensively by the FAA and industry. On a scenario composed of three hundred aircraft and two points of congestion, our results show that an agent based evolutionary computation method, where each agent uses the system evaluation function, achieves % improvement over a direct evolutionary algorithm. In addition by creating agent-specific \"difference evaluation functions\" we achieve an additional % improvement over agents using the system evaluation.|Adrian K. Agogino,Kagan Tumer","58069|GECCO|2007|SwarmArchitect a swarm framework for collaborative construction|Computer game development has become increasingly popular in the field of autonomous systems. One of the main topics studies the building of various architectures in computer games. A realistic human-like architecture is expected in a thematic computer game, since it strongly motivates the game players in an intuitive way. However, the task of building a human-like architecture is non-trivial since the construction is a real time process without human supervision. In this paper, we present a collective building algorithm inspired by social insects for intelligent construction based on multiple agents. A swarm of virtual agents indirectly design edifications, which resemble basic features in human-like architecture by using a stigmergic mechanism along with branching rules. The main idea of the algorithm is to map sensory information to appropriate building actions.|Yifeng Zeng,Jorge Cordero Hernandez,Dennis Plougman Buus","58124|GECCO|2007|Stochastic training of a biologically plausible spino-neuromuscular system model|A primary goal of evolutionary robotics is to create systems that are as robust and adaptive as the human body. Moving toward this goal often involves training control systems that processes sensory information in a way similar to humans. Artificial neural networks have been an increasingly popular option for thisbecause they consist of processing units that approximate thesynaptic activity of biological signal processing units, i.e. neurons. In this paper we train a nonlinear recurrent spino-neuromuscular system model(SNMS) comparing the performance of genetic algorithms (GA)s, particle swarm optimizers (PSO)s, and GAPSO hybrids. This model includes several key features of the SNMS that have previously been modeled individually but have not been combined into a single model as is done here. The results show that each algorithm produces fit solutions and generates fundamental biological behaviors that are not directly trained for such as tonic tension behaviors and tricepsactivation patterns.|Stanley Phillips Gotshall,Terence Soule"],["58031|GECCO|2007|Option pricing model calibration using a real-valued quantum-inspired evolutionary algorithm|Quantum effects are a natural phenomenon and just like evolution, or immune processes, can serve as an inspiration for the design of computing algorithms. This study illustrates how a real-valued quantum-inspired evolutionary algorithm(QEA) can be constructed and examines the utility of the resulting algorithm on an important real-world problem, namely the calibration of an Option Pricing model. The results from the algorithm are shown to be robust and sensitivity analysis is carried out on the algorithm parameters, suggesting that there is useful potential to apply QEA to this domain.|Kai Fan,Anthony Brabazon,Conall O'Sullivan,Michael O'Neill","58226|GECCO|2007|A biologically inspired solution for an evolved simulated agent|Biologically inspired designs can improve the design of artificial agents. In this paper we explain and explore the role of directional light sensors from an Evolutionary Robotics perspective using a dynamical systems approach. It was found that by using directionally specific sensors in the agent, there was a simplification of the neural controller employed. This simplification helped not only with the analysis of this type of controller but also improved the behavioural performance of the agents, thereby showing a good example of the ecological balance principle.|Edgar Bermudez Contreras","80820|VLDB|2007|From Data Privacy to Location Privacy Models and Algorithms|This tutorial presents the definition, the models and the techniques of location privacy from the data privacy perspective. By reviewing and revising the state of art research in data privacy area, the presenter describes the essential concepts, the alternative models, and the suite of techniques for providing location privacy in mobile and ubiquitous data management systems. The tutorial consists of two main components. First, we will introduce location privacy threats and give an overview of the state of art research in data privacy and analyze the applicability of the existing data privacy techniques to location privacy problems. Second, we will present the various location privacy models and techniques effective in either the privacy policy based framework or the location anonymization based framework. The discussion will address a number of important issues in both data privacy and location privacy research, including the location utility and location privacy trade-offs, the need for a careful combination of policy-based location privacy mechanisms and location anonymization based privacy schemes, as well as the set of safeguards for secure transmission, use and storage of location information, reducing the risks of unauthorized disclosure of location information. The tutorial is designed to be self-contained, and gives the essential background for anyone interested in learning about the concept and models of location privacy, and the principles and techniques for design and development of a secure and customizable architecture for privacy-preserving mobile data management in mobile and pervasive information systems. This tutorial is accessible to data management administrators, mobile location based service developers, and graduate students and researchers who are interested in data management in mobile information syhhhstems, pervasive computing, and data privacy.|Ling Liu","80792|VLDB|2007|Performance Evaluation and Experimental Assessment - Conscience or Curse of Database Research|Performance, performance and performance used to be the three things that really mattered in database research. Most of our published works indeed include an experimental evaluation of the proposed techniques. However, such evaluations are sometimes seen as a \"must-have\" eating up the valuable space where one could describe new ideas. The experimental evaluations end up being short, lacking important information to interpret andor reproduce the results, and often end without clear conclusion.|Ioana Manolescu,Stefan Manegold","57952|GECCO|2007|Screening the parameters affecting heuristic performance|This research screens the tuning parameters of a combinatorial optimization heuristic. Specifically, it presents a Design of Experiments (DOE) approach that uses a Fractional Factorial Design to screen the tuning parameters of Ant Colony System (ACS) for the Travelling Sales person problem. Screening is a preliminary step towards building a full Response Surface Model (RSM) . It identifies parametersthat have little influence on performance and can be omittedfrom the RSM design. This reduces the complexity andexpense of the RSM design.  algorithm parameters and  problem characteristics are considered. Open questionson the effect of  parameters on performance are answered.A further parameter, sometimes assumed important, was shown to have no effect on performance. A new problem characteristic that effects performance was identified. A full version of this paper is available .|Enda Ridge,Daniel Kudenko","58089|GECCO|2007|Vulnerability analysis and security framework BeeSec for nature inspired MANET routing protocols|Design, development and evaluation of adaptive, scalable, and power aware BioNature inspired routing protocols has received a significant amount of attention in the recent past. However, to the best of our knowledge no attempts have been made to systematically analyze their security vulnerabilities. In this paper, we investigate the security vulnerabilities of BeeAdHoc, a well known power aware, BioNature inspired routing protocol. Our analysis clearly demonstrates that the malicious nodes in an untrusted MANET, where BeeAdHoc is used for routing, can significantly disrupt the normal routing behavior. We then propose a security framework, BeeSec, for BeeAdHoc that enables it to counter the different types of threats. We also designed an empirical framework, embedded into a well known simulator, ns-, to systematically validate the operational security of BeeSec. An interesting outcome of the research is that BeeSec, even with significant overhead of the security framework, achieves better performance as compared to state-of-the-art, non-secure, classical routing protocols AODV and DSR.|Nauman Mazhar,Muddassar Farooq","58042|GECCO|2007|Novel ways of improving cooperation and performance in ensemble classifiers|There are two common methods of evolving teams of genetic programs. Research suggests Island approaches produce teams of strong individuals that cooperate poorly and Team approaches produce teams of weak individuals that cooperate strongly. Ideally, teams should be composed of strong individuals that cooperate well. In this paper we present a new class of algorithms called Orthogonal Evolution of Teams (OET) that overcomes the weaknesses of current Island and Team approaches by applying evolutionary pressure at both the level of teams and individuals during selection and replacement. We present four novel algorithms in this new class and compare their performance to Island and Team approaches as well as multi-class Adaboost on a number of classification problems.|Russell Thomason,Terence Soule","57918|GECCO|2007|Analyzing heuristic performance with response surface models prediction optimization and robustness|This research uses a Design of Experiments (DOE) approach to build a predictive model of the performance of a combinatorial optimization heuristic over a range of heuristic tuning parameter settings and problem instance characteristics. The heuristic is Ant Colony System (ACS) for the Travelling Salesperson Problem.  heurstic tuning parameters and  problem characteristics are considered. Response Surface Models (RSM) of the solution quality and solution time predicted ACS performance on both new instances from a publicly available problem generator and new real-world instances from the TSPLIB benchmark library. A numerical optimisation of the RSMs is used to find the tuning parameter settings that yield optimal performance in terms of solution quality and solution time. This paper is the first use of desirability functions, a well-established technique in DOE, to simultaneously optimise these conflicting goals. Finally, overlay plots are used to examine the robustness of the performance of the optimised heuristic across a range of problem instance characteristics. These plots give predictions on the range of problem instances for which a given solutionquality can be expected within a given solution time.|Enda Ridge,Daniel Kudenko","80839|VLDB|2007|GeRoMeSuite A System for Holistic Generic Model Management|Manipulation of models and mappings is a common task in the design and development of information systems. Research in Model Management aims at supporting these tasks by providing a set of operators to manipulate models and mappings. As a framework, GeRoMeSuite provides an environment to simplify the implementation of model management operators. GeRoMeSuite is based on the generic role based metamodel GeRoMe , which represents models from different modeling languages (such as XML Schema, OWL, SQL) in a generic way. Thereby, the management of models in a polymorphic fashion is enabled, i.e. the same operator implementations are used regardless of the original modeling language of the schemas. In addition to providing a framework for model management, GeRoMeSuite implements several fundamental operators such as Match, Merge, and Compose.|David Kensche,Christoph Quix,Xiang Li 0002,Yong Li","80750|VLDB|2007|Adaptive Aggregation on Chip Multiprocessors|The recent introduction of commodity chip multiprocessors requires that the design of core database operations be carefully examined to take full advantage of on-chip parallelism. In this paper we examine aggregation in a multi-core environment, the Sun UltraSPARC T, a chip multiprocessor with eight cores and a shared L cache. Aggregation is an important aspect of query processing that is seemingly easy to understand and implement. Our research, however, demonstrates that a chip multiprocessor adds new dimensions to understanding hash-based aggregation performance---concurrent sharing of aggregation data structures and contentious accesses to frequently used values. We also identify a trade off between private data structures assigned to each thread versus shared data structures for aggregation. Depending on input characteristics, different aggregation strategies are optimal and choosing the wrong strategy can result in a performance penalty of over an order of magnitude. We provide a thorough explanation of the factors affecting aggregation performance on chip multiprocessors and identify three key input characteristics that dictate performance () average run length of identical group-by values, () locality of references to the aggregation hash table, and () frequency of repeated accesses to the same hash table location. We then introduce an adaptive aggregation operator that performs lightweight sampling of the input to choose the correct aggregation strategy with high accuracy. Our experiments verify that our adaptive algorithm chooses the highest performing aggregation strategy on a number of common input distributions.|John Cieslewicz,Kenneth A. Ross"],["80826|VLDB|2007|Bridging the Application and DBMS Profiling Divide for Database Application Developers|In today's world, tools for profiling and tuning application code remain disconnected from the profiling and tuning tools for relational DBMSs. This makes it challenging for developers of database applications to profile, tune and debug their applications, for example, identifying application code that causes deadlocks in the server. We have developed an infrastructure that simultaneously captures both the application context as well as the database context, thereby enabling a rich class of tuning, profiling and debugging tasks that is not possible today. We have built a tool using this infrastructure that enables developers to seamlessly profile, tune and debug ADO.NET applications over Microsoft SQL Server by taking advantage of information across the application and database contexts. We describe and evaluate several tasks that can be accomplished using this tool.|Surajit Chaudhuri,Vivek R. Narasayya,Manoj Syamala","58004|GECCO|2007|Dimensionality reduction in evolutionary multiobjective design case study|Real-world applications of Pareto-based optimisation commonly involve many objectives. It causes difficulties because of reduced selection pressure for better solutions. Dimensionality Reduction (DR) is a very appealing approach to overcome this problem. A case study of multiobjective Electric Machine (EM) design based on DR of the novel model  is considered.|Piotr Wozniak","57918|GECCO|2007|Analyzing heuristic performance with response surface models prediction optimization and robustness|This research uses a Design of Experiments (DOE) approach to build a predictive model of the performance of a combinatorial optimization heuristic over a range of heuristic tuning parameter settings and problem instance characteristics. The heuristic is Ant Colony System (ACS) for the Travelling Salesperson Problem.  heurstic tuning parameters and  problem characteristics are considered. Response Surface Models (RSM) of the solution quality and solution time predicted ACS performance on both new instances from a publicly available problem generator and new real-world instances from the TSPLIB benchmark library. A numerical optimisation of the RSMs is used to find the tuning parameter settings that yield optimal performance in terms of solution quality and solution time. This paper is the first use of desirability functions, a well-established technique in DOE, to simultaneously optimise these conflicting goals. Finally, overlay plots are used to examine the robustness of the performance of the optimised heuristic across a range of problem instance characteristics. These plots give predictions on the range of problem instances for which a given solutionquality can be expected within a given solution time.|Enda Ridge,Daniel Kudenko","80734|VLDB|2007|Example-driven design of efficient record matching queries|Record matching is the task of identifying records that match the same real world entity. This is a problem of great significance for a variety of business intelligence applications. Implementations of record matching rely on exact as well as approximate string matching (e.g., edit distances) and use of external reference data sources. Record matching can be viewed as a query composed of a small set of primitive operators. However, formulating such record matching queries is difficult and depends on the specific application scenario. Specifically, the number of options both in terms of string matching operations as well as the choice of external sources can be daunting. In this paper, we exploit the availability of positive and negative examples to search through this space and suggest an initial record matching query. Such queries can be subsequently modified by the programmer as needed. We ensure that the record matching queries our approach produces are () efficient these queries can be run on large datasets by leveraging operations that are well-supported by RDBMSs, and () explainable the queries are easy to understand so that they may be modified by the programmer with relative ease. We demonstrate the effectiveness of our approach on several real-world datasets.|Surajit Chaudhuri,Bee-Chung Chen,Venkatesh Ganti,Raghav Kaushik","57919|GECCO|2007|Parallel skeleton for multi-objective optimization|Many real-world problems are based on the optimization of more than one objective function. This work presents a tool for the resolution of multi-objective optimization problems based on the cooperation of a set of algorithms. The invested time in the resolution is decreased by means of a parallel implementation of an evolutionary team algorithm. This model keeps the advantages of heterogeneous island models but also allows to assign more computational resources to the algorithms with better expectations. The elitist scheme applied aims to improve the results obtained with single executions of independent evolutionary algorithms. The user solves the problem without the need of knowing the internal operation details of the used evolutionary algorithms. The computational results obtained on a cluster of PCs for some tests available in the literature are presented.|Coromoto León,Gara Miranda,Carlos Segura","80768|VLDB|2007|Automating the Detection of Snapshot Isolation Anomalies|Snapshot isolation (SI) provides significantly improved concurrency over PL, allowing reads to be non-blocking. Unfortunately, it can also lead to non-serializable executions in general. Despite this, it is widely used, supported in many commercial databases, and is in fact the highest available level of consistency in Oracle and Post-greSQL. Sufficient conditions for detecting whether SI anomalies could occur in a given set of transactions were presented recently, and extended to necessary conditions for transactions without predicate reads. In this paper we address several issues in extending the earlier theory to practical detectioncorrection of anomalies. We first show how to mechanically find a set of programs which is large enough so that we ensure that all executions will be free of SI anomalies, by modifying these programs appropriately. We then address the problem of false positives, i.e., transaction programs wrongly identified as possibly leading to anomalies, and present techniques that can significantly reduce such false positives. Unlike earlier work, our techniques are designed to be automated, rather than manually carried out. We describe a tool which we are developing to carry out this task. The tool operates on descriptions of the programs either taken from the application code itself, or taken from SQL query traces. It can be used with any database system. We have used our tool on two real world applications in production use at IIT Bombay, and detected several anomalies, some of which have caused real world problems. We believe such a tool will be invaluable for ensuring safe execution of the large number of applications which are already running under SI.|Sudhir Jorwekar,Alan Fekete,Krithi Ramamritham,S. Sudarshan","57894|GECCO|2007|Self-adaptive ant colony optimisation applied to function allocation in vehicle networks|Modern vehicles possess an increasing number of softwareand hardware components that are integrated in electroniccontrol units (ECUs). Finding an optimal allocation forall components is a multi-objective optimisation problem,since every valid allocation can be rated according to multipleobjectives like costs, busload, weight, etc. Additionally,several constraints mainly regarding the availability of resourceshave to be considered. This paper introduces a newvariant of the well-known ant colony optimisation, whichhas been applied to the real-world problem described above.Since it concerns a multi-objective optimisation problem,multiple ant colonies are employed. In the course of thiswork, pheromone updating strategies specialised on constrainthandling are developed. To reduce the effort neededto adapt the algorithm to the optimisation problem by tuningstrategic parameters, self-adaptive mechanisms are establishedfor most of them. Besides the reduction of theeffort, this step also improves the algorithm's convergencebehaviour.|Manuel Förster,Bettina Bickel,Bernd Hardung,Gabriella Kókai","80776|VLDB|2007|LCS-TRIM Dynamic Programming Meets XML Indexing and Querying|In this article, we propose a new approach for querying and indexing a database of trees with specific applications to XML datasets. Our approach relies on representing both the queries and the data using a sequential encoding and then subsequently employing an innovative variant of the longest common subsequence (LCS) matching algorithm to retrieve the desired results. A key innovation here is the use of a series of inter-linked early pruning steps, coupled with a simple index structure that enable us to reduce the search space and eliminate a large number of false positive matches prior to applying the more expensive LCS matching algorithm. Additionally, we also present mechanisms that enable the user to specify constraints on the retrieved output and show how such constraints can be pushed deep into the retrieval process, leading to improved response times. Mechanisms supporting the retrieval of approximate matches are also supported. When compared with state-of-the-art approaches, the query processing time of our algorithms is shown to be up to two to three orders of magnitude faster on several real datasets on realistic query workloads. Finally, we show that our approach is suitable for emerging multi-core server architectures when retrieving data for more expensive queries.|Shirish Tatikonda,Srinivasan Parthasarathy,Matthew Goyder","58195|GECCO|2007|ICSPEA evolutionary five-axis milling path optimisation|ICSPEA is a novel multi-objective evolutionary algorithm which integrates aspects from the powerful variation operators of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and the well proven multi-objective Strength Pareto Evaluation Scheme of the SPEA . The CMA-ES has already shown excellent performance on various kinds of complex single-objective problems. The evaluation scheme of the SPEA  selects individuals with respect to their current position in the objective space using a scalar index in order to form proper Pareto front approximations. These indices can be used by the CMA-part of ICSPEA for learning and guiding the search towards better Pareto front approximations. The ICSPEA is applied to complex benchmark functions such as an extended n-dimensional Schaffer's function or Quagliarella's problem. The results show that the CMA operator allows ICSPEA to find the Pareto set of the generalised Schaffer's function faster than SPEA . Furthermore, this concept is tested on the complex real-world application of the multi-objective optimization of five-axis milling NC-paths. An application of ICSPEA to the milling-path optimisation problem yielded efficient sets of five-axis NC-paths.|Jörn Mehnen,Rajkumar Roy,Petra Kersting,Tobias Wagner","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"],["57927|GECCO|2007|Improving the human readability of features constructed by genetic programming|The use of machine learning techniques to automatically analyse data for information is becoming increasingly widespread. In this paper we examine the use of Genetic Programming and a Genetic Algorithm to pre-process data before it is classified by an external classifier. Genetic Programming is combined with a Genetic Algorithm to construct and select new features from those available in the data, a potentially significant process for data mining since it gives consideration to hidden relationships between features. We then examine techniques to improve the human readability of these new features and extract more information about the domain.|Matthew Smith,Larry Bull","58097|GECCO|2007|Mining breast cancer data with XCS|In this paper, we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C..|Faten Kharbat,Larry Bull,Mohammed Odeh","80815|VLDB|2007|Data Access Patterns in The Amazoncom Technology Platform|The Amazon.com technology platform provides a set of highly advanced business and infrastructure services implemented using ultra-scalable distributed systems technologies. Within this environment we can identify a number of specific data access patterns, each with their own availability, consistency, performance and operational requirements in order to serve a collection of highly diverse business processes. In this presentation we will reviews these different patterns in detail and discuss which technologies are required to support them in an always-on environment.|Werner Vogels","57978|GECCO|2007|Controlling overfitting with multi-objective support vector machines|Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs.|Ingo Mierswa","80813|VLDB|2007|Efficient Computation of Reverse Skyline Queries|In this paper, for the first time, we introduce the concept of Reverse Skyline Queries. At first, we consider for a multidimensional data set P the problem of dynamic skyline queries according to a query point q. This kind of dynamic skyline corresponds to the skyline of a transformed data space where point q becomes the origin and all points of P are represented by their distance vector to q. The reverse skyline query returns the objects whose dynamic skyline contains the query object q. In order to compute the reverse skyline of an arbitrary query point, we first propose a Branch and Bound algorithm (called BBRS), which is an improved customization of the original BBS algorithm. Furthermore, we identify a super set of the reverse skyline that is used to bound the search space while computing the reverse skyline. To further reduce the computational cost of determining if a point belongs to the reverse skyline, we propose an enhanced algorithm (called RSSA) that is based on accurate pre-computed approximations of the skylines. These approximations are used to identify whether a point belongs to the reverse skyline or not. Through extensive experiments with both real-world and synthetic datasets, we show that our algorithms can efficiently support reverse skyline queries. Our enhanced approach improves reversed skyline processing by up to an order of magnitude compared to the algorithm without the usage of pre-computed approximations.|Evangelos Dellis,Bernhard Seeger","80843|VLDB|2007|Over-encryption Management of Access Control Evolution on Outsourced Data|Data outsourcing is emerging today as a successful paradigm allowing users and organizations to exploit external services for the distribution of resources. A crucial problem to be addressed in this context concerns the enforcement of selective authorization policies and the support of policy updates in dynamic scenarios. In this paper, we present a novel solution to the enforcement of access control and the management of its evolution. Our proposal is based on the application of selective encryption as a means to enforce authorizations. Two layers of encryption are imposed on data the inner layer is imposed by the owner for providing initial protection, the outer layer is imposed by the server to reflect policy modifications. The combination of the two layers provides an efficient and robust solution. The paper presents a model, an algorithm for the management of the two layers, and an analysis to identify and therefore counteract possible information exposure risks.|Sabrina De Capitani di Vimercati,Sara Foresti,Sushil Jajodia,Stefano Paraboschi,Pierangela Samarati","57932|GECCO|2007|Genetically designed multiple-kernels for improving the SVM performance|Classical kernel-based classifiers only use a single kernel, butthe real world applications have emphasized the need to con-sider a combination of kernels also known as a multiple kernel in order to boost the performance. Our purpose isto automatically find the mathematical expression of a multiple kernel by evolutionary means. In order to achieve this purpose we propose a hybrid model that combines a Genetic Programming (GP) algorithm and a kernel-based Support Vector Machine (SVM) classifier. Each GP chromosome isa tree encoding the mathematical expression of a multiple kernel. Numerical experiments show that the SVM embedding the evolved multiple kernel performs better than the standard kernels for the considered classification problems.|Laura Diosan,Mihai Oltean,Alexandrina Rogozan,Jean-Pierre Pécuchet","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono","58029|GECCO|2007|Hybrid coevolutionary algorithms vs SVM algorithms|As a learning method support vector machine is regarded as one of the best classifiers with a strong mathematical foundation. On the other hand, evolutionary computational technique is characterized as a soft computing learning method with its roots in the theory of evolution. During the past decade, SVM has been commonly used as a classifier for various applications. The evolutionary computation has also attracted a lot of attention in pattern recognition and has shown significant performance improvement on a variety of applications. However, there has been no comparison of the two methods. In this paper, first we propose an improvement of a coevolutionary computational classification algorithm, called Improved Coevolutionary Feature Synthesized EM (I-CFS-EM) algorithm. It is a hybrid of coevolutionary genetic programming and EM algorithm applied on partially labeled data. It requires less labeled data and it makes the test in a lower dimension, which speeds up the testing. Then, we provide a comprehensive comparison between SVM with different kernel functions and I-CFS-EM on several real datasets. This comparison shows that I-CFS-EM outperforms SVM in the sense of both the classification performance and the computational efficiency in the testing phase. We also give an intensive analysis of the pros and cons of both approaches.|Rui Li,Bir Bhanu,Krzysztof Krawiec"],["80757|VLDB|2007|On the Production of Anorexic Plan Diagrams|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduction has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, identifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches. We investigate here the plan reduction issue from theoretical, statistical and empirical perspectives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPCH-based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to \"anorexic\" (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs.|Harish D.,Pooja N. Darera,Jayant R. Haritsa","80732|VLDB|2007|Supporting Time-Constrained SQL Queries in Oracle|The growing nature of databases, and the flexibility inherent in the SQL query language that allows arbitrarily complex formulations, can result in queries that take inordinate amount of time to complete. To mitigate this problem, strategies that are optimized to return the 'first-few rows' or 'top-k rows' (in case of sorted results) are usually employed. However, both these strategies can lead to unpredictable query processing times. Thus, in this paper we propose supporting time-constrained SQL queries. Specifically, a user issues a SQL query as before but additionally provides nature of constraint (soft or hard), an upper bound for query processing time, and acceptable nature of results (partial or approximate). The DBMS takes the criteria (constraint type, time limit, quality of result) into account in generating the query execution plan, which is expected (guaranteed) to complete in the allocated time for soft (hard) time constraint. If partial results are acceptable then the technique of reducing result set cardinality (i.e. returning first few or top-k rows) is used, whereas if approximate results are acceptable then sampling is used, to compute query results within the specified time limit. For the latter case, we argue that trading off quality of results for predictable response time is quite useful. However, for this case, we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval. This paper presents the notion of time-constrained SQL queries, discusses the challenges in supporting such a construct, describes a framework for supporting such queries, and outlines its implementation in Oracle Database by exploiting Oracle's cost-based optimizer and extensibility capabilities.|Ying Hu,Seema Sundara,Jagannathan Srinivasan","80772|VLDB|2007|Unifying Data and Domain Knowledge Using Virtual Views|The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB  PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.|Lipyeow Lim,Haixun Wang,Min Wang","80737|VLDB|2007|Staying FIT Efficient Load Shedding Techniques for Distributed Stream Processing|In distributed stream processing environments, large numbers of continuous queries are distributed onto multiple servers. When one or more of these servers become overloaded due to bursty data arrival, excessive load needs to be shed in order to preserve low latency for the query results. Because of the load dependencies among the servers, load shedding decisions on these servers must be well-coordinated to achieve end-to-end control on the output quality. In this paper, we model the distributed load shedding problem as a linear optimization problem, for which we propose two alternative solution approaches a solver-based centralized approach, and a distributed approach based on metadata aggregation and propagation, whose centralized implementation is also available. Both of our solutions are based on generating a series of load shedding plans in advance, to be used under certain input load conditions. We have implemented our techniques as part of the Borealis distributed stream processing system. We present experimental results from our prototype implementation showing the performance of these techniques under different input and query workloads.|Nesime Tatbul,Ugur \u2021etintemel,Stanley B. Zdonik","80823|VLDB|2007|A genetic approach for random testing of database systems|Testing a database engine has been and continues to be a challenging task. The space of possible SQL queries along with their possible access paths is practically unbounded. Moreover, this space is continuously increasing in size as the feature set of modern DBMS systems expands with every product release. To tackle these problems, random query generator tools have been used to create large numbers of test cases. While such test case generators enable the creation of complex and syntactically correct SQL queries, they do not guarantee that the queries produced return results or exercise desired DBMS components. Very often the generated queries contain logical contradictions, which cause \"short-circuits\" at the query optimization layer, failing to exercise the lower layers of the database engine (query optimization, query execution, access methods, etc.) In this paper we present a random test case generation technique, which provides solutions to the above problems. Our technique utilizes execution feedback, obtained from the DBMS under test, in order to guide the test generation process toward specific DBMS subcomponents and rarely exercised code paths. Test cases are created incrementally using a genetic approach, which synthesizes query characteristics that are of interest for the purposes of test coverage. Our experiments indicate that our technique can outperform other methods of random testing in terms of efficiency and code coverage. We also provide experimental results which show that the use of execution feedback improves code coverage of specific DBMS components. Finally, we share our experiences gained from using this testing approach during the development cycles of Microsoft SQL Server.|Hardik Bati,Leo Giakoumakis,Steve Herbert,Aleksandras Surna","80802|VLDB|2007|Eliminating Impedance Mismatch in C|Recently, the C and the VISUAL BASIC communities were tantalized by the advent of LINQ ---the Language INtegrated Query technology from Microsoft. LINQ represents a set of language extensions relying on advanced (some say hard to understand) techniques drawn from functional languages such as type inference, &lambda-expressions and most importantly, monads. The rd edition of C just as the th of VISUAL BASIC allow programmer to directly access relational and XML-based databases from within the programming language. We show that very similar capabilities can be achieved in the C++ programming language without relying on any language extensions, compiler modifications, external processing tools, or any other vendor specific machinery ARATAT is a C++ template library whose objective is type safe generation of SQL statements for access relational database systems. Learning curve is minimal since ARARAT resembles relational algebra, which is at the core of SQL.|Joseph Gil,Keren Lenz","80854|VLDB|2007|Sum-Max Monotonic Ranked Joins for Evaluating Top-K Twig Queries on Weighted Data Graphs|In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirabilitypenalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.|Yan Qi 0002,K. Selçuk Candan,Maria Luisa Sapino","80838|VLDB|2007|Graph Indexing Tree  Delta  Graph|Recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs. As a result, it is of special interest to process graph containment queries effectively on large graph databases. Given a graph database G, and a query raph q, the graph containment query is to retrieve all graphs in G which contain q as subgraph(s). Due to the vast number of graphs in G and the nature of complexity for subgraph isomorphism testing, it is desirable to make use of high-quality graph indexing mechanisms to reduce the overall query processing cost. In this paper, we propose a new cost-effective graph indexing method based on frequent tree-features of the graph database. We analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects feature size, feature selection cost, and pruning power. In order to achieve better pruning ability than existing graph-based indexing methods, we select, in addition to frequent tree-features (Tree), a small number of discriminative graphs (&Delta) on demand, without a costly graph mining process beforehand. Our study verifies that (Tree+&Delta) is a better choice than graph for indexing purpose, denoted (Tree+&Delta &geGraph), to address the graph containment query problem. It has two implications () the index construction by (Tree+&Delta) is efficient, and () the graph containment query processing by (Tree+&Delta) is efficient. Our experimental studies demonstrate that (Tree+&Delta) has a compact index structure, achieves an order of magnitude better performance in index construction, and most importantly, outperforms up-to-date graph-based indexing methods gIndex and C-Tree, in graph containment query processing.|Peixiang Zhao,Jeffrey Xu Yu,Philip S. Yu","80764|VLDB|2007|Querying Complex Structured Databases|Correctly generating a structured query (e.g., an XQuery or a SQL query) requires the user to have a full understanding of the database schema, which can be a daunting task. Alternative query models have been proposed to give users the ability to query the database without schema knowledge. Those models, including simple keyword search and labeled keyword search, aim to extract meaningful data fragments that match the structure-free query conditions (e.g., keywords) based on various matching semantics. Typically, the matching semantics are content-based they are defined on data node inter-relationships and incur significant query evaluation cost. Our first contribution is a novel matching semantics based on analyzing the database schema. We show that query models employing a schema-based matching semantics can reduce query evaluation cost significantly while maintaining or even improving result quality. The adoption of schema-based matching semantics does not change the nature of those query models they are still schema-ignorant, i.e., users express no schema knowledge (except the labels in labeled keyword search) in the query. While those models work well for some queries on some databases, they often encounter problems when applied to complex queries on databases with complex schemas. Our second contribution is a novel query model that incorporates partial schema knowledge through the use of schema summary. This new summary-aware query model, called Meaningful Summary Query (MSQ), seamlessly integrates summary-based structural conditions and structure-free conditions, and enables ordinary users to query complex databases. We design algorithms for evaluating MSQ queries, and demonstrate that MSQ queries can produce better results against complex databases when compared with previous approaches, and that they can be efficiently evaluated.|Cong Yu,H. V. Jagadish","80776|VLDB|2007|LCS-TRIM Dynamic Programming Meets XML Indexing and Querying|In this article, we propose a new approach for querying and indexing a database of trees with specific applications to XML datasets. Our approach relies on representing both the queries and the data using a sequential encoding and then subsequently employing an innovative variant of the longest common subsequence (LCS) matching algorithm to retrieve the desired results. A key innovation here is the use of a series of inter-linked early pruning steps, coupled with a simple index structure that enable us to reduce the search space and eliminate a large number of false positive matches prior to applying the more expensive LCS matching algorithm. Additionally, we also present mechanisms that enable the user to specify constraints on the retrieved output and show how such constraints can be pushed deep into the retrieval process, leading to improved response times. Mechanisms supporting the retrieval of approximate matches are also supported. When compared with state-of-the-art approaches, the query processing time of our algorithms is shown to be up to two to three orders of magnitude faster on several real datasets on realistic query workloads. Finally, we show that our approach is suitable for emerging multi-core server architectures when retrieving data for more expensive queries.|Shirish Tatikonda,Srinivasan Parthasarathy,Matthew Goyder"],["80830|VLDB|2007|GhostDB Hiding Data from Prying Eyes|Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed, say, to the prying eyes of some insurance fraudster. GhostDB is an architecture and system to achieve this. You carry private data in a smart USB device (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose and the public data you access. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB device) in which data flows in only one direction from public to private. This demonstration shows GhostDB's query processing in action.|Christophe Salperwyck,Nicolas Anciaux,Mehdi Benzine,Luc Bouganim,Philippe Pucheral,Dennis Shasha","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","80777|VLDB|2007|FuSem - Exploring Different Semantics of Data Fusion|Data fusion is the final step of a typical data integration process, after schematic conflicts have been overcome and after duplicates have been correctly identified. We present the relational data fusion system FuSem, which uses schema mappings and information about duplicates to decide what to fuse, i.e., which tuples to merge into one. The aspect emphasized by the demo is how to fuse the duplicates with FuSem. First, it offers several conflict resolution functions to handle data conflicts among duplicates. Furthermore, different fusion semantics proposed in the literature, such as MatchJoin or ConQuer, can be compared and visually explored. Optimized execution allows interactive access to the data and thus to explore the different data fusion procedures.|Jens Bleiholder,Karsten Draba,Felix Naumann","80820|VLDB|2007|From Data Privacy to Location Privacy Models and Algorithms|This tutorial presents the definition, the models and the techniques of location privacy from the data privacy perspective. By reviewing and revising the state of art research in data privacy area, the presenter describes the essential concepts, the alternative models, and the suite of techniques for providing location privacy in mobile and ubiquitous data management systems. The tutorial consists of two main components. First, we will introduce location privacy threats and give an overview of the state of art research in data privacy and analyze the applicability of the existing data privacy techniques to location privacy problems. Second, we will present the various location privacy models and techniques effective in either the privacy policy based framework or the location anonymization based framework. The discussion will address a number of important issues in both data privacy and location privacy research, including the location utility and location privacy trade-offs, the need for a careful combination of policy-based location privacy mechanisms and location anonymization based privacy schemes, as well as the set of safeguards for secure transmission, use and storage of location information, reducing the risks of unauthorized disclosure of location information. The tutorial is designed to be self-contained, and gives the essential background for anyone interested in learning about the concept and models of location privacy, and the principles and techniques for design and development of a secure and customizable architecture for privacy-preserving mobile data management in mobile and pervasive information systems. This tutorial is accessible to data management administrators, mobile location based service developers, and graduate students and researchers who are interested in data management in mobile information syhhhstems, pervasive computing, and data privacy.|Ling Liu","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80842|VLDB|2007|XSeek A Semantic XML Search Engine Using Keywords|We present XSeek, a keyword search engine that enables users to easily access XML data without the need of learning XPath or XQuery and studying possibly complex data schemas. XSeek addresses a challenge in XML keyword search that has been neglected in the literature how to determine the desired return information, analogous to inferring a \"return\" clause in XQuery. To infer the search semantics, XSeek recognizes possible entities and attributes in the data, differentiates search predicates and return specifications in the keywords, and generates meaningful search results based on the analysis.|Ziyang Liu,Jeffrey Walker,Yi Chen","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80800|VLDB|2007|Answering Aggregation Queries in a Secure System Model|As more sensitive data is captured in electronic form, security becomes more and more important. Data encryption is the main technique for achieving security. While in the past enterprises were hesitant to implement database encryption because of the very high cost, complexity, and performance degradation, they now have to face the ever-growing risk of data theft as well as emerging legislative requirements. Data encryption can be done at multiple tiers within the enterprise. Different choices on where to encrypt the data offer different security features that protect against different attacks. One class of attack that needs to be taken seriously is the compromise of the database server, its software or administrator. A secure way to address this threat is for a DBMS to directly process queries on the ciphertext, without decryption. We conduct a comprehensive study on answering SUM and AVG aggregation queries in such a system model by using a secure homomorphic encryption scheme in a novel way. We demonstrate that the performance of such a solution is comparable to a traditional symmetric encryption scheme (e.g., DES) in which each value is decrypted and the computation is performed on the plaintext. Clearly this traditional encryption scheme is not a viable solution to the problem because the server must have access to the secret key and the plaintext, which violates our system model and security requirements. We study the problem in the setting of a read-optimized DBMS for data warehousing applications, in which SUM and AVG are frequent and crucial.|Tingjian Ge,Stanley B. Zdonik","80767|VLDB|2007|Materialized Views in Probabilistic Databases for Information Exchange and Query Optimization|Views over probabilistic data contain correlations between tuples, and the current approach is to capture these correlations using explicit lineage. In this paper we propose an alternative approach to materializing probabilistic views, by giving conditions under which a view can be represented by a block-independent disjoint (BID) table. Not all views can be represented as BID tables and so we propose a novel partial representation that can represent all views but may not define a unique probability distribution. We then give conditions on when a query's value on a partial representation will be uniquely defined. We apply our theory to two applications query processing using views and information exchange using views. In query processing on probabilistic data, we can ignore the lineage and use materialized views to more efficiently answer queries. By contrast, if the view has explicit lineage, the query evaluation must reprocess the lineage to compute the query resulting in dramatically slower execution. The second application is information exchange when we do not wish to disclose the entire lineage, which otherwise may result in shipping the entire database. The paper contains several theoretical results that completely solve the problem of deciding whether a conjunctive view can be represented as a BID and whether a query on a partial representation is uniquely determined. We validate our approach experimentally showing that representable views exist in real and synthetic workloads and show over three magnitudes of improvement in query processing versus a lineage based approach.|Christopher Re,Dan Suciu","80752|VLDB|2007|Update Exchange with Mappings and Provenance|We consider systems for data sharing among heterogeneous peers related by a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to ask queries over related data from other peers as well. To achieve this, every peer's updates propagate along the mappings to the other peers. However, this update exchange is filtered by trust conditions --- expressing what data and sources a peer judges to be authoritative --- which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. These systems target scientific data sharing applications, and their general principles and architecture have been described in . In this paper we present methods for realizing such systems. Specifically, we extend techniques from data integration, data exchange, and incremental view maintenance to propagate updates along mappings we integrate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance we discuss strategies for implementing our techniques in conjunction with an RDBMS and we experimentally demonstrate the viability of our techniques in the ORCHESTRA prototype system.|Todd J. Green,Grigoris Karvounarakis,Zachary G. Ives,Val Tannen"],["57951|GECCO|2007|An estimation of distribution algorithm with guided mutation for a complex flow shop scheduling problem|An Estimation of Distribution Algorithm (EDA) is proposed toapproach the Hybrid Flow Shop with Sequence Dependent Setup Times and Uniform Machines in parallel (HFS-SDST-UM) problem. The latter motivated by the needs of a real world company. The proposed EDA implements a fairly new mechanism to improve the search of more traditional EDAs. This is the Guided Mutation (GM). EDA-GM generates new solutions by using the information from a probability model, as all EDAs, and the local information from a good known solution. The approach is tested on several instances of HFS-SDST-UM and compared with adaptations of meta-heuristics designed for very similarproblems. Encouraging results are reported.|Abdellah Salhi,José Antonio Vázquez Rodríguez,Qingfu Zhang","58030|GECCO|2007|Addressing sampling errors and diversity loss in UMDA|Estimation of distribution algorithms replace the typical crossover and mutation operators by constructing a probabilistic model and generating offspring according to this model. In previous studies, it has been shown that this generally leads to diversity loss due to sampling errors. In this paper, for the case of the simple Univariate Marginal Distribution Algorithm (UMDA), we propose and test several methods for counteracting diversity loss. The diversity loss can come in two phases sampling from the probability model (offspring generation) and selection. We show that it is possible to completely remove the sampling error during offspring generation. Furthermore, we examine several plausible model construction variants which counteract diversity loss during selection and demonstrate that these update rules work better than the standard update on a variety of simple test problems.|Jürgen Branke,Clemens Lode,Jonathan L. Shapiro","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","58170|GECCO|2007|Parameter cross-validation and early-stopping in univariate marginal distribution algorithm|In this paper, a cross-validation and early-stopping algorithm is devised for parameter updating in the Univariate Marginal Distribution Algorithm (UMDA) to reduce overftting. Our hypothesis is that the well-known problem of diversity loss in UMDA is a consequence of overfitting during the parameter estimation step at each generation. It is tested by experiments on two different optimization problems.|Hao Wu,Jonathan L. Shapiro","57946|GECCO|2007|SDR a better trigger for adaptive variance scaling in normal EDAs|Recently, advances have been made in continuous, normal-distribution-based Estimation-of-DistributionAlgorithms (EDAs) by scaling the variance upfrom the maximum-likelihood estimate. When doneproperly, such scaling has been shown to preventpremature convergence on slope-like regions ofthe search space. In this paper we specificallyfocus on one way of scaling that was previouslyintroduced as Adaptive Variance Scaling (AVS). It wasfound that when using AVS, the average number offitness evaluations grows subquadratically withthe dimensionality on a wide range of unimodaltest-problems, competitively with the CMA-ES.Still, room for improvement exists because thevariance doesn't always have to be scaled. Apreviously introduced trigger based on correlationthat determines when to apply scaling was shownto fail on higher dimensional problems. Here weprovide a new solution called the Standard-DeviationRatio (SDR) trigger that is integrated with theIterated Density-Estimation Evolutionary Algorithm(IDEA). Intuitively put, scaling istriggered with SDR only if improvements are foundto be far away from the mean. SDR works even inhigh dimensions as a result of factorizing thedecision rule behind the trigger according to theestimated Bayesian factorization. We evaluateSDR-AVS-IDEA on the same set ofbenchmark problems and compare it with AVS-IDEAand CMA-ES. We find that the addition of SDR givesAVS-IDEA an important extra edgefor it to be used in future research and inapplications both in single-objective optimizationas well as in multi-objective and dynamicoptimization. In addition, we provide practical rulesof thumb for parameter settings for usingSDR-AVS-IDEA that result in anasymptotic scale-up behavior that is sublinearfor the population size (O(l.)) andsubquadratic (O(l.)) for thenumber of evaluations.|Peter A. N. Bosman,Jörn Grahl,Franz Rothlauf","58243|GECCO|2007|Analyzing probabilistic models in hierarchical BOA on traps and spin glasses|The hierarchical Bayesian optimization algorithm (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling probabilistic models of promising solutions. This paper analyzes probabilistic models in hBOA on two common test problems concatenated traps and D Ising spin glasses with periodic boundary conditions. We argue that although Bayesian networks with local structures can encode complex probability distributions, analyzing these models in hBOA is relatively straightforward and the results of such analyses may provide practitioners with useful information about their problems. The results show that the probabilistic models in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in subsequent iterations of BOA, and creating adequate probabilistic models by hand is not straightforward even with complete knowledge of the optimization problem.|Mark Hauschild,Martin Pelikan,Cláudio F. Lima,Kumara Sastry","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","58197|GECCO|2007|Order or not does parallelization of model building in hBOA affect its scalability|It has been shown that model building in the hierarchical Bayesian optimization algorithm (hBOA) can be efficiently parallelized by randomly generating an ancestral ordering of the nodes of the network prior to learning the network structure and allowing only dependencies consistent with the generated ordering. However, it has not been thoroughly shown that this approach to restricting probabilistic models does not affect scalability of hBOA on important classes of problems. This paper demonstrates that although the use of a random ancestral ordering restricts the structure of considered models to allow efficient parallelization of model building, its effects on hBOA performance and scalability are negligible.|Martin Pelikan,James D. Laury Jr.","57996|GECCO|2007|Why is parity hard for estimation of distribution algorithms|We describe a k-bounded and additively separable test problem on which the hierarchical Bayesian Optimization Algorithm (hBOA) scales exponentially.|David Jonathan Coffin,Robert Elliott Smith","57969|GECCO|2007|Dependency trees permutations and quadratic assignment problem|This paper describes and analyzes an estimation of distribution algorithm based on dependency tree models (dtEDA), which can explicitly encode probabilistic models for permutations. dtEDA is tested on deceptive ordering problems and a number of instances of the quadratic assignment problem. The performance of dtEDA is compared to that of the standard genetic algorithm with the partially matched crossover (PMX) and the linear order crossover (LOX). In the quadratic assignment problem, the robust tabu search is also included in the comparison.|Martin Pelikan,Shigeyoshi Tsutsui,Rajiv Kalapala"],["58146|GECCO|2007|A synthesis of optimal stopping time in compact genetic algorithm based on real options approach|This paper introduces the real options approach, which is an evaluation tool for investment under uncertainty, to analyze optimal stopping time in genetic algorithms. This paper focuses on the simple model of EDAs named the compact genetic algorithm. This algorithm employs the probability vector as a model that scales well with the problem size. We analyze optimal stopping time of trap problems and propose an optimal stopping criterion as a decision contour. The proposed criterion also provides a stopping boundary, where termination is optimal on one side and continuation is on the other. This region suggests when it is worth continuing the algorithm and helps save computational effort by stopping early. Moreover, when the reset method is applied, the algorithm can reach a higher solution quality. The proposed technique can also be applied to analyze other problems.|Sunisa Rimcharoen,Daricha Sutivong,Prabhas Chongstitvatana","58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","58134|GECCO|2007|Getting the most from search-based refactoring|Object-oriented systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying design. This problem must often be remedied in a costly refactoring phase before further maintenance programming can take place. Recently search-based approaches to automating the task of softwarere factoring, based on the concept of treating object-oriented designas a combinatorial optimisation problem, have been proposed. However, because search-based refactoring is a novel approach it has yet to be established which search techniques are most suitable forthe task. In this paper we report the results of an empirical comparison of simulated annealing, genetic algorithm and multiple ascent hill-climbing in search-based refactoring. A prototype automated refactoring tool is employed, capable of making radical changes to the design of an existing program in order that it conform more closely to a contemporary quality model. Results show multiple-ascent hill climbing to outperform both simulated annealing and genetic algorithm over a set of four input programs.|Mark Kent O'Keeffe,Mel \u201C Cinnéide","58057|GECCO|2007|Synthesis of analog filters on an evolvable hardware platform using a genetic algorithm|This work presents a novel approach to filter synthesis on a fieldprogrammable analog array (FPAA) architecture using a genetic algorithm (GA). First, a Matlab model of the FPAA is created and verified for compliance with transistor-level simulations of the FPAA. Using this model, differentfilter structures are built using an active-RC approach and evaluated. Secondly, a robust genetic algorithm is implemented in Matlab, which allows synthesis of analog filters on the given structure. Optimal parameters and operators of the genetic algorithm are identified by gradual adaptation andperformance evaluation, and the general feasibility is shown. Finally, the GA is used to overcome quantization-limitations of the FPAA structure and find configurations of filters, which would not have been achievable with traditional synthesis methods. The system is not only a platform for theoretical investigation offilter structures on the given chip structure but also provides aframework for evolution and instantiation of filters on actual chiphardware.|Joachim Becker,Stanis Trendelenburg,Fabian Henrici,Yiannos Manoli","58190|GECCO|2007|Multi-objective hybrid PSO using -fuzzy dominance|This paper describes a PSO-Nelder Mead Simplex hybrid multi-objective optimization algorithm based on a numerical metric called  -fuzzy dominance. Within each iteration of this approach, in addition to the position and velocity update of each particle using PSO, the k-means algorithm is applied to divide the population into smaller sized clusters. The Nelder-Mead simplex algorithm is used separately within each cluster for added local search. The proposed algorithm is shown to perform better than MOPSO on several test problems as well as for the optimization of a genetic model for flowering time control in Arabidopsis. Adding the local search achieves faster convergence, an important feature in computationally intensive optimization of gene networks.|Praveen Koduru,Sanjoy Das,Stephen Welch","58142|GECCO|2007|A novel ab-initio genetic-based approach for protein folding prediction|In this paper, a model based on genetic algorithms for protein folding prediction is proposed. The most important features of the proposed approach are i) Heuristic secondary structure information is used in the initialization of the genetic algorithm ii) An enhanced D spatial representation called cube-octahedron is used, also, an expansion technique is proposed in order to reduce the computational complexity and spatial constraints iii) Data preprocessing of geometric features to characterize the cube-octahedron using twelve basic vectors to define the nodes. Additionally, biological information (torsion angles, bond angles and secondary structure conformations) was pre-processed through an analysis of all possible combinations of the basic vectors which satisfy the biological constrains defined by the spatial representation and iv) Hashing techniques were used to improve the computational efficiency. The pre-processed information was stored in hash tables, which are intensively used by the genetic algorithm. Some experiments were carried out to validate the proposed model obtaining very promising results.|Sergio Raul Duarte Torres,David Camilo Becerra Romero,Luis Fernando Niño Vasquez,Yoan José Pinzón Ardila","58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","57955|GECCO|2007|Genetic algorithms for water quality management in an urban watershed|This paper describes the use of genetic algorithms for water quality management in an urban watershed. This is achieved by linking a genetic algorithm-based optimization model in a disaggregated manner with a water quality simulation model.|Mohammad Tufail,Lindell E. Ormsbee","58101|GECCO|2007|An online implementable differential evolution tuned optimal guidance law|This paper proposes a novel application of differential evolution to solve a difficult dynamic optimisation or optimal control problem. The miss distance in a missile-target engagement is minimised using differential evolution. The difficulty of solving it by existing conventional techniques in optimal control theory is caused by the nonlinearity of the dynamic constraint equation, inequality constraint on the control input and inequality constraint on another parameter that enters problem indirectly. The optimal control problem of finding the minimum miss distance has an analytical solution subject to several simplifying assumptions. In the approach proposed in this paper, the initial population is generated around the seed value given by this analytical solution. Thereafter, the algorithm progresses to an acceptable final solution within a few generations, satisfying the constraints at every iteration. Since this solution or the control input has to be obtained in real time to be of any use in practice, the feasibility of online implementation is also illustrated.|Raghunathan Thangavelu,S. Pradeep","58067|GECCO|2007|Quality time tradeoff operator for designing efficient multi level genetic algorithms|We present a novel cost benefit operator that assists multi levelgenetic algorithm searches. Through the use of the cost benefitoperator, it is possible to dynamically constrain the search of thebase level genetic algorithms, to suit the users requirements. We note that the current literature has abundant studies on metaevolutionary GAs, however these approaches have not identifiedan efficient approach to the termination of base GA searchs or ameans to balance practical consideration such as quality ofsolution and the expense of computation. Our Quality timetradeoff operator (QTT) is user defined, and acts as a base leveltermination operator and also provides a fitness value for themeta-level GA. In this manner, the amount of computation timespent on less encouraging configurations can be specified by theuser. Our approach was applied to a computationally intensive test problem which evaluates a large set of configuration settings forthe base GAs to find suitable configuration settings (populationsize, crossover operator and rate, mutation operator and rate,repair or penalty and the use of adaptive mutation rates) forselected TSP problems.|George G. Mitchell,Barry McMullin,James Decraene,Ciaran Kelly"]]},"title":{"entropy":5.428950324403226,"topics":["genetic programming, genetic algorithm, using genetic, genetic for, for network, programming and, genetic network, using programming, programming for, neural network, rule mining, and using, genetic and, using algorithm, with fuzzy, evolving for, with, genetic with, cartesian programming, selection using","algorithm for, for, genetic algorithm, for and, genetic for, for problem, and, evolutionary algorithm, for optimization, framework for, analysis and, for selection, evolutionary for, and evolutionary, and its, and algorithm, for the, analysis for, framework and, selection algorithm","particle swarm, the, particle optimization, swarm optimization, for the, optimization algorithm, the problem, the effects, evolutionary algorithm, for optimization, algorithm the, mutation operator, evolution strategies, building block, evolutionary computation, for problem, estimation distribution, multiobjective optimization, evolution for, optimization","data, for query, efficient for, approach data, for systems, for detection, for data, efficient search, immune systems, top-k queries, data management, and databases, and querying, for queries, for top-k, and search, efficient, search using, for streams, data using","genetic programming, genetic for, genetic algorithm, using programming, using genetic, programming and, programming for, genetic and, evolving for, cartesian programming, cartesian genetic, parallel genetic","genetic network, for network, genetic with, algorithm with, rule mining, network programming, programming with, neural network, network and, genetic learning, with and, algorithm network, with for, learning with, rule with, network with, network, new, with","for and, framework for, and, metric for, framework and, model and, model for, optimisation, analyzing","performance and, performance for, for adaptive, heuristic for, performance, adaptive, heuristic, towards, continuous, eda, improving, variance, scaling, better, machine","evolutionary algorithm, the evolutionary, using evolutionary, evolutionary for, evolutionary approach, algorithm the, method for, evolutionary computation, evolutionary design, using multi-objective, evolutionary study, and evolutionary, using algorithm, multi-objective approach, with evolutionary, the multi-objective, for multi-objective, evolutionary, multi-objective, based","particle swarm, for optimization, optimization algorithm, particle optimization, swarm optimization, the optimization, for design, swarm for, and diversity, multiobjective optimization, particle for, particle algorithm, swarm algorithm, optimization and, algorithm the, particle the, swarm the, using particle, the design, algorithm for","and querying, indexing, time, views, information, xml","efficient search, for streams, for search, search, application, solution, web, distributed, over, with"],"ranking":[["57904|GECCO|2007|Association rule mining for continuous attributes using genetic network programming|Most association rule mining algorithms make use of discretization algorithms for handling continuous attributes. However, by means of methods of discretization, it is difficult to get highest attribute interdependency and at the same time to get lowest number of intervals. We propose a method using a new graph-based evolutionary algorithm named \"Genetic Network Programming (GNP)\" that can deal with continues values directly, that is, without using any discretization method as a preprocessing step. GNP is one of the evolutionary optimization techniques, which uses directed graph structures as solutions and is composed of three kinds of nodes start node, judgment node and processing node. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the judgment and connection from the current activated node. The features of GNP are described as follows. First, it is possible to reuse nodes because of this, the structure is compact. Second, GNP can find solutions of problems without bloat, which can be sometimes found in Genetic Programming (GP), because of the fixed number of nodes in GNP. Third, nodes that are not used at the current program executions will be used for future evolution. Fourth, GNP is able to cope with partially observable Markov processes. In this paper, we propose a method that can deal with continuous attributes, where attributes in databases correspond to judgment nodes in GNP and each continuous attribute is checked whether its value is greater than a threshold value and the association rules are represented as the connections of the judgment nodes. Threshold ai is firstly determined by calculating the mean i and standard deviation si of all attribute values of Ai. Then, initial threshold ai is selected randomly between the interval i - aisi, i + aisi where ai is a parameter to determine the range of the interval. Once the threshold ai is selected for all attributes, each value of the attribute Ai is checked if it is greater than the threshold ai in the judgment nodes of the proposed method. In addition to that, the threshold ai is also evolved by mutation between i - aisi, i + aisi in every generation in order to obtain as many association rules as possible. The features of the proposed method are as follows compared with other methods ) Extracts rules without identifying frequent itemsets used in Apriori-like mining methods. ) Stores extracted important association rules in a pool all together through generations. ) Measures the significance of associations via the chi-squared test. ) Extracts important rules sufficient enough for user's purpose in a short time. ) The pool is updated in every generation and only important association rules with higher chi-squared value are stored when the identical rules are stored. We have evaluated the proposed method by doing two simulations. Simulation  uses fixed threshold values that is, they remain fixed at initial thresholds during evolution. In simulation ,thresholds are evolved by mutation in every generation. Fig.  shows the number of rules extracted in the pool in simulation . It is found that the number of rules extracted has been increased, which means simulation  outperforms simulation .|Karla Taboada,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","58049|GECCO|2007|The genetic programming collaboration network and its communities|Useful information about scientific collaboration structures and patterns can be inferred from computer databases of published papers. The genetic programming bibliography is the most complete reference of papers on GP. In addition to locating publications, it contains coauthor and coeditor relationships from which a more complete picture of the field emerges. We treat these relationships as undirected small world graphs whose study reveals the community structure of the GP collaborative social network. Automatic analysis discovers new communities and highlights new facets of them. The investigation reveals many similarities between GP and coauthorship networks in other scientific fields but also some subtle differences such as a smaller central network component and a high clustering.|Leslie Luthi,Marco Tomassini,Mario Giacobini,William B. Langdon","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57965|GECCO|2007|Effects of passengers arrival distribution to double-deck elevator group supervisory control systems using genetic network programming|The Elevator Group Supervisory Control Systems (EGSCS) are the control systems that systematically manage three or more elevators in order to efficiently transport the passengers in buildings. Double-deck elevators, where two cages are connected with each other, are expected to be the next generation elevator systems. Meanwhile, Destination Floor Guidance Systems (DFGS) are also expected in Double-Deck Elevator Systems (DDES). With these, the passengers could be served at two consecutive floors and could input their destinations at elevator halls instead of conventional systems without DFGS. Such systems become more complex than the traditional systems and require new control methods Genetic Network Programming (GNP), a graph-based evolutionary method, has been applied to EGSCS and its advantages are shown in some previous papers. GNP can obtain the strategy of a new hall call assignment to the optimal elevator because it performs crossover and mutation operations to judgment nodes and processing nodes. In studies so far, the passenger's arrival has been assumed to take Exponential distribution for many years. In this paper, we have applied Erlang distribution and Binomial distribution in order to study how the passenger's arrival distribution affects EGSCS. We have found that the passenger's arrival distribution has great influence on EGSCS. It has been also clarified that GNP makes good performances under different conditions.|Lu Yu,Jin Zhou,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu,Sandor Markon","57974|GECCO|2007|Trading rules on stock markets using genetic network programming with sarsa learning|In this paper, the Genetic Network Programming (GNP) for creating trading rules on stocks is described. GNP is an evolutionary computation, which represents its solutions using graph structures and has some useful features inherently. It has been clarified that GNP works well especially in dynamic environments since GNP can create quite compact programs and has an implicit memory function. In this paper, GNP is applied to creating a stock trading model. There are three important points The first important point is to combine GNP with Sarsa Learning which is one of the reinforcement learning algorithms. Evolution-based methods evolve their programs after task execution because they must calculate fitness values, while reinforcement learning can change programs during task execution, therefore the programs can be created efficiently. The second important point is that GNP uses candlestick chart and selects appropriate technical indices to judge the buying and selling timing of stocks. The third important point is that sub-nodes are used in each node to determine appropriate actions (buyingselling) and to select appropriate stock price information depending on the situation. In the simulations, the trading model is trained using the stock prices of  brands in ,  and . Then the generalization ability is tested using the stock prices in . From the simulation results, it is clarified that the trading rules of the proposed method obtain much higher profits than Buy&Hold method and its effectiveness has been confirmed.|Yan Chen,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","57893|GECCO|2007|A data parallel approach to genetic programming using programmable graphics hardware|In recent years the computing power of graphics cards has increased significantly. Indeed, the growth in the computing power of these graphics cards is now several orders of magnitude greater than the growth in the power of computer processor units. Thus these graphics cards are now beginning to be used by the scientific community aslow cost, high performance computing platforms. Traditional genetic programming is a highly computer intensive algorithm but due to its parallel nature it can be distributed over multiple processors to increase the speed of the algorithm considerably. This is not applicable for single processor architectures but graphics cards provide a mechanism for developing a data parallel implementation of genetic programming. In this paper we will describe the technique of general purpose computing using graphics cards and how to extend this technique to genetic programming. We will demonstrate the improvement in the performance of genetic programming on single processor architectures which can be achieved by harnessing the computing power of these next generation graphics cards.|Darren M. Chitty","57884|GECCO|2007|Genetic network programming with parallel processing for association rule mining in large and dense databases|Several methods of extracting association rules have been reported. A new evolutionary computation method named Genetic Network Programming (GNP) has also been developed recently and its efectiveness is shown for small datasets. However, it has not been tested for large datasets, particularly in datasets with a large number of attributes. The aim of this paper is to extract association rules from large and dense datasets using GNP considering a real world database with a huge number of attributes. We propose a new method where a large database is divided into many small datasets, then each GNP deals with one dataset having attributes with appropiate size, which was selected randomly from a large dataset and generated genetically. These GNPs are processed in parallel. We then propose some new genetic operations to improve the number of rules extracted and their quality as well. The proposed method improves remarkably on simulations. Fig.  shows the architecture of the proposed method. We use the CLIENTSERVER model. CLIENT side carries out preprocessing of large database, assignment of files to each server, rule checking, and genetic operations on files. SERVER side carries out processing of each file using conventional GNP based mining method independently. The features and advantages of the proposed method are the following Rule extraction is done in parallel. Each file generates its local pool of the rules. Files or datasets are treated as individuals in order to do new genetic operations over them and improve the rule extraction. Extracted rules are stored in a global pool. The rules are verified to avoid redundancy among them and it is assured that only new rules are stored.|Eloy Gonzales,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","57895|GECCO|2007|Peptide detectability following ESI mass spectrometry prediction using genetic programming|The accurate quantification of proteins is important in several areas of cell biology, biotechnology and medicine. Both relative and absolute quantification of proteins is often determined following mass spectrometric analysis of one or more of their constituent peptides. However, in order for quantification to be successful, it is important that the experimenter knows which peptides are readily detectable under the mass spectrometric conditions used for analysis. In this paper, genetic programming is used to develop a function which predicts the detectability of peptides from their calculated physico-chemical properties. Classification is carried out in two stages the selection of a good classifier using the AUROC objective function and the setting of an appropriate threshold. This allows the user to select the balance point between conflicting priorities in an intuitive way. The success of this method is found to be highly dependent on the initial selection of input parameters. The use of brood recombination and a modified version of the multi-objective FOCUS method are also investigated. While neither has a significant effect on predictive accuracy, the use of the FOCUS method leads to considerably more compact solutions.|David C. Wedge,Simon J. Gaskell,Simon J. Hubbard,Douglas B. Kell,King Wai Lau,Claire Eyers","57898|GECCO|2007|Evolving controllers for simulated car racing using object oriented genetic programming|Several different controller representations are compared on anon-trivial problem in simulated car racing, with respect tolearning speed and final fitness. The controller representations arebased either on Neural Networks or Genetic Programming, and alsodiffer in regards to whether they allow for stateful controllers orjust reactive ones. Evolved GP trees are analysed, and attempts aremade at explaining the performance differences observed.|Alexandros Agapitos,Julian Togelius,Simon M. Lucas","57990|GECCO|2007|Solving real-valued optimisation problems using cartesian genetic programming|Classical Evolutionary Programming (CEP) and Fast Evolutionary Programming (FEP) have been applied to real-valued function optimisation. Both of these techniques directly evolve the real-values that are the arguments of the real-valued function. In this paper we have applied a form of genetic programming called Cartesian Genetic Programming (CGP) to a number of real-valued optimisation benchmark problems. The approach we have taken is to evolve a computer program that controls a writing-head, which moves along and interacts with a finite set of symbols that are interpreted as real numbers, instead of manipulating the real numbers directly. In other studies, CGP has already been shown to benefit from a high degree of neutrality. We hope to exploit this for real-valued function optimisation problems to avoid being trapped on local optima. We have also used an extended form of CGP called Embedded CGP (ECGP) which allows the acquisition, evolution and re-use of modules. The effectiveness of CGP and ECGP are compared and contrasted with CEP and FEP on the benchmark problems. Results show that the new techniques are very effective.|James Alfred Walker,Julian Francis Miller"],["58092|GECCO|2007|Guided hyperplane evolutionary algorithm|A new evolutionary technique for multicriteria optimization called Guiding Hyper-plane Evolutionary Algorithm (GHEA) is proposed. The originality of the approach consists in the fact that the fitness assignment is realized by using a guiding hyperplane and a new non Pareto optimality concept. Numerical experiments illustrate the performance of GHEA compared with the popular NSGA-II and SPEA.|Corina Rotar,D. Dumitrescu,Rodica Ioana Lung","58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","57984|GECCO|2007|Discrimination of metabolic flux profiles using a hybrid evolutionary algorithm|Studying metabolic fluxes is a crucial aspect of understanding biological phenotypes. However, it is often not possible to measure these fluxes directly. As an alternative, fluxome profiling provides indirect information about fluxes in a high-throughput setting. In this paper, we consider a scenario where fluxome profiling is used to investigate characteristic differences between a number of bacterial mutant strains. The goal is to identify groups of mutants that show maximally different fluxome profiles. We propose an evolutionary algorithm for this optimization problem and demonstrate that it outperforms alternative methods based on principle component analysis and independent component analysis on both real and synthetic data sets.|Stefan Bleuler,Eckart Zitzler","58021|GECCO|2007|GARS an improved genetic algorithm with reserve selection for global optimization|This paper investigates how genetic algorithms (GAs) can be improved to solve large-scale and complex problems more efficiently. First of all, we review premature convergence, one of the challenges confronted with when applying GAs to real-world problems. Next, some of the methods now available to prevent premature convergence and their intrinsic defects are discussed. A qualitative analysis is then done on the cause of premature convergence that is the loss of building blocks hosted in less-fit individuals during the course of evolution. Thus, we propose a new improver - GAs with Reserve Selection (GARS), where a reserved area is set up to save potential building blocks and a selection mechanism based on individual uniqueness is employed to activate the potentials. Finally, case studies are done in a few standard problems well known in the literature, where the experimental results demonstrate the effectiveness and robustness of GARS in suppressing premature convergence, and also an enhancement is found in global optimization capacity.|Yang Chen,Jinglu Hu,Kotaro Hirasawa,Songnian Yu","58229|GECCO|2007|On the runtime analysis of the -ANT ACO algorithm|The runtime analysis of randomized search heuristics is a growing field where, in the last two decades, many rigorous results have been obtained. These results, however, apply particularly to classical search heuristics such as Evolutionary Algorithms (EAs) and Simulated Annealing. First runtime analyses of modern search heuristics have been conducted only recently w.r.t a simple Ant Colony Optimization (ACO) algorithm called -ANT. In particular, the influence of the evaporation factor in the pheromone update mechanism and the robustness of this parameter w.r.t the runtime behavior have been determined for the example function OneMax.This paper puts forward the rigorous runtime analysis of the -ANT on example functions, namely on the functions LeadingOnes and BinVal. With respect to EAs, such analyses have been essential to develop methods for the analysis on more complicated problems. The proof techniques required for the -ANT, unfortunately, differ significantly from those for EAs, which means that a new reservoir of methods has to be built up. Again, the influence of the evaporation factor is analyzed rigorously, and it is proved that its choice can be very crucial to allow efficient runtimes. Moreover, the analyses provide insight into the working principles of ACO algorithms and, in terms of their robustness, describe essential differences to other randomized search heuristics.|Benjamin Doerr,Frank Neumann,Dirk Sudholt,Carsten Witt","58182|GECCO|2007|ExGA II an improved exonic genetic algorithm for the multiple knapsack problem|ExGA I, a previously presented genetic algorithm, successfully solved numerous instances of the multiple knapsack problem (MKS) by employing an adaptive repair function that made use of the algorithm's modular encoding. Here we present ExGA II, an extension of ExGA I that implements additional features which allow the algorithm to perform more reliably across a larger set of benchmark problems. In addition to some basic modifications of the algorithm's framework, more specific extensions include the use of a biased mutation operator and adaptive control sequences which are used to guide the repair procedure. The success rate of ExGA II is superior to its predecessor, and other algorithms in the literature, without an overall increase in the number of function evaluations required to reach the global optimum. In fact, the new algorithm exhibits a significant reduction in the number of function evaluations required for the largest problems investigated. We also address the computational cost of using a repair function and show that the algorithm remains highly competitive when this cost is accounted for.|Philipp Rohlfshagen,John A. Bullinaria","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif","58167|GECCO|2007|Disburdening the species conservation evolutionary algorithm of arguing with radii|The present paper investigates the hybridization of two well-known multimodal optimization methods, i.e. species conservation and multinational algorithms. The topological species conservation algorithm embraces the vision of the existence of subpopulations around seeds (the best local individuals) and the preservation of these dominating individuals from one generation to another, but detects multimodality by means of the hill-valley mechanism employed by multinational algorithms. The aim is to inherit the strengths of both parent techniques and at the same time overcome their flaws. The species conservation algorithm efficiently keeps track of several good search space regions at once, but is difficult to parametrize without prior problem knowledge. Conversely, the multinational algorithms use many functionevaluations to establish subpopulations, but do not depend onprovided radius parameter values. Experiments with all threealgorithms are made on a wide range of test problems in order toinvestigate their advantages and shortcomings.|Catalin Stoean,Mike Preuss,Ruxandra Stoean,Dumitru Dumitrescu"],["58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58208|GECCO|2007|Two-level of nondominated solutions approach to multiobjective particle swarm optimization|In multiobjective particle swarm optimization (MOPSO) methods, selecting the local best and the global best for each particle of the population has a great impact on the convergence and diversity of solutions, especially when optimizing problems with high number of objectives. This paper presents a two-level of nondominated solutions approach to MOPSO. The ability of the proposed approach to detect the true Pareto optimal solutions and capture the shape of the Pareto front is evaluated through experiments on well-known non-trivial test problems. The diversity of the nondominated solutions obtained is demonstrated through different measures. The proposed approach has been assessed through a comparative study with the reported results in the literature.|M. A. Abido","58150|GECCO|2007|Geometric particle swarm optimization for the sudoku puzzle|Geometric particle swarm optimization (GPSO) is a recentlyintroduced generalization of traditional particle swarm optimization(PSO) that applies to all combinatorial spaces. The aim of thispaper is to demonstrate the applicability of GPSO to non-trivialcombinatorial spaces. The Sudoku puzzle is a perfect candidate totest new algorithmic ideas because it is entertaining andinstructive as well as a non-trivial constrained combinatorialproblem. We apply GPSO to solve the sudoku puzzle.|Alberto Moraglio,Julian Togelius","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang"],["80738|VLDB|2007|Indexable PLA for Efficient Similarity Search|Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the \"dimensionality curse\". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).|Qiuxia Chen,Lei Chen 0002,Xiang Lian,Yunhao Liu,Jeffrey Xu Yu","80825|VLDB|2007|Efficiently Answering Top-k Typicality Queries on Large Databases|Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognition science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. To answer questions like \"Who are the top-k most typical NBA players\", the measure of simple typicality is developed. To answer questions like \"Who are the top-k most typical guards distinguishing guards from other players\", the notion of discriminative typicality is proposed. Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations. () The randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers. () The direct local typicality approximation using VP-trees provides an approximation quality guarantee. () A VP-tree can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree data structure. An extensive performance study using two real data sets and a series of synthetic data sets clearly show that top-k typicality queries are meaningful and our methods are practical.|Ming Hua,Jian Pei,Ada Wai-Chee Fu,Xuemin Lin,Ho-fung Leung","80835|VLDB|2007|Ad-hoc Top-k Query Answering for Data Streams|A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Nikos Sarkas","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","58081|GECCO|2007|A multi-objective approach to search-based test data generation|There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage.The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade-offs between the two simultaneous objectives.|Kiran Lakhotia,Mark Harman,Phil McMinn","80794|VLDB|2007|Fast nGram-Based String Search Over Data Encoded Using Algebraic Signatures|We propose a novel string search algorithm for data stored once and read many times. Our search method combines the sublinear traversal of the record (as in Boyer Moore or Knuth-Morris-Pratt) with the agglomeration of parts of the record and search pattern into a single character -- the algebraic signature -- in the manner of Karp-Rabin. Our experiments show that our algorithm is up to seventy times faster for DNA data, up to eleven times faster for ASCII, and up to a six times faster for XML documents compared with an implementation of Boyer-Moore. To obtain this speed-up, we store records in encoded form, where each original character is replaced with an algebraic signature. Our method applies to records stored in databases in general and to distributed implementations of a Database As Service (DAS) in particular. Clients send records for insertion and search patterns already in encoded form and servers never operate on records in clear text. No one at a node can involuntarily discover the content of the stored data.|Witold Litwin,Riad Mokadem,Philippe Rigaux,Thomas J. E. Schwarz","80854|VLDB|2007|Sum-Max Monotonic Ranked Joins for Evaluating Top-K Twig Queries on Weighted Data Graphs|In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirabilitypenalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.|Yan Qi 0002,K. Selçuk Candan,Maria Luisa Sapino","80728|VLDB|2007|Mining Approximate Top-K Subspace Anomalies in Multi-Dimensional Time-Series Data|Market analysis is a representative data analysis process with many applications. In such an analysis, critical numerical measures, such as profit and sales, fluctuate over time and form time-series data. Moreover, the time series data correspond to market segments, which are described by a set of attributes, such as age, gender, education, income level, and product-category, that form a multi-dimensional structure. To better understand market dynamics and predict future trends, it is crucial to study the dynamics of time-series in multi-dimensional market segments. This is a topic that has been largely ignored in time series and data cube research. In this study, we examine the issues of anomaly detection in multi-dimensional time-series data. We propose time-series data cube to capture the multi-dimensional space formed by the attribute structure. This facilitates the detection of anomalies based on expected values derived from higher level, \"more general\" time-series. Anomaly detection in a time-series data cube poses computational challenges, especially for high-dimensional, large data sets. To this end, we also propose an efficient search algorithm to iteratively select subspaces in the original high-dimensional space and detect anomalies within each one. Our experiments with both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed solution.|Xiaolei Li,Jiawei Han","57897|GECCO|2007|Efficient priority optimization in complex distributed embedded systems through search space adaptation|In this paper we present a framework for dynamic search space adaptation during evolutionary design space exploration. Compared to previous approaches our framework is capable of adapting the search space dynamically during exploration leading to better search space exploitation in the same exploration time. The application of our framework to priority optimization in complex distributed embedded systems shows that dynamic search space adaptation can significantly increase exploration efficiency, both in terms of exploration time and quality of achieved results.|Arne Hamann,Rolf Ernst","80852|VLDB|2007|Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data|The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.|Man Lung Yiu,Nikos Mamoulis"],["58132|GECCO|2007|Self-modifying cartesian genetic programming|In nature, systems with enormous numbers of components (i.e. cells) are evolved from a relatively small genotype. It has not yet been demonstrated that artificial evolution is sufficient to make such a system evolvable. Consequently researchers have been investigating forms of computational development that may allow more evolvable systems. The approaches taken have largely used re-writing, multi- cellularity, or genetic regulation. In many cases it has been difficult to produce general purpose computation from such systems.In this paper we introduce computational development using a form of Cartesian Genetic Programming that includes self-modification operations. One advantage of this approach is that ab initio the system can be used to solve computational problems. We present results on a number of problems and demonstrate the characteristics and advantages that self-modification brings.|Simon Harding,Julian Francis Miller,Wolfgang Banzhaf","58113|GECCO|2007|Best SubTree genetic programming|The result of the program encoded into a Genetic Programming(GP) tree is usually returned by the root of that tree. However, this is not a general strategy. In this paper we present and investigate a new variant where the best subtree is chosen to provide the solution of the problem. The other nodes (not belonging to the best subtree) are deleted. This will reduce the size of the chromosome in those cases where its best subtree is different from the entire tree. We have tested this strategy on a wide range of regression and classification problems. Numerical experiments have shown that the proposed approach can improve both the search speed and the quality of results.|Oana Muntean,Laura Diosan,Mihai Oltean","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57893|GECCO|2007|A data parallel approach to genetic programming using programmable graphics hardware|In recent years the computing power of graphics cards has increased significantly. Indeed, the growth in the computing power of these graphics cards is now several orders of magnitude greater than the growth in the power of computer processor units. Thus these graphics cards are now beginning to be used by the scientific community aslow cost, high performance computing platforms. Traditional genetic programming is a highly computer intensive algorithm but due to its parallel nature it can be distributed over multiple processors to increase the speed of the algorithm considerably. This is not applicable for single processor architectures but graphics cards provide a mechanism for developing a data parallel implementation of genetic programming. In this paper we will describe the technique of general purpose computing using graphics cards and how to extend this technique to genetic programming. We will demonstrate the improvement in the performance of genetic programming on single processor architectures which can be achieved by harnessing the computing power of these next generation graphics cards.|Darren M. Chitty","57960|GECCO|2007|A new crossover technique for Cartesian genetic programming|Genetic Programming was first introduced by Koza using tree representation together with a crossover technique in which random sub-branches of the parents' trees are swapped to create the offspring. Later Miller and Thomson introduced Cartesian Genetic Programming, which uses directed graphs as a representation to replace the tree structures originally introduced by Koza. Cartesian Genetic Programming has been shown to perform better than the traditional Genetic Programming but it does not use crossover to create offspring, it is implemented using mutation only. In this paper a new crossover method in Genetic Programming is introduced. The new technique is based on an adaptation of the Cartesian Genetic Programming representation and is tested on two simple regression problems. It is shown that by implementing the new crossover technique, convergence is faster than that of using mutation only in the Cartesian Genetic Programming method.|Janet Clegg,James Alfred Walker,Julian Francis Miller","58106|GECCO|2007|Linear genetic programming of metaheuristics|We suggest a flavour of linear Genetic Programming indomain-specific languages that acts as a hyperheuristic (HH).|Robert E. Keller,Riccardo Poli","57895|GECCO|2007|Peptide detectability following ESI mass spectrometry prediction using genetic programming|The accurate quantification of proteins is important in several areas of cell biology, biotechnology and medicine. Both relative and absolute quantification of proteins is often determined following mass spectrometric analysis of one or more of their constituent peptides. However, in order for quantification to be successful, it is important that the experimenter knows which peptides are readily detectable under the mass spectrometric conditions used for analysis. In this paper, genetic programming is used to develop a function which predicts the detectability of peptides from their calculated physico-chemical properties. Classification is carried out in two stages the selection of a good classifier using the AUROC objective function and the setting of an appropriate threshold. This allows the user to select the balance point between conflicting priorities in an intuitive way. The success of this method is found to be highly dependent on the initial selection of input parameters. The use of brood recombination and a modified version of the multi-objective FOCUS method are also investigated. While neither has a significant effect on predictive accuracy, the use of the FOCUS method leads to considerably more compact solutions.|David C. Wedge,Simon J. Gaskell,Simon J. Hubbard,Douglas B. Kell,King Wai Lau,Claire Eyers","57898|GECCO|2007|Evolving controllers for simulated car racing using object oriented genetic programming|Several different controller representations are compared on anon-trivial problem in simulated car racing, with respect tolearning speed and final fitness. The controller representations arebased either on Neural Networks or Genetic Programming, and alsodiffer in regards to whether they allow for stateful controllers orjust reactive ones. Evolved GP trees are analysed, and attempts aremade at explaining the performance differences observed.|Alexandros Agapitos,Julian Togelius,Simon M. Lucas","58164|GECCO|2007|Controller design based on genetic programming|Three genetic programming-based approaches are proposed for continuous-time process control design. Two approaches are represented using a network of interconnected continuous-time or discrete-time elementary dynamic building blocs. In the third approach the control algorithm is represented as a recurrent function of discrete-time input variables.all.|Ivan Sekaj,Juraj Perkacz,Tomas Palenik","57990|GECCO|2007|Solving real-valued optimisation problems using cartesian genetic programming|Classical Evolutionary Programming (CEP) and Fast Evolutionary Programming (FEP) have been applied to real-valued function optimisation. Both of these techniques directly evolve the real-values that are the arguments of the real-valued function. In this paper we have applied a form of genetic programming called Cartesian Genetic Programming (CGP) to a number of real-valued optimisation benchmark problems. The approach we have taken is to evolve a computer program that controls a writing-head, which moves along and interacts with a finite set of symbols that are interpreted as real numbers, instead of manipulating the real numbers directly. In other studies, CGP has already been shown to benefit from a high degree of neutrality. We hope to exploit this for real-valued function optimisation problems to avoid being trapped on local optima. We have also used an extended form of CGP called Embedded CGP (ECGP) which allows the acquisition, evolution and re-use of modules. The effectiveness of CGP and ECGP are compared and contrasted with CEP and FEP on the benchmark problems. Results show that the new techniques are very effective.|James Alfred Walker,Julian Francis Miller"],["57882|GECCO|2007|Use of a genetic algorithm to evolve an extended artificial regulatory network for cell pattern generation|Cell pattern formation has a crucial role in both artificial and natural development. We present results from experiments in which a genetic algorithm was used to evolve an extended artificial regulatory network to produce predefined D cell patterns through the selective activation and inhibition of genes.|Arturo Chavoya,Yves Duthen","57961|GECCO|2007|Heuristic speciation for evolving neural network ensemble|Speciation is an important concept in evolutionary computation. It refers to an enhancements of evolutionary algorithms to generate a set ofdiverse solutions. The concept is studied intensively in the evolutionary design of neural network ensembles. Thediversity and cooperation of individual networks are among the essential criteria of the design.This paper proposes a speciation framework for ensemble design which integratesa collection of new techniques. Its characteristic features are(a) the population of networks are speciated as such thatthe mutual information between the networks' outputs and genotypic representations is preserved. (b) The ensemble is designed incrementally,upon discovery of a species of networks which enhances the ensembleperformance. (c) Multiple species are evolved andindividual networks are evaluated according to therole of their respective species in the ensemble.This framework provides an implementation of evolutionary algorithm which performs simultaneous single-objective optimizations.The new algorithm is evaluated with a series of classification benchmarks andshows an improvement over other evolutionary training strategiesand a statistical algorithm.|Shin Ando","58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","57904|GECCO|2007|Association rule mining for continuous attributes using genetic network programming|Most association rule mining algorithms make use of discretization algorithms for handling continuous attributes. However, by means of methods of discretization, it is difficult to get highest attribute interdependency and at the same time to get lowest number of intervals. We propose a method using a new graph-based evolutionary algorithm named \"Genetic Network Programming (GNP)\" that can deal with continues values directly, that is, without using any discretization method as a preprocessing step. GNP is one of the evolutionary optimization techniques, which uses directed graph structures as solutions and is composed of three kinds of nodes start node, judgment node and processing node. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the judgment and connection from the current activated node. The features of GNP are described as follows. First, it is possible to reuse nodes because of this, the structure is compact. Second, GNP can find solutions of problems without bloat, which can be sometimes found in Genetic Programming (GP), because of the fixed number of nodes in GNP. Third, nodes that are not used at the current program executions will be used for future evolution. Fourth, GNP is able to cope with partially observable Markov processes. In this paper, we propose a method that can deal with continuous attributes, where attributes in databases correspond to judgment nodes in GNP and each continuous attribute is checked whether its value is greater than a threshold value and the association rules are represented as the connections of the judgment nodes. Threshold ai is firstly determined by calculating the mean i and standard deviation si of all attribute values of Ai. Then, initial threshold ai is selected randomly between the interval i - aisi, i + aisi where ai is a parameter to determine the range of the interval. Once the threshold ai is selected for all attributes, each value of the attribute Ai is checked if it is greater than the threshold ai in the judgment nodes of the proposed method. In addition to that, the threshold ai is also evolved by mutation between i - aisi, i + aisi in every generation in order to obtain as many association rules as possible. The features of the proposed method are as follows compared with other methods ) Extracts rules without identifying frequent itemsets used in Apriori-like mining methods. ) Stores extracted important association rules in a pool all together through generations. ) Measures the significance of associations via the chi-squared test. ) Extracts important rules sufficient enough for user's purpose in a short time. ) The pool is updated in every generation and only important association rules with higher chi-squared value are stored when the identical rules are stored. We have evaluated the proposed method by doing two simulations. Simulation  uses fixed threshold values that is, they remain fixed at initial thresholds during evolution. In simulation ,thresholds are evolved by mutation in every generation. Fig.  shows the number of rules extracted in the pool in simulation . It is found that the number of rules extracted has been increased, which means simulation  outperforms simulation .|Karla Taboada,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","58049|GECCO|2007|The genetic programming collaboration network and its communities|Useful information about scientific collaboration structures and patterns can be inferred from computer databases of published papers. The genetic programming bibliography is the most complete reference of papers on GP. In addition to locating publications, it contains coauthor and coeditor relationships from which a more complete picture of the field emerges. We treat these relationships as undirected small world graphs whose study reveals the community structure of the GP collaborative social network. Automatic analysis discovers new communities and highlights new facets of them. The investigation reveals many similarities between GP and coauthorship networks in other scientific fields but also some subtle differences such as a smaller central network component and a high clustering.|Leslie Luthi,Marco Tomassini,Mario Giacobini,William B. Langdon","58194|GECCO|2007|An effective genetic algorithm for improving wireless sensor network lifetime|An important scheme for extending sensor network lifetime is to divide sensor nodes into disjoint groups such that each group covers all targets and works alternatively. This scheme can be transformated to the Disjoint Set Covers (DSC) problem, which is proved to be NP-complete. Existing heuristic algorithms either get barely satisfactory solutions or take exponential time complexity. In this paper, we present a genetic algorithm to solve the DSC problem. Simulation results show that the proposed genetic algorithm can improve the most constrained-minimum constraining heuristic algorithm (MCMCC) in solution quality by $%$ with only polynomial computation time complexity.|Chih-Chung Lai,Chuan-Kang Ting,Ren-Song Ko","57965|GECCO|2007|Effects of passengers arrival distribution to double-deck elevator group supervisory control systems using genetic network programming|The Elevator Group Supervisory Control Systems (EGSCS) are the control systems that systematically manage three or more elevators in order to efficiently transport the passengers in buildings. Double-deck elevators, where two cages are connected with each other, are expected to be the next generation elevator systems. Meanwhile, Destination Floor Guidance Systems (DFGS) are also expected in Double-Deck Elevator Systems (DDES). With these, the passengers could be served at two consecutive floors and could input their destinations at elevator halls instead of conventional systems without DFGS. Such systems become more complex than the traditional systems and require new control methods Genetic Network Programming (GNP), a graph-based evolutionary method, has been applied to EGSCS and its advantages are shown in some previous papers. GNP can obtain the strategy of a new hall call assignment to the optimal elevator because it performs crossover and mutation operations to judgment nodes and processing nodes. In studies so far, the passenger's arrival has been assumed to take Exponential distribution for many years. In this paper, we have applied Erlang distribution and Binomial distribution in order to study how the passenger's arrival distribution affects EGSCS. We have found that the passenger's arrival distribution has great influence on EGSCS. It has been also clarified that GNP makes good performances under different conditions.|Lu Yu,Jin Zhou,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu,Sandor Markon","57974|GECCO|2007|Trading rules on stock markets using genetic network programming with sarsa learning|In this paper, the Genetic Network Programming (GNP) for creating trading rules on stocks is described. GNP is an evolutionary computation, which represents its solutions using graph structures and has some useful features inherently. It has been clarified that GNP works well especially in dynamic environments since GNP can create quite compact programs and has an implicit memory function. In this paper, GNP is applied to creating a stock trading model. There are three important points The first important point is to combine GNP with Sarsa Learning which is one of the reinforcement learning algorithms. Evolution-based methods evolve their programs after task execution because they must calculate fitness values, while reinforcement learning can change programs during task execution, therefore the programs can be created efficiently. The second important point is that GNP uses candlestick chart and selects appropriate technical indices to judge the buying and selling timing of stocks. The third important point is that sub-nodes are used in each node to determine appropriate actions (buyingselling) and to select appropriate stock price information depending on the situation. In the simulations, the trading model is trained using the stock prices of  brands in ,  and . Then the generalization ability is tested using the stock prices in . From the simulation results, it is clarified that the trading rules of the proposed method obtain much higher profits than Buy&Hold method and its effectiveness has been confirmed.|Yan Chen,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","57884|GECCO|2007|Genetic network programming with parallel processing for association rule mining in large and dense databases|Several methods of extracting association rules have been reported. A new evolutionary computation method named Genetic Network Programming (GNP) has also been developed recently and its efectiveness is shown for small datasets. However, it has not been tested for large datasets, particularly in datasets with a large number of attributes. The aim of this paper is to extract association rules from large and dense datasets using GNP considering a real world database with a huge number of attributes. We propose a new method where a large database is divided into many small datasets, then each GNP deals with one dataset having attributes with appropiate size, which was selected randomly from a large dataset and generated genetically. These GNPs are processed in parallel. We then propose some new genetic operations to improve the number of rules extracted and their quality as well. The proposed method improves remarkably on simulations. Fig.  shows the architecture of the proposed method. We use the CLIENTSERVER model. CLIENT side carries out preprocessing of large database, assignment of files to each server, rule checking, and genetic operations on files. SERVER side carries out processing of each file using conventional GNP based mining method independently. The features and advantages of the proposed method are the following Rule extraction is done in parallel. Each file generates its local pool of the rules. Files or datasets are treated as individuals in order to do new genetic operations over them and improve the rule extraction. Extracted rules are stored in a global pool. The rules are verified to avoid redundancy among them and it is assured that only new rules are stored.|Eloy Gonzales,Kaoru Shimada,Shingo Mabu,Kotaro Hirasawa,Jinglu Hu","58127|GECCO|2007|A doubly distributed genetic algorithm for network coding|We present a genetic algorithm which is distributed in two novel ways along genotype and temporal axes. Our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather thana subset of the population to each. This genotype distribution is shown to offer a significant gain in running time. Then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions intopipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. This temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.|Minkyu Kim,Varun Aggarwal,Una-May O'Reilly,Muriel Médard"],["57915|GECCO|2007|A spatial model of the red queen effect|Van Valen first discovered the \"Red Queen Effect\" (RQE), where two species can dramatically co-evolve their phenotypes over time but never gain a relative advantage . In the ideal version of the RQE, regardless of the actual values that the species evolve to obtain, they have not moved in relation to each other. Though previous models of the RQE exist, we developed an agent-based model (ABM) which has a base ontology more similar to real world coevolutionary systems than equation-based models (EBMs). For instance, this model contains spatial information and an individuallevel reproduction mechanism. Yet this model recreates traditional EBM results. For instance Dieckmann et al show that there are three possible outcomes of competitive coevolution predator dominance, prey dominance and evolutionary cycling (RQE) . By reconceptualizing this EBM using an ABM, we make it easier for students and researchers to understand, manipulate, and modify this model . The model is written in the NetLogo agent-based modeling environment . The model is initialized with  predator and  prey agents. Predator agents have a resistance level r and prey agents have a poison p. The agents are initially randomly distributed on a toroidal real-valued  by  grid. The initial resistance and poison values for the predators and prey are drawn from normal distributions with means r and p and a standard deviation of . During a model timestep, each agent moves one unit at a random heading. If at the end of its move a predator is within  unit of a prey, then it will challenge the prey. The predator will compare its resistance value to the prey's poison value and which ever agent has the larger value will win the challenge and the other agent will be killed. At the end of an agent's turn if the total number of agents is less than the maximum carrying capacity the agent will reproduce with a % probability. The new agent's initial poison  resistance will be drawn from a normal distribution with the parent's poison  resistance as the mean value and a standard deviation of . Our goal was to investigate whether this ABMwould replicate the results of the EBM of . Our parameter of interest was p we held r constant at  and varied p from  to  at increments of . For each value we ran the model  times for  timesteps. Figure  illustrates the final average values of both the resistance and the poison for the various initial values. If there are no predators or prey then a value of  is plotted for the respective final trait. In most cases, one species drives the other to extinction, and there is little change in the initial trait values. However when the value of p is similar to r then neither species is completely destroyed, but if there is any significant difference between r and p then one species will die off. The highest final trait values are found when p  r, in this case we see the results of the RQE since the final trait values are much higher than the initial values. These final values are more than . orders of magnitude larger than the initial values. This model reproduces classical models of the RQE, but has two different mobile species interacting on a spatial grid over time which is a closer representation of reality than traditional models. This closer representation makes ABMs excellent teaching and experimental tools because their basic assumptions can easily be manipulated and explored. Acknowledgments We thank the National Science Foundation and NICO for supporting this research.|Jules Ottino-Loffler,William Rand,Uri Wilensky","58090|GECCO|2007|A framework for the emergence of intra-species mutual recognition patterns|This one-page abstract presents a framework for the emergence of recognition patterns that are used by individuals to find each other and mate. A genetic component determines the brain of the individuals, a machine learner architecture, which is then used to transmit knowledge. The a priori information is kept down to a minimum. All species are initially indistinguishable and agents can only find each other by chance at the beginning. Differentiation occurs as a result of the interactions between the genetic and the knowledge parts. Restricted availability of different symbolic values forces the emergence of more elaborated recognition patterns. The sequences that form as the result of this simulation cannot be related to either the genetic or the environmental initial conditions. A Baldwin effect is observed where parts of the machine learning architecture adapts to accommodate for each species dominant sequence, which further stabilizes it. Mutually infertile species, limited learning capacities, and a restricted imperfect communication are thus all that is needed for the emergence of stable recognition patterns.|Nicolas Brodu","58131|GECCO|2007|A unified model of optimisation problems|In this work, a conceptual software model of optimisation problems is developed. Problem-specific aspects are clearly identified as such. To achieve the desired separation between problems and solvers, the details of the problem are encapsulated, and emphmechanisms capable of supporting the optimisation process are provided in a problem-independent way, allowing optimisers to be formulated at a more abstract level. The proposed model has been prototyped in Python.|Cristina C. Vieira,Carlos M. Fonseca","58121|GECCO|2007|Is the island model fault tolerant|This paper presents a research about the Fault Tolerancenature of the Island Model when applied to Distributed-Parallel Genetic Algorithms (GAs). Parallel and distributed models have been extensively applied to GAs when researchers tackle hard problems. Nevertheless, there are few works dealing with the problem of failures that are usually present when a distributed infrastructure is employed. The main results from this research suggest that the GAs Island Modelsare fault tolerant by nature.|José Ignacio Hidalgo,Francisco Fernández de Vega,Juan Lanchares,Daniel Lombraña Gonzalez","57947|GECCO|2007|A NSGA-II web-enabled parallel optimization framework for NLP and MINLP|Engineering design increasingly uses computer simulation models coupled with optimization algorithms to find the best design that meets the customer constraints within a time constrained deadline. The continued application of Moore's law combined with linear speedups of coarse grained parallelization will allow more designs to be evaluated in shorter periods of time. This paper presents a scalable, standards based framework that uses web services and grid services with a multiple objective genetic algorithm to solve continuous, mixed integer, single objective or multiple objective nonlinear, constrained design problems. Test data is provided to validate a linear speedup based on the number of processors and to show the robustness of the genetic algorithm on a set of  design problems.|David J. Powell,Joel K. Hollingsworth","80743|VLDB|2007|SQLB A Query Allocation Framework for Autonomous Consumers and Providers|In large-scale distributed information systems, where participants are autonomous and have special interests for some queries, query allocation is a challenge. Much work in this context has focused on distributing queries among providers in a way that maximizes overall performance (typically throughput and response time). However, preserving the participants\" interests is also important. In this paper, we make two main contributions. First, we provide a model to define participants' perception of the system w.r.t. their interests and propose metrics to evaluate the quality of query allocation methods. This model facilitates the design and evaluation of new query allocation methods that take into account the participants' interests. Second, we propose a framework for query allocation called Satisfaction-based Query Load Balancing (SQLB). To be fair, SQLB dynamically trades consumers' interests for providers' interests. And it continuously adapts to changes in participants' interests and to the workload. We implemented SQLB and compared it, through experimentation, to two important baseline query allocation methods, namely Capacity based and Mariposa-like. The results demonstrate that SQLB yields high efficiency while satisfying the participants' interests and significantly outperforms the baseline methods.|Jorge-Arnulfo Quiané-Ruiz,Philippe Lamarre,Patrick Valduriez","80790|VLDB|2007|A General Framework for Modeling and Processing Optimization Queries|An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function andor a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that IO optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.|Michael Gibas,Ning Zheng,Hakan Ferhatosmanoglu","58048|GECCO|2007|Automated red teaming a proposed framework for military application|In this paper, we describe Automated Red Teaming (ART), a concept that uses Evolutionary Algorithm (EA), Parallel Computing and Simulation to complement the manual Red Teaming effort to uncover system vulnerabilities or to find exploitable gaps in military operational concepts. The overall goal is to reduce surprises, improve and ensure the robustness of the Blue ops concepts. The design of key components and techniques that are required to develop an ART framework are described and discussed. An experiment with a military scenario in Urban Operations (UO) was conducted and the results analyzed to demonstrate the capability of the ART framework. Results showed that Red Force survivability can be improved by % just by modifying behavioral parameters alone. These findings could be used by Blue Force to refine their tactics and strategy thereby ensuring robustness of plans and higher mission success.|Chwee Seng Choo,Ching Lian Chua,Su-Han Victor Tay","58069|GECCO|2007|SwarmArchitect a swarm framework for collaborative construction|Computer game development has become increasingly popular in the field of autonomous systems. One of the main topics studies the building of various architectures in computer games. A realistic human-like architecture is expected in a thematic computer game, since it strongly motivates the game players in an intuitive way. However, the task of building a human-like architecture is non-trivial since the construction is a real time process without human supervision. In this paper, we present a collective building algorithm inspired by social insects for intelligent construction based on multiple agents. A swarm of virtual agents indirectly design edifications, which resemble basic features in human-like architecture by using a stigmergic mechanism along with branching rules. The main idea of the algorithm is to map sensory information to appropriate building actions.|Yifeng Zeng,Jorge Cordero Hernandez,Dennis Plougman Buus","57983|GECCO|2007|A framework of quantum-inspired multi-objective evolutionary algorithms and its convergence condition|A general framework of quantum-inspired multi-objective evolutionary algorithms as well as one of its sufficient convergence conditions to Pareto optimal set is proposed.|Zhiyong Li,Günter Rudolph"],["58174|GECCO|2007|On quality performance of heuristic and evolutionary algorithms for biobjective minimum spanning trees|In this paper, we consider a biobjective minimum spanning tree problem (MOST) and minimize two objectives - tree cost and diameter - in terms of Pareto-optimality. We assess the quality of obtained MOEA solutions in comparison to well-known diameter-constrained minimum spanning tree (dc-MST) algorithms and further improve MOEA solutions using problem-specific knowledge.|Rajeev Kumar,Pramod Kumar Singh","58222|GECCO|2007|Cross entropy and adaptive variance scaling in continuous EDA|This paper deals with the adaptive variance scaling issue incontinuous Estimation of Distribution Algorithms. A phenomenon is discovered that current adaptive variance scaling method in EDA suffers from imprecise structure learning. A new type of adaptation method is proposed to overcome this defect. The method tries to measure the difference between the obtained population and the prediction of the probabilistic model, then calculate the scaling factor by minimizing the cross entropy between these two distributions. This approach calculates the scaling factor immediately rather than adapts it incrementally. Experiments show that this approach extended the class of problems that can be solved, and improve the search efficiency in some cases. Moreover, the proposed approach features in that each decomposed subspace can be assigned an individual scaling factor, which helps to solve problems with special dimension property.|Yunpeng Cai,Xiaomin Sun,Hua Xu,Peifa Jia","57952|GECCO|2007|Screening the parameters affecting heuristic performance|This research screens the tuning parameters of a combinatorial optimization heuristic. Specifically, it presents a Design of Experiments (DOE) approach that uses a Fractional Factorial Design to screen the tuning parameters of Ant Colony System (ACS) for the Travelling Sales person problem. Screening is a preliminary step towards building a full Response Surface Model (RSM) . It identifies parametersthat have little influence on performance and can be omittedfrom the RSM design. This reduces the complexity andexpense of the RSM design.  algorithm parameters and  problem characteristics are considered. Open questionson the effect of  parameters on performance are answered.A further parameter, sometimes assumed important, was shown to have no effect on performance. A new problem characteristic that effects performance was identified. A full version of this paper is available .|Enda Ridge,Daniel Kudenko","57946|GECCO|2007|SDR a better trigger for adaptive variance scaling in normal EDAs|Recently, advances have been made in continuous, normal-distribution-based Estimation-of-DistributionAlgorithms (EDAs) by scaling the variance upfrom the maximum-likelihood estimate. When doneproperly, such scaling has been shown to preventpremature convergence on slope-like regions ofthe search space. In this paper we specificallyfocus on one way of scaling that was previouslyintroduced as Adaptive Variance Scaling (AVS). It wasfound that when using AVS, the average number offitness evaluations grows subquadratically withthe dimensionality on a wide range of unimodaltest-problems, competitively with the CMA-ES.Still, room for improvement exists because thevariance doesn't always have to be scaled. Apreviously introduced trigger based on correlationthat determines when to apply scaling was shownto fail on higher dimensional problems. Here weprovide a new solution called the Standard-DeviationRatio (SDR) trigger that is integrated with theIterated Density-Estimation Evolutionary Algorithm(IDEA). Intuitively put, scaling istriggered with SDR only if improvements are foundto be far away from the mean. SDR works even inhigh dimensions as a result of factorizing thedecision rule behind the trigger according to theestimated Bayesian factorization. We evaluateSDR-AVS-IDEA on the same set ofbenchmark problems and compare it with AVS-IDEAand CMA-ES. We find that the addition of SDR givesAVS-IDEA an important extra edgefor it to be used in future research and inapplications both in single-objective optimizationas well as in multi-objective and dynamicoptimization. In addition, we provide practical rulesof thumb for parameter settings for usingSDR-AVS-IDEA that result in anasymptotic scale-up behavior that is sublinearfor the population size (O(l.)) andsubquadratic (O(l.)) for thenumber of evaluations.|Peter A. N. Bosman,Jörn Grahl,Franz Rothlauf","58042|GECCO|2007|Novel ways of improving cooperation and performance in ensemble classifiers|There are two common methods of evolving teams of genetic programs. Research suggests Island approaches produce teams of strong individuals that cooperate poorly and Team approaches produce teams of weak individuals that cooperate strongly. Ideally, teams should be composed of strong individuals that cooperate well. In this paper we present a new class of algorithms called Orthogonal Evolution of Teams (OET) that overcomes the weaknesses of current Island and Team approaches by applying evolutionary pressure at both the level of teams and individuals during selection and replacement. We present four novel algorithms in this new class and compare their performance to Island and Team approaches as well as multi-class Adaboost on a number of classification problems.|Russell Thomason,Terence Soule","57918|GECCO|2007|Analyzing heuristic performance with response surface models prediction optimization and robustness|This research uses a Design of Experiments (DOE) approach to build a predictive model of the performance of a combinatorial optimization heuristic over a range of heuristic tuning parameter settings and problem instance characteristics. The heuristic is Ant Colony System (ACS) for the Travelling Salesperson Problem.  heurstic tuning parameters and  problem characteristics are considered. Response Surface Models (RSM) of the solution quality and solution time predicted ACS performance on both new instances from a publicly available problem generator and new real-world instances from the TSPLIB benchmark library. A numerical optimisation of the RSMs is used to find the tuning parameter settings that yield optimal performance in terms of solution quality and solution time. This paper is the first use of desirability functions, a well-established technique in DOE, to simultaneously optimise these conflicting goals. Finally, overlay plots are used to examine the robustness of the performance of the optimised heuristic across a range of problem instance characteristics. These plots give predictions on the range of problem instances for which a given solutionquality can be expected within a given solution time.|Enda Ridge,Daniel Kudenko","58082|GECCO|2007|Parallel genetic algorithm assessment of performance in multidimensional scaling|Visualization of multidimensional data by means of Multidimensional Scaling (MDS) is a popular technique of exploratory data analysis widely usable, e.g. in analysis of bio-medical data, behavioral science, marketing research, etc. Implementations of MDS methods include a subroutine for an auxiliary global optimization problem. The latter is difficult because of high dimensionality, absence of overall smoothness, and a large number of local minima. In such a situation application of a genetic algorithm (GA) seems reasonable. A favorable assessment of application of GAs in MDS in previous publications is based on heuristic arguments without estimating quantitatively the precision of GA while applied to the solution of corresponding global optimization problems. Indeed, the estimation of precision is difficult because of complexity to find the actual global minimum not only in routine use but also in unique research experiments. Quantitatively the precision of GA was estimated, at least in the experimental problems of modest dimensionality, using global minima found by means of the developed parallel version of explicit enumeration algorithm. To cope with high complexity of the minimization problem a parallel version of GA is developed, and its efficiency for problem of higher dimensionality is investigated.|Antanas Zilinskas,Julius Zilinskas","57932|GECCO|2007|Genetically designed multiple-kernels for improving the SVM performance|Classical kernel-based classifiers only use a single kernel, butthe real world applications have emphasized the need to con-sider a combination of kernels also known as a multiple kernel in order to boost the performance. Our purpose isto automatically find the mathematical expression of a multiple kernel by evolutionary means. In order to achieve this purpose we propose a hybrid model that combines a Genetic Programming (GP) algorithm and a kernel-based Support Vector Machine (SVM) classifier. Each GP chromosome isa tree encoding the mathematical expression of a multiple kernel. Numerical experiments show that the SVM embedding the evolved multiple kernel performs better than the standard kernels for the considered classification problems.|Laura Diosan,Mihai Oltean,Alexandrina Rogozan,Jean-Pierre Pécuchet","58218|GECCO|2007|Adaptive variance scaling in continuous multi-objective estimation-of-distribution algorithms|Recent research into single-objective continuous Estimation-of-Distribution Algorithms (EDAs)has shown that when maximum-likelihood estimationsare used for parametric distributions such as thenormal distribution, the EDA can easily suffer frompremature convergence. In this paper we argue thatthe same holds for multi-objective optimization.Our aim in this paper is to transfer a solutioncalled Adaptive Variance Scaling (AVS) from thesingle-objective case to the multi-objectivecase. To this end, we zoom in on an existing EDAfor continuous multi-objective optimization, theMIDEA, which employs mixturedistributions. We propose a means to combine AVSwith the normal mixture distribution, as opposedto the single normal distribution for which AVS wasintroduced. In addition, we improve the AVS schemeusing the Standard-Deviation Ratio(SDR) trigger. Intuitively put, variance scalingis triggered by the SDR trigger only ifimprovements are found to be far awayfrom the mean. For the multi-objective case,this addition is important to keep the variancefrom being scaled to excessively large values.From experiments performed on five well-knownbenchmark problems, the addition of SDR andAVS is found to enlarge the class of problems thatcontinuous multi-objective EDAs can solve reliably.|Peter A. N. Bosman,Dirk Thierens","58169|GECCO|2007|Success effort for performance comparisons|This paper looks at the production of a confidence interval for a statistic we rename success effort.|Matthew Walker,Howard Edwards,Chris H. Messom"],["58176|GECCO|2007|A study on metamodeling techniques ensembles and multi-surrogates in evolutionary computation|Surrogate-Assisted Memetic Algorithm (SAMA) is a hybrid evolutionary algorithm, particularly a memetic algorithm that employs surrogate models in the optimization search. Since most of the objective function evaluations in SAMA are approximated, the search performance of SAMA is likely to be affected by the characteristics of the models used. In this paper, we study the search performance of using different meta modeling techniques, ensembles, and multi-surrogates in SAMA. In particular, we consider the SAMA-TRF, a SAMA model management framework that incorporates a trust region scheme for interleaving use of exact objective function with computationally cheap local meta models during local searches. Four different metamodels, namely Gaussian Process (GP), Radial Basis Function (RBF), Polynomial Regression (PR), and Extreme Learning Machine (ELM) neural network are used in the study. Empirical results obtained show that while some metamodeling techniques perform best on particular benchmark problems, ensemble of metamodels and multisurrogates yield robust and improved solution quality on the benchmark problems in general, for the same computational budget.|Dudy Lim,Yew-Soon Ong,Yaochu Jin,Bernhard Sendhoff","58092|GECCO|2007|Guided hyperplane evolutionary algorithm|A new evolutionary technique for multicriteria optimization called Guiding Hyper-plane Evolutionary Algorithm (GHEA) is proposed. The originality of the approach consists in the fact that the fitness assignment is realized by using a guiding hyperplane and a new non Pareto optimality concept. Numerical experiments illustrate the performance of GHEA compared with the popular NSGA-II and SPEA.|Corina Rotar,D. Dumitrescu,Rodica Ioana Lung","58193|GECCO|2007|An evolutionary approach to collective communication scheduling|In this paper, we describe two evolutionary algorithms aimed at scheduling collective communications on interconnection networks of parallel computers. To avoid contention for links and associated delays, collective communications proceed in synchronized steps. Minimum number of steps is sought for the given network topology, wormhole (pipelined) switching, minimum routing and given sets of sender andor receiver nodes. Used algorithms are able not only re-invent optimum schedules for known symmetric topologies like hyper-cubes, but they can find schedules even for any asymmetric or irregular topologies in case of general many-to-many collective communications. In most cases does the number of steps reach the theoretical lower bound for the given type of collective communication if it does not, non-minimum routing can provide further improvement. Optimum schedules may serve for writing high-performance communication routines for application-specific networks on chip or for development of communication libraries in case of general-purpose interconnection networ.|Jirí Jaros,Milos Ohlídal,Vaclav Dvorak","58118|GECCO|2007|Interactive evolutionary multi-objective optimization and decision-making using reference direction method|In this paper, we borrow the concept of reference direction approach from the multi-criterion decision-making literature and combine it with an EMOprocedure to develop an algorithm for finding a single preferred solution in a multi-objective optimization scenario efficiently. EMO methodologies are adequately used to find a set of representative efficient solutions over the past decade. This study is timely in addressing the issue of optimizing and choosing a single solution using certain preference information. In this approach, the user supplies one or more reference directions in the objective space. The population approach of EMO methodologies is exploited to find a set of efficient solutions corresponding to a number of representative points along the reference direction. By using a utility function, a single solution is chosen for further analysis. This procedure is continued till no further improvement is possible. The working of the procedure is demonstrated on a set of test problems having two to ten objectives and on an engineering design problem. Results are verified with theoretically exact solutions on two-objective test problems.|Kalyanmoy Deb,Abhishek Kumar","57984|GECCO|2007|Discrimination of metabolic flux profiles using a hybrid evolutionary algorithm|Studying metabolic fluxes is a crucial aspect of understanding biological phenotypes. However, it is often not possible to measure these fluxes directly. As an alternative, fluxome profiling provides indirect information about fluxes in a high-throughput setting. In this paper, we consider a scenario where fluxome profiling is used to investigate characteristic differences between a number of bacterial mutant strains. The goal is to identify groups of mutants that show maximally different fluxome profiles. We propose an evolutionary algorithm for this optimization problem and demonstrate that it outperforms alternative methods based on principle component analysis and independent component analysis on both real and synthetic data sets.|Stefan Bleuler,Eckart Zitzler","57997|GECCO|2007|Feature selection and classification in noisy epistatic problems using a hybrid evolutionary approach|A hybrid evolutionary approach is proposed for the combined problem of feature selection (using a genetic algorithm with IntersectionUnion recombination and a fitness function based on a counter-propagation artificial neural network) and subsequent classifier construction (using strongly-typed genetic programming), for use in nonlinear association studies with relatively large potential feature sets and noisy class data. The method was tested using synthetic data with various degrees of injected noise, based on a proposed mental health database.allResults show the algorithm has good potential for feature selection, classification and function characterization.|Drew DeHaas,Jesse Craig,Colin Rickert,Margaret J. Eppstein,Paul Haake,Kirsten Stor","58198|GECCO|2007|Using evolutionary computation and local search to solve multi-objective flexible job shop problems|Finding realistic schedules for Flexible Job Shop Problems has attracted many researchers recently due to its NP-hardness. In this paper, we present an efficient approach for solving the multi-objective flexible job shop by combining Evolutionary Algorithm and Guided Local Search. Instead of applying random local search to find neighborhood solutions, we introduce a guided local search procedure to accelerate the process of convergence to Pareto-optimal solutions. The main improvement of this combination is to help diversify the population towards the Pareto-front. Empirical studies show that ) the gaps between the obtained results and known lower bounds are small, and ) the multi-objective solutions of our algorithms dominate previous designs for solving the same benchmarks while incurring less computational time.|Nhu Binh Ho,Joc Cing Tay","58088|GECCO|2007|Carbon-friendly travel plan construction using an evolutionary algorithm|This paper discusses the use of an evolutionary algorithm to design workplace travel plans, to promote of car sharing and reduce carbon emissions from single-occupancy motor vehicles.|Neil Urquhart","57983|GECCO|2007|A framework of quantum-inspired multi-objective evolutionary algorithms and its convergence condition|A general framework of quantum-inspired multi-objective evolutionary algorithms as well as one of its sufficient convergence conditions to Pareto optimal set is proposed.|Zhiyong Li,Günter Rudolph","58099|GECCO|2007|Automatic analog IC layout generation based on a evolutionary computation approach|This paper describes an innovative analog IC layout generation approach based on evolutionary computation techniques.|Nuno C. Lourenço,Nuno C. G. Horta"],["58040|GECCO|2007|Variable selection for wind power prediction using particle swarm optimization|Wind energy has an increasing influence on the energy supply in many countries, but in contrast to conventional power plants it is a fluctuating energy source. For its integration in the electricity supply structure it is necessary to predict the wind power hours or days ahead. There are models based on physical, statistical or artificial intelligence approaches for the prediction of wind power. In this paper a new short-term prediction method is described based on variable selection using particle swarm optimization and nearest neighbour search. As input variables for this prediction method weather data of a numerical weather prediction model and measured power data from wind farms of several locations in a spread area are used. Additionally a prediction model based on neural networks is described and the results of the new method are compared to the results of the neural network approach. As a result we get a reduction of the prediction error by using the new prediction method. An additional error reduction is possible by using the mean model output of the neural network model and of the nearest neighbour search based prediction approach.|René Jursa","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58208|GECCO|2007|Two-level of nondominated solutions approach to multiobjective particle swarm optimization|In multiobjective particle swarm optimization (MOPSO) methods, selecting the local best and the global best for each particle of the population has a great impact on the convergence and diversity of solutions, especially when optimizing problems with high number of objectives. This paper presents a two-level of nondominated solutions approach to MOPSO. The ability of the proposed approach to detect the true Pareto optimal solutions and capture the shape of the Pareto front is evaluated through experiments on well-known non-trivial test problems. The diversity of the nondominated solutions obtained is demonstrated through different measures. The proposed approach has been assessed through a comparative study with the reported results in the literature.|M. A. Abido","58150|GECCO|2007|Geometric particle swarm optimization for the sudoku puzzle|Geometric particle swarm optimization (GPSO) is a recentlyintroduced generalization of traditional particle swarm optimization(PSO) that applies to all combinatorial spaces. The aim of thispaper is to demonstrate the applicability of GPSO to non-trivialcombinatorial spaces. The Sudoku puzzle is a perfect candidate totest new algorithmic ideas because it is entertaining andinstructive as well as a non-trivial constrained combinatorialproblem. We apply GPSO to solve the sudoku puzzle.|Alberto Moraglio,Julian Togelius"],["80775|VLDB|2007|MIST Distributed Indexing and Querying in Sensor Networks using Statistical Models|The modeling of high level semantic events from low level sensor signals is important in order to understand distributed phenomena. For such content-modeling purposes, transformation of numeric data into symbols and the modeling of resulting symbolic sequences can be achieved using statistical models---Markov Chains (MCs) and Hidden Markov Models (HMMs). We consider the problem of distributed indexing and semantic querying over such sensor models. Specifically, we are interested in efficiently answering (i) range queries return all sensors that have observed an unusual sequence of symbols with a high likelihood, (ii) top- queries return the sensor that has the maximum probability of observing a given sequence, and (iii) -NN queries return the sensor (model) which is most similar to a query model. All the above queries can be answered at the centralized base station, if each sensor transmits its model to the base station. However, this is communication-intensive. We present a much more efficient alternative---a distributed index structure, MIST (Model-based Index STructure), and accompanying algorithms for answering the above queries. MIST aggregates two or more constituent models into a single composite model, and constructs an in-network hierarchy over such composite models. We develop two kinds of composite models the first kind captures the average behavior of the underlying models and the second kind captures the extreme behaviors of the underlying models. Using the index parameters maintained at the root of a subtree, we bound the probability of observation of a query sequence from a sensor in the subtree. We also bound the distance of a query model to a sensor model using these parameters. Extensive experimental evaluation on both real-world and synthetic data sets show that the MIST schemes scale well in terms of network size and number of model states. We also show its superior performance over the centralized schemes in terms of update, query, and total communication costs.|Arnab Bhattacharya,Anand Meka,Ambuj K. Singh","80771|VLDB|2007|Inferring XML Schema Definitions from XML Data|Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.|Geert Jan Bex,Frank Neven,Stijn Vansummeren","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80774|VLDB|2007|Towards Graph Containment Search and Indexing|Given a set of model graphs D and a query graph q, containment search aims to find all model graphs g &epsilon D such that q contains g (q &supe g). Due to the wide adoption of graph models, fast containment search of graph data finds many applications in various domains. In comparison to traditional graph search that retrieves all the graphs containing q (q &sube g), containment search has its own indexing characteristics that have not yet been examined. In this paper, we perform a systematic study on these characteristics and propose a contrast subgraph-based indexing model, called cIndex. Contrast subgraphs capture the structure differences between model graphs and query graphs, and are thus perfect for indexing due to their high selectivity. Using a redundancy-aware feature selection process, cIndex can sort out a set of significant and distinctive contrast subgraphs and maximize its indexing capability. We show that it is NP-complete to choose the best set of indexing features, and our greedy algorithm can approximate the one-level optimal index within a ratio of -- e. Taking this solution as a base indexing model, we further extend it to accommodate hierarchical indexing methodologies and apply data space clustering and sampling techniques to reduce the index construction time. The proposed methodology provides a general solution to containment search and indexing, not only for graphs, but also for any data with transitive relations as well. Experimental results on real test data show that cIndex achieves near-optimal pruning power on various containment search workloads, and confirms its obvious advantage over indices built for traditional graph search in this new scenario.|Chen Chen,Xifeng Yan,Philip S. Yu,Jiawei Han,Dong-Qing Zhang,Xiaohui Gu","80767|VLDB|2007|Materialized Views in Probabilistic Databases for Information Exchange and Query Optimization|Views over probabilistic data contain correlations between tuples, and the current approach is to capture these correlations using explicit lineage. In this paper we propose an alternative approach to materializing probabilistic views, by giving conditions under which a view can be represented by a block-independent disjoint (BID) table. Not all views can be represented as BID tables and so we propose a novel partial representation that can represent all views but may not define a unique probability distribution. We then give conditions on when a query's value on a partial representation will be uniquely defined. We apply our theory to two applications query processing using views and information exchange using views. In query processing on probabilistic data, we can ignore the lineage and use materialized views to more efficiently answer queries. By contrast, if the view has explicit lineage, the query evaluation must reprocess the lineage to compute the query resulting in dramatically slower execution. The second application is information exchange when we do not wish to disclose the entire lineage, which otherwise may result in shipping the entire database. The paper contains several theoretical results that completely solve the problem of deciding whether a conjunctive view can be represented as a BID and whether a query on a partial representation is uniquely determined. We validate our approach experimentally showing that representable views exist in real and synthetic workloads and show over three magnitudes of improvement in query processing versus a lineage based approach.|Christopher Re,Dan Suciu","80764|VLDB|2007|Querying Complex Structured Databases|Correctly generating a structured query (e.g., an XQuery or a SQL query) requires the user to have a full understanding of the database schema, which can be a daunting task. Alternative query models have been proposed to give users the ability to query the database without schema knowledge. Those models, including simple keyword search and labeled keyword search, aim to extract meaningful data fragments that match the structure-free query conditions (e.g., keywords) based on various matching semantics. Typically, the matching semantics are content-based they are defined on data node inter-relationships and incur significant query evaluation cost. Our first contribution is a novel matching semantics based on analyzing the database schema. We show that query models employing a schema-based matching semantics can reduce query evaluation cost significantly while maintaining or even improving result quality. The adoption of schema-based matching semantics does not change the nature of those query models they are still schema-ignorant, i.e., users express no schema knowledge (except the labels in labeled keyword search) in the query. While those models work well for some queries on some databases, they often encounter problems when applied to complex queries on databases with complex schemas. Our second contribution is a novel query model that incorporates partial schema knowledge through the use of schema summary. This new summary-aware query model, called Meaningful Summary Query (MSQ), seamlessly integrates summary-based structural conditions and structure-free conditions, and enables ordinary users to query complex databases. We design algorithms for evaluating MSQ queries, and demonstrate that MSQ queries can produce better results against complex databases when compared with previous approaches, and that they can be efficiently evaluated.|Cong Yu,H. V. Jagadish","80838|VLDB|2007|Graph Indexing Tree  Delta  Graph|Recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs. As a result, it is of special interest to process graph containment queries effectively on large graph databases. Given a graph database G, and a query raph q, the graph containment query is to retrieve all graphs in G which contain q as subgraph(s). Due to the vast number of graphs in G and the nature of complexity for subgraph isomorphism testing, it is desirable to make use of high-quality graph indexing mechanisms to reduce the overall query processing cost. In this paper, we propose a new cost-effective graph indexing method based on frequent tree-features of the graph database. We analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects feature size, feature selection cost, and pruning power. In order to achieve better pruning ability than existing graph-based indexing methods, we select, in addition to frequent tree-features (Tree), a small number of discriminative graphs (&Delta) on demand, without a costly graph mining process beforehand. Our study verifies that (Tree+&Delta) is a better choice than graph for indexing purpose, denoted (Tree+&Delta &geGraph), to address the graph containment query problem. It has two implications () the index construction by (Tree+&Delta) is efficient, and () the graph containment query processing by (Tree+&Delta) is efficient. Our experimental studies demonstrate that (Tree+&Delta) has a compact index structure, achieves an order of magnitude better performance in index construction, and most importantly, outperforms up-to-date graph-based indexing methods gIndex and C-Tree, in graph containment query processing.|Peixiang Zhao,Jeffrey Xu Yu,Philip S. Yu","80776|VLDB|2007|LCS-TRIM Dynamic Programming Meets XML Indexing and Querying|In this article, we propose a new approach for querying and indexing a database of trees with specific applications to XML datasets. Our approach relies on representing both the queries and the data using a sequential encoding and then subsequently employing an innovative variant of the longest common subsequence (LCS) matching algorithm to retrieve the desired results. A key innovation here is the use of a series of inter-linked early pruning steps, coupled with a simple index structure that enable us to reduce the search space and eliminate a large number of false positive matches prior to applying the more expensive LCS matching algorithm. Additionally, we also present mechanisms that enable the user to specify constraints on the retrieved output and show how such constraints can be pushed deep into the retrieval process, leading to improved response times. Mechanisms supporting the retrieval of approximate matches are also supported. When compared with state-of-the-art approaches, the query processing time of our algorithms is shown to be up to two to three orders of magnitude faster on several real datasets on realistic query workloads. Finally, we show that our approach is suitable for emerging multi-core server architectures when retrieving data for more expensive queries.|Shirish Tatikonda,Srinivasan Parthasarathy,Matthew Goyder","80769|VLDB|2007|Structured Materialized Views for XML Queries|The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad  prototype and we present a performance analysis.|Andrei Arion,Véronique Benzaken,Ioana Manolescu,Yannis Papakonstantinou","80811|VLDB|2007|Lazy Maintenance of Materialized Views|Materialized views can speed up query processing greatly but they have to be kept up to date to be useful. Today, database systems typically maintain views eagerly in the same transaction as the base table updates. This has the effect that updates pay for view maintenance while beneficiaries (queries) get a free ride View maintenance overhead can be significant and it seems unfair to have updates bear the cost. We present a novel way to lazily maintain materialized views that relieves updates of this overhead. Maintenance of a view is postponed until the system has free cycles or the view is referenced by a query. View maintenance is fully or partly hidden from queries depending on the system load. Ideally, views are maintained entirely on system time at no cost to updates and queries. The efficiency of lazy maintenance is improved by combining updates from several transactions into a single maintenance operation, by condensing multiple updates of the same row into a single update, and by exploiting row versioning. Experiments using a prototype implementation in Microsoft SQL Server show much faster response times for updates and also significant reduction in maintenance cost when combining updates.|Jingren Zhou,Per-\u2026ke Larson,Hicham G. Elmongui"],["80738|VLDB|2007|Indexable PLA for Efficient Similarity Search|Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the \"dimensionality curse\". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).|Qiuxia Chen,Lei Chen 0002,Xiang Lian,Yunhao Liu,Jeffrey Xu Yu","80751|VLDB|2007|FluxCapacitor Efficient Time-Travel Text Search|An increasing number of temporally versioned text collections is available today with Web archives being a prime example. Search on such collections, however, is often not satisfactory and ignores their temporal dimension completely. Time-travel text search solves this problem by evaluating a keyword query on the state of the text collection as of a user-specified time point. This work demonstrates our approach to efficient time-travel text search and its implementation in the FLUXCAPACITOR prototype.|Klaus Berberich,Srikanta J. Bedathur,Thomas Neumann,Gerhard Weikum","80804|VLDB|2007|Multi-Probe LSH Efficient Indexing for High-Dimensional Similarity Search |Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar time-efficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and  to  times fewer number of hash tables.|Qin Lv,William Josephson,Zhe Wang,Moses Charikar,Kai Li","80794|VLDB|2007|Fast nGram-Based String Search Over Data Encoded Using Algebraic Signatures|We propose a novel string search algorithm for data stored once and read many times. Our search method combines the sublinear traversal of the record (as in Boyer Moore or Knuth-Morris-Pratt) with the agglomeration of parts of the record and search pattern into a single character -- the algebraic signature -- in the manner of Karp-Rabin. Our experiments show that our algorithm is up to seventy times faster for DNA data, up to eleven times faster for ASCII, and up to a six times faster for XML documents compared with an implementation of Boyer-Moore. To obtain this speed-up, we store records in encoded form, where each original character is replaced with an algebraic signature. Our method applies to records stored in databases in general and to distributed implementations of a Database As Service (DAS) in particular. Clients send records for insertion and search patterns already in encoded form and servers never operate on records in clear text. No one at a node can involuntarily discover the content of the stored data.|Witold Litwin,Riad Mokadem,Philippe Rigaux,Thomas J. E. Schwarz","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80774|VLDB|2007|Towards Graph Containment Search and Indexing|Given a set of model graphs D and a query graph q, containment search aims to find all model graphs g &epsilon D such that q contains g (q &supe g). Due to the wide adoption of graph models, fast containment search of graph data finds many applications in various domains. In comparison to traditional graph search that retrieves all the graphs containing q (q &sube g), containment search has its own indexing characteristics that have not yet been examined. In this paper, we perform a systematic study on these characteristics and propose a contrast subgraph-based indexing model, called cIndex. Contrast subgraphs capture the structure differences between model graphs and query graphs, and are thus perfect for indexing due to their high selectivity. Using a redundancy-aware feature selection process, cIndex can sort out a set of significant and distinctive contrast subgraphs and maximize its indexing capability. We show that it is NP-complete to choose the best set of indexing features, and our greedy algorithm can approximate the one-level optimal index within a ratio of -- e. Taking this solution as a base indexing model, we further extend it to accommodate hierarchical indexing methodologies and apply data space clustering and sampling techniques to reduce the index construction time. The proposed methodology provides a general solution to containment search and indexing, not only for graphs, but also for any data with transitive relations as well. Experimental results on real test data show that cIndex achieves near-optimal pruning power on various containment search workloads, and confirms its obvious advantage over indices built for traditional graph search in this new scenario.|Chen Chen,Xifeng Yan,Philip S. Yu,Jiawei Han,Dong-Qing Zhang,Xiaohui Gu","58074|GECCO|2007|The effects of solution density in the search space on finding spatially robust solutions|The common definition for robust solutions considers a solution robust if it remains optimal (or near optimal) when the parameters defining the fitness function are perturbed. We call this parameter robustness or temporal robustness. In this paper we propose an alternate definition for robustness, which we call spatial or solution robustness, if both the solution and the neighbourhood around the solution has high fitness. With this definition, we created a set of functions with useful properties to allow for the testing of solution robustness. We then focus on the effect of the precision (density) of the search space and find that it has a drastic effect on both the number of solutions and their quality.|Grzegorz Drzadzewski,Mark Wineberg","58240|GECCO|2007|Symbiotic tabu search|Recombination in the Genetic Algorithm (GA) is supposed to extract the component characteristics from two parents and reassemble them in different combinations hopefully producing an offspring that has the good characteristics of both parents. Symbiotic Combination is formerly introduced as an alternative for sexual recombination operator to overcome the need of explicit design of recombination operators in GA all. This paper presents an optimization algorithm based on using this operator in Tabu Search. The algorithm is benchmarked on two problem sets and is compared with standard genetic algorithm and symbiotic evolutionary adaptation model, showing success rates higher than both cited algorithms.|Ramin Halavati,Saeed Bagheri Shouraki,Bahareh Jafari Jashmi,Mojdeh Jalali Heravi","57897|GECCO|2007|Efficient priority optimization in complex distributed embedded systems through search space adaptation|In this paper we present a framework for dynamic search space adaptation during evolutionary design space exploration. Compared to previous approaches our framework is capable of adapting the search space dynamically during exploration leading to better search space exploitation in the same exploration time. The application of our framework to priority optimization in complex distributed embedded systems shows that dynamic search space adaptation can significantly increase exploration efficiency, both in terms of exploration time and quality of achieved results.|Arne Hamann,Rolf Ernst","57900|GECCO|2007|Pareto optimal search based refactoring at the design level|Refactoring aims to improve the quality of a software systems' structure, which tends to degrade as the system evolves. While manually determining useful refactorings can be challenging, search based techniques can automatically discover useful refactorings. Current search based refactoring approaches require metrics to be combined in a complex fashion, and producea single sequence of refactorings. In this paper we show how Pareto optimality can improve search based refactoring, making the combination of metrics easier, and aiding the presentation of multiple sequences of optimal refactorings to users.|Mark Harman,Laurence Tratt"]]}}