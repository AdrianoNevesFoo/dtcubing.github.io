{"abstract":{"entropy":6.592475385465383,"topics":["evolutionary algorithm, artificial intelligence, evolution strategies, evolutionary computation, particle swarm, evolutionary multi-objective, optimization problem, testing test, multi-objective problem, software quality, operator crossover, evolutionary search, fitness landscapes, software evolution, algorithm optimization, multi-objective algorithm, paper operator, multi-objective optimization, differential evolution, solving multi-objective","data, mapping schema, knowledge base, web, queries relational, efficient xml, query processing, consider problem, sensor network, stream systems, learning task, search engine, semantic web, information integration, mining classification, support data, continuous queries, machine data, large data, xml data","markov decision, markov processes, decision processes, partially observable, classifier xcs, spanning tree, classifier systems, reinforcement learning, agents, processes mdp, time data, markov mdp, recent years, agents environments, actions effects, decision mdp, areas research, agents information, xcs systems, learning systems","genetic algorithm, genetic programming, algorithm, algorithm problem, present novel, present algorithm, solving problem, evolutionary algorithm, programming cartesian, multiple voting, agents voting, embedded cartesian, genetic cartesian, extension cartesian, present approach, novel approach, algorithm search, general voting, extension programming, problem","evolutionary search, testing test, algorithm search, use evolutionary, search test, recent work, search, work, game, use, computational, different, recent, applied, mutation, significant, article, field, methods, metaheuristics","evolutionary algorithm, multi-objective problem, evolutionary computation, evolutionary multi-objective, evolutionary solutions, multi-objective optimization, optimization techniques, multi-objective algorithm, solving multi-objective, important role, algorithm optimization, evolutionary problem, present evolutionary, evolutionary operator, dynamic evolutionary, dynamic algorithm, evolutionary, techniques algorithm, techniques, important","data, large data, queries relational, efficient xml, xml, machine data, xml data, widely used, used xml, queries database, highly data, queries xml, queries, data relational, problem data, used data, queries data, automatically, complex, key","present similarity, search engine, address problem, applications, problem information, applications problem, problem large, information, retrieval, similarity, report, network, challenging, current, various, statistical, domains, important, measures, large","spanning tree, time data, recent years, areas research, research, tool, sets, real, virtual, able, attention, ubiquitous, components, autonomous, demonstrate, large","classifier xcs, systems, classifier systems, xcs systems, embedded systems, recommender systems, systems complex, investigate systems, investigate, study, concepts, increasingly, become, interactive, standard, complexity, efficiency, behavior, following, use","solving problem, problem, combinatorial problem, algorithm solving, used problem, algorithm problem, constraint problem, commonly problem, constraint, recently, memory, probabilistic, variables, context, optimal, reasoning, introduced, approaches, shown, definition","present novel, solve problem, paper approach, present problem, algorithm called, present based, present approach, distribution algorithm, novel approach, algorithm learning, paper problem, based approach, present learning, present algorithm, approach problem, paper novel, paper present, approach, present, scheme"],"ranking":[["57700|GECCO|2006|Incorporating directional information within a differential evolution algorithm for multi-objective optimization|The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.|Antony W. Iorio,Xiaodong Li","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57729|GECCO|2006|A new multi-objective evolutionary algorithm for solving high complex multi-objective problems|In this paper, a new multi-objective evolutionary algorithm for solving high complex multi-objective problems is presented based on the rule of energy minimizing and the law of entropy increasing of particle systems in phase space, Through the experiments it proves that this algorithm can quickly obtains the Pareto solutions with high precision and uniform distribution. And the results of the experiments show that this algorithm can avoid the premature phenomenon of problems better than the traditional evolutionary algorithm because it can drive all the individuals to participate in the evolving operation in each generation.|Kangshun Li,Xuezhi Yue,Lishan Kang,Zhangxin Chen","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","57815|GECCO|2006|Single and multi-objective genetic operators in object-oriented conceptual software design|This poster paper investigates the potential of single and multi-objective genetic operators with an object-oriented conceptual design space. Using cohesion as an objective fitness function, genetic operators inspired by genetic algorithms and evolutionary programming are compared against a simple case study. Also, using both cohesion and coupling as objective fitness functions, multi-objective genetic operators inspired by a non-dominated sorting algorithm have been developed. Cohesion and coupling values achieved are similar to human performed designs and a large number and variety of optimal solutions are arrived at, which could not have been produced by the human software engineer. We conclude that this mass of optimal design variants offers significant potential for design support when integrated with user-centric, computationally intelligent tools.|Christopher L. Simons,Ian C. Parmee","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan","57843|GECCO|2006|Multiobjective evolutionary optimization for visual data mining with virtual reality spaces application to Alzheimer gene expressions|This paper introduces a multi-objective optimization approach to the problem of computing virtual reality spaces for the visual representation of relational structures (e.g. databases), symbolic knowledge and others, in the context of visual data mining and knowledge discovery. Procedures based on evolutionary computation are discussed. In particular, the NSGA-II algorithm is used as a framework for an instance of this methodology simultaneously minimizing Sammon's error for dissimilarity measures, and mean cross-validation error on a k-nn pattern classifier. The proposed approach is illustrated with an example from genomics (in particular, Alzheimer's disease) by constructing virtual reality spaces resulting from multi-objective optimization. Selected solutions along the Pareto front approximation are used as nonlinearly transformed features for new spaces that compromise similarity structure preservation (from an unsupervised perspective) and class separability (from a supervised pattern recognition perspective), simultaneously. The possibility of spanning a range of solutions between these two important goals, is a benefit for the knowledge discovery and data understanding process. The quality of the set of discovered solutions is superior to the ones obtained separately, from the point ofview of visual data mining.|Julio J. Valdés,Alan J. Barton","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"],["80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80714|VLDB|2006|R-SOX Runtime Semantic Query Optimization over XML Streams|Optimizing queries over XML streams has been an important and non-trivial issue with the emergence of complex XML stream applications such as monitoring sensor networks and online transaction processing. Our system, R-SOX, provides a platform for runtime query optimization based on dynamic schema knowledge embedded in the XML streams. Such information provides refined runtime schema knowledge thus dramatically enlarged the opportunity for schema-based query optimizations. In this demonstration, we focus on the following three aspects () annotation of runtime schema knowledge () incremental maintenance of run-time schema knowledge () dynamic semantic query optimization techniques. The overall framework for runtime semantic query optimization, including several classes of dynamic optimization techniques, will be shown in this demonstration.|Song Wang,Hong Su,Ming Li,Mingzhu Wei,Shoushen Yang,Drew Ditto,Elke A. Rundensteiner,Murali Mani","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80603|VLDB|2006|A Middleware for Fast and Flexible Sensor Network Deployment|A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from httpglobalsn.sourceforge.net.|Karl Aberer,Manfred Hauswirth,Ali Salehi","80621|VLDB|2006|Containment of Conjunctive Object Meta-Queries|We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.|Andrea Calì,Michael Kifer","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","57721|GECCO|2006|Genetic algorithms for action set selection across domains a demonstration|Action set selection in Markov Decision Processes (MDPs) is an area of research that has received little attention. On the other hand, the set of actions available to an MDP agent can have a significant impact on the ability of the agent to gain optimal rewards. Last year at GECCO', the first automated action set selection tool powered by genetic algorithms was presented. The demonstration of its capabilities, though intriguing, was limited to a single domain. In this paper, we apply the tool to a more challenging problem of oil sand image interpretation. In the new experiments, genetic algorithms evolved a compact high-performance set of image processing operators, decreasing interpretation time by % while improving image interpretation accuracy by %. These results exceed the original performance and suggest certain cross-domain portability of the approach.|Greg Lee,Vadim Bulitko","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","65772|AAAI|2006|Controlled Search over Compact State Representations in Nondeterministic Planning Domains and Beyond|Two of the most efficient planners for planning in nondeterministic domains are MBP and ND-SHOP. MBP achieves its efficiency by using Binary Decision Diagrams (BDDs) to represent sets of states that share some common properties, so it can plan for all of these states simultaneously. ND-SHOP achieves its efficiency by using HTN task decomposition to focus the search. In some environments, ND-SHOP runs exponentially faster than MBP, and in others the reverse is true. In this paper, we discuss the following  We describe how to combine ND-SHOP's HTNs with MBP's BDDs. Our new planning algorithm, YoYo, performs task decompositions over classes of states that are represented as BDDs. In our experiments, YoYo easily outperformed both MBP and ND-SHOP, often by several orders of magnitude.  HTNs are just one of several techniques that are originally developed for classical planning domains and that can be adapted to work in nondeterministic domains. By combining those techniques with a BDD representation, it should be possible to get great speedups just as we did here. We discuss how these same ideas can be generalized for use in several other research areas, such as planning with Markov Decision Processes, synthesizing controllers for hybrid systems, and composing Semantic Web Services.|Ugur Kuter,Dana S. Nau","65811|AAAI|2006|Factored MDP Elicitation and Plan Display|The software suite we will demonstrate at AAAI' was designed around planning with factored Markov decision processes (MDPs). It is a user-friendly suite that facilitates domain elicitation, preference elicitation, planning, and MDP policy display. The demo will concentrate on user interactions for domain experts and those for whom plans are made.|Krol Kevin Mathias,Casey Lengacher,Derek Williams,Austin Cornett,Alex Dekhtyar,Judy Goldsmith","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aurélie Beynier,Abdel-Illah Mouaddib","65864|AAAI|2006|Targeting Specific Distributions of Trajectories in MDPs|We define TTD-MDPs, a novel class of Markov decision processes where the traditional goal of an agent is changed from finding an optimal trajectory through a state space to realizing a specified distribution of trajectories through the space. After motivating this formulation, we show how to convert a traditional MDP into a TTD-MDP. We derive an algorithm for finding non-deterministic policies by constructing a trajectory tree that allows us to compute locally-consistent policies. We specify the necessary conditions for solving the problem exactly and present a heuristic algorithm for constructing policies when an exact answer is impossible or impractical. We present empirical results for our algorithm in two domains a synthetic grid world and stories in an interactive drama or game.|David L. Roberts,Mark J. Nelson,Charles Lee Isbell Jr.,Michael Mateas,Michael L. Littman","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","57727|GECCO|2006|Nonlinear parametric regression in genetic programming|Function approximation or regression is the problem of finding a function that best explains the relationship between independent variables and a dependent variable from the observed data. Genetic programming has been considered a promising approach for the problem since it is possible to optimize both the functional form and the coefficients. Genetic programming has been considered a promising approach for function approximation since it is possible to optimize both the functional form and the coefficients. However, it is not easy to find an optimal set of coefficients by using only non-adjustable constant nodes in genetic programming. To overcome the problem, there have been some studies on genetic programming using adjustable parameters in linear or nonlinear models. Although the nonlinear parametric model has a merit over the linear parametric model, there have been few studies on it. In this paper, we propose a nonlinear parametric genetic programming which uses a nonlinear gradient method to estimate parameters. The most notable feature in the proposed genetic programming is that we design a parameter attachment algorithm using as few redundant parameters as possible. It showed a significant performance improvement over the traditional genetic programming approaches on real-world application problems.|Yung-Keun Kwon,Sung-Soon Choi,Byung Ro Moon","57603|GECCO|2006|Instance similarity and the effectiveness of case injection in a genetic algorithm for binary quadratic programming|When an evolutionary algorithm addresses a sequence of instances of the same problem, it can seed its population with solutions that it found for previous instances. This technique is called case injection. How similar must the instances be for case injection to help an EA's search We consider this question by applying a genetic algorithm, without and with case injection, to sequences of instances of binary quadratic programming. When the instances are similar, case injection helps when the instances differ sufficiently, case injection is no help at all.|Jason Amunrud,Bryant A. Julstrom","57850|GECCO|2006|A multi-chromosome approach to standard and embedded cartesian genetic programming|Embedded Cartesian Genetic Programming (ECGP) is an extension of Cartesian Genetic Programming (CGP) that can automatically acquire, evolve and re-use partial solutions in the form of modules. In this paper, we introduce for the first time a new multi-chromosome approach to CGP and ECGP that allows difficult problems with multiple outputs to be broken down into many smaller, simpler problems with single outputs, whilst still encoding the entire solution in a single genotype. We also propose a multi-chromosome evolutionary strategy which selects the best chromosomes from the entire population to form the new fittest individual, which may not have been present in the population. The multi-chromosome approach to CGP and ECGP is tested on a number of multiple output digital circuits. Computational Effort figures are calculated for each problem and compared against those for CGP and ECGP. The results indicate that the use of multiple chromosomes in both CGP and ECGP provide a significant performance increase on all problems tested.|James Alfred Walker,Julian Francis Miller,Rachel Cavill","57849|GECCO|2006|Embedded cartesian genetic programming and the lawnmower and hierarchical-if-and-only-if problems|Embedded Cartesian Genetic Programming (ECGP) is an extension of the directed graph based Cartesian Genetic Programming (CGP), which is capable of automatically acquiring, evolving and re-using partial solutions in the form of modules. In this paper, we apply for the first time, CGP and ECGP to the well known Lawnmower problem and to the Hierarchical-if-and-Only-if problem. The latter is normally associated with Genetic Algorithms. Computational effort figures are calculated from the results of both CGP and ECGP and our results compare favourably with other techniques.|James Alfred Walker,Julian Francis Miller","57623|GECCO|2006|Simulated annealing for improving software quality prediction|In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions.We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available expert and it compares favorably with the expert build via a customized genetic algorithm.|Salah Bouktif,Houari A. Sahraoui,Giuliano Antoniol","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm","57675|GECCO|2006|A tree-based genetic algorithm for building rectilinear Steiner arborescences|A rectilinear Steiner arborescence (RSA) is a tree, whose nodes include a prescribed set of points, termed the vertices, in the first quadrant of the Cartesian plane, and whose tree edges from parent to child nodes must head either straight to the right or straight above. A minimal RSA (a MRSA) is one for which the total path length of the edges in the tree is minimal. RSAs have application in VLSI design. Curiously, although a RSA is a tree, to our knowledge, previous genetic attacks on the MRSA problem have not used tree-based approaches to representation, nor to the operations of crossover and mutation. We show why some care is needed in the choice of such genetic operators. Then we present tree-based operators for crossover and mutation, which are successful in creating true RSAs from source RSAs without the need of repair steps. We compare our results to two earlier researches, and find that our approach gives good results, but not results that are consistently better than those earlier approaches.|William A. Greene"],["57775|GECCO|2006|The parallel Nash Memory for asymmetric games|Coevolutionary algorithms search for test cases as part of the search process. The resulting adaptive evaluation function takes away the need to define a fixed evaluation function, but may also be unstable and thereby prevent reliable progress. Recent work in coevolution has therefore focused on algorithms that guarantee progress with respect to a given solution concept. The Nash Memory archive guarantees monotonicity with respect to the game-theoretic solution concept of the Nash equilibrium, but is limited to symmetric games. We present an extension of the Nash Memory that guarantees monotonicity for asymmetric games. The Parallel Nash Memory is demonstrated in experiments, and its performance on general sum games is discussed.|Frans A. Oliehoek,Edwin D. de Jong,Nikos A. Vlassis","65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","57636|GECCO|2006|A specification-based fitness function for evolutionary testing of object-oriented programs|Encapsulation of states in object-oriented programs hinders the search for test data using evolutionary testing. As client code is oblivious to the internal state of a server object, no guidance is available to test the client code using evolutionary testing i.e., it is difficult to determine the fitness or goodness of test data, as it may depend on the hidden internal state. Nevertheless, evolutionary testing is a promising new approach of which effectiveness has been shown by several researchers. We propose a specification-based fitness function for evolutionary testing of object-oriented programs. Our approach is modular in that fitness value calculation doesn't depend on source code of server classes, thus it works even if the server implementation is changed or no code is available----which is frequently the case for reusable object-oriented class libraries and frameworks.|Yoonsik Cheon,Myoung Kim","57842|GECCO|2006|Pairwise sequence comparison for fitness evaluation in evolutionary structural software testing|Evolutionary algorithms are among the metaheuristic search methods that have been applied to the structural test data generation problem. Fitness evaluation methods play an important role in the performance of evolutionary algorithms and various methods have been devised for this problem. In this paper, we propose a new fitness evaluation method based on pairwise sequence comparison also used in bioinformatics. Our preliminary study shows that this method is easy to implement and produces promising results.|H. Turgut Uyar,A. Sima Etaner-Uyar,A. Emre Harmanci","57836|GECCO|2006|Improving evolutionary real-time testing|Embedded systems are often used in a safety-critical context, e.g. in airborne or vehicle systems. Typically, timing constraints must be satisfied so that real-time embedded systems work properly and safely. Execution time testing involves finding the best and worst case execution times to determine if timing constraints are respected. Evolutionary real-time testing (ERTT) is used to dynamically search for the extreme execution times. It can be shown that ERTT outperforms the traditional methods based on static analysis. However, during the evolutionary search, some parts of the source code are never accessed. Moreover, it turns out that ERTT delivers different extreme execution times in a high number of generations for the same test object, the results are neither reliable nor efficient. We propose a new approach to ERTT which makes use of seeding the evolutionary algorithm with test data achieving a high structural coverage. Using such test data ensures a comprehensive exploration of the search space and leads to rise the confidence in the results. We present also another improvement method based on restricting the range of the input variables in the initial population in order to reduce the search space. Experiments with these approaches demonstrate an increase of reliability in terms of constant extreme execution times and a gain in efficiency in terms of number of generations needed.|Marouane Tlili,Stefan Wappler,Harmen Sthamer","57853|GECCO|2006|Use of statistical outlier detection method in adaptive evolutionary algorithms|In this paper, the issue of adapting probabilities for Evolutionary Algorithm (EA) search operators is revisited. A framework is devised for distinguishing between measurements of performance and the interpretation of those measurements for purposes of adaptation. Several examples of measurements and statistical interpretations are provided. Probability value adaptation is tested using an EA with  search operators against  test problems with results indicating that both the type of measurement and its statistical interpretation play significant roles in EA performance. We also find that selecting operators based on the prevalence of outliers rather than on average performance is able to provide considerable improvements to adaptive methods and soundly outperforms the non-adaptive case.|James M. Whitacre,Q. Tuan Pham,Ruhul A. Sarker","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","57851|GECCO|2006|Evolutionary unit testing of object-oriented software using strongly-typed genetic programming|Evolutionary algorithms have successfully been applied to software testing. Not only approaches that search for numeric test data for procedural test objects have been investigated, but also techniques for automatically generating test programs that represent object-oriented unit test cases. Compared to numeric test data, test programs optimized for object-oriented unit testing are more complex. Method call sequences that realize interesting test scenarios must be evolved. An arbitrary method call sequence is not necessarily feasible due to call dependences which exist among the methods that potentially appear in a method call sequence. The approach presented in this paper relies on a tree-based representation of method call sequences by which sequence feasibility is preserved throughout the entire search process. In contrast to other approaches in this area, neither repair of individuals nor penalty mechanisms are required. Strongly-typed genetic programming is employed to generate method call trees. In order to deal with runtime exceptions, we use an extended distance-based fitness function. We performed experiments with four test objects. The initial results are promising high code coverages were achieved completely automatically for all of the test objects.|Stefan Wappler,Joachim Wegener","57738|GECCO|2006|An empirical investigation of how and why neutrality affects evolutionary search|The effects of neutrality on evolutionary search have been considered in a number of studies, the results of which, however, have been contradictory. Some have found neutrality to be beneficial to aid evolution whereas others have argued that neutrality in the evolutionary process is useless. We believe that this confusion is due to several reasons many studies have based their conclusions on performance statistics rather than a more in-depth analysis of population dynamics, studies often consider problems, representations and search algorithms that are relatively complex and so results represent the compositions of multiple effects, there is not a single definition of neutrality and different studies have added neutrality to problems in radically different ways. In this paper, we try to shed some light on neutrality by addressing these problems. That is, we use the simplest possible definition of neutrality (a neutral network of constant fitness, identically distributed in the whole search space), we consider one of the simplest possible algorithms (a mutation based, binary genetic algorithm) applied to two simple problems (a unimodal landscape and a deceptive landscape), and analyse both performance figures and, critically, population flows from and to the neutral network and the basins of attraction of the optima.|Edgar Galván López,Riccardo Poli","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"],["57684|GECCO|2006|Dynamic multi-objective optimization with evolutionary algorithms a forward-looking approach|This work describes a forward-looking approach for the solution of dynamic (time-changing) problems using evolutionary algorithms. The main idea of the proposed method is to combine a forecasting technique with an evolutionary algorithm. The location, in variable space, of the optimal solution (or of the Pareto optimal set in multi-objective problems) is estimated using a forecasting method. Then, using this forecast, an anticipatory group of individuals is placed on and near the estimated location of the next optimum. This prediction set is used to seed the population when a change in the objective landscape arrives, aiming at a faster convergence to the new global optimum. The forecasting model is created using the sequence of prior optimum locations, from which an estimate for the next location is extrapolated. Conceptually this approach encompasses advantages of memory methods by making use of information available from previous time steps. Combined with a convergencediversity balance mechanism it creates a robust algorithm for dynamic optimization. This strategy can be applied to single objective and multi-objective problems, however in this work it is tested on multi-objective problems. Initial results indicate that the approach improves algorithm performance, especially in problems where the frequency of objective change is high.|Iason Hatzakis,David Wallace","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57729|GECCO|2006|A new multi-objective evolutionary algorithm for solving high complex multi-objective problems|In this paper, a new multi-objective evolutionary algorithm for solving high complex multi-objective problems is presented based on the rule of energy minimizing and the law of entropy increasing of particle systems in phase space, Through the experiments it proves that this algorithm can quickly obtains the Pareto solutions with high precision and uniform distribution. And the results of the experiments show that this algorithm can avoid the premature phenomenon of problems better than the traditional evolutionary algorithm because it can drive all the individuals to participate in the evolving operation in each generation.|Kangshun Li,Xuezhi Yue,Lishan Kang,Zhangxin Chen","57648|GECCO|2006|Towards estimating nadir objective vector using evolutionary approaches|Nadir point plays an important role in multi-objective optimization because of its importance in estimating the range of objective values corresponding to desired Pareto-optimal solutions and also in using many classical interactive optimization techniques. Since this point corresponds to the worst Pareto-optimal solution of each objective, the task of estimating the nadir point necessitates information about the whole Pareto optimal frontier and is reported to be a difficult task using classical means. In this paper, for the first time, we have proposed a couple of modifications to an existing evolutionary multi-objective optimization procedure to focus its search towards the extreme objective values front-wise. On up to -objective optimization problems, both proposed procedures are found to be capable of finding a near nadir point quickly and reliably. Simulation results are interesting and should encourage further studies and applications in estimating the nadir point, a process which should lead to a better interactive procedure of finding and arriving at a desired Pareto-optimal solution.|Kalyanmoy Deb,Shamik Chaudhuri,Kaisa Miettinen","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan","57843|GECCO|2006|Multiobjective evolutionary optimization for visual data mining with virtual reality spaces application to Alzheimer gene expressions|This paper introduces a multi-objective optimization approach to the problem of computing virtual reality spaces for the visual representation of relational structures (e.g. databases), symbolic knowledge and others, in the context of visual data mining and knowledge discovery. Procedures based on evolutionary computation are discussed. In particular, the NSGA-II algorithm is used as a framework for an instance of this methodology simultaneously minimizing Sammon's error for dissimilarity measures, and mean cross-validation error on a k-nn pattern classifier. The proposed approach is illustrated with an example from genomics (in particular, Alzheimer's disease) by constructing virtual reality spaces resulting from multi-objective optimization. Selected solutions along the Pareto front approximation are used as nonlinearly transformed features for new spaces that compromise similarity structure preservation (from an unsupervised perspective) and class separability (from a supervised pattern recognition perspective), simultaneously. The possibility of spanning a range of solutions between these two important goals, is a benefit for the knowledge discovery and data understanding process. The quality of the set of discovered solutions is superior to the ones obtained separately, from the point ofview of visual data mining.|Julio J. Valdés,Alan J. Barton","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"],["80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","80723|VLDB|2006|Crimson A Data Management System to Support Evaluating Phylogenetic Tree Reconstruction Algorithms|Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a \"gold standard\" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.|Yifeng Zheng,Stephen Fisher,Shirley Cohen,Sheng Guo,Junhyong Kim,Susan B. Davidson","80699|VLDB|2006|GORDIAN Efficient and Scalable Discovery of Composite Keys|Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets.|Yannis Sismanis,Paul Brown,Peter J. Haas,Berthold Reinwald","80599|VLDB|2006|FIX Feature-based Indexing Technique for XML Documents|Indexing large XML databases is crucial for efficient evaluation of XML twig queries. In this paper, we propose a feature-based indexing technique, called FIX, based on spectral graph theory. The basic idea is that for each twig pattern in a collection of XML documents, we calculate a vector of features based on its structural properties. These features are used as keys for the patterns and stored in a B+tree. Given an XPath query, its feature vector is first calculated and looked up in the index. Then a further refinement phase is performed to fetch the final results. We experimentally study the indexing technique over both synthetic and real data sets. Our experiments show that FIX provides great pruning power and could gain an order of magnitude performance improvement for many XPath queries over existing evaluation techniques.|Ning Zhang 0002,M. Tamer \u2013zsu,Ihab F. Ilyas,Ashraf Aboulnaga","80638|VLDB|2006|iDM A Unified and Versatile Data Model for Personal Dataspace Management|Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML . Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace  of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) , , . This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles","80645|VLDB|2006|Efficient Discovery of XML Data Redundancies|As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often \"casually designed\" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.|Cong Yu,H. V. Jagadish","80644|VLDB|2006|SMOQE A System for Providing Secure Access to XML|XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.|Wenfei Fan,Floris Geerts,Xibei Jia,Anastasios Kementsietsidis","80611|VLDB|2006|On the Path to Efficient XML Queries|XQuery and SQLXML are powerful new languages for querying XML data. However, they contain a number of stumbling blocks that users need to be aware of to get the expected results and performance. For example, certain language features make it hard if not impossible to exploit XML indexes.The major database vendors provide XQuery and SQLXML support in their current or upcoming product releases. In this paper, we identify common pitfalls gleaned from the experiences of early adopters of this functionality. We illustrate these pitfalls through concrete examples, explain the unexpected query behavior, and show alternative formulations of the queries that behave and perform as anticipated. As results we provide guidelines for XQuery and SQLXML users, feedback on the language standards, and food for thought for emerging languages and APIs.|Andrey Balmin,Kevin S. Beyer,Fatma \u2013zcan,Matthias Nicola","80701|VLDB|2006|AQAX A System for Approximate XML Query Answers|On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.|Joshua Spiegel,Emmanuel D. Pontikakis,Suratna Budalakoti,Neoklis Polyzotis","80619|VLDB|2006|Type-Based XML Projection|XML data projection (or pruning) is one of the main optimization techniques recently adopted in the context of main-memory XML query-engines. The underlying idea is quite simple given a query Q over a document D, the subtrees of D not necessary to evaluate Q are pruned, thus obtaining a smaller document D'. Then Q is executed over D', hence avoiding to allocate and process nodes that will never be reached by navigational specifications in Q.In this article, we propose a new approach, based on types, that greatly improves current solutions. Besides providing comparable or greater precision and far lesser pruning overhead our solution, unlike current approaches, takes into account backward axes, predicates, and can be applied to multiple queries rather than just to single ones. A side contribution is a new type system for XPath able to handle backward axes, which we devise in order to apply our solution.The soundness of our approach is formally proved. Furthermore, we prove that the approach is also complete (i.e., yields the best possible type-driven pruning) for a relevant class of queries and DTDs, which include nearly all the queries used in the XMark and XPathMark benchmarks. These benchmarks are also used to test our implementation and show and gauge the practical benefits of our solution.|Véronique Benzaken,Giuseppe Castagna,Dario Colazzo,Kim Nguyen"],["65869|AAAI|2006|Identification and Evaluation of Weak Community Structures in Networks|Identifying intrinsic structures in large networks is a fundamental problem in many fields, such as engineering, social science and biology. In this paper, we are concerned with communities, which are densely connected sub-graphs in a network, and address two critical issues for finding community structures from large experimental data. First, most existing network clustering methods assume sparse networks and networks with strong community structures. In contrast, we consider sparse and dense networks with weak community structures. We introduce a set of simple operations that capture local neighborhood information of a node to identify weak communities. Second, we consider the issue of automatically determining the most appropriate number of communities, a crucial problem for all clustering methods. This requires to properly evaluate the quality of community structures. Built atop a function for network cluster evaluation by Newman and Girvan, we extend their work to weighted graphs. We have evaluated our methods on many networks of known structures, and applied them to analyze a collaboration network and a genetic network. The results showed that our methods can find superb community structures and correct numbers of communities. Comparing to the existing approaches, our methods performed significantly better on networks with weak community structures and equally well on networks with strong community structures.|Jianhua Ruan,Weixiong Zhang","65694|AAAI|2006|Inexact Matching of Ontology Graphs Using Expectation-Maximization|We present a new method for mapping ontology schemas that address similar domains. The problem of ontology mapping is crucial since we are witnessing a decentralized development and publication of ontological data. We formulate the problem of inferring a match between two ontologies as a maximum likelihood problem, and solve it using the technique of expectation-maximization (EM). Specifically, we adopt directed graphs as our model for ontologies and use a generalized version of EM to arrive at a mapping between the nodes of the graphs. We exploit the structural and lexical similarity between the graphs, and improve on previous approaches by generating a many-one correspondence between the concept nodes. We provide preliminary experimental results in support of our method and outline its limitations.|Prashant Doshi,Christopher Thomas","65827|AAAI|2006|Corpus-based and Knowledge-based Measures of Text Semantic Similarity|This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to % error rate reduction with respect to the traditional vector-based similarity metric.|Rada Mihalcea,Courtney Corley,Carlo Strapparava","65738|AAAI|2006|Object Boundary Detection in Images using a Semantic Ontology|We present a novel method for detecting the boundaries between objects in images that uses a large, hierarchical, semantic ontology - WordNet. The semantic object hierarchy in WordNet grounds this ill-posed segmentation problem, so that true boundaries are defined as edges between instances of different classes, and all other edges are clutter. To avoid fully classifying each pixel, which is very difficult in generic images, we evaluate the semantic similarity of the two regions bounding each edge in an initial oversegmentation. Semantic similarity is computed using WordNet enhanced with appearance information, and is largely orthogonal to visual similarity. Hence two regions with very similar visual attributes, but from different categories, can have a large semantic distance and therefore evidence of a strong boundary between them, and vice versa. The ontology is trained with images from the UC Berkeley image segmentation benchmark, extended with manual labeling of the semantic content of each image segment. Results on boundary detection against the benchmark images show that semantic similarity computed through WordNet can significantly improve boundary detection compared to generic segmentation.|Anthony Hoogs,Roderic Collins","80720|VLDB|2006|Using High Dimensional Indexes to Support Relevance Feedback Based Interactive Images Retrival|Image retrieval has found more and more applications. Due to the well recognized semantic gap problem, the accuracy and the recall of image similarity search are often still low. As an effective method to improve the quality of image retrieval, the relevance feedback approach actively applies users' feedback to refine the search. As searching a large image database is often costly, to improve the efficiency, high dimensional indexes may help. However, many existing database indexes are not adaptive to updates of distance measures caused by users' feedback. In this paper, we propose a demo to illustrate the relevance feedback based interactive images retrieval procedure, and examine the effectiveness and the efficiency of various indexes. Particularly, audience can interactively investigate the effect of updated distance measures on the data space where the images are supposed to be indexed, and on the distributions of the similar images in the indexes. We also introduce our new B+-tree-like index method based on cluster splitting and iDistance.|Junqi Zhang,Xiangdong Zhou,Wei Wang 0009,Baile Shi,Jian Pei","80708|VLDB|2006|Similarity Search A Matching Based Approach|Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks ) it leaves many partial similarities uncovered ) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper.The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved, which is especially useful for information retrieval from multiple systems. We can also apply the proposed algorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that ) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities ) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.|Anthony K. H. Tung,Rui Zhang 0003,Nick Koudas,Beng Chin Ooi","65793|AAAI|2006|Semi-supervised Multi-label Learning by Constrained Non-negative Matrix Factorization|We present a novel framework for multi-label learning that explicitly addresses the challenge arising from the large number of classes and a small size of training data. The key assumption behind this work is that two examples tend to have large overlap in their assigned class memberships if they share high similarity in their input patterns. We capitalize this assumption by first computing two sets of similarities, one based on the input patterns of examples, and the other based on the class memberships of the examples. We then search for the optimal assignment of class memberships to the unlabeled data that minimizes the difference between these two sets of similarities. The optimization problem is formulated as a constrained Non-negative Matrix Factorization (NMF) problem, and an algorithm is presented to efficiently find the solution. Compared to the existing approaches for multi-label learning, the proposed approach is advantageous in that it is able to explore both the unlabeled data and the correlation among different classes simultaneously. Experiments with text categorization show that our approach performs significantly better than several state-of-the-art classification techniques when the number of classes is large and the size of training data is small.|Yi Liu,Rong Jin,Liu Yang","65807|AAAI|2006|Bookmark Hierarchies and Collaborative Recommendation|GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely. The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web. GiveALink does not use tags, content, or links in the submitted Web pages. Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users. In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network. Search results can be personalized using the bookmarks submitted by a user. We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.|Benjamin Markines,Lubomira Stoilova,Filippo Menczer","80666|VLDB|2006|LBKeogh Supports Exact Indexing of Shapes under Rotation Invariance with Arbitrary Representations and Distance Measures|The matching of two-dimensional shapes is an important problem with applications in domains as diverse as biometrics, industry, medicine and anthropology. The distance measure used must be invariant to many distortions, including scale, offset, noise, partial occlusion, etc. Most of these distortions are relatively easy to handle, either in the representation of the data or in the similarity measure used. However rotation invariance seems to be uniquely difficult. Current approaches typically try to achieve rotation invariance in the representation of the data, at the expense of discrimination ability, or in the distance measure, at the expense of efficiency. In this work we show that we can take the slow but accurate approaches and dramatically speed them up. On real world problems our technique can take current approaches and make them four orders of magnitude faster, without false dismissals. Moreover, our technique can be used with any of the dozens of existing shape representations and with all the most popular distance measures including Euclidean distance, Dynamic Time Warping and Longest Common Subsequence.|Eamonn J. Keogh,Li Wei,Xiaopeng Xi,Sang-Hee Lee,Michail Vlachos","80709|VLDB|2006|Reference-based Indexing of Sequence Databases|We consider the problem of similarity search in a very large sequence database with edit distance as the similarity measure. Given limited main memory, our goal is to develop a reference-based index that reduces the number of costly edit distance computations in order to answer a query. The idea in reference-based indexing is to select a small set of reference sequences that serve as a surrogate for the other sequences in the database. We consider two novel strategies for selecting references as well as a new strategy for assigning references to database sequences. Our experimental results show that our selection and assignment methods far outperform competitive methods. For example, our methods prune up to  times as many sequences as the Omni method, and as many as  times as many sequences as frequency vectors. Our methods also scale nicely for databases containing many andor very long sequences.|Jayendra Venkateswaran,Deepak Lachwani,Tamer Kahveci,Christopher M. Jermaine"],["80626|VLDB|2006|To Tune or not to Tune A Lightweight Physical Design Alerter|In recent years there has been considerable research on automating the physical design in database systems. Current techniques provide good recommendations, but are resource intensive. This makes DBAs somewhat conservative when deciding to launch a resource-intensive tuning session. In this paper, we introduce an alerter that helps determining when a physical design tool should be invoked. The alerter is a lightweight mechanism that provides guaranteed lower (and upper bounds) on the improvement that a DBA could expect by invoking a comprehensive physical design tool. Moreover, it produces an accompanying recommendation that serves as a \"proof\" for the lower bound. We show experimentally that the alerter handles large workloads with little overhead, and help judiciously decide on launching subsequent tuning sessions.|Nicolas Bruno,Surajit Chaudhuri","80700|VLDB|2006|IT Policy Leading to u-Korea|With the strategic early adoption of IT infrastructure technologies such as the broadband and the CDMA wireless communication in Korea, IT industry has been the major contributor to the recent Korean economic growth, accounting for .% of the real GDP in .In , Korean Ministry of Information and Communication has established the so-called IT strategy as its new IT initiative. IT means  new IT services which will be deployed within the next three to four years so that service operators invest on  new wireless broadband and secure communication infrastructures to offer high-quality ubiquitous service. For rich user experience,  hardware and software component industries are defined as the growth engine. The total twenty industry sectors of the IT strategy form the IT industry value chain.In , Korean Ministry of Information and Communication updated its IT structure by replacing some of its components to explicitly address the u-Korea framework.This IT strategy will contribute not only to IT industry but bring qualitative changes to the economic and social paradigm. It ultimately aims to realize a ubiquitous world by forming a virtuous circle of developing new services, infrastructure, and growth engines.To sustain the current momentum, Korea must become proactive in the global collaboration. Korean Ministry of Information and Communication invests in drawing leading global IT companies into Korea for cooperation with Korea R&D partners. During the past two years, twelve global companies established local R&D laboratories in Korea with partial funding from Korean government.|Jung-Hee Song","80683|VLDB|2006|Continuous Nearest Neighbor Monitoring in Road Networks|Recent research has focused on continuous monitoring of nearest neighbors (NN) in highly dynamic scenarios, where the queries and the data objects move frequently and arbitrarily. All existing methods, however, assume the Euclidean distance metric. In this paper we study k-NN monitoring in road networks, where the distance between a query and a data object is determined by the length of the shortest path connecting them. We propose two methods that can handle arbitrary object and query moving patterns, as well as fluctuations of edge weights. The first one maintains the query results by processing only updates that may invalidate the current NN sets. The second method follows the shared execution paradigm to reduce the processing time. In particular, it groups together the queries that fall in the path between two consecutive intersections in the network, and produces their results by monitoring the NN sets of these intersections. We experimentally verify the applicability of the proposed techniques to continuous monitoring of large data and query sets.|Kyriakos Mouratidis,Man Lung Yiu,Dimitris Papadias,Nikos Mamoulis","80669|VLDB|2006|myPortal Robust Extraction and Aggregation of Web Content|We demonstrate myPortal - an application for web content block extraction and aggregation. The research issues behind the tool are also explained, with an emphasis on robustness of web content extraction.|Marek Kowalkiewicz,Tomasz Kaczmarek,Witold Abramowicz","80633|VLDB|2006|Debugging Schema Mappings with Routes|A schema mapping is a high-level declarative specification of the relationship between two schemas it specifies how data structured under one schema, called the source schema, is to be converted into data structured under a possibly different schema, called the target schema. Schema mappings are fundamental components for both data exchange and data integration. To date, a language for specifying (or programming) schema mappings exists. However, developmental support for programming schema mappings is still lacking. In particular, a tool for debugging schema mappings has not yet been developed. In this paper, we propose to build a debugger for understanding and exploring schema mappings. We present a primary feature of our debugger, called routes, that describes the relationship between source and target data with the schema mapping. We present two algorithms for computing all routes or one route for selected target data. Both algorithms execute in polynomial time in the size of the input. In computing all routes, our algorithm produces a concise representation that factors common steps in the routes. Furthermore, every minimal route for the selected data can, essentially, be found in this representation. Our second algorithm is able to produce one route fast, if there is one, and alternative routes as needed. We demonstrate the feasibility of our route algorithms through a set of experimental results on both synthetic and real datasets.|Laura Chiticariu,Wang Chiew Tan","65866|AAAI|2006|A Fast Decision Tree Learning Algorithm|There is growing interest in scaling up the widely-used decision-tree learning algorithms to very large data sets. Although numerous diverse techniques have been proposed, a fast tree-growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential. In this paper, we present a novel, fast decision-tree learning algorithm that is based on a conditional independence assumption. The new algorithm has a time complexity of O(m  n), where m is the size of the training data and n is the number of attributes. This is a significant asymptotic improvement over the time complexity O(m  n) of the standard decision-tree learning algorithm C., with an additional space increase of only O(n). Experiments show that our algorithm performs competitively with C. in accuracy on a large number of UCI benchmark data sets, and performs even better and significantly faster than C. on a large number of text classification data sets. The time complexity of our algorithm is as low as naive Bayes'. Indeed, it is as fast as naive Bayes but outperforms naive Bayes in accuracy according to our experiments. Our algorithm is a core tree-growing algorithm that can be combined with other scaling-up techniques to achieve further speedup.|Jiang Su,Harry Zhang","80610|VLDB|2006|An Incrementally Maintainable Index for Approximate Lookups in Hierarchical Data|Several recent papers argue for approximate lookups in hierarchical data and propose index structures that support approximate searches in large sets of hierarchical data. These index structures must be updated if the underlying data changes. Since the performance of a full index reconstruction is prohibitive, the index must be updated incrementally.We propose a persistent and incrementally maintainable index for approximate lookups in hierarchical data. The index is based on small tree patterns, called pq-grams. It supports efficient updates in response to structure and value changes in hierarchical data and is based on the log of tree edit operations. We prove the correctness of the incremental maintenance for sequences of edit operations. Our algorithms identify a small set of pq-grams that must be updated to maintain the index. The experimental results with synthetic and real data confirm the scalability of our approach.|Nikolaus Augsten,Michael H. Böhlen,Johann Gamper","57721|GECCO|2006|Genetic algorithms for action set selection across domains a demonstration|Action set selection in Markov Decision Processes (MDPs) is an area of research that has received little attention. On the other hand, the set of actions available to an MDP agent can have a significant impact on the ability of the agent to gain optimal rewards. Last year at GECCO', the first automated action set selection tool powered by genetic algorithms was presented. The demonstration of its capabilities, though intriguing, was limited to a single domain. In this paper, we apply the tool to a more challenging problem of oil sand image interpretation. In the new experiments, genetic algorithms evolved a compact high-performance set of image processing operators, decreasing interpretation time by % while improving image interpretation accuracy by %. These results exceed the original performance and suggest certain cross-domain portability of the approach.|Greg Lee,Vadim Bulitko","80627|VLDB|2006|Globalization Challenges to Database Community|Globalization is flattening the world. As database researcher, we are proud that information technology is a critical enabler of globalization. At the same time, we are seeing that research and development of information technology is also being globalized.In recent years, many R&D labs were established in Asia, especially, in India and China, by global IT companies. Some of our colleagues have moved with globalization to establish new labs or to lead R&D in newly established labs. For those who have not moved physically, it is common to work with colleagues at remote labs with time difference.The objective of this panel is to invite pioneers leading R&D globalization and to share their vision and challenges, and to discuss how globalization impacts the future of database and information management research, education, and industry.|Sang Kyun Cha,P. Anandan,Meichun Hsu,C. Mohan,Rajeev Rastogi,Vishal Sikka,Honesty C. Young","80639|VLDB|2006|Answering Top-k Queries Using Views|The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.|Gautam Das,Dimitrios Gunopulos,Nick Koudas,Dimitris Tsirogiannis"],["65680|AAAI|2006|Building Explainable Artificial Intelligence Systems|As artificial intelligence (AI) systems and behavior models in military simulations become increasingly complex, it has been difficult for users to understand the activities of computer-controlled entities. Prototype explanation systems have been added to simulators, but designers have not heeded the lessons learned from work in explaining expert system behavior. These new explanation systems are not modular and not portable they are tied to a particular AI system. In this paper, we present a modular and generic architecture for explaining the behavior of simulated entities. We describe its application to the Virtual Humans, a simulation designed to teach soft skills such as negotiation and cultural awareness.|Mark G. Core,H. Chad Lane,Michael van Lent,Dave Gomboc,Steve Solomon,Milton Rosenberg","65840|AAAI|2006|Approximate Compilation for Embedded Model-based Reasoning|The use of embedded technology has become widespread. Many complex engineered systems comprise embedded features to perform self-diagnosis or self-reconfiguration. These features require fast response times in order to be useful in domains where embedded systems are typically deployed. Researchers often advocate the use of compilation-based approaches to store the set of environments (resp. solutions) to a diagnosis (resp. reconfiguration) problem, in some compact representation. However, the size of a compiled representation may be exponential in the treewidth of the problem. In this paper we propose a novel method for compiling the most preferred environments in order to reduce the large space requirements of our compiled representation. We show that approximate compilation is an effective means of generating the highest-valued environments, while obtaining a representation whose size can be tailored to any embedded application. The method also provides a graceful way to tradeoff space requirements with the completeness of our coverage of the environment space.|Barry O'Sullivan,Gregory M. Provan","80635|VLDB|2006|Foundations of Automated Database Tuning|Our society is more dependent on information systems than ever before. However, managing the information systems infrastructure in a cost-effective manner is a growing challenge. The total cost of ownership (TCO) of information technology is increasingly dominated by people costs. In fact, mistakes in operations and administration of information systems are the single most reasons for system outage and unacceptable performance. For information systems to provide value to their customers, we must reduce the complexity associated with their deployment and usage.|Surajit Chaudhuri,Gerhard Weikum","57768|GECCO|2006|An anticipatory approach to improve XCSF|XCSF is a novel version of learning classifier systems (LCS) which extends the typical concept of LCS by introducing computable classifier prediction. In XCSF Classifier prediction is computed as a linear combination of classifier inputs and a weight vector associated to each classifier. Learning process takes place using a weight update mechanism. Initial results show that XCSF can be used to evolve accurate approximations of some functions. In this paper, we try to add an anticipatory component to XCSF improving its performance.|Amin Nikanjam,Adel Torkaman Rahmani","65706|AAAI|2006|A Two-Step Hierarchical Algorithm for Model-Based Diagnosis|For many large systems the computational complexity of complete model-based diagnosis is prohibitive. In this paper we investigate the speedup of the diagnosis process by exploiting the hierarchylocality as is typically present in well-engineered systems. The approach comprises a compile-time and a run-time step. In the first step, a hierarchical CNF representation of the system is compiled to hierarchical DNF of adjustable hierarchical depth. In the second step, the diagnoses are computed from the hierarchical DNF and the actual observations. Our hierarchical algorithm, while sound and complete, allows large models to be diagnosed, where compiletime investment directly translates to run-time speedup. The benefits of our approach are illustrated by using weak-fault models of real-world systems, including the ISCAS- combinatorial circuits. Even for these non-optimally partitioned problems the speedup compared to traditional approaches ranges in the hundreds.|Alexander Feldman,Arjan J. C. van Gemund","80634|VLDB|2006|Entirely Declarative Sensor Network Systems|The database and sensor network community have both recognized the utility of SQL for interfacing with sensor network systems. Recently there have been proposals to construct Internet protocols declaratively in variants of Datalog. We take these ideas to their logical extreme, and demonstrate entire distributed sensor network systems built declaratively. Our demo exposes the rapidity, flexibility, and efficiency of our approach by building several fully-functional yet widely-varying sensor network applications and services declaratively. As a result of our declarative construction, we are able to highlight a wealth of previously underexposed similarities between sensor networks and database concepts. In addition, we tackle many database systems challenges in building multiple layers of a declarative database for an embedded, distributed system.|David Chu,Arsalan Tavakoli,Lucian Popa 0002,Joseph M. Hellerstein","57777|GECCO|2006|Bounding XCSs parameters for unbalanced datasets|This paper analyzes the behavior of the XCS classifier system on imbalanced datasets. We show that XCS with standard parameter settings is quite robust to considerable class imbalances. For high class imbalances, XCS suffers from biases toward the majority class. We analyze XCS's behavior under such extreme imbalances and prove that appropriate parameter tuning improves significantly XCS's performance. Specifically, we counterbalance the imbalance ratio by equalizing the reproduction probabilities of the most occurring and least occurring niches. The study provides guidelines to tune XCS's parameters for unbalanced datasets, based on the dataset imbalance ratio. We propose a method to estimate the imbalance ratio during XCS's training and adapt XCS's parameters online.|Albert Orriols-Puig,Ester Bernadó-Mansilla","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","65766|AAAI|2006|Lessons on Applying Automated Recommender Systems to Information-Seeking Tasks|Automated recommender systems predict user preferences by applying machine learning techniques to data on products, users, and past user preferences for products. Such systems have become increasingly popular in entertainment and e-commerce domains, but have thus far had little success in information-seeking domains such as identifying published research of interest. We report on several recent publications that show how recommenders can be extended to more effectively address information-seeking tasks by expanding the focus from accurate prediction of user preferences to identifying a useful set of items to recommend in response to the user's specific information need. Specific research demonstrates the value of diversity in recommendation lists, shows how users value lists of recommendations as something different from the sum of the individual recommendations within, and presents an analytic model for customizing a recommender to match user information-seeking needs.|Joseph A. Konstan,Sean M. McNee,Cai-Nicolas Ziegler,Roberto Torres,Nishikant Kapoor,John Riedl","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"],["57629|GECCO|2006|GRASP - evolution for constraint satisfaction problems|There are several evolutionary approaches for solving random binary Constraint Satisfaction Problems (CSPs). In most of these strategies we find a complex use of information regarding the problem at hand. Here we present a hybrid Evolutionary Algorithm that outperforms previous approaches in terms of effectiveness and compares well in terms ofefficiency. Our algorithm is conceptual and simple, featuring a GRASP-like (GRASP stands for Greedy Randomized Adaptive Search Procedure) mechanism for genotype-to-phenotype mapping, and without considering any specific knowledge of the problem. Therefore, we provide a simple algorithm that harnesses generality while boosting performance.|Manuel Cebrián,Iván Dotú","65843|AAAI|2006|Strategy Variations in Analogical Problem Solving|While it is commonly agreed that analogy is useful in human problem solving, exactly how analogy can and should be used remains an intriguing problem. VanLehn () for instance argues that there are differences in how novices and experts use analogy, but the VanLehn and Jones () Cascade model does not implement these differences. This paper analyzes several variations in strategies for using analogy to explore possible sources of noviceexpert differences. We describe a series of ablation experiments on an expert model to examine the effects of strategy variations in using analogy in problem solving. We provide evidence that failing to use qualitative reasoning when encoding problems, being careless in validating analogical inferences, and not using multiple retrievals can degrade the efficiency of problem-solving.|Tom Y. Ouyang,Kenneth D. Forbus","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65702|AAAI|2006|DNNF-based Belief State Estimation|As embedded systems grow increasingly complex, there is a pressing need for diagnosing and monitoring capabilities that estimate the system state robustly. This paper is based on approaches that address the problem of robustness by reasoning over declarative models of the physical plant, represented as a variant of factored Hidden Markov Models, called Probabilistic Concurrent Constraint Automata. Prior work on Mode Estimation of PCCAs is based on a Best-First Trajectory Enumeration (BFTE) algorithm. Two algorithms have since made improvements to the BFTE algorithm ) the Best-First Belief State Update (BFBSU) algorithm has improved the accuracy of BFTE and ) the MEXEC algorithm has introduced a polynomial-time bounded algorithm using a smooth deterministic decomposable negation normal form (sd-DNNF) representation. This paper introduces a new DNNF-based Belief State Estimation (DBSE) algorithm that merges the polynomial time bound of the MEXEC algorithm with the accuracy of the BFBSU algorithm. This paper also presents an encoding of a PCCA as a CNF with probabilistic data, suitable for compilation into an sd-DNNF-based representation. The sd-DNNF representation supports computing k belief states from k previous belief states in the DBSE algorithm.|Paul Elliott,Brian C. Williams","65722|AAAI|2006|Exploiting Tree Decomposition and Soft Local Consistency In Weighted CSP|Several recent approaches for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local consistency enforcing. Graph decomposition exploits the problem structure and offers space and time complexity bounds while hard information propagation provides practical improvements of space and time behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local consistency enforcing for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency.|Simon de Givry,Thomas Schiex,Gérard Verfaillie","57609|GECCO|2006|An agent-based algorithm for generalized graph colorings|This paper presents an algorithm for solving a number of generalized graph coloring problems. Specifically, it gives an agent-based algorithm for the Bandwidth Coloring problem. Using a standard method for preprocessing the input, the same algorithm can also be used to solve the Multicoloring and Bandwidth Multicoloring problems. In the algorithm a number of agents, called ants, each of which colors a portion of the graph, collaborate to obtain a coloring of the entire graph. This coloring is then further improved by a local optimization algorithm. Experimental results on a set of benchmark graphs for these generalized coloring problems show that this algorithm performs very well compared to other heuristic approaches.|Thang Nguyen Bui,ThanhVu H. Nguyen","57845|GECCO|2006|Heterogeneous cooperative coevolution strategies of integration between GP and GA|Cooperative coevolution has proven to be a promising technique for solving complex combinatorial optimization problems. In this paper, we present four different strategies which involve cooperative coevolution of a genetic program and of a population of constants evolved by a genetic algorithm. The genetic program evolves expressions that solve a problem, while the genetic algorithm provides \"good\" values for the numeric terminal symbols used by those expressions. Experiments have been performed on three symbolic regression problems and on a \"real-world\" biomedical application. Results are encouraging and confirm that our coevolutionary algorithms can be used effectively in different domains.|Leonardo Vanneschi,Giancarlo Mauri,Andrea Valsecchi,Stefano Cagnoni","57735|GECCO|2006|On the utility of the multimodal problem generator for assessing the performance of evolutionary algorithms|This paper looks in detail at how an evolutionary algorithm attempts to solve instances from the multimodal problem generator. The paper shows that in order to consistently reach the global optimum, an evolutionary algorithm requires a population size that should grow at least linearly with the number of peaks. A close relationship is also shown between the supply and decision making issues that have been identified previously in the context of population sizing models for additively decomposable problems.The most important result of the paper, however, is that solving an instance of the multimodal problem generator is like solving a peak-in-a-haystack, and it is argued that evolutionary algorithms are not the best algorithms for such a task. Finally, and as opposed to what several researchers have been doing, it is our strong belief that the multimodal problem generator is not adequate for assessing the performance of evolutionary algorithms.|Fernando G. Lobo,Cláudio F. Lima","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57650|GECCO|2006|Innovization innovating design principles through optimization|This paper introduces a new design methodology (we call it \"innovization\") in the context of finding new and innovative design principles by means of optimization techniques. Although optimization algorithms are routinely used to find an optimal solution corresponding to an optimization problem, the task of innovization stretches the scope beyond an optimization task and attempts to unveil new, innovative, and important design principles relating to decision variables and objectives, so that a deeper understanding of the problem can be obtained. The variety of problems chosen in the paper and the resulting innovations obtained for each problem amply demonstrate the usefulness of the innovization task. The results should encourage a wide spread applicability of the proposed innovization procedure (which is not simply an optimization procedure) to other problem-solving tasks.|Kalyanmoy Deb,Aravind Srinivasan"],["65837|AAAI|2006|Sequential and Parallel Algorithms for Frontier A with Delayed Duplicate Detection|We present sequential and parallel algorithms for Frontier A* (FA*) algorithm augmented with a form of Delayed Duplicate Detection (DDD). The sequential algorithm, FA*-DDD, overcomes the leak-back problem associated with the combination of FA* and DDD. The parallel algorithm, PFA*-DDD, is a parallel version of FA*-DDD that features a novel workload distribution strategy based on intervals. We outline an implementation of PFA*-DDD designed to run on a cluster of workstations. The implementation computes intervals at run-time that are tailored to fit the workload at hand. Because the implementation distributes the workload in a manner that is both automated and adaptive, it does not require the user to specify a workload mapping function, and, more importantly, it is applicable to arbitrary problems that may be irregular. We present the results of an experimental evaluation of the implementation where it is used to solve instances of the multiple sequence alignment problem on a cluster of workstations running on top of a commodity network. Results demonstrate that the implementation offers improved capability in addition to improved performance.|Robert Niewiadomski,José Nelson Amaral,Robert C. Holte","80604|VLDB|2006|Scalable Continuous Query Processing by Tracking Hotspots|This paper considers the problem of scalably processing a large number of continuous queries. We propose a flexible framework with novel data structures and algorithms for group-processing and indexing continuous queries by exploiting potential overlaps in query predicates. Our approach partitions the collection of continuous queries into groups based on the clustering patterns of the query ranges, and then applies specialized processing strategies to those heavily-clustered groups (or hotspots). To maintain the partition dynamically, we present efficient algorithms that maintain a nearly optimal partition in nearly amortized logarithmic time. We show how to use the hotspots to scalably process large numbers of continuous select-join and band-join queries, which are much more challenging than simple range selection queries. Experiments demonstrate that this approach can improve the processing throughput by orders of magnitude. As another application of hotspots, we show how to use them to build a high-quality histogram for intervals in linear time.|Pankaj K. Agarwal,Junyi Xie,Jun Yang 0001,Hai Yu","65755|AAAI|2006|Bayesian Calibration for Monte Carlo Localization|Localization is a fundamental challenge for autonomous robotics. Although accurate and efficient techniques now exist for solving this problem, they require explicit probabilistic models of the robot's motion and sensors. These models are usually obtained from time-consuming and error-prone measurement or tedious manual tuning. In this paper we examine automatic calibration of sensor and motion models from a Bayesian perspective. We introduce an efficient MCMC procedure for sampling from the posterior distribution of the model parameters. We also present a novel extension of particle filters to make use of our posterior parameter samples. Finally, we demonstrate our approach both in simulation and on a physical robot. Our results demonstrate effective inference of model parameters as well as a paradoxical result that using posterior parameter samples can produce more accurate position estimates than the true parameters.|Armita Kaboli,Michael H. Bowling,Petr Musílek","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","65932|AAAI|2006|A Unified Knowledge Based Approach for Sense Disambiguation and Semantic Role Labeling|In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our approach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.|Peter Z. Yeh,Bruce W. Porter,Ken Barker","65793|AAAI|2006|Semi-supervised Multi-label Learning by Constrained Non-negative Matrix Factorization|We present a novel framework for multi-label learning that explicitly addresses the challenge arising from the large number of classes and a small size of training data. The key assumption behind this work is that two examples tend to have large overlap in their assigned class memberships if they share high similarity in their input patterns. We capitalize this assumption by first computing two sets of similarities, one based on the input patterns of examples, and the other based on the class memberships of the examples. We then search for the optimal assignment of class memberships to the unlabeled data that minimizes the difference between these two sets of similarities. The optimization problem is formulated as a constrained Non-negative Matrix Factorization (NMF) problem, and an algorithm is presented to efficiently find the solution. Compared to the existing approaches for multi-label learning, the proposed approach is advantageous in that it is able to explore both the unlabeled data and the correlation among different classes simultaneously. Experiments with text categorization show that our approach performs significantly better than several state-of-the-art classification techniques when the number of classes is large and the size of training data is small.|Yi Liu,Rong Jin,Liu Yang","57623|GECCO|2006|Simulated annealing for improving software quality prediction|In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions.We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available expert and it compares favorably with the expert build via a customized genetic algorithm.|Salah Bouktif,Houari A. Sahraoui,Giuliano Antoniol","65704|AAAI|2006|When a Decision Tree Learner Has Plenty of Time|The majority of the existing algorithms for learning decision trees are greedy--a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Furthermore, the greedy algorithms require a fixed amount of time and are not able to generate a better tree if additional time is available. To overcome this problem. we present a lookahead-based algorithm for anytime induction of decision trees which allows trading computational speed for tree quality. The algorithm uses a novel strategy for evaluating candidate splits a stochastic version of ID is repeatedly invoked to estimate the size of the tree in which each split results, and the split that minimizes the expected size is preferred. Experimental results indicate that for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.|Saher Esmeir,Shaul Markovitch","57811|GECCO|2006|Estimating the destructiveness of crossover on binary tree representations|In some cases, evolutionary algorithms represent individuals as typical binary trees with n leaves and n- internal nodes. When designing a crossover operator for a particular representation and application, it is desirable to quantify the operator's destructiveness in order to estimate its effectiveness at using building blocks. For the case of binary tree representations, we present a novel approach for empirically estimating the destructiveness of any crossover operator by computing and summarizing the distribution of Robinson-Foulds distances from the parent to the entire neighborhood of possible children. We demonstrate the approach by quantifying the destructiveness of a popular tree-based crossover operator as applied to the problem of phylogenetic inferencing. We discuss the benefits and limitations of the destructiveness metric.|Luke Sheneman,James A. Foster","57774|GECCO|2006|Parisian evolution with honeybees for three-dimensional reconstruction|This paper introduces a novel analogy with the way in which honeybee colonies operate in order to solve the problem of sparse and quasi dense reconstruction. To successfully solve increasingly complex problems, we must develop effective techniques for evolving cooperative solutions in the form of interacting coadapted subcomponents. A new adaptive behavior strategy is presented based on the \"divide and conquer\" approach used by the honeybee colony to solve search problems. The general ideas that explain the honeybee behavior are translated into a computational algorithm following the evolutionary computing paradigm. Experiments demonstrate the importance of the proposed communication system to reduce dramatically the number of outliers.|Gustavo Olague,Cesar Puente"]]},"title":{"entropy":6.35334449496945,"topics":["for learning, system for, reinforcement learning, neural networks, learning, learning with, classifier system, learning classifier, learning system, system, human-robot interaction, model for, for networks, for sets, sensor networks, learning and, for constraint, and system, intelligent system, language and","the problem, the, for the, evolutionary algorithms, the web, the algorithms, evolutionary for, multi-objective optimization, the and, for optimization, multi-objective algorithms, for multi-objective, particle swarm, multi-objective evolutionary, optimization algorithms, the performance, new for, evolutionary, optimization, evolutionary optimization","genetic algorithms, genetic for, algorithms for, genetic programming, and algorithms, using genetic, genetic and, for problem, for and, hybrid for, flexible scheduling, genetic problem, algorithms problem, for optimal, genetic with, the and, local search, solving problem, search for, and flexible","for data, and its, semantic web, based approach, its quality, and quality, its applications, based control, quality mutation, based mutation, control mutation, data, based its, approach and, control and, approach control, quality control, its mutation, query processing, and web","for constraint, for structure, and constraint, constraint, detection, towards, agent, distributed, clustering, improving, classification, random, coordination, image, weighted, via, belief, and, using","model for, for sets, and sets, bayesian and, knowledge and, with and, probabilistic and, efficient with, efficient for, model, knowledge, efficient, inference, text, representation, modeling, support, csp, value, adaptive","evolutionary algorithms, evolutionary for, the evolutionary, evolutionary, the population, the algorithms, evolutionary computation, evolutionary testing, assignment, space, music, comparison, sequence","the problem, for the, the web, the and, the, the algorithms, the performance, using web, analysis for, using the, memory for, for agent, analysis the, discovery the, analysis, matching, agent, schema, logic, autonomous","the and, for and, how and, and, representation and, planning and, planning, compact, simple, when, elicitation, partial, approximation, sequence, approaches","search for, for optimal, for graph, local search, algorithms graph, crossover operator, search, genetic for, genetic search, for, operator for, algorithms for, for partitioning, crossover for, state, robustness, constrained, generalized, hierarchical, maximum","for, pattern, computing, novel, ranking, description, tree, stream, integration","semantic web, semantic for, and web, the and, for web, semantic and, the semantic, for and, databases, from, user, cooperative, coevolution, continuous, feature, and"],"ranking":[["57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","57594|GECCO|2006|Smart crossover operator with multiple parents for a Pittsburgh learning classifier system|This paper proposes a new smart crossover operator for a Pittsburgh Learning Classifier System. This operator, unlike other recent LCS approaches of smart recombination, does not learn the structure of the domain, but it merges the rules of N parents (N  ) to generate a new offspring. This merge process uses an heuristic that selects the minimum subset of candidate rules that obtains maximum training accuracy. Moreover the operator also includes a rule pruning scheme to avoid the inclusion of over-specific rules, and to guarantee as much as possible the robust behaviour of the LCS. This operator takes advantage from the fact that each individual in a Pittsburgh LCS is a complete solution, and the system has a global view of the solution space that the proposed rule selection algorithm exploits. We have empirically evaluated this operator using a recent LCS called GAssist. First with the standard LCS benchmark, the  bits multiplexer, and later using  standard real datasets. The results of the experiments over these datasets indicate that the new operator manages to increase the accuracy of the system over the classical crossover in  of the  datasets, and never having a significantly worse performance than the classic operator.|Jaume Bacardit,Natalio Krasnogor","65913|AAAI|2006|Action Selection in Bayesian Reinforcement Learning|My research attempts to address on-line action selection in reinforcement learning from a Bayesian perspective. The idea is to develop more effective action selection techniques by exploiting information in a Bayesian posterior, while also selecting actions by growing an adaptive, sparse lookahead tree. I further augment the approach by considering a new value function approximation strategy for the belief-state Markov decision processes induced by Bayesian learning.|Tao Wang","57803|GECCO|2006|Reward allotment in an event-driven hybrid learning classifier system for online soccer games|This paper describes our study into the concept of using rewards in a classifier system applied to the acquisition of decision-making algorithms for agents in a soccer game. Our aim is to respond to the changing environment of video gaming that has resulted from the growth of the Internet, and to provide bug-free programs in a short time. We have already proposed a bucket brigade algorithm (a reinforcement learning method for classifiers) and a procedure for choosing what to learn depending on the frequency of events with the aim of facilitating real-time learning while a game is in progress. We have also proposed a hybrid system configuration that combines existing algorithm strategies with a classifier system, and we have reported on the effectiveness of this hybrid system. In this paper, we report on the results of performing reinforcement learning with different reward values assigned to reflect differences in the roles performed by forward, midfielder and defense players, and we describe the results obtained when learning is performed with different combinations of success rewards for various type of play such as dribbling and passing. In  matches played against an existing soccer game incorporating an algorithm devised by humans, a better win ratio and better convergence were observed compared with the case where learning was performed with no roles assigned to all of the in-game agents.|Yuji Sato,Yosuke Akatsuka,Takenori Nishizono","65717|AAAI|2006|Overview of AutoFeed An Unsupervised Learning System for Generating Webfeeds|The AutoFeed system automatically extracts data from semistructured web sites. Previously, researchers have developed two types of supervised learning approaches for extracting web data methods that create precise, site-specific extraction rules and methods that learn less-precise site-independent extraction rules. In either case, significant training is required. AutoFeed follows a third, more ambitious approach, in which unsupervised learning is used to analyze sites and discover their structure. Our method relies on a set of heterogeneous \"experts\", each of which is capable of identifying certain types of generic structure. Each expert represents its discoveries as \"hints\". Based on these hints, our system clusters the pages and identifies semi-structured data that can be extracted. To identify a good clustering, we use a probabilistic model of the hint-generation process. This paper summarizes our formulation of the fully-automatic web-extraction problem, our clustering approach, and our results on a set of experiments.|Bora Gazen,Steven Minton","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci","65909|AAAI|2006|Reinforcement Learning with Human Teachers Evidence of Feedback and Guidance with Implications for Learning Performance|As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacherrobot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance.|Andrea Lockerd Thomaz,Cynthia Breazeal"],["57684|GECCO|2006|Dynamic multi-objective optimization with evolutionary algorithms a forward-looking approach|This work describes a forward-looking approach for the solution of dynamic (time-changing) problems using evolutionary algorithms. The main idea of the proposed method is to combine a forecasting technique with an evolutionary algorithm. The location, in variable space, of the optimal solution (or of the Pareto optimal set in multi-objective problems) is estimated using a forecasting method. Then, using this forecast, an anticipatory group of individuals is placed on and near the estimated location of the next optimum. This prediction set is used to seed the population when a change in the objective landscape arrives, aiming at a faster convergence to the new global optimum. The forecasting model is created using the sequence of prior optimum locations, from which an estimate for the next location is extrapolated. Conceptually this approach encompasses advantages of memory methods by making use of information available from previous time steps. Combined with a convergencediversity balance mechanism it creates a robust algorithm for dynamic optimization. This strategy can be applied to single objective and multi-objective problems, however in this work it is tested on multi-objective problems. Initial results indicate that the approach improves algorithm performance, especially in problems where the frequency of objective change is high.|Iason Hatzakis,David Wallace","57670|GECCO|2006|On the effect of populations in evolutionary multi-objective optimization|Multi-objective evolutionary algorithms (MOEAs) have become increasingly popular as multi-objective problem solving techniques. An important open problem is to understand the role of populations in MOEAs. We present a simple bi-objective problem which emphasizes when populations are needed. Rigorous runtime analysis point out an exponential runtime gap between the population-based algorithm Simple Evolutionary Multi-objective Optimizer (SEMO) and several single individual-based algorithms on this problem. This means that among the algorithms considered, only the population-based MOEA is successful and all other algorithms fail.|Oliver Giel,Per Kristian Lehre","57816|GECCO|2006|Comparison of multi-modal optimization algorithms based on evolutionary algorithms|Many engineering optimization tasks involve finding more than one optimum solution. The present study provides a comprehensive review of the existing work done in the field of multi-modal function optimization and provides a critical analysis of the existing methods. Existing niching methods are analyzed and an improved niching method is proposed. To achieve this purpose, we first give an introduction to niching and diversity preservation, followed by discussion of a number of algorithms. Thereafter, a comparison of clearing, clustering, deterministic crowding, probabilistic crowding, restricted tournament selection, sharing, species conserving genetic algorithms is made. A modified niching-based technique -- modified clearing approach -- is introduced and also compared with existing methods. For comparison, a versatile hump test function is also proposed and used together with two other functions. The ability of the algorithms in finding, locating, and maintaining multiple optima is judged using two performance measures (i) number of peaks maintained, and (ii) computational time. Based on the results, we conclude that the restricted tournament selection and the proposed modified clearing approaches are better in terms of finding and maintaining the multiple optima.|Gulshan Singh,Kalyanmoy Deb","57621|GECCO|2006|Combining gradient techniques for numerical multi-objective evolutionary optimization|Recently, gradient techniques for solving numerical multi-objective optimization problems have appeared in the literature. Although promising results have already been obtained when combined with multi-objective evolutionary algorithms (MOEAs), an important question remains what is the best way to integrate the use of gradient techniques in the evolutionary cycle of a MOEA. In this paper, we present an adaptive resource-allocation scheme that uses three gradient techniques in addition to the variation operator in a MOEA. During optimization, the effectivity of the gradient techniques is monitored and the available computational resources are redistributed to allow the (currently) most effective operator to spend the most resources. In addition, we indicate how the multi-objective search can be stimulated to also search $mboxemphalong $ the Pareto front, ultimately resulting in a better and wider spread of solutions. We perform tests on a few well-known benchmark problems as well as two novel benchmark problems with specific gradient properties. We compare the results of our adaptive resource-allocation scheme with the same MOEA without the use of gradient techniques and a scheme in which resource allocation is constant. The results show that our proposed adaptive resource-allocation scheme makes proper use of the gradient techniques only when required and thereby leads to results that are close to the best results that can be obtained by fine-tuning the resource allocation for a specific problem.|Peter A. N. Bosman,Edwin D. de Jong","57649|GECCO|2006|Reference point based multi-objective optimization using evolutionary algorithms|Evolutionary multi-objective optimization (EMO) methodologies have been amply applied to find a representative set of Pareto-optimal solutions in the past decade and beyond. Although there are advantages of knowing the range of each objective for Pareto-optimality and the shape of the Pareto-optimal frontier itself in a problem for an adequate decision-making, the task of choosing a single preferred Pareto-optimal solution is also an important task which has received a lukewarm attention so far. In this paper, we combine one such preference based strategy with an EMO methodology and demonstrate how, instead of one solution, a preferred set solutions near the reference points can be found parallely. We propose a modified EMO procedure based on the elitist non-dominated sorting GAor NSGA-II. On two-objective to -objective optimization problems, the modified NSGA-II approach shows its efficacy in finding an adequate set of Pareto-optimal points. Such procedures will provide the decision-maker with a set of solutions near herhis preference so that a better and a more reliable decision can be made.|Kalyanmoy Deb,J. Sundar","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","57739|GECCO|2006|MOGE GP classification problem decomposition using multi-objective optimization|A novel approach to classification is proposed in which a Pareto-based ranking of individuals is used to encourage multiple individuals to participate in the solution. To do so, the classification problem is re-expressed as a cluster consistency problem, thus allowing utilization of techniques from multi-objective optimization. Such a formulation enables classification problems to be automatically decomposed and solved by several specialist classifiers rather than by a single 'super' individual. In this paper, we demonstrate the proposed approach to two benchmark binary problems and recommend a natural extension to multi-class problems. Results indicate the general appropriateness of the approach.|Andrew R. McIntyre,Malcolm I. Heywood","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57773|GECCO|2006|Comparison of multi-objective evolutionary algorithms in optimizing combinations of reinsurance contracts|Our paper concerns optimal combinations of different types of reinsurance contracts. We introduce a novel approach based on the Mean-Variance-Criterion to solve this task. Two state-of-the-art MOEAs are used to perform an optimization of yet unresolved problem instances. In addition to that, we focus on finding a dense set of solutions to derive analogies to theoretic results of easier problem instances.|Ingo Oesterreicher,Andreas Mitschele,Frank Schlottmann,Detlef Seese","57813|GECCO|2006|Dynamic fitness inheritance proportion for multi-objective particle swarm optimization|In this paper, we propose a dynamic mechanism to vary the probability by which fitness inheritance is applied throughout the run of a multi-objective particle swarm optimizer, in order to obtain a greater reduction in computational cost (than the obtained with a fixed probability), without dramatically affecting the quality of the results. The results obtained show that it is possible to reduce the computational cost by % without affecting the quality of the obtained Pareto front.|Margarita Reyes Sierra,Carlos A. Coello Coello"],["57709|GECCO|2006|Cutting stock waste reduction using genetic algorithms|A new model for the One-dimensional Cutting Stock problem using Genetic Algorithms (GA) is developed to optimize construction steel bars waste. One-dimensional construction stocks (i.e., steel rebars, steel sections, dimensional lumber, etc.) are one of the major contributors to the construction waste stream. Construction wastes account for a significant portion of municipal waste stream. Cutting one-dimensional stocks to suit needed project lengths results in trim losses, which are the main causes of one-dimensional stock wastes. The model developed and the results obtained were compared with real life case studies from local steel workshops. Cutting schedules produced by our new GA model were tested in the shop against the current cutting schedules. The comparisons show the superiority of this new GA model in terms of waste minimization.|Yaser M. A. Khalifa,O. Salem,A. Shahin","57754|GECCO|2006|Estimating photometric redshifts with genetic algorithms|Photometry is used as a cheap and easy way to estimate redshifts of galaxies, which would otherwise require considerable amounts of expensive telescope time. However, the analysis of photometric redshift datasets is a task where it is sometimes difficult to achieve a high classification accuracy. This work presents a custom Genetic Algorithm (GA) for mining the Hubble Deep Field North (HDF-N) datasets to achieve accurate IF-THEN classification rules. This kind of knowledge representation has the advantage of being intuitively comprehensible to the user, facilitating astronomers' interpretation of discovered knowledge. The GA is tested against the state of the art decision tree algorithm C.  achieving significantly better results.|Nick Miles,Alex Alves Freitas,Stephen Serjeant","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","57688|GECCO|2006|ORDERTREE a new test problem for genetic programming|In this paper, we describe a new test problem for genetic programming (GP), ORDERTREE. We argue that it is a natural analogue of ONEMAX, a popular GA test problem, and that it also avoids some of the known weaknesses of other benchmark problems for Genetic Programming. Through experiments, we show that the difficulty of the problem can be tuned not only by increasing the size of the problem, but also by increasing the non-linearity in the fitness structure.|Tuan Hao Hoang,Nguyen Xuan Hoai,Nguyen Thi Hien,Robert I. McKay,Daryl Essam","57667|GECCO|2006|Solving identification problem for asynchronous finite state machines using genetic algorithms|A Genetic Algorithm, embedded in a simulation-based method, is applied to the identification of Asynchronous Finite State Machines. Two different coding schemes and their associated crossover operations are examined. It is shown that one operator  coding pair outperforms the other in that the scheme reduces noticeably the production of invalid chromosomes thus increasing the efficiency and the convergence rate of the evolution process.|Xiaojun Geng","57628|GECCO|2006|Variable length genetic algorithms with multiple chromosomes on a variant of the Onemax problem|The dynamics of variable length representations in evolutionary computation have been shown to be complex and different from those seen in standard fixed length genetic algorithms. This paper explores a simple variable length genetic algorithm with multiple chromosomes and its underlying dynamics when used for the onemax problem. The changes in length of the chromosomes are especially observed and explanations for these fluctuations are sought.|Rachel Cavill,Stephen L. Smith,Andy M. Tyrrell","57593|GECCO|2006|Candlestick stock analysis with genetic algorithms|Candlestick analysis, a form of stock market technical analysis, is well suited for use with a genetic search algorithm. This paper explores an implementation of marrying these two techniques by creating agents that attempt to identify stocks that will change in price. The best of run individuals, produced by the genetic algorithm, performed statistically better than an agent that makes random investment decisions.|Peter Belford","57814|GECCO|2006|Anisotropic selection in cellular genetic algorithms|In this paper we introduce a new selection scheme in cellular genetic algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows accurate control of the selective pressure. First we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure we can obtain the same takeover time with the two techniques although the spreading of the best individual is different. We then give experimental results that show to what extent AS promotes the emergence of niches that support low coupling and high cohesion. Finally, using a cGA with anisotropic selection on a Quadratic Assignment Problem we show the existence of an anisotropic optimal value for which the best average performance is observed. Further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme.|David Simoncini,Sébastien Vérel,Philippe Collard,Manuel Clergue","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom"],["65868|AAAI|2006|Expressive Commerce and Its Application to Sourcing|Sourcing professionals buy several trillion dollars worth of goods and services yearly. We introduced a new paradigm called expressive commerce and applied it to sourcing. It combines the advantages of highly expressive human negotiation with the advantages of electronic reverse auctions. The idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auctions, and are algorithmically cleared. This creates a Pareto efficiency improvement in the allocation (a win-win between the buyer and the sellers) but the market clearing problem is a highly complex combinatorial optimization problem. We developed the world's fastest tree search algorithms for solving it. We have hosted $ billion of sourcing using the technology, and created $. billion of hard-dollar savings. The suppliers also benefited by being able to express production efficiencies and creativity, and through exposure problem removal. Supply networks were redesigned, with quantitative understanding of the tradeoffs, and implemented in weeks instead of months.|Tuomas Sandholm","65681|AAAI|2006|Adaptive Sampling Based Large-Scale Stochastic Resource Control|We consider closed-loop solutions to stochastic optimization problems of resource allocation type. They concern with the dynamic allocation of reusable resources over time to non-preemtive interconnected tasks with stochastic durations. The aim is to minimize the expected value of a regular performance measure. First, we formulate the problem as a stochastic shortest path problem and argue that our formulation has favorable properties, e.g., it has finite horizon, it is acyclic, thus, all policies are proper, and moreover, the space of control policies can be safely restricted. Then, we propose an iterative solution. Essentially, we apply a reinforcement learning based adaptive sampler to compute a sub-optimal control policy. We suggest several approaches to enhance this solution and make it applicable to large-scale problems. The main improvements are () the value function is maintained by feature-based support vector regression () the initial exploration is guided by rollout algorithms () the state space is partitioned by clustering the tasks while keeping the precedence constraints satisfied () the action space is decomposed and, consequently, the number of available actions in a state is decreased and, finally, () we argue that the sampling can be effectively distributed among several processors. The effectiveness of the approach is demonstrated by experimental results on both artificial (benchmark) and real-world (industry related) data.|Balázs Csanád Csáji,László Monostori","57870|GECCO|2006|Both robust computation and mutation operation in dynamic evolutionary algorithm are based on orthogonal design|A robust dynamic evolutionary algorithm (labeled RODEA), where both the robust calculation and mutation operator are based on an orthogonal design, is proposed in this paper. Previous techniques calculate the mean effective objective (for robust) by using samples without much evenly distributing over the neighborhood. The samples by using orthogonal array distribute evenly. Therefore the calculation of mean effective objective more robust. The new technique is generalized from the ODEA algorithm . An orthogonal design method is employed on the niches for the mutation operator to find a potentially good solution that may become the representative in the niche. The fitness of the offspring is therefore likely to be higher than that of its parent. We propose a complex benchmark, consisting of moving function peaks, to test our new approach. Numerical experiments show that the moving solutions of the algorithm are a little worse in objective value but robust.|Sanyou Y. Zeng,Rui Wang,Hui Shi,Guang Chen,Hugo de Garis,Lishan Kang,Lixin X. Ding","80680|VLDB|2006|IPAC - An Interactive Approach to Access Control for Semi-structured Data|We propose IPAC(Interactive aPproach to Access Control for semi-structured data), a framework for XML access constraint specification and security view selection. IPAC clearly demarcates access constraint specification, access control strategy and security mechanism (implementation). It features a declarative access constraint specification language, a global access control strategy configuration unit, and an automatic security view generation and ranking tool. IPAC is the first system that assists the DBA in specifying access control strategies and access constraints on XML data, and helps the DBA in choosing the optimal plan that implements the specified strategy and access constraints accurately and efficiently.|Sriram Mohan,Yuqing Wu","65674|AAAI|2006|An Edge Deletion Semantics for Belief Propagation and its Practical Impact on Approximation Quality|We show in this paper that the influential algorithm of iterative belief propagation can be understood in terms of exact inference on a polytree, which results from deleting enough edges from the original network. We show that deleting edges implies adding new parameters into a network, and that the iterations of belief propagation are searching for values of these new parameters which satisfy intuitive conditions that we characterize. The new semantics lead to the following question Can one improve the quality of approximations computed by belief propagation by recovering some of the deleted edges, while keeping the network easy enough for exact inference We show in this paper that the answer is yes, leading to another question How do we choose which edges to recover To answer, we propose a specific method based on mutual information which is motivated by the edge deletion semantics. Empirically, we provide experimental results showing that the quality of approximations can be improved without incurring much additional computational cost. We also show that recovering certain edges with low mutual information may not be worthwhile as they increase the computational complexity, without necessarily improving the quality of approximations.|Arthur Choi,Adnan Darwiche","65780|AAAI|2006|A Look at Parsing and Its Applications|This paper provides a brief introduction to recent work in statistical parsing and its applications. We highlight successes to date, remaining challenges, and promising future work.|Matthew Lease,Eugene Charniak,Mark Johnson,David McClosky","57653|GECCO|2006|Evolutionary design of pseudorandom sequence generators based on cellular automata and its applicability in current cryptosystems|In this work, a genetic algorithm is used to find cellular automata rules that make cellular automata behave like good pseudorandom sequence generators. Pseudorandom sequence generators based on one-dimensional cellular automata with non-homogeneous rules and arbitrary neighbors are proposed. The fitness function combines entropy measures and standard statistical tests for random sequences. The generators found are statistically compared to some well-known pseudorandom sequences generators.|David Delgado,David Vidal,German Hernandez","57755|GECCO|2006|Extraction of landscape information based on a quality control approach and its applications to mutation in GAExtraction of landscape information based on a quality control approach and its applications to mutation in GA|We introduce an attraction hypothesis and repulsion hypothesis on combinations of genes and we characterize \"genelocus pair\" as a \"Unique Inheritance\" if the pair satisfies one of the hypotheses. We propose a method based on a statistical approach to extract a set of gene-locus pairs characterized as \"Unique Inheritance\", and also two new genetic operations, attraction mutation and repulsion mutation.|Mitsukuni Matayoshi,Morikazu Nakamura,Hayao Miyagi","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao","80677|VLDB|2006|Quality Views Capturing and Exploiting the User Perspective on Data Quality|There is a growing awareness among life scientists of the variability in quality of the data in public repositories, and of the threat that poor data quality poses to the validity of experimental results. No standards are available, however, for computing quality levels in this data domain. We argue that data processing environments used by life scientists should feature facilities for expressing and applying quality-based, personal data acceptability criteria.We propose a framework for the specification of users' quality processing requirements, called quality views. These views are compiled and semi-automatically embedded within the data processing environment. The result is a quality management toolkit that promotes rapid prototyping and reuse of quality components. We illustrate the utility of the framework by showing how it can be deployed within Taverna, a scientific workflow management tool, and applied to actual workflows for data analysis in proteomics.|Paolo Missier,Suzanne M. Embury,R. Mark Greenwood,Alun D. Preece,Binling Jin"],["65676|AAAI|2006|Constraint Symmetry and Solution Symmetry|Symmetry in constraint satisfaction problems (CSPs) has been considered in two fundamentally different ways as an operation preserving the solutions of a CSP instance, or as an operation preserving the constraints. To reflect these two views, we define solution symmetry and constraint symmetry. We discuss how these concepts are related and show that some CSP instances have many more solution symmetries than constraint symmetries.|David A. Cohen,Peter Jeavons,Christopher Jefferson,Karen E. Petrie,Barbara M. Smith","57629|GECCO|2006|GRASP - evolution for constraint satisfaction problems|There are several evolutionary approaches for solving random binary Constraint Satisfaction Problems (CSPs). In most of these strategies we find a complex use of information regarding the problem at hand. Here we present a hybrid Evolutionary Algorithm that outperforms previous approaches in terms of effectiveness and compares well in terms ofefficiency. Our algorithm is conceptual and simple, featuring a GRASP-like (GRASP stands for Greedy Randomized Adaptive Search Procedure) mechanism for genotype-to-phenotype mapping, and without considering any specific knowledge of the problem. Therefore, we provide a simple algorithm that harnesses generality while boosting performance.|Manuel Cebrián,Iván Dotú","65835|AAAI|2006|Constraint-Based Random Stimuli Generation for Hardware Verification|We report on random stimuli generation for hardware verification in IBM as a major application of various artificial intelligence technologies, including knowledge representation, expert systems, and constraint satisfaction. The application has been developed for almost a decade, with huge payoffs. Research and development around this application is still thriving, as we continue to cope with the ever-increasing complexity of modern hardware systems and demanding business environments.|Yehuda Naveh,Michal Rimon,Itai Jaeger,Yoav Katz,Michael Vinov,Eitan Marcus,Gil Shurek","65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","65728|AAAI|2006|Analysis of Privacy Loss in Distributed Constraint Optimization|Distributed Constraint Optimization (DCOP) is rapidly emerging as a prominent technique for multi agent coordination. However, despite agent privacy being a key motivation for applying DCOPs in many applications, rigorous quantitative evaluations of privacy loss in DCOP algorithms have been lacking. Recently, Maheswaran et al.  introduced a framework for quantitative evaluations of privacy in DCOP algorithms, showing that some DCOP algorithms lose more privacy than purely centralized approaches and questioning the motivation for applying DCOPs. This paper addresses the question of whether state-of-the art DCOP algorithms suffer from a similar shortcoming by investigating several of the most efficient DCOP algorithms, including both DPOP and ADOPT. Furthermore, while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions (e.g. constraint-graph topology, asynchrony, message-contents), this paper examines the privacy aspect of such decisions, providing an improved understanding of privacy-efficiency tradeoffs.|Rachel Greenstadt,Jonathan P. Pearce,Milind Tambe","65853|AAAI|2006|ODPOP An Algorithm for OpenDistributed Constraint Optimization|We propose ODPOP, a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domains (Faltings & Macho-Gonzalez ). The ODPOP algorithm explores the same search space as the dynamic programming algorithm DPOP (Petcu & Faltings b) or ADOPT (Modi et at. ). but does so in an incremental, best-first fashion suitable for open problems. ODPOP has several advantages over DPOP. First, it uses messages whose size only grows linearly with the treewidth of the problem. Second, by letting agents explore values in a best-first order, it avoids incurring always the worst case complexity as DPOP, and on average it saves a significant amount of computation and information exchange. To show the merits of our approach, we report on experiments with practically sized distributed meeting scheduling problems on a multiagent system.|Adrian Petcu,Boi Faltings","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65857|AAAI|2006|A Quadratic Propagator for the Inter-Distance Constraint|We present a new propagator achieving bound consistency for the INTER-DISTANCE constraint. This constraint ensures that, among a set of variables X,..., Xn, the difference between two variables is at least p. This restriction models, in particular scheduling problems in which tasks require p contiguous units of a resource to be completed. Until now, the best known propagator for bound consistency had time complexity O(n). In this work we propose a quadratic propagator for the same level of consistency. We then show that this theoretical gain gives savings of an order of magnitude in our benchmark of scheduling problems.|Claude-Guy Quimper,Alejandro López-Ortiz,Gilles Pesant"],["65808|AAAI|2006|Efficient Haplotype Inference with Boolean Satisfiability|One of the main topics of research in genornics is determining the relevance of mutations, described in haplotype data, as causes of some genetic diseases. However, due to technological limitations, genotype data rather than haplotype data is usually obtained. The haplotype inference by pure parsimony (HIPP) problem consists in inferring haplotypes from genotypes S.t. the number of required haplotypes is minimum. Previous approaches to the HIPP problem have focused on integer programming models and branch-and-bound algorithms. In contrast, this paper proposes the utilization of Boolean Satisfiability (SAT). The proposed solution entails a SAT model, a number of key pruning techniques, and an iterative algorithm that enumerates the possible solution values for the target optimization problem. Experimental results, obtained on a wide range of instances, demonstrate that the SAT-based approach can be several orders of magnitude faster than existing solutions. Besides being more efficient, the SAT-based approach is also the only capable of computing the solution for a large number of instances.|Inês Lynce,João Marques-Silva","57635|GECCO|2006|Adaptive discretization for probabilistic model building genetic algorithms|This paper proposes an adaptive discretization method, called Split-on-Demand (SoD), to enable the probabilistic model building genetic algorithm (PMBGA) to solve optimization problems in the continuous domain. The procedure, effect, and usage of SoD are described in detail. As an example, the integration of SoD and the extended compact genetic algorithm (ECGA), named real-coded ECGA (rECGA), is presented and numerically examined. The experimental results indicate that rECGA works well and SoD is effective. The behavior of SoD is analyzed and discussed, followed by the potential future work for SoD.|Chao-Hong Chen,Wei-Nan Liu,Ying-Ping Chen","65886|AAAI|2006|Memory-Efficient Inference in Relational Domains|Propositionalization of a first-order theory followed by satisfiability testing has proved to be a remarkably efficient approach to inference in relational domains such as planning (Kautz & Selman ) and verification (Jackson ). More recently, weighted satisfiability solvers have been used successfully for MPE inference in statistical relational learners (Singla & Domingos ). However, fully instantiating a finite first-order theory requires memory on the order of the number of constants raised to the arity of the clauses, which significantly limits the size of domains it can be applied to. In this paper we propose LazySAT, a variation of the Walk-SAT solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domains (i.e., only a small fraction of ground atoms are true, and most clauses are trivially satisfied). Experiments on entity resolution and planning problems show that LazySAT reduces memory usage by orders of magnitude compared to Walk-SAT, while taking comparable time to run and producing the same solutions.|Parag Singla,Pedro Domingos","65792|AAAI|2006|PPCP Efficient Probabilistic Planning with Clear Preferences in Partially-Known Environments|For most real-world problems the agent operates in only partially-known environments. Probabilistic planners can reason over the missing information and produce plans that take into account the uncertainty about the environment. Unfortunately though, they can rarely scale up to the problems that are of interest in real-world. In this paper, however, we show that for a certain subset of problems we can develop a very efficient probabilistic planner. The proposed planner, called PPCP, is applicable to the problems for which it is clear what values of the missing information would result in the best plan. In other words, there exists a clear preference for the actual values of the missing information. For example, in the problem of robot navigation in partially-known environments it is always preferred to find out that an initially unknown location is traversable rather than not. The planner we propose exploits this property by using a series of deterministic A*-like searches to construct and refine a policy in anytime fashion. On the theoretical side, we show that once converged, the policy is guaranteed to be optimal under certain conditions. On the experimental side, we show the power of PPCP on the problem of robot navigation in partially-known terrains. The planner can scale up to very large environments with thousands of initially unknown locations. We believe that this is several orders of magnitude more unknowns than what the current probabilistic planners developed for the same problem can handle. Also, despite the fact that the problem we experimented on in general does not satisfy the conditions for the solution optimality, PPCP still produces the solutions that are nearly always optimal.|Maxim Likhachev,Anthony Stentz","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","65735|AAAI|2006|New Inference Rules for Efficient Max-SAT Solving|In this paper we augment the Max-SAT solver of (Larrosa & Heras ) with three new inference rules. The three of them are special cases of Max-SAT resolution with which better lower bounds and more value pruning is achieved. Our experimental results on several domains show that the resulting algorithm can be orders of magnitude faster than state-of-the-art Max-SAT solvers and the best Weighted CSP solver.|Federico Heras,Javier Larrosa","65785|AAAI|2006|Performing Incremental Bayesian Inference by Dynamic Model Counting|The ability to update the structure of a Bayesian network when new data becomes available is crucial for building adaptive systems. Recent work by Sang, Beame, and Kautz (AAAI ) demonstrates that the well-known Davis-Putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in Bayesian networks with high density and width. In this paper, we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple runs on a series of problems with similar structure. This allows us to perform Bayesian inference incrementally as the structure of the network changes. Experimental results show that our approach yields significant improvements over the previous model counting approaches on multiple challenging Bayesian network instances.|Wei Li 0002,Peter van Beek,Pascal Poupart","65855|AAAI|2006|An Efficient Way of Breaking Value Symmetries|Several methods for breaking value symmetries have been proposed recently in the constraint programming community. They can be used in conjunction with variable symmetry breaking methods. However, this combination does not break all symmetries in general. We present a combination of lex constraints and element constrants that can be used to break all combinations of variable and value symmetries. It is the first time to our knowledge that it is possible to break all combinations of value and variable symmetries by adding constraints. This method is quite efficient when the number of symmetries is not too large, as shown by experiments using graceful graph problems. We also present a new global constraint that deals with the case where there are too many value symmetries. Experiments show that this is highly effective.|Jean-François Puget","65854|AAAI|2006|Sound and Efficient Inference with Probabilistic and Deterministic Dependencies|Reasoning with both probabilistic and deterministic dependencies is important for many real-world problems, and in particular for the emerging field of statistical relational learning. However, probabilistic inference methods like MCMC or belief propagation tend to give poor results when deterministic or near-deterministic dependencies are present, and logical ones like satisfiability testing are inapplicable to probabilistic ones. In this paper we propose MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. MC-SAT is based on Markov logic, which defines Markov networks using weighted clauses in first-order logic. From the point of view of MCMC, MC-SAT is a slice sampler with an auxiliary variable per clause, and with a satisfiability-based method for sampling the original variables given the auxiliary ones. From the point of view of satisfiability, MCSAT wraps a procedure around the SampleSAT uniform sampler that enables it to sample from highly non-uniform distributions over satisfying assignments. Experiments on entity resolution and collective classification problems show that MC-SAT greatly outperforms Gibbs sampling and simulated tempering over a broad range of problem sizes and degrees of determinism.|Hoifung Poon,Pedro Domingos","80690|VLDB|2006|An Algebraic Query Model for Effective and Efficient Retrieval of XML Fragments|Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.|Sujeet Pradhan"],["57707|GECCO|2006|Autonomous evolutionary music composer|A second-generation autonomous music composition tool is developed using Genetic Algorithms. The composition is conducted in two Stages. The first Stage generates and identifies musically sound patterns (motifs). In the second Stage, methods to combine different generated motifs and their transpositions are applied. These combinations are evaluated and as a result, musically fit phrases are generated. Four musical phrases are generated at the end of each program run. The generated music pieces will be translated into Guido Music Notation (GMN) and have alternate representation in Musical Instrument Digital Interface (MIDI). The Autonomous Evolutionary Music Composer (AEMC) was able to create interesting pieces of music that were both innovative and musically sound.|Yaser M. A. Khalifa,Mohamed Basel Al-Mourad","57842|GECCO|2006|Pairwise sequence comparison for fitness evaluation in evolutionary structural software testing|Evolutionary algorithms are among the metaheuristic search methods that have been applied to the structural test data generation problem. Fitness evaluation methods play an important role in the performance of evolutionary algorithms and various methods have been devised for this problem. In this paper, we propose a new fitness evaluation method based on pairwise sequence comparison also used in bioinformatics. Our preliminary study shows that this method is easy to implement and produces promising results.|H. Turgut Uyar,A. Sima Etaner-Uyar,A. Emre Harmanci","57816|GECCO|2006|Comparison of multi-modal optimization algorithms based on evolutionary algorithms|Many engineering optimization tasks involve finding more than one optimum solution. The present study provides a comprehensive review of the existing work done in the field of multi-modal function optimization and provides a critical analysis of the existing methods. Existing niching methods are analyzed and an improved niching method is proposed. To achieve this purpose, we first give an introduction to niching and diversity preservation, followed by discussion of a number of algorithms. Thereafter, a comparison of clearing, clustering, deterministic crowding, probabilistic crowding, restricted tournament selection, sharing, species conserving genetic algorithms is made. A modified niching-based technique -- modified clearing approach -- is introduced and also compared with existing methods. For comparison, a versatile hump test function is also proposed and used together with two other functions. The ability of the algorithms in finding, locating, and maintaining multiple optima is judged using two performance measures (i) number of peaks maintained, and (ii) computational time. Based on the results, we conclude that the restricted tournament selection and the proposed modified clearing approaches are better in terms of finding and maintaining the multiple optima.|Gulshan Singh,Kalyanmoy Deb","57724|GECCO|2006|Biobjective evolutionary and heuristic algorithms for intersection of geometric graphs|Wire routing in a VLSI chip often requires minimization of ire-length as well as the number of intersections among multiple nets. Such an optimization problem is computationally hard for which no efficient algorithm or good heuristic is known to exist. Additionally, in a biobjective setting, the major challenge to solve a problem is to obtain representative diverse solutions across the (near-) Pareto-front.In this work, we consider the problem of constructing spanning trees of two geometric graphs corresponding to two nets, each with multiple terminals, with a goal to minimize the total edge cost and the number of intersections among the edges of the two trees. We first design simple heuristics to obtain the extreme points in the solution space, which however, could not produce diverse solutions. Search algorithms based on evolutionary multiobjective optimization (EMO) are then proposed to obtain diverse solutions in the feasible solution space. Each element of this solution set is a tuple of two spanning trees corresponding to the given geometric graphs. Empirical evidence shows that the proposed evolutionary algorithms cover a larger range and are much superior to the heuristics.|Rajeev Kumar,Pramod Kumar Singh,Bhargab B. Bhattacharya","57854|GECCO|2006|Credit assignment in adaptive evolutionary algorithms|In this paper, a new method for assigning credit to search operators is presented. Starting with the principle of optimizing search bias, search operators are selected based on an ability to create solutions that are historically linked to future generations. Using a novel framework for defining performance measurements, distributing credit for performance, and the statistical interpretation of this credit, a new adaptive method is developed and shown to outperform a variety of adaptive and non-adaptive competitors.|James M. Whitacre,Q. Tuan Pham,Ruhul A. Sarker","57661|GECCO|2006|Evolutionary interactive music composition|This paper proposes the CFE framework---Composition, Feedback, and Evolution---and presents an interactive music composition system. The system composes short, manageable pieces of music by interacting with users. The most important features of the system include creating customized music according to the user preference and providing the facilities specifically designed for producing large amounts of music. We present the structure as well as the implementation of the system and the auxiliary functionalities that enhance the system. We also introduce the auto-feedback test with which we verify and evaluate the interactive music composition system.|Tao-yang Fu,Tsu-yu Wu,Chin-te Chen,Kai-chu Wu,Ying-Ping Chen","57767|GECCO|2006|A method for parameter calibration and relevance estimation in evolutionary algorithms|We present and evaluate a method for estimating the relevance and calibrating the values of parameters of an evolutionary algorithm. The method provides an information theoretic measure on how sensitive a parameter is to the choice of its value. This can be used to estimate the relevance of parameters, to choose between different possible sets of parameters, and to allocate resources to the calibration of relevant parameters. The method calibrates the evolutionary algorithm to reach a high performance, while retaining a maximum of robustness and generalizability. We demonstrate the method on an agent-based application from evolutionary economics and show how the method helps to design an evolutionary algorithm that allows the agents to achieve a high welfare with a minimum of algorithmic complexity.|Volker Nannen,A. E. Eiben","57836|GECCO|2006|Improving evolutionary real-time testing|Embedded systems are often used in a safety-critical context, e.g. in airborne or vehicle systems. Typically, timing constraints must be satisfied so that real-time embedded systems work properly and safely. Execution time testing involves finding the best and worst case execution times to determine if timing constraints are respected. Evolutionary real-time testing (ERTT) is used to dynamically search for the extreme execution times. It can be shown that ERTT outperforms the traditional methods based on static analysis. However, during the evolutionary search, some parts of the source code are never accessed. Moreover, it turns out that ERTT delivers different extreme execution times in a high number of generations for the same test object, the results are neither reliable nor efficient. We propose a new approach to ERTT which makes use of seeding the evolutionary algorithm with test data achieving a high structural coverage. Using such test data ensures a comprehensive exploration of the search space and leads to rise the confidence in the results. We present also another improvement method based on restricting the range of the input variables in the initial population in order to reduce the search space. Experiments with these approaches demonstrate an increase of reliability in terms of constant extreme execution times and a gain in efficiency in terms of number of generations needed.|Marouane Tlili,Stefan Wappler,Harmen Sthamer","57773|GECCO|2006|Comparison of multi-objective evolutionary algorithms in optimizing combinations of reinsurance contracts|Our paper concerns optimal combinations of different types of reinsurance contracts. We introduce a novel approach based on the Mean-Variance-Criterion to solve this task. Two state-of-the-art MOEAs are used to perform an optimization of yet unresolved problem instances. In addition to that, we focus on finding a dense set of solutions to derive analogies to theoretic results of easier problem instances.|Ingo Oesterreicher,Andreas Mitschele,Frank Schlottmann,Detlef Seese","57736|GECCO|2006|Revisiting evolutionary algorithms with on-the-fly population size adjustment|In an evolutionary algorithm, the population has a very important role as its size has direct implications regarding solution quality, speed, and reliability. Theoretical studies have been done in the past to investigate the role of population sizing in evolutionary algorithms. In addition to those studies, several self-adjusting population sizing mechanisms have been proposed in the literature. This paper revisits the latter topic and pays special attention to the genetic algorithm with adaptive population size (APGA), for which several researchers have claimed to be very effective at autonomously (re)sizing the population.As opposed to those previous claims, this paper suggests a complete opposite view. Specifically, it shows that APGA is not capable of adapting the population size at all. This claim is supported on theoretical grounds and confirmed by computer simulations.|Fernando G. Lobo,Cláudio F. Lima"],["65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","57728|GECCO|2006|Mixed-integer optimization of coronary vessel image analysis using evolution strategies|In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.|Rui Li,Michael Emmerich,Jeroen Eggermont,Ernst G. P. Bovenkamp","65767|AAAI|2006|Novel Relationship Discovery Using Opinions Mined from the Web|This paper proposes relationship discovery models using opinions mined from the Web instead of only conventional collocations. Web opinion mining extracts subjective information from the Web for specific targets, summarizes the polarity and the degree of the information, and tracks the development over time. Targets which gain similar opinionated tendencies within a period of time may be correlated. This paper detects event bursts from the tracking plots of opinions, and decides the strength of the relationship using the coverage of the plots. Companies are selected as the experimental targets. A total of ,, economics-related documents are collected from  Web sources between August  and May  for experiments. Models that discover relations are then proposed and compared on the basis of their performance. There are three types of models, collocation-based, opinion-based, and integration models, and respectively, four, two and two variants of each type. For evaluation, company pairs which demonstrate similar oscillation of stock prices are considered correlated and are selected as the gold standard. The results show that collocation-based models and opinion-based models are complementary, and the integration models perform the best. The top ,  and  answers discovered by the best integration model achieve precision rates of , . and ., respectively.|Lun-Wei Ku,Hsiu-Wei Ho,Hsin-Hsi Chen","57593|GECCO|2006|Candlestick stock analysis with genetic algorithms|Candlestick analysis, a form of stock market technical analysis, is well suited for use with a genetic search algorithm. This paper explores an implementation of marrying these two techniques by creating agents that attempt to identify stocks that will change in price. The best of run individuals, produced by the genetic algorithm, performed statistically better than an agent that makes random investment decisions.|Peter Belford","65668|AAAI|2006|Using Semantics to Identify Web Objects|Many common web tasks can be automated by algorithms that are able to identify web objects relevant to the user's needs. This paper presents a novel approach to web object identificalion that finds relationships between the user's actions and linguistic information associated with web objects. From a single training example involving demonstration and a natural language description, we create a parameterized object description. The approach performs as well as a popular web wrapper on a routine task, but it has the additional capability of performing in dynamic environments and the attractive property of being reusable in other domains without additional training.|Nathanael Chambers,James F. Allen,Lucian Galescu,Hyuckchul Jung,William Taysom","57825|GECCO|2006|On the analysis of the  memetic algorithm|Memetic algorithms are evolutionary algorithms incorporating local search to increase exploitation. This hybridization has been fruitful in countless applications. However, theory on memetic algorithms is still in its infancy.Here, we introduce a simple memetic algorithm, the (+) Memetic Algorithm (+(MA)), working with a population size of  and no crossover. We compare it with the well-known (+) EA and randomized local search and show that these algorithms can outperform each other drastically.On problems like, e.g., long path problems it is essential to limit the duration of local search. We investigate the (+) MA with a fixed maximal local search duration and define a class of fitness functions where a small variation of the local search duration has a large impact on the performance of the (+) MA.All results are proved rigorously without assumptions.|Dirk Sudholt","65729|AAAI|2006|Predicting Electricity Distribution Feeder Failures Using Machine Learning Susceptibility Analysis|A Machine Learning (ML) System known as ROAMS (Ranker for Open-Auto Maintenance Scheduling) was developed to create failure-susceptibility rankings for almost one thousand .kV-kV energy distribution feeder cables that supply electricity to the boroughs of New York City. In Manhattan, rankings are updated every  minutes and displayed on distribution system operators' screens. Additionally, a separate system makes seasonal predictions of failure susceptibility. These feeder failures, known as \"Open Autos\" or \"OAs,\" are a significant maintenance problem. A year's sustained research has led to a system that demonstrates high accuracy % of the feeders that actually failed over the summer of  were in the % of feeders ranked as most at-risk. By the end of the summer, the  most susceptible feeders as ranked by the ML system were accounting for up to % of all OAs that subsequently occurred each day. The system's algorithm also identifies the factors underlying failures which change over time and with varying conditions (especially temperature), providing insights into the operating properties and failure causes in the feeder system.|Philip Gross,Albert Boulanger,Marta Arias,David L. Waltz,Philip M. Long,Charles Lawson,Roger Anderson,Matthew Koenig,Mark Mastrocinque,William Fairechio,John A. Johnson,Serena Lee,Frank Doherty,Arthur Kressner","57665|GECCO|2006|Memory analysis and significance test for agent behaviours|Many agent problems in a grid world have a restricted sensory information and motor actions. The environmental conditions need dynamic processing of internal memory. In this paper, we handle the artificial ant problem, an agent task to model ant trail following in a grid world, which is one of the difficult problems that purely reactive systems cannot solve. We provide an evolutionary approach to quantify the amount of memory needed for the agent problem and explore a systematic analysis over the memory usage. We apply two types of memory-based control structures, Koza's genetic programming and finite state machines, to recognize the relevance of internal memory. Statistical significance test based on beta distribution differentiates the characteristics and performances of the two control structures.|DaeEun Kim","57847|GECCO|2006|Comparative analysis of the sailor assignment problem|In this work the performance of several local search and metaheuristic methods is compared to previously reported work using evolutionary algorithms. The results show that while multiple algorithms are competitive on the Sailor Assignment Problem, the state-of-the-art evolutionary algorithm and a simulated annealing algorithm tend to provide the best performance. Additionally, some relevant features of the Sailor Assignment Problem are analyzed and used to explain the observed performance characteristics.|Joseph Vannucci,Deon Garrett,Dipankar Dasgupta","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","65841|AAAI|2006|A Compact Representation Scheme for Coalitional Games in Open Anonymous Environments|Coalition formation is an important capability of automated negotiation among self-interested agents. In order for coalitions to be stable, a key question that must be answered is how the gains from cooperation are to be distributed. Recent research has revealed that traditional solution concepts, such as the Shapley value, core, least core, and nucleolus, are vulnerable to various manipulations in open anonymous environments such as the Internet. These manipulations include submitting false names, collusion, and hiding some skills. To address this, a solution concept called the anonymity-proof core, which is robust against such manipulations, was developed. However, the representation size of the outcome function in the anonymity-proof core (and similar concepts) requires space exponential in the number of agentsskills. This paper proposes a compact representation of the outcome function, given that the characteristic function is represented using a recently introduced compact language that explicitly specifies only coalitions that introduce synergy. This compact representation scheme can successfully express the outcome function in the anonymity-proof core. Furthermore, this paper develops a new solution concept, the anonymity-proof nucleolus, that is also expressible in this compact representation. We show that the anonymity-proof nucleolus always exists, is unique, and is in the anonymity-proof core (if the latter is nonempty). and assigns the same value to symmetric skills.|Naoki Ohta,Atsushi Iwasaki,Makoto Yokoo,Kohki Maruono,Vincent Conitzer,Tuomas Sandholm","65659|AAAI|2006|Factored Planning How When and When Not|Automated domain factoring, and planning methods that utilize them, have long been of interest to planning researchers. Recent work in this area yielded new theoretical insight and algorithms, but left many questions open How to decompose a domain into factors How to work with these factors And whether and when decomposition-based methods are useful This paper provides theoretical analysis that answers many of these questions it proposes a novel approach to factored planning proves its theoretical superiority over previous methods provides insight into how to factor domains and uses its novel complexity results to analyze when factored planning is likely to perform well, and when not. It also establishes the key role played by the domain's causal graph in the complexity analysis of planning algorithms.|Ronen I. Brafman,Carmel Domshlak","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65943|AAAI|2006|Mechanisms for Partial Information Elicitation The Truth but Not the Whole Truth|We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios.|Aviv Zohar,Jeffrey S. Rosenschein","65747|AAAI|2006|Darshak - An Intelligent Cinematic Camera Planning System|A virtual camera is a powerful communicative tool in virtual environments. It is a window through which a viewer perceives the virtual world. For virtual environments with an underlying narrative component, there is a need for automated camera planning systems that account for the situational parameters of the interaction and not just the graphical arrangement of the virtual world. I propose a camera planning system called Darshak that takes as input a story in the form of sequence of events and generates a sequence of camera actions based on cinematic idioms. The camera actions, when executed in the virtual environment update a list of geometric constraints on the camera. A constraint solver then places the camera based on these constraints.|Arnav Jhala","65875|AAAI|2006|Contingent Planning with Goal Preferences|The importance of the problems of contingent planning with actions that have non-deterministic effects and of planning with goal preferences has been widely recognized, and several works address these two problems separately. However, combining conditional planning with goal preferences adds some new difficulties to the problem. Indeed, even the notion of optimal plan is far from trivial, since plans in nondeterministic domains can result in several different behaviors satisfying conditions with different preferences. Planning for optimal conditional plans must therefore take into account the different behaviors, and conditionally search for the highest preference that can be achieved. In this paper, we address this problem. We formalize the notion of optimal conditional plan, and we describe a correct and complete planning algorithm that is guaranteed to find optimal solutions. We implement the algorithm using BDD-based techniques, and show the practical potentialities of our approach through a preliminary experimental evaluation.|Dmitry Shaparau,Marco Pistore,Paolo Traverso","65772|AAAI|2006|Controlled Search over Compact State Representations in Nondeterministic Planning Domains and Beyond|Two of the most efficient planners for planning in nondeterministic domains are MBP and ND-SHOP. MBP achieves its efficiency by using Binary Decision Diagrams (BDDs) to represent sets of states that share some common properties, so it can plan for all of these states simultaneously. ND-SHOP achieves its efficiency by using HTN task decomposition to focus the search. In some environments, ND-SHOP runs exponentially faster than MBP, and in others the reverse is true. In this paper, we discuss the following  We describe how to combine ND-SHOP's HTNs with MBP's BDDs. Our new planning algorithm, YoYo, performs task decompositions over classes of states that are represented as BDDs. In our experiments, YoYo easily outperformed both MBP and ND-SHOP, often by several orders of magnitude.  HTNs are just one of several techniques that are originally developed for classical planning domains and that can be adapted to work in nondeterministic domains. By combining those techniques with a BDD representation, it should be possible to get great speedups just as we did here. We discuss how these same ideas can be generalized for use in several other research areas, such as planning with Markov Decision Processes, synthesizing controllers for hybrid systems, and composing Semantic Web Services.|Ugur Kuter,Dana S. Nau","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","65698|AAAI|2006|Cost-Optimal External Planning|This paper considers strategies for external memory based optimal planning. An external breadth-first search exploration algorithm is devised that is guaranteed to find the costoptimal solution. We contribute a procedure for finding the upper bound on the locality of the search in planning graphs that dictates the number of layers that have to be kept to avoid re-openings. We also discuss an external variant of Enforced Hill Climbing. Using relaxed-plan heuristic without helpful-action pruning we have been able to perform large explorations on metric planning problems, providing better plan lengths than have been reported earlier. A novel approach to plan reconstruction in external setting with linear IO complexity is proposed. We provide external exploration results on some recently proposed planning domains.|Stefan Edelkamp,Shahid Jabbar"],["57818|GECCO|2006|A simple line search operator for ridged landscapes|This paper describes a new simple operator for Evolutionary Algorithms (EA) to climb ridged landscapes.|Andrea Soltoggio","57869|GECCO|2006|Genetic local search for multicast routing|We describe a population-based search algorithm for cost minimization of multicast routing. The algorithm utilizes the partially mixed crossover operation (PMX) and a landscape analysis in a pre-processing step. The aim of the landscape analysis is to estimate the depth  of the deepest local minima in the landscape generated by the routing tasks and the objective function. The analysis employs simulated annealing with a logarithmic cooling schedule (LSA). The local search performs alternating sequences of descending and ascending steps for each individual of the population, where the length of a sequence with uniform direction is controlled by the estimated value of . We present results from computational experiments on a synthetic routing tasks, and we provide experimental evidence that our genetic local search procedure, that combines LSA and PMX, performs better than algorithms using either LSA or PMX only.|Mohammed S. Zahrani,Martin J. Loomes,James A. Malcolm,Andreas Alexander Albrecht","57710|GECCO|2006|Geometric crossover for multiway graph partitioning|Geometric crossover is a representation-independent generalization of the traditional crossover defined using the distance of the solution space. Using a distance tailored to the problem at hand, the formal definition of geometric crossover allows to design new problem-specific crossovers that embed problem-knowledge in the search. The standard encoding for multiway graph partitioning is highly redundant each solution has a number of representations, one for each way of labeling the represented partition. Traditional crossover does not perform well on redundant encodings. We propose a new geometric crossover for graph partitioning based on a labeling-independent distance that filters the redundancy of the encoding. A correlation analysis of the fitness landscape based on this distance shows that it is well suited to graph partitioning. Our new genetic algorithm outperforms existing ones.|Yong-Hyuk Kim,Yourim Yoon,Alberto Moraglio,Byung Ro Moon","57765|GECCO|2006|Generalized cycle crossover for graph partitioning|We propose a new crossover that generalizes cycle crossover to permutations with repetitions and naturally suits partition problems. We tested it on graph partitioning problems obtaining excellent results.|Alberto Moraglio,Yong-Hyuk Kim,Yourim Yoon,Byung Ro Moon,Riccardo Poli","57671|GECCO|2006|Maximum cardinality matchings on trees by randomized local search|To understand the working principles of randomized search heuristics like evolutionary algorithms they are analyzed on optimization problems whose structure is well-studied. The idea is to investigate when it is possible to simulate clever optimization techniques for combinatorial optimization problems by random search. The maximum matching problem is well suited for this approach since long augmenting paths do not allow immediate improvements by local changes. It is known that randomized search heuristics like simulated annealing, the Metropolis algorithm, the (+) EA and randomized local search efficiently approximate maximum matchings for any graph however, there are graphs where they fail to find maximum matchings in polynomial time. In this paper, we examine randomized local search (RLS) for graphs whose structure is simple. We show that RLS finds maximum matchings on trees in expected polynomial time.|Oliver Giel,Ingo Wegener","65939|AAAI|2006|A Breadth-First Approach to Memory-Efficient Graph Search|Recent work shows that the memory requirements of A* and related graph-search algorithms can be reduced substantially by only storing nodes that are on or near the search frontier, using special techniques to prevent node regeneration, and recovering the solution path by a divide-and-conquer technique. When this approach is used to solve graph-search problems with unit edge costs, we have shown that a breadth-first search strategy can be more memory-efficient than a best-first strategy. We provide an overview of our work using this approach, which we call breadth-first heuristic search.|Rong Zhou,Eric A. Hansen","57617|GECCO|2006|The no free lunch and realistic search algorithms|The No-Free-Lunch theorems (NFLTs) are criticized for being too general to be of any relevance to the real world scenario. This paper investigates, both formally and empirically, the implications of the NFLTs for realistic search algorithms. In the first part of the paper, by restricting ourselves to a specific performance measure, we derive a new NFL result for a class of problems which is not closed under permutations. In the second part, we discuss properties of this set which are likely to be true for realistic search algorithms. We provide empirical support for this in .|Yossi Borenstein,Riccardo Poli","57741|GECCO|2006|A crossover operator for the anonymity problem|Recent dissemination of personal data has created an important optimization problem what is the minimal transformation of a dataset that is needed to guarantee the anonymity of the underlying individuals One natural representation for this problem is a bit-string, which makes a genetic algorithm a logical choice for optimization. Unfortunately, under certain realistic conditions, not all bit combinations will represent valid solutions. This means that in many instances, useful solutions are sparse in the search space. We implement a new crossover operator that preserves valid solutions under this representation. Our results show that this reproductive strategy is more efficient, effective, and robust than previous work. We also investigate how the population size and uniqueness can affect the performance of genetic search on this application.|Monte Lunacek,Darrell Whitley,Indrakshi Ray","57753|GECCO|2006|Spectral techniques for graph bisection in genetic algorithms|Various applications of spectral techniques for enhancing graph bisection in genetic algorithms are investigated. Several enhancements to a genetic algorithm for graph bisection are introduced based on spectral decompositions of adjacency matrices of graphs and subpopulation matrices. First, the spectral decompositions give initial populations for the genetic algorithm to start with. Next, spectral techniques are used to engineer new individuals and reorder the schema to strategically group certain sets of vertices together on the chromosome. The operators and techniques are found to be beneficial when added to a plain genetic algorithm and when used in conjunction with other local optimization techniques for graph bisection. In addition, several world record minimum bisections have been obtained from the methods described in this study.|Jacob G. Martin","57630|GECCO|2006|Optimal mutation rates for genetic search|Using a set of model landscapes we examine how different mutation rates affect different search metrics. We show that very universal heuristics, such as N and the error threshold, can generally be improved upon if one has some qualitative information about the landscape. In particular, we show in the case of multiple optima (signals) how mutation affects which signal dominates and how passing between the dominance of one to another depends on the relative height and size of the peaks and their relative positions in the configuration space.|Jorge Cervantes,Christopher R. Stephens"],["80607|VLDB|2006|On Biased Reservoir Sampling in the Presence of Stream Evolution|The method of reservoir based sampling is often used to pick an unbiased sample from a data stream. A large portion of the unbiased sample may become less relevant over time because of evolution. An analytical or mining task (eg. query estimation) which is specific to only the sample points from a recent time-horizon may provide a very inaccurate result. This is because the size of the relevant sample reduces with the horizon itself. On the other hand, this is precisely the most important case for data stream algorithms, since recent history is frequently analyzed. In such cases, we show that an effective solution is to bias the sample with the use of temporal bias functions. The maintenance of such a sample is non-trivial, since it needs to be dynamically maintained, without knowing the total number of points in advance. We prove some interesting theoretical properties of a large class of memory-less bias functions, which allow for an efficient implementation of the sampling algorithm. We also show that the inclusion of bias in the sampling process introduces a maximum requirement on the reservoir size. This is a nice property since it shows that it may often be possible to maintain the maximum relevant sample with limited storage requirements. We not only illustrate the advantages of the method for the problem of query estimation, but also show that the approach has applicability to broader data mining problems such as evolution analysis and classification.|Charu C. Aggarwal","80670|VLDB|2006|Answering Tree Pattern Queries Using Views|We study the query answering using views (QAV) problem for tree pattern queries. Given a query and a view, the QAV problem is traditionally formulated in two ways (i) find an equivalent rewriting of the query using only the view, or (ii) find a maximal contained rewriting using only the view. The former is appropriate for classical query optimization and was recently studied by Xu and Ozsoyoglu for tree pattern queries (TP). However, for information integration, we cannot rely on equivalent rewriting and must instead use maximal contained rewriting as shown by Halevy. Motivated by this, we study maximal contained rewriting for TP, a core subset of XPath, both in the absence and presence of a schema. In the absence of a schema, we show there are queries whose maximal contained rewriting (MCR) can only be expressed as the union of exponentially many TPs. We characterize the existence of a maximal contained rewriting and give a polynomial time algorithm for testing the existence of an MCR. We also give an algorithm for generating the MCR when one exists. We then consider QAV in the presence of a schema. We characterize the existence of a maximal contained rewriting when the schema contains no recursion or union types, and show that it consists of at most one TP. We give an efficient polynomial time algorithm for generating the maximal contained rewriting whenever it exists. Finally, we discuss QAV in the presence of recursive schemas.|Laks V. S. Lakshmanan,Hui Wang,Zheng (Jessica) Zhao","80629|VLDB|2006|TwigStack Bottom-up Processing of Generalized-Tree-Pattern Queries over XML Documents|Tree pattern matching is one of the most fundamental tasks for XML query processing. Holistic twig query processing techniques ,  have been developed to minimize the intermediate results, namely, those root-to-leaf path matches that are not in the final twig results. However, useless path matches cannot be completely avoided, especially when there is a parent-child relationship in the twig query. Furthermore, existing approaches do not consider the fact that in practice, in order to process XPath or XQuery statements, a more powerful form of twig queries, namely, Generalized-Tree-Pattern (GTP)  queries, is required. Most existing works on processing GTP queries generally calls for costly post-processing for eliminating redundant data andor grouping of the matching results.In this paper, we first propose a novel hierarchical stack encoding scheme to compactly represent the twig results. We introduce TwigStack, a bottom-up algorithm for processing twig queries based on this encoding scheme. Then we show how to efficiently enumerate the query results from the encodings for a given GTP query. To our knowledge, this is the first GTP matching solution that avoids any post path-join, sort, duplicate elimination and grouping operations. Extensive performance studies on various data sets and queries show that the proposed TwigStack algorithm not only has better twig query processing performance than state-of-the-art algorithms, but is also capable of efficiently processing the more complex GTP queries.|Songting Chen,Hua-Gang Li,Jun'ichi Tatemura,Wang-Pin Hsiung,Divyakant Agrawal,K. Selçuk Candan","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","65791|AAAI|2006|A Modular Action Description Language|\"Toy worlds\" involving actions, such as the blocks world and the Missionaries and Cannibals puzzle, are often used by researchers in the areas of commonsense reasoning and planning to illustrate and test their ideas. We would like to create a datahase of general-purpose knowledge about actions that encodes common features of many action domains of this kind. in the same way as abstract algebra and topology represent common features of specific number systems. This paper is a report on the first stage of this project--the design of an action description language in which this database will be written The new language is an extension of the action language C+. Its main distinctive feature is the possibility of referring to other action descriptions in the definition of a new action domain.|Vladimir Lifschitz,Wanwan Ren","65763|AAAI|2006|Hand Grip Pattern Recognition for Mobile User Interfaces|This paper presents a novel user interface for handheld mobile devices by recognizing hand grip patterns. Particularly, we consider the scenario where the device is provided with an array of capacitive touch sensors underneath the exterior cover. In order to provide the users with intuitive and natural manipulation experience, we use pattern recognition techniques for identifying the users' band grips from the touch sensors. Preliminary user studies suggest that filtering out unintended user hand grip is one of the most important issues to be resolved. We discuss the details of the prototype implementation, as well as engineering challenges for practical deployment.|Kee-Eung Kim,Wook Chang,Sung-Jung Cho,Junghyun Shim,Hyunjeong Lee,Joonah Park,Youngbeom Lee,Sangryoung Kim","80725|VLDB|2006|A Semantic Information Integration Tool Suite|We describe a prototype software tool suite for semantic information integration it has the following features. First, it can import local metadata as well as a domain ontology. Imported metadata is stored persistently in an ontological format. Second, it provides a semantic query facility that allows users to retrieve information across multiple data sources using the domain ontology directly. Third, it has a GUI for users to define mappings between the local metadata and the domain ontology. Fourth, it incorporates a novel mechanism to improve system reliability by dynamically adapting query execution upon detecting various types of environmental changes. In addition, this tool suite is compatible with WC Semantic Web specifications such as RDF and OWL. It also uses the query engine of Commercial EII products for low level query processing.|Jun Yuan,Ali Bahrami,Changzhou Wang,Marie O. Murray,Anne Hunt","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","65709|AAAI|2006|Mining and Re-ranking for Answering Biographical Queries on the Web|The rapid growth of the Web has made itself a huge and valuable knowledge base. Among them, biographical information is of great interest to society. However, there has not been an efficient and complete approach to automated biography creation by querying the web. This paper describes an automatic web-based question answering system for biographical queries. Ad-hoc improvements on pattern learning approaches are proposed for mining biographical knowledge. Using bootstrapping, our approach learns surface text patterns from the web, and applies the learned patterns to extract relevant information. To reduce human labeling cost, we propose a new IDF-inspired reranking approach and compare it with pattern's precision-based re-ranking approach. A comparative study of the two re-ranking models is conducted. The tested system produces promising results for answering biographical queries.|Donghui Feng,Deepak Ravichandran,Eduard H. Hovy","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["65715|AAAI|2006|Mixed Collaborative and Content-Based Filtering with User-Contributed Semantic Features|We describe a recommender system which uses a unique combination of content-based and collaborative methods to suggest items of interest to users, and also to learn and exploit item semantics. Recommender systems typically use techniques from collaborative filtering, in which proximity measures between users are formulated to generate recommendations, or content-based filtering, in which users are compared directly to items. Our approach uses similarity measures between users, but also directly measures the attributes of items that make them appealing to specific users. This can be used to directly make recommendations to users, but equally importantly it allows these recommendations to be justified. We introduce a method for predicting the preference of a user for a movie by estimating the user's attitude toward features with which other users have described that movie. We show that this method allows for accurate recommendations for a sub-population of users, but not for the entire user population. We describe a hybrid approach in which a user-specific recommendation mechanism is learned and experimentally evaluated. It appears that such a recommender system can achieve significant improvements in accuracy over alternative methods, while also retaining other advantages.|Matthew Garden,Gregory Dudek","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65742|AAAI|2006|Deciding Semantic Matching of Stateless Services|We present a novel approach to describe and reason about stateless information processing services. It can be seen as an extension of standard descriptions which makes explicit the relationship between inputs and outputs and takes into account OWL ontologies to fix the meaning of the terms used in a service description. This allows us to define a notion of matching between services which yields high precision and recall for service location. We explain why matching is decidable, and provide biomedical example services to illustrate the utility of our approach.|Duncan Hull,Evgeny Zolin,Andrey Bovykin,Ian Horrocks,Ulrike Sattler,Robert Stevens","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","80637|VLDB|2006|HISA A Query System Bridging The Semantic Gap For Large Image Databases|We propose a novel system called HISA for organizing very large image databases. HISA implements the first known data structure to capture both the ontological knowledge and visual features for effective and effcient retrieval of images by either keywords, image examples, or both. HISA employs automatic image annotation technique, ontology analysis and statistical analysis of domain knowledge to precompute the data structure. Using these techniques, HISA is able to bridge the gap between the image semantics and the visual features, therefore providing more user-friendly and high-performance queries. We demonstrate the novel data structure employed by HISA, the query algorithms, and the pre-computation process.|Gang Chen,Xiaoyan Li,Lidan Shou,Jinxiang Dong,Chun Chen","65749|AAAI|2006|OntoSearch A Full-Text Search Engine for the Semantic Web|OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of hisher queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance.|Xing Jiang,Ah-Hwee Tan","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"]]}}