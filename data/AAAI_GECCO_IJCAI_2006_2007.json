{"abstract":{"entropy":6.4945921494653955,"topics":["machine learning, novel approach, present novel, description logic, present approach, natural language, knowledge base, learning systems, data mining, sense disambiguation, word sense, learning, classifier systems, semantic web, web pages, learning classifier, present systems, reinforcement learning, dimensionality reduction, modal logic","genetic algorithm, evolutionary algorithm, genetic programming, particle swarm, markov decision, algorithm, optimization problem, markov processes, present algorithm, algorithm optimization, decision processes, evolutionary computation, partially observable, multi-objective evolutionary, estimation distribution, evolution strategies, optimization, swarm optimization, particle optimization, describe algorithm","artificial intelligence, agents, real world, systems, play role, multi-agent systems, autonomous agents, mobile robots, spatial reasoning, actions uncertainty, goal agents, immune systems, games player, decision making, systems problem, problem agents, agents environment, research area, planning uncertainty, resource allocation","recent years, search heuristic, solving problem, constraint satisfaction, support vector, constraint problem, search space, satisfaction csp, search algorithm, problem, constraint csp, search problem, addresses problem, combinatorial problem, solutions problem, search, voting rules, local search, satisfaction problem, building blocks","machine learning, reinforcement learning, dimensionality reduction, neural network, social network, learning systems, bayesian network, information extraction, network, information retrieval, learning domains, systems user, research learning, important network, applying learning, learning neural, statistical learning, user, information, support machine","present novel, novel approach, present approach, present, present systems, approach, present framework, approach learning, present learning, novel learning, framework learning, paper approach, describe approach, approach based, framework argumentation, novel agents, present agents, introduce approach, approach systems, paper novel","evolution strategies, xcs fault, interactive configuration, learning xcs, analyze xcs, xcs systems, classifier xcs, games playing, gradient xcs, general playing, framework xcs, evolution strategy, first xcs, called xcs, learning function, work function, xcs, function, present evolution, evolution","markov decision, markov processes, decision processes, partially observable, observable pomdps, observable markov, partially pomdps, partially markov, markov model, observable decision, testing test, observable processes, markov pomdps, partially decision, markov mdps, partially processes, decision pomdps, processes mdps, algorithm markov, processes pomdps","multi-agent systems, agents environment, artificial immune, immune systems, systems, systems problem, systems agents, cognitive architecture, address problem, detection problem, agents model, students problem, intelligent tutoring, allows agents, problem agents, agents, multi-agent agents, describe systems, intelligent systems, systems control","autonomous agents, real world, play role, mobile robots, games player, monitoring systems, diagnosis systems, consider problem, important agents, agents able, important problem, important applications, world actions, applications problem, applications, distributed systems, distributed agents, model-based systems, autonomous robots, important role","recent years, solving problem, reinforcement learning, satisfiability problem, deals problem, recent research, problem learning, recent problem, years research, recent, recent algorithm, techniques problem, growing interest, efficient algorithm, recent interest, satisfiability efficient, techniques planning, problem reinforcement, years interest, problem interest","algorithm problem, combinatorial problem, optimization problem, problem, combinatorial optimization, solving problem, problem finding, consider problem, building blocks, solutions problem, algorithm gas, genetic gas, stock market, challenging problem, problem network, solve problem, study problem, online auctions, stochastic problem, optimal solutions"],"ranking":[["65959|AAAI|2007|A Robot That Uses Existing Vocabulary to Infer Non-Visual Word Meanings from Observation|The authors present TWIG, a visually grounded word-learning system that uses its existing knowledge of vocabulary, grammar, and action schemas to help it learn the meanings of new words from its environment. Most systems built to learn word meanings from sensory data focus on the \"base case\" of learning words when the robot knows nothing, and do not incorporate grammatical knowledge to aid the process of inferring meaning. The present study shows how using existing language knowledge can aid the word-learning process in three ways. First, partial parses of sentences can focus the robot's attention on the correct item or relation in the environment. Second, grammatical inference can suggest whether a new word refers to a unary or binary relation. Third, the robot's existing predicate schemas can suggest possibilities for a new predicate. The authors demonstrate that TWIG can use its understanding of the phrase '\"got the ball\" while watching a game of catch to learn that \"I\" refers to the speaker, \"you\" refers to the addressee, and the names refer to particular people. The robot then uses these new words to learn that \"am\" and \"are\" refer to the identity relation.|Kevin Gold,Brian Scassellati","57768|GECCO|2006|An anticipatory approach to improve XCSF|XCSF is a novel version of learning classifier systems (LCS) which extends the typical concept of LCS by introducing computable classifier prediction. In XCSF Classifier prediction is computed as a linear combination of classifier inputs and a weight vector associated to each classifier. Learning process takes place using a weight update mechanism. Initial results show that XCSF can be used to evolve accurate approximations of some functions. In this paper, we try to add an anticipatory component to XCSF improving its performance.|Amin Nikanjam,Adel Torkaman Rahmani","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","65761|AAAI|2006|Learning Systems of Concepts with an Infinite Relational Model|Relationships between concepts account for a large proportion of semantic knowledge. We present a nonparametric Bayesian model that discovers systems of related concepts. Given data involving several sets of entities, our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely. We apply our approach to four problems clustering objects and features, learning ontologies, discovering kinship systems, and discovering structure in political data.|Charles Kemp,Joshua B. Tenenbaum,Thomas L. Griffiths,Takeshi Yamada,Naonori Ueda","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","66054|AAAI|2007|Recognizing Textual Entailment Using a Subsequence Kernel Method|We present a novel approach to recognizing Textual Entailment. Structural features are constructed from abstract tree descriptions, which are automatically extracted from syntactic dependency trees. These features are then applied in a subsequence-kernel-based classifier to learn whether an entailment relation holds between two texts. Our method makes use of machine learning techniques using a limited data set, no external knowledge bases (e.g. WordNet), and no handcrafted inference rules. We achieve an accuracy of .% for text pairs in the Information Extraction and Question Answering task, .% for the RTE- test data, and .% for the RET- test data.|Rui Wang 0005,Günter Neumann","66224|AAAI|2007|PLOW A Collaborative Task Learning Agent|To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies deep natural language understanding, knowledge representation and reasoning, dialogue systems, planningagent-based systems and machine learning. A formal evaluation shows the approach has great promise.|James F. Allen,Nathanael Chambers,George Ferguson,Lucian Galescu,Hyuckchul Jung,Mary D. Swift,William Taysom","16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli","65952|AAAI|2007|Enabling Domain-Awareness for a Generic Natural Language Interface|In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.|Yunyao Li,Ishan Chaudhuri,Huahai Yang,Satinder Singh,H. V. Jagadish"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58046|GECCO|2007|Obtaining ground states of ising spin glasses via optimizing bonds instead of spins|Frustrated Ising spin glasses represent a rich class of challenging optimization problems that share many features with other complex, highly multimodal optimization and combinatorial problems. This paper shows that transforming candidate solutions to an alternative representation that is strongly tied to the energy function simplifies the exploration of the space of potential spin configurations and that it significantly improves performance of evolutionary algorithms with simple variation operators on Ising spin glasses. The proposed techniques are incorporated into the simple genetic algorithm, the univariate marginal distribution algorithm, and the hierarchical Bayesian optimization algorithm.|Martin Pelikan,Alexander K. Hartmann","57731|GECCO|2006|Automating the drug scheduling with different toxicity clearance in cancer chemotherapy via evolutionary computation|The toxicity of an anticancer drug is cleared from the body by different processes, including saturable metabolic and nonsaturable renal-excretion pathways. According to the principles of toxicokinetics, we propose a new anticancer drug scheduling model with different toxic elimination processes in this paper. We also present a sophisticated automating drug scheduling approach based on evolutionary computation and computer modeling. To explore multiple efficient drug scheduling policies, we use a multimodal optimization algorithm --- adaptive elitist-population based genetic algorithm (AEGA) to solve the new model, and discuss the situation of multiple optimal solutions under different parameter settings. The simulation results obtained by the new model match well with the clinical treatment experience, and can provide much more drug scheduling policies for a doctor to choose depending on the particular conditions of the patients.|Yong Liang,Kwong-Sak Leung,Tony Shu Kam Mok","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57860|GECCO|2006|The LEM implementation of learnable evolution model and its testing on complex function optimization problems|Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to ). In every experiment, LEM outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.|Janusz Wojtusiak,Ryszard S. Michalski","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","57970|GECCO|2007|A multi-objective approach for the prediction of loan defaults|Credit institutions are seldom faced with problems dealing with single objectives. Often, decisions involving optimizing two or more competing goals simultaneously need to be made, and conventional optimization routines and models are incapable of handling the problems. This study applies Fuzzy dominance based Simplex Genetic Algorithm (a multi-objective evolutionary optimization algorithm) in generating decision rules for predicting loan default in a typical credit institution.|Oluwarotimi Odeh,Praveen Koduru,Sanjoy Das,Allen M. Featherstone,Stephen Welch","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57751|GECCO|2006|Evolutionary learning with kernels a generic solution for large margin problems|In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters. We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.|Ingo Mierswa","57975|GECCO|2007|Scalability of particle swarm algorithms|When dealing with complex optimisations problems, evolutionary computation techniques have proven tobe very helpful. Amongst optimisation algorithms driven by evolutionary computation techniques,particle swarm algorithms have proven to be a very good alternative to genetic algorithms because of their faster convergence. However they can still suffer from premature convergence to local optima. Premature convergence occurs when the particles of the swarm are too close to each other to enable further exploration of the space. To put it another way, the dispersion or distribution of the swarm throughout the search space has been localised to a small region with a consequent stagnation of the search process. Many strategies have been used to try to prevent convergence to a local optimum. However little work has been done on problems of high dimensions. By high dimensions, we mean dimensions of  and above.This paper focuses, therefore, on the limitations of classical particle swarm algorithms when dealing with high dimensional problems. We compare different versions of Particle Swarm Algorithms GBest, LBest with ringor random neighbourhood of size citeClerc, and GCPSOciteVanDenBergh. Those algorithmswere run twice, with a linearly decreasing inertia weight, and with the use of a constriction factor.We also used two repulsive-based algorithms Charged PSOciteBlackwell and Predator PreyciteSilva.Our main focus is problems of high dimensionality. In particular, we applied the different algorithmsto the benchmarks functions Ackley and Rosenbrock, in the following dimensions , , .Even though these represent problems of relatively small dimensionality in a real-world context,experiments on higher dimensions have not been necessary to show the limits of the algorithms studied.Each experiment was run  times. The swarm size was chosen from $    $ and so that it isnot greater than the problem size. Each experiment is  iterations long.A one-way analysis of variance (ANOVA) was used to compare the performance of each algorithms. We found that the LBest algorithms perform significantly better when used with the constriction factor.GBest and GCPSO perform better with linearly decreasing inertia with a small swarm size, but better with the constriction factorwith a big swarm size. The improvement of GCPSO on GBest is not statistically significant in our experiments. The LBest algorithms with the constriction factor seem to be the best algorithms to handle problems of high dimensionality. The LBest algorithm with fixed neighbourhood seems to be less sensitive to the size of the swarm than the LBest algorithm with randomneighbourhood. Especially, in the case of the Rosenbrock function of size , increasing the sizeof the swarm does not improve the performance of LBest with constricted factor and fixed neighbourhood.The algorithms based on repulsion between particles, i.e. Charged Swarm and Predator Prey, do not perform very well.Indeed, even if the predator prey algorithm gives quite good results, it is trapped in a local optimum, as the fitness value stagnates on a constant value for the last % of iterations. This may come from a too low levelof repulsion. Tuning the parameters used for repulsion seems to be very important for high dimensionality problems. Experiments show that almost all the algorithms managed to solve the problems for dimension  butnone of the algorithms managed to solve the problem in the case of problems of dimension  and . The LBest algorithm with random neighbourhood and constriction factor performed the best. Further work will be done on modelling the size of the swarm required to be able to solve the problems. Other particle swarm algorithms will also be included.|Sébastien Piccand,Michael O'Neill,Jacqueline Walker"],["16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","66270|AAAI|2007|Dominance and Equivalence for Sensor-Based Agents|This paper describes recent results from the robotics community that develop a theory, similar in spirit to the theory of computation, for analyzing sensor-based agent systems. The central element to this work is a notion of dominance of one such system over another. This relation is formally based on the agents' progression through a derived information space, but may informally be understood as describing one agent's ability to \"simulate\" another. We present some basic properties of this dominance relation and demonstrate its usefulness by applying it to a basic problem in robotics. We argue that this work is of interest to a broad audience of artificial intelligence researchers for two main reasons. First, it calls attention to the possibility of studying belief spaces in way that generalizes both probabilistic and nondeterministic uncertainty models. Second, it provides a means for evaluating the information that an agent is able to acquire (via its sensors and via conformant actions), independent of any optimality criterion and of the task to be completed.|Jason M. O'Kane,Steven M. LaValle","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","57711|GECCO|2006|Dominance hierarchies and social diversity in multi-agent systems|In this study, we investigate self-organizing social hierarchies in multi-agent systems. Agents occupy the nodes of a small-world network and interact exclusively with other agents in their local neighbourhood. Here, the interactions represent competition for a limited resource. Monte-Carlo simulations show that the changes in a network's structure can alter the steady-state attributes for fixed rewardpenalty mechanisms. The results suggest that the expected phase transition from a homogeneous to a hierarchical society depends on (a) the relative strengths of the feedback mechanisms employed, (b) the underlying communication topology, and (c) whether previously dominated agents are replaced in the population by agents with higher social status. A key contribution of this paper is the coherent picture painted of the relationship between social differentiation and spatial structure in a multi-agent system.|Michael Kirley","16601|IJCAI|2007|An Axiomatic Approach to Personalized Ranking Systems|Personalized ranking systems and trust systems are an essential tool for collaboration in a multi-agent environment. In these systems, trust relations between many agents are aggregated to produce a personalized trust rating of the agents. In this paper we introduce the first extensive axiomatic study of this setting, and explore a wide array of well-known and new personalized ranking systems. We adapt several axioms (basic criteria) from the literature on global ranking systems to the context of personalized ranking systems, and prove strong properties implied by the combination of these axioms.|Alon Altman,Moshe Tennenholtz","16505|IJCAI|2007|Market Based Resource Allocation with Incomplete Information|Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).|Bo An,Chunyan Miao,Zhiqi Shen","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir"],["16396|IJCAI|2007|Quantified Constraint Satisfaction Problems From Relaxations to Explanations|The Quantified Constraint Satisfaction Problem (QCSP) is a generalisation of the classical CSP in which some of variables can be universally quantified. In this paper, we extend two well-known concepts in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency. We show that the generality of the QCSP allows for a number of different forms of relaxation not available in classical CSP. We further present an algorithmfor computing a generalisation of conflict-based explanations of inconsistency for the QCSP.|Alex Ferguson,Barry O'Sullivan","57629|GECCO|2006|GRASP - evolution for constraint satisfaction problems|There are several evolutionary approaches for solving random binary Constraint Satisfaction Problems (CSPs). In most of these strategies we find a complex use of information regarding the problem at hand. Here we present a hybrid Evolutionary Algorithm that outperforms previous approaches in terms of effectiveness and compares well in terms ofefficiency. Our algorithm is conceptual and simple, featuring a GRASP-like (GRASP stands for Greedy Randomized Adaptive Search Procedure) mechanism for genotype-to-phenotype mapping, and without considering any specific knowledge of the problem. Therefore, we provide a simple algorithm that harnesses generality while boosting performance.|Manuel Cebrián,Iván Dotú","66120|AAAI|2007|Filtering Decomposition and Search Space Reduction for Optimal Sequential Planning|We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.|Stéphane Grandcolas,C. Pain-Barre","66125|AAAI|2007|Transposition Tables for Constraint Satisfaction|In this paper, a state-based approach for the Constraint Satisfaction Problem (CSP) is proposed. The key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks. Classical techniques to avoid the resurgence of previously encountered conflicts involve recording conflict sets. This contrasts with our state-based approach which records subnetworks - a snapshot of some selected domains - already explored. This knowledge is later used to either prune inconsistent states or avoid recomputing the solutions of these subnetworks. Interestingly enough, the two approaches present some complementarity different states can be pruned from the same partial instantiation or conflict set, whereas different partial instantiations can lead to the same state that needs to be explored only once. Also, our proposed approach is able to dynamically break some kinds of symmetries (e.g. neighborhood interchangeability). The obtained experimental results demonstrate the promising prospects of state-based search.|Christophe Lecoutre,Lakhdar Sais,Sébastien Tabary,Vincent Vidal","66118|AAAI|2007|Best-First Search for Treewidth|Finding the exact treewidth of a graph is central to many operations in a variety of areas, including probabilistic reasoning and constraint satisfaction. Treewidth can be found by searching over the space of vertex elimination orders. This search space differs from those where best-first search is typically applied, because a solution path is evaluated by its maximum edge cost instead of the sum of its edge costs. We show how to make best-first search admissible on max-cost problem spaces. We also employ breadth-first heuristic search to reduce the memory requirement while still eliminating all duplicate nodes in the search space. Our empirical results show that our algorithms find the exact treewidth an order of magnitude faster than the previous state-of-the-art algorithm on hard benchmark graphs.|P. Alex Dow,Richard E. Korf","65700|AAAI|2006|Extending Dynamic Backtracking to Solve Weighted Conditional CSPs|Many planning and design problems can be characterized as optimal search over a constrained network of conditional choices with preferences. To draw upon the advanced methods of constraint satisfaction to solve these types of problems, many dynamic and flexible CSP variants have been proposed. One such variant is the Weighted Conditional CSP (WCCSP). So far, however, little work has been done to extend the full suite of CSP search algorithms to solve these CSP variants. In this paper, we extend Dynamic Backtracking and similar backjumping-based CSP search algorithms to solve WCCSPs by utilizing activity constraints and soft constraints in order to quickly prune infeasible and suboptimal regions of the search space. We provide experimental results on randomly generated WCCSP instances to prove these claims.|Robert T. Effinger,Brian C. Williams","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","65736|AAAI|2006|A New Approach to Distributed Task Assignment using Lagrangian Decomposition and Distributed Constraint Satisfaction|We present a new formulation of distributed task assignment, called Generalized Mutual Assignment Problem (GMAP), which is derived from an NP-hard combinatorial optimization problem that has been studied for many years in the operations research community. To solve the GMAP, we introduce a novel distributed solution protocol using Lagrangian decomposition and distributed constraint satisfaction, where the agents solve their individual optimization problems and coordinate their locally optimized solutions through a distributed constraint satisfaction technique. Next, to produce quick agreement between the agents on a feasible solution with reasonably good quality, we provide a parameter that controls the range of \"noise\" mixed with an incrementdecrement in a Lagrange multiplier. Our experimental results indicate that the parameter may allow us to control tradeoffs between the quality of a solution and the cost of finding it.|Katsutoshi Hirayama","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","65988|AAAI|2007|Using Expectation Maximization to Find Likely Assignments for Solving CSPs|We present a new probabilistic framework for finding likely variable assignments in difficult constraint satisfaction problems. Finding such assignments is key to efficient search, but practical efforts have largely been limited to random guessing and heuristically designed weighting systems. In contrast, we derive a new version of Belief Propagation (BP) using the method of Expectation Maximization (EM). This allows us to differentiate between variables that are strongly biased toward particular values and those that are largely extraneous. Using EM also eliminates the threat of non-convergence associated with regular BP. Theoretically, the derivation exhibits appealing primaldual semantics. Empirically, it produces an \"EMBP\"-based heuristic for solving constraint satisfaction problems, as illustrated with respect to the Quasigroup with Holes domain. EMBP outperforms existing techniques for guiding variable and value ordering during backtracking search on this problem.|Eric I. Hsu,Matthew Kitching,Fahiem Bacchus,Sheila A. McIlraith"],["16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","16529|IJCAI|2007|Inferring Long-term User Properties Based on Users Location History|Recent development of location technologies enables us to obtain the location history of users. This paper proposes a new method to infer users' longterm properties from their respective location histories. Counting the instances of sensor detection for every user, we can obtain a sensor-user matrix. After generating features from the matrix, a machine learning approach is taken to automatically classify users into different categories for each user property. Inspired by information retrieval research, the problem to infer user properties is reduced to a text categorization problem. We compare weightings of several features and also propose sensor weighting. Our algorithms are evaluated using experimental location data in an office environment.|Yutaka Matsuo,Naoaki Okazaki,Kiyoshi Izumi,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideyuki Nakashima","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","65987|AAAI|2007|Fish Inspection System using a Parallel Neural Network Chip and Image Knowledge Builder Application|A generic image learning system, CogniSight, is being used for the inspection of fishes before filleting off-shore. More than thirty systems have been deployed on seven fishing vessels in Norway and Iceland over the past three years. Each CogniSight uses four neural network chips (a total of  neurons) based on a natively parallel hardwired architecture perfonning real time learning and non-linear classification (RBF). These systems are trained by the ship crew using Image Knowledge Builder, a \"show and tell\" interface for easy training and validation. Fishermen can reinforce the learning at anytime when needed. The use of CogniSight has reduced significantly the number of crewmembers on the boats (by up to six persons) and the time at sea has shortened by %. The prompt and strong return of the investment to the fishing fleet has increased significantly the market shares of Pisces Industries, the company integrating CogniSight systems to its filleting machines.|Anne Menendez,Guy Paillet","16517|IJCAI|2007|Learning to Count by Think Aloud Imitation|Although necessary, learning to discover new solutions is often long and difficult, even for supposedly simple tasks such as counting. On the other hand, learning by imitation provides a simple way to acquire knowledge by watching other agents do. In order to learn more complex tasks by imitation than mere sequences of actions, a Think Aloud protocol is introduced, with a new neuro-symbolic network. The latter uses time in the same way as in a Time Delay Neural Network, and is added basic first order logic capacities. Tested on a benchmark counting task, learning is very fast, generalization is accurate, whereas there is no initial bias toward counting.|Laurent Orseau","16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli","65766|AAAI|2006|Lessons on Applying Automated Recommender Systems to Information-Seeking Tasks|Automated recommender systems predict user preferences by applying machine learning techniques to data on products, users, and past user preferences for products. Such systems have become increasingly popular in entertainment and e-commerce domains, but have thus far had little success in information-seeking domains such as identifying published research of interest. We report on several recent publications that show how recommenders can be extended to more effectively address information-seeking tasks by expanding the focus from accurate prediction of user preferences to identifying a useful set of items to recommend in response to the user's specific information need. Specific research demonstrates the value of diversity in recommendation lists, shows how users value lists of recommendations as something different from the sum of the individual recommendations within, and presents an analytic model for customizing a recommender to match user information-seeking needs.|Joseph A. Konstan,Sean M. McNee,Cai-Nicolas Ziegler,Roberto Torres,Nishikant Kapoor,John Riedl","16723|IJCAI|2007|Generalized Additive Bayesian Network Classifiers|Bayesian network classifiers (BNC) have received considerable attention in machine learning field. Some special structure BNCs have been proposed and demonstrate promise performance. However, recent researches show that structure learning in BNs may lead to a non-negligible posterior problem, i.e, there might be many structures have similar posterior scores. In this paper, we propose a generalized additive Bayesian network classifiers, which transfers the structure learning problem to a generalized additive models (GAM) learning problem. We first generate a series of very simple BNs, and put them in the framework of GAM, then adopt a gradient-based algorithm to learn the combining parameters, and thus construct a more powerful classifier. On a large suite of benchmark data sets, the proposed approach outperforms many traditional BNCs, such as naive Bayes, TAN, etc, and achieves comparable or better performance in comparison to boosted Bayesian network classifiers.|Jianguo Li,Changshui Zhang,Tao Wang,Yimin Zhang","16640|IJCAI|2007|Learning Implied Global Constraints|Finding a constraint network that will be efficiently solved by a constraint solver requires a strong expertise in Constraint Programming. Hence, there is an increasing interest in automatic reformulation. This paper presents a general framework for learning implied global constraints in a constraint network assumed to be provided by a non-expert user. The learned global constraints can then be added to the network to improve the solving process. We apply our technique to global cardinality constraints. Experiments show the significance of the approach.|Christian Bessière,Remi Coletta,Thierry Petit"],["16579|IJCAI|2007|Team Programming in Golog under Partial Observability|In this paper, we present the agent programming language TEAMGOLOG, which is a novel approach to programming a team of cooperative agents under partial observability. Every agent is associated with a partial control program in Golog, which is completed by the TEAMGOLOG interpreter in an optimal way by assuming a decision-theoretic semantics. The approach is based on the key concepts of a synchronization state and a communication state, which allow the agents to passively resp. actively coordinate their behavior, while keeping their belief states, observations, and activities invisible to the other agents. We show the usefulness of the approach in a rescue simulated domain.|Alessandro Farinelli,Alberto Finzi,Thomas Lukasiewicz","16736|IJCAI|2007|A Lattice-Based Approach to Computing Warranted Beliefs in Skeptical Argumentation Frameworks|This paper introduces a novel approach to model warrant computation in a skeptical abstract argumentation framework. We show that such search space can be defined as a lattice, and illustrate how the so-called dialectical constraints can play a role for guiding the efficient computation of warranted arguments.|Carlos Iván Chesñevar,Guillermo Ricardo Simari","66078|AAAI|2007|A Search via Approximate Factoring|We present a novel method for creating A* estimates for structured search problems originally described in Haghighi, DeNero, & Klein (). In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein & Manning (), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and demonstrate its effectiveness.|Aria Haghighi,John DeNero,Dan Klein","65793|AAAI|2006|Semi-supervised Multi-label Learning by Constrained Non-negative Matrix Factorization|We present a novel framework for multi-label learning that explicitly addresses the challenge arising from the large number of classes and a small size of training data. The key assumption behind this work is that two examples tend to have large overlap in their assigned class memberships if they share high similarity in their input patterns. We capitalize this assumption by first computing two sets of similarities, one based on the input patterns of examples, and the other based on the class memberships of the examples. We then search for the optimal assignment of class memberships to the unlabeled data that minimizes the difference between these two sets of similarities. The optimization problem is formulated as a constrained Non-negative Matrix Factorization (NMF) problem, and an algorithm is presented to efficiently find the solution. Compared to the existing approaches for multi-label learning, the proposed approach is advantageous in that it is able to explore both the unlabeled data and the correlation among different classes simultaneously. Experiments with text categorization show that our approach performs significantly better than several state-of-the-art classification techniques when the number of classes is large and the size of training data is small.|Yi Liu,Rong Jin,Liu Yang","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","16364|IJCAI|2007|Correlation Clustering for Crosslingual Link Detection|The crosslingual link detection problem calls for identifying news articles in multiple languages that report on the same news event. This paper presents a novel approach based on constrained clustering. We discuss a general way for constrained clustering using a recent, graph-based clustering framework called correlation clustering. We introduce a correlation clustering implementation that features linear program chunking to allow processing larger datasets. We show how to apply the correlation clustering algorithm to the crosslingual link detection problem and present experimental results that show correlation clustering improves upon the hierarchical clustering approaches commonly used in link detection, and, hierarchical clustering approaches that take constraints into account.|Jurgen Van Gael,Xiaojin Zhu","16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli","65952|AAAI|2007|Enabling Domain-Awareness for a Generic Natural Language Interface|In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.|Yunyao Li,Ishan Chaudhuri,Huahai Yang,Satinder Singh,H. V. Jagadish","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"],["57615|GECCO|2006|Hyper-ellipsoidal conditions in XCS rotation linear approximation and solution structure|The learning classifier system XCS is an iterative rule-learning system that evolves rule structures based on gradient-based prediction and rule quality estimates. Besides classification and reinforcement learning tasks, XCS was applied as an effective function approximator. Hereby, XCS learns space partitions to enable a maximally accurate and general function approximation. Recently, the function approximation approach was improved by replacing () hyperrectangular conditions with hyper-ellipsoids and () iterative linear approximation with the recursive least squares method. This paper combines the two approaches assessing the usefulness of each. The evolutionary process is further improved by changing the mutation operator implementing an angular mutation that rotates ellipsoidal structures explicitly. Both enhancements improve XCS performance in various non-linear functions. We also analyze the evolving ellipsoidal structures confirming that XCS stretches and rotates the evolving ellipsoids according to the shape of the underlying function. The results confirm that improvements in both the evolutionary approach and the gradient approach can result in significantly better performance.|Martin V. Butz,Pier Luca Lanzi,Stewart W. Wilson","57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","57712|GECCO|2006|Standard and averaging reinforcement learning in XCS|This paper investigates reinforcement learning (RL) in XCS. First, it formally shows that XCS implements a method of generalized RL based on linear approximators, in which the usual input mapping function translates the state-action space into a niche relative fitness space. Then, it shows that, although XCS has always been related to standard RL, XCS is actually a method of averaging RL. More precisely, XCS with gradient descent can be actually derived from the typical update of averaging RL. It is noted that the use of averaging RL in XCS introduces an intrinsic preference toward classifiers with a smaller fitness in the niche. It is argued that, because of the accuracy pressure in XCS, this results in an additional preference toward specificity. A very simple experiment is presented to support this hypothesis. The same approach is applied to XCS with computed prediction (XCSF) and similar conclusions are drawn.|Pier Luca Lanzi,Daniele Loiacono","58011|GECCO|2007|Introducing fault tolerance to XCS|In this paper, we introduce fault tolerance to XCS and propose a new XCS framework called XCS with Fault Tolerance (XCSFT). As an important branch of learning classifier systems, XCS has been proven capable of evolving maximally accurate, maximally general problem solutions. However, in practice, it oftentimes generates a lot of rules, which lower the readability of the evolved classification model, and thus, people may not be able to get the desired knowledge or useful information out of the model. Inspired by the fault tolerance mechanism proposed in field of data mining, we devise a new XCS framework by integrating the concept and mechanism of fault tolerance into XCS in order to reduce the number of classification rules and therefore to improve the readability of the generated prediction model. A series of $N$-multiplexer experiments, including -bit, -bit, -bit, and -bit multiplexers, are conducted to examine whether XCSFT can accomplish its goal of design. According to the experimental results, XCSFT can offer the same level of prediction accuracy on the test problems as XCS can, while the prediction model evolved by XCSFT consists of significantly fewer classification rules.|Hong-Wei Chen,Ying-Ping Chen","58201|GECCO|2007|Empirical analysis of generalization and learning in XCS with gradient descent|We analyze generalization and learning in XCS with gradient descent. At first, we show that the addition of gradient in XCS may slow down learning because it indirectly decreases the learning rate. However, in contrast to what was suggested elsewhere, gradient descent has no effect on the achieved generalization. We also show that when gradient descent is combined with roulette wheel selection, which is known to be sensitive to small values of the learning rate, the learning speed can slow down dramatically. Previous results reported no difference in the performance of XCS with gradient descent when roulette wheel selection or tournament selection were used. In contrast, we suggest that gradient descent should always be combined with tournament selection, which is not sensitive to the value of the learning rate. When gradient descent is used in combination with tournament selection, the results show that (i) the slowdown in learning is limited and (ii) the generalization capabilities of XCS are not affected.|Pier Luca Lanzi,Martin V. Butz,David E. Goldberg","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","57616|GECCO|2006|Studying XCSBOA learning in Boolean functions structure encoding and random Boolean functions|Recently, studies with the XCS classifier system on Boolean functions have shown that in certain types of functions simple crossover operators can lead to disruption and, consequently, a more effective recombination mechanism is required. Simple crossover operators were replaced by recombination based on estimation of distribution algorithms (EDAs). The combination showed that XCS with such a statistics-based crossover operator can solve challenging hierarchical functions more efficiently. This study elaborates the gained competence further investigating the coding scheme for the EDA component (BOA in our case) of XCS as well as performance in randomly generated Boolean function problems. Results in hierarchical Boolean functions show that the originally used -bit coding scheme induces a certain learning bias that stresses additional diversity in the evolving XCS population. A -bit coding scheme as well as a restricted -bit coding scheme confirm the suspected bias. The alternative encodings decrease the unnecessary bias towards specificity and increase performance robustness. The paper concludes with a discussion on the challenges ahead for XCS in Boolean function problems as well as on the implications of the obtained results for real-valued and multiple-valued classification problems, multi-step problems, and function approximation problems.|Martin V. Butz,Martin Pelikan","57981|GECCO|2007|Modeling XCS in class imbalances population size and parameter settings|This paper analyzes the scalability of the population size required in XCS to maintain nichesthat are infrequently activated.Facetwise models have been developed to predict the effect of the imbalance ratio--ratio betweenthe number of instances of the majority class and the minority class that are sampled to XCS--on population initialization, andon the creation and deletion of classifiers of the minority class. While theoretical models show that, ideally, XCS scales linearly with the imbalance ratio, XCS with standard configuration scales exponentially. The causes that are potentially responsible for this deviation from the ideal scalability are also investigated. Specifically, the inheritance procedure of classifiers' parameters, mutation, and subsumption are analyzed, and improvements in XCS's mechanisms are proposed to effectively and efficiently handle imbalanced problems. Once the recommendations are incorporated to XCS, empirical results show that the population size in XCS indeed scales linearly with the imbalance ratio.|Albert Orriols-Puig,David E. Goldberg,Kumara Sastry,Ester Bernadó-Mansilla","57633|GECCO|2006|FTXI fault tolerance XCS in integer|In the realm of data mining, several key issues exists in the traditional classification algorithms, such as low readability, large rule number, and low accuracy with information losing. In this paper, we propose a new classification methodology, called fault tolerance XCS in integer (FTXI), by extending XCS to handle conditions in integers and integrating the mechanism of fault tolerance in the context of data mining into the framework of XCS. We also design and generate appropriate artificial data sets for examining and verifying the proposed method. Our experiments indicate that FTXI can provide the least rule number, obtain high prediction accuracy, and offer rule readability, compared to C. and XCS in integer without fault tolerance.|Hong-Wei Chen,Ying-Ping Chen","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","66123|AAAI|2007|Continuous State POMDPs for Object Manipulation Tasks|My research focus is on using continuous state partially observable Markov decision processes (POMDPs) to perform object manipulation tasks using a robotic arm. During object manipulation, object dynamics can be extremely complex, non-linear and challenging to specify. To avoid modeling the full complexity of possible dynamics. I instead use a model which switches between a discrete number of simple dynamics models. By learning these models and extending Porta's continuous state POMDP framework (Porta et at. ) to incorporate this switching dynamics model, we hope to handle tasks that involve absolute and relative dynamics within a single framework. This dynamics model may be applicable not only to object manipulation tasks, but also to a number of other problems, such as robot navigation. By using an explicit model of uncertainty, I hope to create solutions to object manipulation tasks that more robustly handle the noisy sensory information received by physical robots.|Emma Brunskill","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,François Charpillet","65786|AAAI|2006|Incremental Least Squares Policy Iteration for POMDPs|We present a new algorithm, called incremental least squares policy iteration (ILSPI), for finding the infinite-horizon stationary policy for partially observable Markov decision processes (POMDPs). The ILSPI algorithm computes a basis representation of the infinite-horizon value function by minirnizing the square of Bellman residual and performs policy improvenent in reachable belief states. A number of optimal basis functions are determined by the algorithm to minimize the Bellman residual incrementally, via efficient computations. We show that, by using optimally determined basis functions, the policy can be improved successively on a set of most probable belief points sampled from the reachable belief set. As the ILSPI is based on belief sample points, it represents a point-based policy iteration method. The results on four benchmark problems show that the ILSPI compares competitively to its value-iteration counterparts in terms of both performance and computational efficiency.|Hui Li,Xuejun Liao,Lawrence Carin","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein"],["66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","65626|AAAI|2006|Quantifying Incentive Compatibility of Ranking Systems|Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. When the set of agents and the set of alternatives coincide, we get the ranking systems setting. A famous type of ranking systems are page ranking systems in the context of search engines. Such ranking systems do not exist in empty space, and therefore agents' incentives should be carefully considered. In this paper we define three measures for quantifying the incentive compatibility of ranking systems. We apply these measures to several known ranking systems, such as PageRank, and prove tight bounds on the level of incentive compatibility under two basic properties strong monotonicity and non-imposition. We also introduce two novel nonimposing ranking systems, one general, and the other for the case of systems with three participants. A full axiomatization is provided for the latter.|Alon Altman,Moshe Tennenholtz","16601|IJCAI|2007|An Axiomatic Approach to Personalized Ranking Systems|Personalized ranking systems and trust systems are an essential tool for collaboration in a multi-agent environment. In these systems, trust relations between many agents are aggregated to produce a personalized trust rating of the agents. In this paper we introduce the first extensive axiomatic study of this setting, and explore a wide array of well-known and new personalized ranking systems. We adapt several axioms (basic criteria) from the literature on global ranking systems to the context of personalized ranking systems, and prove strong properties implied by the combination of these axioms.|Alon Altman,Moshe Tennenholtz","65871|AAAI|2006|Deeper Natural Language Processing for Evaluating Student Answers in Intelligent Tutoring Systems|This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.|Vasile Rus,Arthur C. Graesser","16391|IJCAI|2007|Near-Optimal Anytime Coalition Structure Generation|Forming effective coalitions is a major research challenge in the field of multi-agent systems. Central to this endeavour is the problem of determining the best set of agents that should participate in a given team. To this end, in this paper, we present a novel, anytime algorithm for coalition structure generation that is faster than previous anytime algorithms designed for this purpose. Our algorithm can generate solutions that either have a tight bound from the optimal or are optimal (depending on the objective) and works by partitioning the space in terms of a small set of elements that represent structures which contain coalitions of particular sizes. It then performs an online heuristic search that prunes the space and only considers valid and non-redundant coalition structures. We empirically show that we are able to find solutions that are, in the worst case, % efficient in .% of the time to find the optimal value by the state of the art dynamic programming (DP) algorithm (for  agents), using % less memory.|Talal Rahwan,Sarvapali D. Ramchurn,Viet Dung Dang,Nicholas R. Jennings","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","16649|IJCAI|2007|Dynamic Verification of Trust in Distributed Open Systems|In open and distributed systems, agents must engage in interactions of which they have no previous experience. Deontic models are widely used to describe aspects of permission, obligation, and trust anticipated by such agents, but no practical mechanism has been developed for testing deontic trust specifications against models of multi-agent interactions. This paper describes a way of doing this an implementation of it via model checking and some preliminary results on a realistic example.|Nardine Osman,David Robertson","65820|AAAI|2006|Strong Mediated Equilibrium|Stability against potential deviations by sets of agents is a most desired property in the design and analysis of multi-agent systems. However, unfortunately, this property is typically not satisfied. In game-theoretic terms, a strong equilibrium, which is a strategy profile immune to deviations by coalition, rarely exists. This paper suggests the use of mediators in order to enrich the set of situations where we can obtain stability against deviations by coalitions. A mediator is defined to be a reliable entity, which can ask the agents for the right to play on their behalf, and is guaranteed to behave in a pre-specified way based on messages received from the agents. However, a mediator cannot enforce behavior that is, agents can play in the game directly, without the mediator's help. A mediator generates a new game for the players, the mediated game. We prove some general results about mediators, and mainly concentrate on the notion of strong mediated equilibrium, which is just a strong equilibrium at the mediated game. We show that desired behaviors, which are stable against deviations by coalitions, can be obtained using mediators in several classes of settings.|Dov Monderer,Moshe Tennenholtz","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","16675|IJCAI|2007|Multi-Agent System that Attains Longevity via Death|We propose a novel approach to self-regenerating systems which require continuous operation, such as security surveillance. For that aim we introduce HADES, a self-regenerating cooperative multi-agent system with local monitoring. When agents of HADES find local failures they repair them. However, in extreme cases repair may not be possible and irregular aggressive agents will multiply. These irregular agents may use all of the system's resources and thus take over the system. To optimize system longevity, we identify protocols for killing these irregular agents. Our primary contribution is a double communication protocol of alert and death signals among the agents, making the multi-agent system robust to failures and attacks.|Megan Olsen,Hava T. Siegelmann"],["65628|AAAI|2006|Keeping in Touch Maintaining Biconnected Structure by Homogeneous Robots|For many distributed autonomous robotic systems, it is important to maintain communication connectivity among the robots. That is, each robot must be able to communicate with each other robot, perhaps through a series of other robots. Ideally, this property should be robust to the removal of any single robot from the system. In (Ahmadi & Stone a) we define a property of a team's communication graph that ensures this property, called biconnectivity. In that paper, a distributed algorithm to check if a team of robots is biconnected and its correctness proof are also presented. In this paper we provide distributed algorithms to add and remove robots tofrom a multi-robot team while maintaining the biconnected property. These two algorithms are implemented and tested in the PlayerStage simulator.|Mazda Ahmadi,Peter Stone","65757|AAAI|2006|Diagnosis of Multi-Robot Coordination Failures Using Distributed CSP Algorithms|With increasing deployment of systems involving multiple coordinating agents, there is a growing need for diagnosing coordination failures in such systems. Previous work presented centralized methods for coordination failure diagnosis however, these are not always applicable, due to the significant computational and communication requirements, and the brittleness of a single point of failure. In this paper we propose a distributed approach to model-based coordination failure diagnosis. We model the coordination between the agents as a constraint graph, and adapt several algorithms from the distributed CSP area, to use as the basis for the diagnosis algorithms. We evaluate the algorithms in extensive experiments with simulated and real Sony Aibo robots and show that in general a trade-off exists between the computational requirements of the algorithms, and their diagnosis results. Surprisingly, in contrast to results in distributed CSPs, the asynchronous backtracking algorithm outperforms stochastic local search in terms of both quality and runtime.|Meir Kalech,Gal A. Kaminka,Amnon Meisels,Yehuda Elmaliach","65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","65629|AAAI|2006|Biconnected Structure for Multi-Robot Systems|Many applications of distributed autonomous robotic systems can benefit from, or even may require, the team of robots staying within communication connectivity. For example, consider the problem of multirobot surveillance (Ahmadi & Stone ), in which a team of robots must collaboratively patrol a given area. If any two robots can directly communicate at all times, the robots can coordinate for efficient behavior. This condition holds trivially in environments that are smaller than the robots' communication range. However in larger environments, the robots must actively maintain physical locations such that any two robots can communicate -- possibly through a series of other robots. Otherwise, the robots may lose track of each others' activities and become miscoordinated. Furthermore, since robots are relatively unreliable andor may need to change tasks (for example if a robot is suddenly called by a human user to perform some other task), in a stable multirobot surveillance system, if one of the robots leaves or crashes, the rest should still be able to communicate. Some examples of other tasks that could benefit from any pair of robots being able to communicate with each other, are multi-robot exploration, search and rescue, and cleaning robots. We say that robot R is connected to robot R if there is a series of robots, each within communication range of the previous, which can pass a message from R to R. It is not possible to maintain connectivity in the face of arbitrary numbers of robot departures if there are any two robots that are not within communication of one another and all other robots simultaneously depart, the system becomes disconnected. Thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust its positioning in response to a departure more quickly than a second departure will occur. In order for the team to stay connected, even in the face of any single departure, it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. We call this property biconnectivity the removal of any one robot from the system does not disconnect the remaining robots from each other.|Mazda Ahmadi,Peter Stone","16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","66107|AAAI|2007|Predictive Exploration for Autonomous Science|Often remote investigations use autonomous agents to observe an environment on behalf of absent scientists. Predictive exploration improves these systems' efficiency with onboard data analysis. Agents can learn the structure of the environment and predict future observations, reducing the remote exploration problem to one of experimental design. In our formulation information gain over a map guides exploration decisions, while a similar criterion suggests the most informative data products for downlink. Ongoing work will develop appropriate models for surface exploration by planetary robots. Experiments will demonstrate these algorithms on kilometer-scale autonomous geology tasks.|David R. Thompson","16612|IJCAI|2007|Efficient Planning of Informative Paths for Multiple Robots|In many sensing applications, including environmental monitoring, measurement systems must cover a large space with only limited sensing resources. One approach to achieve required sensing coverage is to use robots to convey sensors within this space. Planning the motion of these robots - coordinating their paths in order to maximize the amount of information collected while placing bounds on their resources (e.g., path length or energy capacity) - is aNP-hard problem. In this paper, we present an efficient path planning algorithm that coordinates multiple robots, each having a resource constraint, to maximize the \"informativeness\" of their visited locations. In particular, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to characterize the amount of information collected. We provide strong theoretical approximation guarantees for our algorithm by exploiting the submodularity property of mutual information. In addition, we improve the efficiency of our approach by extending the algorithm using branch and bound and a region-based decomposition of the space. We provide an extensive empirical analysis of our algorithm, comparing with existing heuristics on datasets from several real world sensing applications.|Amarjeet Singh 0003,Andreas Krause,Carlos Guestrin,William J. Kaiser,Maxim A. Batalin","65896|AAAI|2006|Real-Time Interactive Learning in the NERO Video Game|In the NeuroEvolving Robotic Operatives (NERO) video game, the player trains a team of virtual robots for combat against other players' teams. The virtual robots learn in real time through interacting with the player. Since NERO was originally released in June, , it has been downloaded over , times, appeared on Slashdot, and won several honors. The real-time NeuroEvolution of Augmenting Topologies (rt-NEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, drives the robots' learning, making possible this entirely new genre of video game. The live demo will show how agents in NERO adapt in real time as they interact with the player. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.|Kenneth O. Stanley,Igor Karpov,Risto Miikkulainen,Aliza Gold","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir"],["16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","65832|AAAI|2006|Probabilistic Goal Recognition in Interactive Narrative Environments|Recent years have witnessed a growing interest in interactive narrative-centered virtual environments for education, training, and entertainment. Narrative environments dynamically craft engaging story-based experiences for users, who are themselves active participants in unfolding stories. A key challenge posed by interactive narrative is recognizing users' goals so that narrative planners can dynamically orchestrate plot elements and character actions to create rich, customized stories. In this paper we present an inductive approach to predicting users' goals by learning probabilistic goal recognition models. This approach has been evaluated in a narrative environment for the domain of microbiology in which the user plays the role of a medical detective solving a science mystery. An empirical evaluation of goal recognition based on n-gram models and Bayesian networks suggests that the models offer significant predictive power.|Bradford W. Mott,Sunyoung Lee,James C. Lester","66236|AAAI|2007|Optimal Regression for Reasoning about Knowledge and Actions|We show how in the propositional case both Reiter's and Scherl & Levesque's solutions to the frame problem can be modelled in dynamic epistemic logic (DEL), and provide an optimal regression algorithm for the latter. Our method is as follows we extend Reiter's framework by integrating observation actions and modal operators of knowledge, and encode the resulting formalism in DEL with announcement and assignment operators. By extending Lutz' recent satisfiability-preserving reduction to our logic, we establish optimal decision procedures for both Reiter's and Scherl & Levesque's approaches satisfiability is NP-complete for one agent, PSPACE-complete for multiple agents and EXPTIME-complete when common knowledge is involved.|Hans P. van Ditmarsch,Andreas Herzig,Tiago De Lima","65653|AAAI|2006|On the Difficulty of Modular Reinforcement Learning for Real-World Partial Programming|In recent years there has been a great deal of interest in \"modular reinforcement learning\" (MRL). Typically, problems are decomposed into concurrent subgoals, allowing increased scalability and state abstraction. An arbitrator combines the subagents' preferences to select an action. In this work, we contrast treating an MRL agent as a set of subagents with the same goal with treating an MRL agent as a set of subagents who may have different, possibly conflicting goals. We argue that the latter is a more realistic description of real-world problems, especially when building partial programs. We address a range of algorithms for single-goal MRL, and leveraging social choice theory, we present an impossibility result for applications of such algorithms to multigoal MRL. We suggest an alternative formulation of arbitration as scheduling that avoids the assumptions of comparability of preference that are implicit in single-goal MRL. A notable feature of this formulation is the explicit codification of the tradeoffs between the subproblems. Finally, we introduce ABL, a language that encapsulates many of these ideas.|Sooraj Bhat,Charles Lee Isbell Jr.,Michael Mateas","66051|AAAI|2007|Understanding Performance Tradeoffs in Algorithms for Solving Oversubscribed Scheduling|In recent years, planning and scheduling research has paid increasing attention to problems that involve resource oversubscription, where cumulative demand for resources outstrips their availability and some subset of goals or tasks must be excluded. Two basic classes of techniques to solve oversubscribed scheduling problems have emerged searching directly in the space of possible schedules and searching in an alternative space of task permutations (by relying on a schedule builder to provide a mapping to schedule space). In some problem contexts, permutation-based search methods have been shown to outperform schedule-space search methods, while in others the opposite has been shown to be the case. We consider two techniques for which this behavior has been observed TaskSwap (TS), a schedule-space repair search procedure, and Squeaky Wheel Optimization (SWO), a permutation-space scheduling procedure. We analyze the circumstances under which one can be expected to dominate the other. Starting from a real-world scheduling problem where SWO has been shown to outperform TS, we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem, where TS has been found to outperform SWO. Experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate.|Laurence A. Kramer,Laura Barbulescu,Stephen F. Smith","57612|GECCO|2006|A new hybrid evolutionary algorithm for the huge -cardinality tree problem|In recent years it has been shown that an intelligent combination of metaheuristics with other optimization techniques can significantly improve over the application of a pure metaheuristic. In this paper, we combine the evolutionary computation paradigm with dynamic programming for the application to the NP-hard k-cardinality tree problem. Given an undirected graph G with node and edge weights, this problem consists of finding a tree in G with exactly k edges such that the sum of the weights is minimal. The genetic operators of our algorithm are based on an existing dynamic programming algorithm from the literature for finding optimal subtrees in a given tree. The simulation results show that our algorithm is able to improve the best known results for benchmark problems from the literature in  cases.|Christian Blum","16449|IJCAI|2007|Edge Partitioning in External-Memory Graph Search|There is currently much interest in using external memory, such as disk storage, to scale up graph-search algorithms. Recent work shows that the local structure of a graph can be leveraged to substantially improve the efficiency of external-memory graph search. This paper introduces a technique, called edge partitioning, which exploits a form of local structure that has not been considered in previous work. The new technique improves the scalability of structured approaches to external-memory graph search, and also guarantees the applicability of these approaches to any graph-search problem. We show its effectiveness in an external-memory graph-search algorithm for domain-independent STRIPS planning.|Rong Zhou,Eric A. Hansen","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. Gürgen"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","57678|GECCO|2006|Exploring network topology evolution through evolutionary computations|We present an evolutionary methodology that explores the evolution of network topology when a uniform growth of the network traffic is considered. The network redesign problem is formulated as an optimization problem, subject to a set of design and performance constraints, while minimizing the redesign cost by maintaining as many as possible of the network devices that constitute the original topology. The experimental results for a -level network redesign problem (consisting of  client nodes) demonstrate the value of the search technique within the genetic algorithms in finding good solutions with respect to redesign cost and time.|Sami J. Habib,Alice C. Parker","65868|AAAI|2006|Expressive Commerce and Its Application to Sourcing|Sourcing professionals buy several trillion dollars worth of goods and services yearly. We introduced a new paradigm called expressive commerce and applied it to sourcing. It combines the advantages of highly expressive human negotiation with the advantages of electronic reverse auctions. The idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auctions, and are algorithmically cleared. This creates a Pareto efficiency improvement in the allocation (a win-win between the buyer and the sellers) but the market clearing problem is a highly complex combinatorial optimization problem. We developed the world's fastest tree search algorithms for solving it. We have hosted $ billion of sourcing using the technology, and created $. billion of hard-dollar savings. The suppliers also benefited by being able to express production efficiencies and creativity, and through exposure problem removal. Supply networks were redesigned, with quantitative understanding of the tradeoffs, and implemented in weeks instead of months.|Tuomas Sandholm","16748|IJCAI|2007|Truthful Risk-Managed Combinatorial Auctions|Given a winning-bid withdrawal in a combinatorial auction, finding an alternative repair solution of adequate revenue without causing undue disturbance to the remaining winning bids in the original solution may be difficult or even impossible. This \"bid-takers exposure problem\" may be preemptively addressed by finding a solution that is robust to winning-bid withdrawal. We introduce the concept of monotonicity-in-expectation. We provide impossibility results concerning truthful mechanisms for robust solutions with bounded social-welfare losses in which the bid-taker cannot rescind items from winning bidders to repair a solution. We also show that this result extends to combinatorial auctions that include a form of leveled-commitment contract. However, we present a positive result regarding truthfulness for combinatorial auctions in a restricted setting that comprises a computationally efficient allocation algorithm that seeks to maximize expected social welfare.|Alan Holland,Barry O'Sullivan","57890|GECCO|2007|Optimal antenna placement using a new multi-objective chc algorithm|Radio network design (RND) is a fundamental problem in cellular networks for telecommunications. In these networks, the terrain must be covered by a set of base stations (or antennae), each of which defines a covered area called cell. The problem may be reduced to figure out the optimal placement of antennae out of a list of candidate sites trying to satisfy two objectives to maximize the area covered by the radio signal and to reduce the number of used antennae. Consequently, RND is a bi-objective optimization problem. Previous works have solved the problem by using single-objective techniques which combine the values of both objectives. The used techniques have allowed to find optimal solutions according to the defined objective, thus yielding a unique solution instead of the set of Pareto optimal solutions. In this paper, we solve the RND problem using a multi-objective version of the algorithm CHC, which is the metaheuristic having reported the best results when solving the single-objective formulation of RND. This new algorithm, called MOCHC, is compared against a binary-coded NSGA-II algorithm and also against the provided results in the literature. Our experiments indicate that MOCHC outperfoms NSGA-II and, more importantly, it is more efficient finding the optimal solutions than single-objectives techniques.|Antonio J. Nebro,Enrique Alba,Guillermo Molina,J. Francisco Chicano,Francisco Luna,Juan José Durillo","66255|AAAI|2007|Extracting Influential Nodes for Information Diffusion on a Social Network|We consider the combinatorial optimization problem of finding the most influential nodes on a large-scale social network for two widely-used fundamental stochastic diffusion models. It was shown that a natural greedy strategy can give a good approximate solution to this optimization problem. However, a conventional method under the greedy algorithm needs a large amount of computation, since it estimates the marginal gains for the expected number of nodes influenced by a set of nodes by simulating the random process of each model many times. In this paper, we propose a method of efficiently estimating all those quantities on the basis of bond percolation and graph theory, and apply it to approximately solving the optimization problem under the greedy algorithm. Using real-world large-scale networks including blog networks, we experimentally demonstrate that the proposed method can outperform the conventional method, and achieve a large reduction in computational cost.|Masahiro Kimura,Kazumi Saito,Ryohei Nakano","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessière,Remi Coletta,Frédéric Koriche,Barry O'Sullivan","57920|GECCO|2007|A study of mutational robustness as the product of evolutionary computation|This paper investigates the ability of a tournament selection based genetic algorithm to find mutationally robust solutions to a simple combinatorial optimization problem. Two distinct algorithms (a stochastic hill climber and a tournament selection based GA) were used to search for optimal walks in several variants of the self avoiding walk problem. The robustness of the solutions obtained by the algorithms were compared, both with each other and with solutions obtained by a random sampling of the optimal solution space. The solutions found by the GA were, for most of the problem variants, significantly more robust than those found by either the hill climbing algorithm or random sampling. The solutions found by the hill climbing algorithm were often significantly less robust than those obtained through random sampling. .|Justin Schonfeld","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","58016|GECCO|2007|An artificial immune system with partially specified antibodies|Artificial Immune System algorithms use antibodies which fully specify the solution of an optimization, learning, or pattern recognition problem. By being restricted to fully specified antibodies, an AIS algorithm can not make use of schemata or classes of partial solutions. This paper presents a symbiotic artificial immune system (SymbAIS) algorithm which is an extension of CLONALG algorithm. It uses partially specified antibodies and gradually builds up building blocks of suitable sub-antibodies. The algorithm is compared with CLONALG on multimodal function optimization and combinatorial optimization problems and it is shown that it can solve problems that CLONALG is unable to solve.|Ramin Halavati,Saeed Bagheri Shouraki,Mojdeh Jalali Heravi,Bahareh Jafari Jashmi"]]},"title":{"entropy":6.453693020975747,"topics":["model for, local search, with, for planning, heuristic search, search for, search, search space, and search, algorithm with, model, method for, bayesian networks, for with, planning with, for networks, artificial immune, inference for, using model, with and","for systems, for and, decision processes, reasoning about, markov processes, sense disambiguation, for reasoning, systems, for agent, markov decision, and systems, language and, situation calculus, and reasoning, decision tree, word disambiguation, description logic, word sense, classifier systems, order crossover","the web, reinforcement learning, for learning, neural networks, semantic web, transfer learning, learning, the semantic, support vector, semantic for, named entity, data mining, image using, using and, learning and, web service, semantic, gene expression, using, for networks","genetic algorithm, algorithm for, genetic programming, particle swarm, genetic for, the problem, the, evolutionary algorithm, for the, for problem, the algorithm, the and, the effects, for optimization, optimization algorithm, genetic and, for, algorithm problem, evolutionary for, and its","for graph, probabilistic and, bounds for, gaussian process, mixture model, for structure, structure and, graph partitioning, generative model, hierarchical model, probabilistic model, probabilistic for, model for, for finding, for partitioning, simple for, incremental for, structure, for discovery, hierarchical","model for, for networks, inference for, efficient for, bayesian networks, new for, using model, for bayesian, for domains, model, algorithm for, for sensor, sensor networks, efficient algorithm, with model, with networks, and inference, efficient with, model algorithm, for selection","for systems, and systems, systems, classifier systems, for learning, learning systems, multi-agent systems, from observation, cognitive for, and environments, cognitive architecture, distributed constraint, systems with, constraint satisfaction, architecture for, mobile robot, for understanding, multiagent for, for intelligent, distributed for","for and, language and, for recognition, and, the and, order crossover, natural language, learning and, for object, integrating and, for generating, large scale, for interfaces, language for, analysis and, cooperative coevolution, and processing, for improved, with and, and aggregation","using and, using, image using, for using, using genetic, feature for, for detection, and image, for improving, for text, from using, representation for, identification and, feature selection, predictive representation, using knowledge, knowledge representation, knowledge and, estimation and, state for","and application, for application, for scheduling, flexible scheduling, bottleneck flexible, hybrid bottleneck, and flexible, bottleneck scheduling, and evaluation, quality approach, extracting from, for flexible, for and, from, bottleneck for, based approach, genetic flexible, extraction from, measures for, and bottleneck","the problem, for the, the, the and, the effects, study the, case study, mechanism design, case the, the dynamic, the functions, the search, automated design, and diversity, using the, the evolutionary, design and, dynamic environments, the crossover, automated mechanism","algorithm for, genetic algorithm, evolutionary algorithm, the algorithm, for optimization, evolutionary for, optimization algorithm, and evolutionary, algorithm and, using algorithm, multi-objective evolutionary, genetic for, multiobjective optimization, evolutionary optimization, using evolutionary, the optimization, the evolutionary, evolutionary with, algorithm with, evolutionary approach"],"ranking":[["66248|AAAI|2007|Automatic Algorithm Configuration Based on Local Search|The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variablevalue ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at httpwww.cs.ubc.calabsbetaProjectsParamILS.|Frank Hutter,Holger H. Hoos,Thomas Stützle","16753|IJCAI|2007|A Heuristic Search Approach to Planning with Temporally Extended Preferences|Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.|Jorge A. Baier,Fahiem Bacchus,Sheila A. McIlraith","16436|IJCAI|2007|Dynamic Weighting A Search-Based MAP Algorithm for Bayesian Networks|In this paper we propose the Dynamic Weighting A* (DWA*) search algorithm for solving MAP problems in Bayesian networks. By exploiting asymmetries in the distribution of MAP variables, the algorithm is able to greatly reduce the search space and offer excellent performance both in terms of accuracy and efficiency.|Xiaoxun Sun,Marek J. Druzdzel,Changhe Yuan","16803|IJCAI|2007|Using Learned Policies in Heuristic-Search Planning|Many current state-of-the-art planners rely on forward heuristic search. The success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. Such estimates are effective in guiding search for many domains, but there remain many other domains where current heuristics are inadequate to guide forward search effectively. In some of these domains, it is possible to learn reactive policies from example plans that solve many problems. However, due to the inductive nature of these learning techniques, the policies are often faulty, and fail to achieve high success rates. In this work, we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. We propose a simple approach that uses the policy to augment the states expanded during each search step. In particular, during each search node expansion, we add not only its neighbors, but all the nodes along the trajectory followed by the policy from the node until some horizon. Empirical results show that our proposed approach benefits both of the leveraged automated techniques, learning and heuristic search, outperforming the state-of-the-art in most benchmark planning domains.|Sung Wook Yoon,Alan Fern,Robert Givan","66120|AAAI|2007|Filtering Decomposition and Search Space Reduction for Optimal Sequential Planning|We present in this paper a hybrid planning system Which combines constraint satisfaction techniques and planning heuristics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mechanisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan's planning graph. This structure is incrementally built Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward chaining search based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner implements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners.|Stéphane Grandcolas,C. Pain-Barre","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","16714|IJCAI|2007|AWA - A Window Constrained Anytime Heuristic Search Algorithm|This work presents an iterative anytime heuristic search algorithm called Anytime Window A* (AWA*) where node expansion is localized within a sliding window comprising of levels of the search treegraph. The search starts in depth-first mode and gradually proceeds towards A* by incrementing the window size. An analysis on a uniform tree model provides some very useful properties of this algorithm. A modification of AWA* is presented to guarantee bounded optimal solutions at each iteration. Experimental results on the  Knapsack problem and TSP demonstrate the efficacy of the proposed techniques over some existing anytime search methods.|Sandip Aine,P. P. Chakrabarti,Rajeev Kumar","58161|GECCO|2007|Improving global numerical optimization using a search-space reduction algorithm|We have developed an algorithm for reduction of search-space, called Domain Optimization Algorithm (DOA), applied to global optimization. This approach can efficiently eliminate search-space regions with low probability of containing a global optimum. DOA basically worksusing simple models for search-space regions to identify and eliminate non-promising regions. The proposed approach has shown relevant results for tests using hard benchmark functions.|Vinicius Veloso de Melo,Alexandre C. B. Delbem,Dorival Leao Pinto Junior,Fernando Marques Federson","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","16422|IJCAI|2007|Graph Connectivity Measures for Unsupervised Word Sense Disambiguation|Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most \"important\" node among the set of graph nodes representing its senses. We propose a variety of measures that analyze the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.|Roberto Navigli,Mirella Lapata","16719|IJCAI|2007|Average-Reward Decentralized Markov Decision Processes|Formal analysis of decentralized decision making has become a thriving research area in recent years, producing a number of multi-agent extensions of Markov decision processes. While much of the work has focused on optimizing discounted cumulative reward, optimizing average reward is sometimes a more suitable criterion. We formalize a class of such problems and analyze its characteristics, showing that it is NP complete and that optimal policies are deterministic. Our analysis lays the foundation for designing two optimal algorithms. Experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.|Marek Petrik,Shlomo Zilberstein","65753|AAAI|2006|Kernel Methods for Word Sense Disambiguation and Acronym Expansion|The scarcity of manually labeled data for supervised machine learning methods presents a significant limitation on their ability to acquire knowledge. The use of kernels in Support Vector Machines (SVMs) provides an excellent mechanism to introduce prior knowledge into the SVM learners, such as by using unlabeled text or existing ontologies as additional knowledge sources. Our aim is to develop three kernels - one that makes use of knowledge derived from unlabeled text, the second using semantic knowledge from ontologies, and finally a third, additive kernel consisting of the first two kernels - and study their effect on the tasks of word sense disambiguation and automatic expansion of ambiguous acronyms.|Mahesh Joshi,Ted Pedersen,Richard Maclin,Serguei V. S. Pakhomov","16413|IJCAI|2007|Word Sense Disambiguation through Sememe Labeling|Currently most word sense disambiguation (WSD) systems are relatively individual word sense experts. Scarcely do these systems take word sense transitions between senses of linearly consecutive words or syntactically dependent words into consideration. Word sense transitions are very important. They embody the fluency of semantic expression and avoid sparse data problem effectively. In this paper, How Net knowledge base is used to decompose every word sense into several sememes. Then one transition between two words' senses becomes multiple transitions between sememes. Sememe transitions are much easier to be captured than word sense transitions due to much less sememes. When sememes are labeled, WSD is done. In this paper, multi-layered conditional random fields (MLCRF) is proposed to model sememe transitions. The experiments show that MLCRF performs better than a base-line system and a maximum entropy model. Syntactic and hypernym features can enhance the performance significantly.|Xiangyu Duan,Jun Zhao,Bo Xu","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","66010|AAAI|2007|A Unification of Extensive-Form Games and Markov Decision Processes|We describe a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization is that it places arbitrary convex optimization problems at decision nodes, in place of the finite action sets typically considered. The possibly-infinite action sets mean we must \"forget\" the exact action taken (feasible solution to the optimization problem), remembering instead only some statistic sufficient for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games.|H. Brendan McMahan,Geoffrey J. Gordon","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aurélie Beynier,Abdel-Illah Mouaddib"],["65967|AAAI|2007|Using Multiresolution Learning for Transfer in Image Classification|Our work explores the transfer of knowledge at multiple levels of abstraction to improve learning. By exploiting the similarities between objects at various levels of detail, multiresolution learning can facilitate transfer between image classification tasks. We extract features from images at multiple levels of resolution, then use these features to create models at different resolutions. Upon receiving a new task, the closest-matching stored model can be generalized (adapted to the appropriate resolution) and transferred to the new task.|Eric Eaton,Marie desJardins,John Stevenson","66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy Lécué,Alexandre Delteil","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","66013|AAAI|2007|Towards Large Scale Argumentation Support on the Semantic Web|This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW) a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Users can also attack or support parts of existing arguments, use existing parts of an argument in the creation of new arguments, or create new argumentation schemes. As such, this initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.|Iyad Rahwan,Fouad Zablith,Chris Reed","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|Cécile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","16483|IJCAI|2007|Learning Semantic Descriptions of Web Information Sources|The Internet is full of information sources providing various types of data from weather forecasts to travel deals. These sources can be accessed via web-forms, Web Services or RSS feeds. In order to make automated use of these sources, one needs to first model them semantically. Writing semantic descriptions for web sources is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions for web sources, in which we actively invoke sources and compare the data they produce with that of known sources of information. We perform an inductive search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. The paper includes an empirical evaluation demonstrating the effectiveness of our approach on real-world web sources.|Mark James Carman,Craig A. Knoblock","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,Kôiti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["57771|GECCO|2006|An effective genetic algorithm for the minimum-label spanning tree problem|Given a connected, undirected graph G with labeled edges, the minimum label spanning tree problem seeks a spanning tree on G to whose edges are attached the smallest possible number of labels. A greedy heuristic for this NP-hard problem greedily chooses labels so as to reduce the number of components in the subgraphs they induce as quickly as possible. A genetic algorithm for the problem encodes candidate solutions as per mutations of the labels an initial segment of such a chromosome lists the labels that appear on the edges in the chromosome's tree. Three versions of the GA apply generic or heuristic crossover and mutation operators and a local search step. In tests on  randomly-generated instances of the minimum-label spanning tree problem, versions of the GA that apply generic operators, with and without the local search step, perform less well than the greedy heuristic, but a version that applies the local search step and operators tailored to the problem returns solutions that require on average  fewer labels than the heuristic's.|Jeremiah Nummela,Bryant A. Julstrom","58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57654|GECCO|2006|The snake in the box problem mathematical conjecture and a genetic algorithm approach|With applications in coding theory and hypercube-based computing and networking, the \"snake in the box\" problem is of great practical importance. Moreover, it is both mathematically elegant and highly difficult. The problem is simply to find the longest \"snake\" in a hypercube. However, as the hypercube grows in dimensionality, the size of the search space increases exponentially. Moreover, as the maximum snake length is only known for the smallest dimensions (where the snakes themselves have already been identified), there is no known stopping criterion for the search in higher dimensions. In this paper we make a mathematical conjecture about the possible maximum length of a snake in a hypercube of dimension d. We use a genetic algorithm for finding snakes in a  hypercube to show some results.|Pedro A. Diaz-Gomez,Dean F. Hougen","58182|GECCO|2007|ExGA II an improved exonic genetic algorithm for the multiple knapsack problem|ExGA I, a previously presented genetic algorithm, successfully solved numerous instances of the multiple knapsack problem (MKS) by employing an adaptive repair function that made use of the algorithm's modular encoding. Here we present ExGA II, an extension of ExGA I that implements additional features which allow the algorithm to perform more reliably across a larger set of benchmark problems. In addition to some basic modifications of the algorithm's framework, more specific extensions include the use of a biased mutation operator and adaptive control sequences which are used to guide the repair procedure. The success rate of ExGA II is superior to its predecessor, and other algorithms in the literature, without an overall increase in the number of function evaluations required to reach the global optimum. In fact, the new algorithm exhibits a significant reduction in the number of function evaluations required for the largest problems investigated. We also address the computational cost of using a repair function and show that the algorithm remains highly competitive when this cost is accounted for.|Philipp Rohlfshagen,John A. Bullinaria","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["66038|AAAI|2007|Probabilistic Community Discovery Using Hierarchical Latent Gaussian Mixture Model|Complex networks exist in a wide array of diverse domains, ranging from biology, sociology, and computer science. These real-world networks, while disparate in nature, often comprise of a set of loose clusters(a.k.a communities), whose members are better connected to each other than to the rest of the network. Discovering such inherent community structures can lead to deeper understanding about the networks and therefore has raised increasing interests among researchers from various disciplines. This paper describes GWN-LDA (Generic weighted network-Latent Dirichlet Allocation) model, a hierarchical Bayesian model derived from the widely-received LDA model, for discovering probabilistic community profiles in social networks. In this model, communities are modeled as latent variables and defined as distributions over the social actor space. In addition, each social actor belongs to every community with different probability. This paper also proposes two different network encoding approaches and explores the impact of these two approaches to the community discovery performance. This model is evaluated on two research collaborative networks CiteSeer and NanoSCI. The experimental results demonstrate that this approach is promising for discovering community structures in large-scale networks.|Haizheng Zhang,C. Lee Giles,Henry C. Foley,John Yen","16561|IJCAI|2007|A Factor Graph Model for Software Bug Finding|Automatic tools for finding software errors require knowledge of the rules a program must obey, or \"specifications,\" before they can identify bugs. We present a method that combines factor graphs and static program analysis to automatically infer specifications directly from programs. We illustrate the approach on inferring functions in C programs that allocate and release resources, and evaluate the approach on three codebases SDL, OpenSSH, and the OS kernel for Mac OS X (XNU). The inferred specifications are highly accurate and with them we have discovered numerous bugs.|Ted Kremenek,Andrew Y. Ng,Dawson R. Engler","58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","57635|GECCO|2006|Adaptive discretization for probabilistic model building genetic algorithms|This paper proposes an adaptive discretization method, called Split-on-Demand (SoD), to enable the probabilistic model building genetic algorithm (PMBGA) to solve optimization problems in the continuous domain. The procedure, effect, and usage of SoD are described in detail. As an example, the integration of SoD and the extended compact genetic algorithm (ECGA), named real-coded ECGA (rECGA), is presented and numerically examined. The experimental results indicate that rECGA works well and SoD is effective. The behavior of SoD is analyzed and discussed, followed by the potential future work for SoD.|Chao-Hong Chen,Wei-Nan Liu,Ying-Ping Chen","57787|GECCO|2006|Sporadic model building for efficiency enhancement of hierarchical BOA|This paper describes and analyzes sporadic model building, which can be used to enhance the efficiency of the hierarchical Bayesian optimization algorithm (hBOA) and other advanced estimation of distribution algorithms (EDAs) that use complex multivariate probabilistic models. With sporadic model building, the structure of the probabilistic model is updated once every few iterations (generations), whereas in the remaining iterations only model parameters (conditional and marginal probabilities) are updated. Since the time complexity of updating model parameters is much lower than the time complexity of learning the model structure, sporadic model building decreases the overall time complexity of model building. The paper shows that for boundedly difficult nearly decomposable and hierarchical optimization problems, sporadic model building leads to a significant model-building speedup that decreases the asymptotic time complexity of model building in hBOA by a factor of (n .) to (n .), where n is the problem size. On the other hand, sporadic model building also increases the number of evaluations until convergence nonetheless, the evaluation slowdown is insignificant compared to the gains in the asymptotic complexity of model building.|Martin Pelikan,Kumara Sastry,David E. Goldberg","16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002","66186|AAAI|2007|Learning Graphical Model Structure Using L-Regularization Paths|Sparsity-promoting L-regularization has recently been succesfully used to learn the structure of undirected graphical models. In this paper, we apply this technique to learn the structure of directed graphical models. Specifically, we make three contributions. First, we show how the decomposability of the MDL score, plus the ability to quickly compute entire regularization paths, allows us to efficiently pick the optimal regularization parameter on a per-node basis. Second, we show how to use L variable selection to select the Markov blanket, before a DAG search stage. Finally, we show how L variable selection can be used inside of an order search algorithm. The effectiveness of these L-based approaches are compared to current state of the art methods on  datasets.|Mark W. Schmidt,Alexandru Niculescu-Mizil,Kevin P. Murphy","65706|AAAI|2006|A Two-Step Hierarchical Algorithm for Model-Based Diagnosis|For many large systems the computational complexity of complete model-based diagnosis is prohibitive. In this paper we investigate the speedup of the diagnosis process by exploiting the hierarchylocality as is typically present in well-engineered systems. The approach comprises a compile-time and a run-time step. In the first step, a hierarchical CNF representation of the system is compiled to hierarchical DNF of adjustable hierarchical depth. In the second step, the diagnoses are computed from the hierarchical DNF and the actual observations. Our hierarchical algorithm, while sound and complete, allows large models to be diagnosed, where compiletime investment directly translates to run-time speedup. The benefits of our approach are illustrated by using weak-fault models of real-world systems, including the ISCAS- combinatorial circuits. Even for these non-optimally partitioned problems the speedup compared to traditional approaches ranges in the hundreds.|Alexander Feldman,Arjan J. C. van Gemund","65724|AAAI|2006|Model Counting A New Strategy for Obtaining Good Bounds|Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained.|Carla P. Gomes,Ashish Sabharwal,Bart Selman","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir"],["58129|GECCO|2007|A chain-model genetic algorithm for Bayesian network structure learning|Bayesian Networks are today used in various fields and domains due to their inherent ability to deal with uncertainty. Learning Bayesian Networks, however is an NP-Hard task . The super exponential growth of the number of possible networks given the number of factors in the studied problem domain has meant that more often, approximate and heuristic rather than exact methods are used. In this paper, a novel genetic algorithm approach for reducing the complexity of Bayesian network structure discovery is presented. We propose a method that uses chain structures as a model for Bayesian networks that can be constructed from given node orderings. The chain model is used to evolve a small number of orderings which are then injected into a greedy search phase which searches for an optimal structure. We present a series of experiments that show a significant reduction can be made in computational cost although with some penalty in success rate.|Ratiba Kabli,Frank Herrmann,John McCall","57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","16436|IJCAI|2007|Dynamic Weighting A Search-Based MAP Algorithm for Bayesian Networks|In this paper we propose the Dynamic Weighting A* (DWA*) search algorithm for solving MAP problems in Bayesian networks. By exploiting asymmetries in the distribution of MAP variables, the algorithm is able to greatly reduce the search space and offer excellent performance both in terms of accuracy and efficiency.|Xiaoxun Sun,Marek J. Druzdzel,Changhe Yuan","66259|AAAI|2007|SUNNY A New Algorithm for Trust Inference in Social Networks Using Probabilistic Confidence Models|In many computing systems, information is produced and processed by many people. Knowing how much a user trusts a source can be very useful for aggregating, filtering, and ordering of information. Furthermore, if trust is used to support decision making, it is important to have an accurate estimate of trust when it is not directly available, as well as a measure of confidence in that estimate. This paper describes a new approach that gives an explicit probabilistic interpretation for confidence in social networks. We describe SUNNY, a new trust inference algorithm that uses a probabilistic sampling technique to estimate our confidence in the trust information from some designated sources. SUNNY computes an estimate of trust based on only those information sources with high confidence estimates. In our experiments, SUNNY produced more accurate trust estimates than the well known trust inference algorithm TIDALTRUST (Golbeck ), demonstrating its effectiveness.|Ugur Kuter,Jennifer Golbeck","65886|AAAI|2006|Memory-Efficient Inference in Relational Domains|Propositionalization of a first-order theory followed by satisfiability testing has proved to be a remarkably efficient approach to inference in relational domains such as planning (Kautz & Selman ) and verification (Jackson ). More recently, weighted satisfiability solvers have been used successfully for MPE inference in statistical relational learners (Singla & Domingos ). However, fully instantiating a finite first-order theory requires memory on the order of the number of constants raised to the arity of the clauses, which significantly limits the size of domains it can be applied to. In this paper we propose LazySAT, a variation of the Walk-SAT solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domains (i.e., only a small fraction of ground atoms are true, and most clauses are trivially satisfied). Experiments on entity resolution and planning problems show that LazySAT reduces memory usage by orders of magnitude compared to Walk-SAT, while taking comparable time to run and producing the same solutions.|Parag Singla,Pedro Domingos","57796|GECCO|2006|Distributed genetic algorithm for energy-efficient resource management in sensor networks|In this work we consider energy-efficient resource management in an environment monitoring and hazard detection sensor network. Our goal is to allocate different detection methods to different sensor nodes in the way such that the required detection probability can be achieved while the network lifetime is maximized. The optimization algorithm is designed based on the Island multi-deme genetic algorithm (GA). The experimental results show that our algorithm increases the network lifetime by approximately .% in average compared with the heuristic approaches. We also investigate the effect of the configuration parameters on the searching quality of the proposed distributed GA. A regression model is derived empirically that estimates the runtime of the distributed GA given the configuration parameters such as the sub-population size, parallelism, and migration rate. Once the model has been fit to a group of data, it can be utilized to find the efficient configurations of the proposed algorithm.|Qinru Qiu,Qing Wu,Daniel J. Burns,Douglas Holzhauer","65741|AAAI|2006|Identifiability in Causal Bayesian Networks A Sound and Complete Algorithm|This paper addresses the problem of identifying causal effects from nonexperimental data in a causal Bayesian network, i.e., a directed acyclic graph that represents causal relationships. The identifiability question asks whether it is possible to compute the probability of some set of (effect) variables given intervention on another set of (intervention) variables, in the presence of non-observable (i.e., hidden or latent) variables. It is well known that the answer to the question depends on the structure of the causal Bayesian network, the set of observable variables, the set of effect variables, and the set of intervention variables. Our work is based on the work of Tian, Pearl, Huang, and Valtorta (Tian & Pearl a b  Huang & Valtorta a) and extends it. We show that the identify algorithm that Tian and Pearl define and prove sound for semi-Markovian models can be transfered to general causal graphs and is not only sound, but also complete. This result effectively solves the identifiability question for causal Bayesian networks that Pearl posed in  (Pearl ), by providing a sound and complete algorithm for identifiability.|Yimin Huang,Marco Valtorta","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","65785|AAAI|2006|Performing Incremental Bayesian Inference by Dynamic Model Counting|The ability to update the structure of a Bayesian network when new data becomes available is crucial for building adaptive systems. Recent work by Sang, Beame, and Kautz (AAAI ) demonstrates that the well-known Davis-Putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in Bayesian networks with high density and width. In this paper, we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple runs on a series of problems with similar structure. This allows us to perform Bayesian inference incrementally as the structure of the network changes. Experimental results show that our approach yields significant improvements over the previous model counting approaches on multiple challenging Bayesian network instances.|Wei Li 0002,Peter van Beek,Pascal Poupart","65685|AAAI|2006|Overlapping Coalition Formation for Efficient Data Fusion in Multi-Sensor Networks|This paper develops new algorithms for coalition formation within multi-sensor networks tasked with performing wide-area surveillance. Specifically, we cast this application as an instance of coalition formation, with overlapping coalitions. We show that within this application area subadditive coalition valuations are typical, and we thus use this structural property of the problem to derive two novel algorithms (an approximate greedy one that operates in polynomial time and has a calculated bound to the optimum, and an optimal branch-and-bound one) to find the optimal coalition structure in this instance. We empirically evaluate the performance of these algorithms within a generic model of a multi-sensor network performing wide area surveillance. These results show that the polynomial algorithm typically generated solutions much closer to the optimal than the theoretical bound, and prove the effectiveness of our pruning procedure.|Viet Dung Dang,Rajdeep K. Dash,Alex Rogers,Nicholas R. Jennings"],["57747|GECCO|2006|Multi-step environment learning classifier systems applied to hyper-heuristics|Heuristic Algorithms (HA) are very widely used to tackle practical problems in operations research. They are simple, easy to understand and inspire confidence. Many of these HAs are good for some problem instances while very poor for other cases. While Meta-Heuristics try to find which is the best heuristic andor parameters to apply for a given problem instance Hyper-Heuristics (HH) try to combine several heuristics in the same solution searching process, switching among them whenever the circumstances vary. Besides, instead to solve a single problem instance it tries to find a general algorithm to apply to whole families of problems. HH use evolutionary methods to search for such a problem-solving algorithm and, once produced, to apply it to any new problem instance desired. Learning Classifier Systems (LCS), and in particular XCS, represents an elegant and simple way to try to fabricate such a composite algorithm. This represents a different kind of problem to those already studied by the LCS community. Previous work, using single step environments, already showed the usefulness of the approach. This paper goes further and studies the novel use of multi-step environments for HH and an alternate way to consider states to see if chains of actions can be learnt. A non-trivial, NP-hard family of problems, the Bin Packing one, is used as benchmark for the procedure. Results of the approach are very encouraging, showing outperformance over all HAs used individually and over previously reported work by the authors, including non-LCS (a GA based approach used for the same BP set of problems) and LCS (using single step environments).|Javier G. Marín-Blázquez,Sonia Schulenburg","65629|AAAI|2006|Biconnected Structure for Multi-Robot Systems|Many applications of distributed autonomous robotic systems can benefit from, or even may require, the team of robots staying within communication connectivity. For example, consider the problem of multirobot surveillance (Ahmadi & Stone ), in which a team of robots must collaboratively patrol a given area. If any two robots can directly communicate at all times, the robots can coordinate for efficient behavior. This condition holds trivially in environments that are smaller than the robots' communication range. However in larger environments, the robots must actively maintain physical locations such that any two robots can communicate -- possibly through a series of other robots. Otherwise, the robots may lose track of each others' activities and become miscoordinated. Furthermore, since robots are relatively unreliable andor may need to change tasks (for example if a robot is suddenly called by a human user to perform some other task), in a stable multirobot surveillance system, if one of the robots leaves or crashes, the rest should still be able to communicate. Some examples of other tasks that could benefit from any pair of robots being able to communicate with each other, are multi-robot exploration, search and rescue, and cleaning robots. We say that robot R is connected to robot R if there is a series of robots, each within communication range of the previous, which can pass a message from R to R. It is not possible to maintain connectivity in the face of arbitrary numbers of robot departures if there are any two robots that are not within communication of one another and all other robots simultaneously depart, the system becomes disconnected. Thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust its positioning in response to a departure more quickly than a second departure will occur. In order for the team to stay connected, even in the face of any single departure, it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. We call this property biconnectivity the removal of any one robot from the system does not disconnect the remaining robots from each other.|Mazda Ahmadi,Peter Stone","66036|AAAI|2007|Centralized Distributed or Something Else Making Timely Decisions in Multi-Agent Systems|In multi-agent systems, agents need to share information in order to make good decisions. Who does what in order to achieve this matters a lot. The assignment of responsibility influences delay and consequently affects agents' abilities to make timely decisions. It is often unclear which approaches are best. We develop a model where one can easily test the impact of different assignments and information sharing protocols by focusing only on the delays caused by computation and communication. Using the model, we obtain interesting results that provide insight about the types of assignments that perform well in various domains and how slight variations in protocols can make great differences in feasibility.|Tim Harbers,Rajiv T. Maheswaran,Pedro A. Szekely","16446|IJCAI|2007|The Value of Observation for Monitoring Dynamic Systems|We consider the fundamental problem of monitoring (i.e. tracking) the belief state in a dynamic system, when the model is only approximately correct and when the initial belief state might be unknown. In this general setting where the model is (perhaps only slightly) mis-specified, monitoring (and consequently planning) may be impossible as errors might accumulate over time. We provide a new characterization, the value of observation, which allows us to bound the error accumulation. The value of observation is a parameter that governs how much information the observation provides. For instance, in Partially Observable MDPs when it is  the POMDP is an MDP while for an unobservable Markov Decision Process the parameter is . Thus, the new parameter characterizes a spectrum from MDPs to unobservable MDPs depending on the amount of information conveyed in the observations.|Eyal Even-Dar,Sham M. Kakade,Yishay Mansour","65699|AAAI|2006|LOCATE Intelligent Systems Demonstration Adapting Help to the Cognitive Styles of Users|LOCATE is workspace layout design software that also serves as a testbed for developing and refining principles of adaptive aiding. This demonstration illustrates LOCATE's ability to determine user cognitive styles and provide help matched to those styles. Users are assessed along a Wholist-Analytic dimension and a Verbal-Imagery-Kinesthetic \"trimension\" and that information is stored in a User Model maintained by LOCATE. Help options provided to users for selecting alternative forms of help permit the system to track those selections and allow for system adaptation to the user's preferred style of help.|Jack L. Edwards,Greg Scott","57779|GECCO|2006|Adaption in distributed systems an evolutionary approach|There is a trend towards networked and distributed systems, complicating the design process of self-adaptive software. Logistics networks can be seen as a distributed system that have to adapt to requirements of companies and customers in a flexible and fast manner. When constructing and planning logistic networks different aspects of complexity have to be considered the number of stores, intermediate stores and transport entities that are required at every stage in a supply chain as well as the sufficient size of every store or transport entity. This paper presents an approach that simulates adaptive logistic networks using a multi-agent system (MAS) based on Evolutionary Computation (EC). Our approach uses fully decentralized operators for reproduction like mutation, recombination and selection, regulated by market mechanisms. The novelty of this approach lies in the decentralized bottom-up adaption method for decentralized systems and we use a logistic scenario as an example. Our proposed method is based on a formal model explaining how adaption occurs in the number and strategies of agents and thus of logistic networks. The implementation and experimental results are given to illustrate the expected outcomes.|Stephan Otto,Stefan Kirn","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","65740|AAAI|2006|Distributed Interactive Learning in Multi-Agent Systems|Both explanation-based and inductive learning techniques have proven successful in a variety of distributed domains. However, learning in multi-agent systems does not necessarily involve the participation of other agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately, or single-agent learning. In this paper we present a new framework, named the Multi-Agent Inductive Learning System (MAILS), that tightly integrates processes of induction between agents. The MAILS framework combines inverse entailment with an epistemic approach to reasoning about knowledge in a multi-agent setting, facilitating a systematic approach to the sharing of knowledge and invention of predicates when required. The benefits of the new approach are demonstrated for inducing declarative program fragments in a multi-agent distributed programming system.|Jian Huang,Adrian R. Pearce","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["16602|IJCAI|2007|Natural Language Query Recommendation in Conversation Systems|In this paper, we address a critical problem in conversation systems limited input interpretation capabilities. When an interpretation error occurs, users often get stuck and cannot recover due to a lack of guidance from the system. To solve this problem, we present a hybrid natural language query recommendation framework that combines natural language generation with query retrieval. When receiving a problematic user query, our system dynamically recommends valid queries that are most relevant to the current user request so that the user can revise his request accordingly. Compared with existing methods, our approach offers two main contributions first, improving query recommendation quality by combining query generation with query retrieval second, adapting generated recommendations dynamically so that they are syntactically and lexically consistent with the original user input. Our evaluation results demonstrate the effectiveness of this approach.|Shimei Pan,James Shaw","65791|AAAI|2006|A Modular Action Description Language|\"Toy worlds\" involving actions, such as the blocks world and the Missionaries and Cannibals puzzle, are often used by researchers in the areas of commonsense reasoning and planning to illustrate and test their ideas. We would like to create a datahase of general-purpose knowledge about actions that encodes common features of many action domains of this kind. in the same way as abstract algebra and topology represent common features of specific number systems. This paper is a report on the first stage of this project--the design of an action description language in which this database will be written The new language is an extension of the action language C+. Its main distinctive feature is the possibility of referring to other action descriptions in the definition of a new action domain.|Vladimir Lifschitz,Wanwan Ren","16582|IJCAI|2007|On Natural Language Processing and Plan Recognition|The research areas of plan recognition and natural language parsing share many common features and even algorithms. However, the dialog between these two disciplines has not been effective. Specifically, significant recent results in parsing mildly context sensitive grammars have not been leveraged in the state of the art plan recognition systems. This paper will outline the relations between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research results in NLP and argue for their applicability to PR.|Christopher W. Geib,Mark Steedman","65871|AAAI|2006|Deeper Natural Language Processing for Evaluating Student Answers in Intelligent Tutoring Systems|This paper addresses the problem of evaluating students' answers in intelligent tutoring environments with mixed-initiative dialogue by modelling it as a textual entailment problem. The problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication. For intelligent tutorial dialogue systems, we show that entailment cases can be detected at various dialog turns during a tutoring session. We report the performance of a lexico-syntactic approach on a set of entailment cases that were collected from a previous study we conducted with AutoTutor.|Vasile Rus,Arthur C. Graesser","65688|AAAI|2006|Societal Grounding Is Essential to Meaningful Language Use|Language engineers often point to tight connections between their systems' linguistic representations and accumulated sensor data as a sign that their systems really mean what they say. While we believe such connections are an important piece in the puzzle of meaning, we argue that perceptual grounding alone does not suffice to explain the specific, stable meanings human speakers attribute to each other. Instead, human attributions of meaning depend on a process of societal grounding by which individual language speakers coordinate their perceptual experience and linguistic usage with other members of their linguistic communities. For system builders, this suggests that implementing a strategy of societal grounding would justify the attribution of bona fide linguistic meaning to a system even if it had little perceptual experience and only modest perceptual accuracy. We illustrate the importance and role of societal grounding using an implemented dialogue system that collaboratively identifies visual objects with human users.|David DeVault,Iris Oved,Matthew Stone","65790|AAAI|2006|Script and Language Identification in Degraded and Distorted Document Images|This paper reports a statistical identification technique that differentiates scripts and languages in degraded and distorted document images. We identify scripts and languages through document vectorization, which transforms each document image into an electronic document vector that characterizes the shape and frequency of the contained character and word images. We first identify scripts based on the density and distribution of vertical runs between character strokes and a vertical scan line. Latin-based languages are then differentiated using a set of word shape codes constructed using horizontal word runs and character extremum points. Experimental results show that our method is tolerant to noise, document degradation, and slight document skew and attains an average identification rate over %.|Shijian Lu,Chew Lim Tan","66020|AAAI|2007|Comprehending and Generating Apt Metaphors A Web-driven Case-based Approach to Figurative Language|Examples of figurative language can range from the explicit and the obvious to the implicit and downright enigmatic. Some simpler forms, like simile, often wear their meanings on their sleeve, while more challenging forms, like metaphor, can make cryptic allusions more akin to those of riddles or crossword puzzles. In this paper we argue that because the same concepts and properties are described in either case, a computational agent can learn from the easy cases (explicit similes) how to comprehend and generate the hard cases (nonexplicit metaphors). We demonstrate that the markedness of similes allows for a large case-base of illustrative examples to be easily acquired from the web, and present a system, called Sardonicus, that uses this case-base both to understand property-attribution metaphors and to generate apt metaphors for a given target on demand. In each case, we show how the text of the web is used as a source of tacit knowledge about what categorizations are allowable and what properties are most contextually appropriate. Overall, we demonstrate that by using the web as a primary knowledge source, a system can achieve a robust and scalable competence with metaphor while minimizing the need for handcrafted resources like WordNet.|Tony Veale,Yanfen Hao","66103|AAAI|2007|Learning Language Semantics from Ambiguous Supervision|This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.|Rohit J. Kate,Raymond J. Mooney","65952|AAAI|2007|Enabling Domain-Awareness for a Generic Natural Language Interface|In this paper, we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquires domain knowledge from user Interactions and Incorporates the knowledge learned to improve the generic system. We have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets. We found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation. We also show that the generic system with domain-awareness enabled by our approach can achieve performance similar to that of previous learning-based domain-specific systems.|Yunyao Li,Ishan Chaudhuri,Huahai Yang,Satinder Singh,H. V. Jagadish","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II"],["65905|AAAI|2006|Cross-Domain Knowledge Transfer Using Structured Representations|Previous work in knowledge transfer in machine learning has been restricted to tasks in a single domain. However, evidence from psychology and neuroscience suggests that humans are capable of transferring knowledge across domains. We present here a novel learning method, based on neuroevolution, for transferring knowledge across domains. We use many-layered, sparsely-connected neural networks in order to learn a structural representation of tasks. Then we mine frequent sub-graphs in order to discover sub-networks that are useful for multiple tasks. These sub-networks are then used as primitives for speeding up the learning of subsequent related tasks, which may be in different domains.|Samarth Swarup,Sylvian R. Ray","65746|AAAI|2006|Improving Approximate Value Iteration Using Memories and Predictive State Representations|Planning in partially-observable dynamical systems is a challenging problem, and recent developments in point-based techniques, such as Perseus significantly improve performance as compared to exact techniques. In this paper, we show how to apply these techniques to new models for non-Markovian dynamical systems called Predictive State Representatiolls (PSRs) and Memory-PSRs (mPSRs). PSRs and mPSRs are models of non-Markovian decision processes that differ from latent-variable models (e.g. HMMs, POMDPs) by representing state using only observable quantities. Further, mPSRs explicitly represent certain structural properties of the dynamical system that are also relevant to planning. We show how planning techniques can be adapted to leverage this structure to improve performance both in terms of execution time as well as quality of the resulting policy.|Michael R. James,Ton Wessling,Nikos A. Vlassis","57873|GECCO|2006|Design synthesis of microelectromechanical systems using genetic algorithms with component-based genotype representation|An automated design synthesis system based on a multi-objective genetic algorithm (MOGA) has been developed for the optimization of surface micromachined MEMS devices. A hierarchical component-based genotype representation is used, which incorporates specific engineering knowledge into the design and optimization process. Each MEMS component is represented by a gene with its own parameters defining its geometry and the way it can be modified from one generation to the next. The object-oriented genotype structures efficiently describe the hierarchical nature typical of engineering designs. They also prevent MOGA from wasting time exploring inappropriate regions of the search space. The automated MEMS design synthesis is demonstrated with surface-micromachined resonator and accelerometer designs.|Ying Zhang,Raffi R. Kamalian,Alice M. Agogino,Carlo H. Séquin","57997|GECCO|2007|Feature selection and classification in noisy epistatic problems using a hybrid evolutionary approach|A hybrid evolutionary approach is proposed for the combined problem of feature selection (using a genetic algorithm with IntersectionUnion recombination and a fitness function based on a counter-propagation artificial neural network) and subsequent classifier construction (using strongly-typed genetic programming), for use in nonlinear association studies with relatively large potential feature sets and noisy class data. The method was tested using synthetic data with various degrees of injected noise, based on a proposed mental health database.allResults show the algorithm has good potential for feature selection, classification and function characterization.|Drew DeHaas,Jesse Craig,Colin Rickert,Margaret J. Eppstein,Paul Haake,Kirsten Stor","65714|AAAI|2006|Overcoming the Brittleness Bottleneck using Wikipedia Enhancing Text Categorization with Encyclopedic Knowledge|When humans approach the task of text categorization, they interpret the specific wording of the document in the much larger context of their background knowledge and experience. On the other hand, state-of-the-art information retrieval systems are quite brittle--they traditionally represent documents as bags of words, and are restricted to learning from individual word occurrences in the (necessarily limited) training set. For instance, given the sentence \"Wal-Mart supply chain goes real time\", how can a text categorization system know that Wal-Mart manages its stock with RFID technology And having read that \"Ciprofloxacin belongs to the quinolones group\", how on earth can a machine know that the drug mentioned is an antibiotic produced by Bayer In this paper we present algorithms that can do just that. We propose to enrich document representation through automatic use of a vast compendium of human knowledge--an encyclopedia. We apply machine learning techniques to Wikipedia, the largest encyclopedia to date, which surpasses in scope many conventional encyclopedias and provides a cornucopia of world knowledge. Each Wikipedia article represents a concept, and documents to be categorized are represented in the rich feature space of words and relevant Wikipedia concepts. Empirical results confirm that this knowledge-intensive representation brings text categorization to a qualitatively new level of performance across a diverse collection of datasets.|Evgeniy Gabrilovich,Shaul Markovitch","16635|IJCAI|2007|From Generic Knowledge to Specific Reasoning for Medical Image Interpretation Using Graph based Representations|In several domains of spatial reasoning, such as medical image interpretation, spatial relations between structures play a crucial role since they are less prone to variability than intrinsic properties of structures. Moreover, they constitute an important part of available knowledge. We show in this paper how this knowledge can be appropriately represented by graphs and fuzzy models of spatial relations, which are integrated in a reasoning process to guide the recognition of individual structures in images. However pathological cases may deviate substantially from generic knowledge. We propose a method to adapt the knowledge representation to take into account the influence of the pathologies on the spatial organization of a set of structures, based on learning procedures. We also propose to adapt the reasoning process, using graph based propagation and updating.|Jamal Atif,Céline Hudelot,Geoffroy Fouquier,Isabelle Bloch,Elsa D. Angelini","65929|AAAI|2006|Improve Web Search Using Image Snippets|The Web has become the largest information repository in the world thus, effectively and efficiently searching the Web becomes a key challenge. Interactive Web search divides the search process into several rounds, and for each round the search engine interacts with the user for more knowledge of the user's information requirement. Previous research mainly uses the text information on Web pages, while little attention is paid to other modalities. This article shows that Web search performance can be significantly improved if imagery is considered in interactive Web search. Compared with text, imagery has its own advantage the time for &ldquoreading&rdquo an image is as little as that for reading one or two words, while the information brought by an image is as much as that conveyed by a whole passage of text. In order to exploit the advantages of imagery, a novel interactive Web search framework is proposed, where image snippets are first extracted from Web pages and then provided, along with the text snippets, to the user for result presentation and relevance feedback, as well as being presented alone to the user for image suggestion. User studies show that it is more convenient for the user to identify the Web pages he or she expects and to reformulate the initial query. Further experiments demonstrate the promise of introducing multimodal techniques into the proposed interactive Web search framework.|Xiao-Bing Xue,Zhi-Hua Zhou,Zhongfei (Mark) Zhang","16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone","65987|AAAI|2007|Fish Inspection System using a Parallel Neural Network Chip and Image Knowledge Builder Application|A generic image learning system, CogniSight, is being used for the inspection of fishes before filleting off-shore. More than thirty systems have been deployed on seven fishing vessels in Norway and Iceland over the past three years. Each CogniSight uses four neural network chips (a total of  neurons) based on a natively parallel hardwired architecture perfonning real time learning and non-linear classification (RBF). These systems are trained by the ship crew using Image Knowledge Builder, a \"show and tell\" interface for easy training and validation. Fishermen can reinforce the learning at anytime when needed. The use of CogniSight has reduced significantly the number of crewmembers on the boats (by up to six persons) and the time at sea has shortened by %. The prompt and strong return of the investment to the fishing fleet has increased significantly the market shares of Pisces Industries, the company integrating CogniSight systems to its filleting machines.|Anne Menendez,Guy Paillet","66177|AAAI|2007|Using Spatial Language in Multi-Modal Knowledge Capture|The ability to understand and communicate spatial relationships is central to many human-level reasoning tasks. People often describe spatial relationships using prepositions (i.e., in, on, under). Being able to use and interpret spatial prepositions could help create interactive systems for many tasks, including knowledge capture. Here I describe my thesis work modeling the learning and use of spatial prepositions and applying this model to the task of knowledge capture.|Kate Lockwood"],["57872|GECCO|2006|Effective genetic approach for optimizing advanced planning and scheduling in flexible manufacturing system|In this paper, a novel approach for designing chromosome has been proposed to improve the effectiveness, which called multistage operation-based genetic algorithm (moGA). The objective is to find the optimal resource selection for assignments, operations sequences, and allocation of variable transfer batches, in order to minimize the total makespan, considering the setup time, transportation time, and operations processing time. The plans and schedules are designed considering flexible flows, resources status, capacities of plants, precedence constraints, and workload balance in Flexible Manufacturing System (FMS). The experimental results of various Advanced Planning and Scheduling (APS) problems have offered to demonstrate the efficiency of moGA by comparing with the previous methods.|Haipeng Zhang,Mitsuo Gen","58193|GECCO|2007|An evolutionary approach to collective communication scheduling|In this paper, we describe two evolutionary algorithms aimed at scheduling collective communications on interconnection networks of parallel computers. To avoid contention for links and associated delays, collective communications proceed in synchronized steps. Minimum number of steps is sought for the given network topology, wormhole (pipelined) switching, minimum routing and given sets of sender andor receiver nodes. Used algorithms are able not only re-invent optimum schedules for known symmetric topologies like hyper-cubes, but they can find schedules even for any asymmetric or irregular topologies in case of general many-to-many collective communications. In most cases does the number of steps reach the theoretical lower bound for the given type of collective communication if it does not, non-minimum routing can provide further improvement. Optimum schedules may serve for writing high-performance communication routines for application-specific networks on chip or for development of communication libraries in case of general-purpose interconnection networ.|Jirí Jaros,Milos Ohlídal,Vaclav Dvorak","57663|GECCO|2006|A hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problemA hybrid of genetic algorithm and bottleneck shifting for flexible job shop scheduling problem|Flexible job shop scheduling problem (fJSP) is an extension of the classical job shop scheduling problem, which provides a closer approximation to real scheduling problems. We develop a new genetic algorithm hybridized with an innovative local search procedure (bottleneck shifting) for the fJSP problem. The genetic algorithm uses two representation methods to represent solutions of the fJSP problem. Advanced crossover and mutation operators are proposed to adapt to the special chromosome structures and the characteristics of the problem. The bottleneck shifting works over two kinds of effective neighborhood, which use interchange of operation sequences and assignment of new machines for operations on the critical path. In order to strengthen the search ability, the neighborhood structure can be adjusted dynamically in the local search procedure. The performance of the proposed method is validated by numerical experiments on several representative problems.|Jie Gao,Mitsuo Gen,Linyan Sun","16784|IJCAI|2007|A Flexible Unsupervised PP-Attachment Method Using Semantic Information|In this paper we revisit the classical NLP problem of prepositional phrase attachment (PP-attachment). Given the pattern V -NP-P-NP in the text, where V is verb, NP is a noun phrase, P is the preposition and NP is the other noun phrase, the question asked is where does P - NP attach V or NP This question is typically answered using both the word and the world knowledge. Word Sense Disambiguation (WSD) and Data Sparsity Reduction (DSR) are the two requirements for PP-attachment resolution. Our approach described in this paper makes use of training data extracted from raw text, which makes it an unsupervised approach. The unambiguous V - P - N and N - P -N tuples of the training corpus TEACH the system how to resolve the attachments in the ambiguous V - N - P - N tuples of the test corpus. A graph based approach to word sense disambiguation (WSD) is used to obtain the accurate word knowledge. Further, the data sparsity problem is addressed by (i) detecting synonymy using the wordnet and (ii) doing a form of inferencing based on the matching of Vs and Ns in the unambiguous patterns of V - P - NP, NP - P - NP. For experimentation, Brown Corpus provides the training data andWall Street Journal Corpus the test data. The accuracy obtained for PP-attachment resolution is close to %. The novelty of the system lies in the flexible use of WSD and DSR phases.|Srinivas Medimi,Pushpak Bhattacharyya","65714|AAAI|2006|Overcoming the Brittleness Bottleneck using Wikipedia Enhancing Text Categorization with Encyclopedic Knowledge|When humans approach the task of text categorization, they interpret the specific wording of the document in the much larger context of their background knowledge and experience. On the other hand, state-of-the-art information retrieval systems are quite brittle--they traditionally represent documents as bags of words, and are restricted to learning from individual word occurrences in the (necessarily limited) training set. For instance, given the sentence \"Wal-Mart supply chain goes real time\", how can a text categorization system know that Wal-Mart manages its stock with RFID technology And having read that \"Ciprofloxacin belongs to the quinolones group\", how on earth can a machine know that the drug mentioned is an antibiotic produced by Bayer In this paper we present algorithms that can do just that. We propose to enrich document representation through automatic use of a vast compendium of human knowledge--an encyclopedia. We apply machine learning techniques to Wikipedia, the largest encyclopedia to date, which surpasses in scope many conventional encyclopedias and provides a cornucopia of world knowledge. Each Wikipedia article represents a concept, and documents to be categorized are represented in the rich feature space of words and relevant Wikipedia concepts. Empirical results confirm that this knowledge-intensive representation brings text categorization to a qualitatively new level of performance across a diverse collection of datasets.|Evgeniy Gabrilovich,Shaul Markovitch","58055|GECCO|2007|Simultaneous optimization of production planning and inspection planning for flexible manufacturing systems|Quality assurance in flexible manufacturing systems (FMSs) has become a matter of great importance in recent years. The possibility for offering high-quality products at lower costs has become an essential for a manufacturer to keep in a competitive edge. In this paper, an approach to the multi-objective optimization of production planning and inspection planning in flexible manufacturing systems is presented. A multi-objective memetic algorithm MOMA is proposed to solve the problems having six objectives minimizing total machining time, machine workload unbalance, greatest machine workload, total tool cost, total inspection time and number of inspections. A schemata-guided local search strategy is proposed for enhancing performances of MOMA. High efficiency of MOMA arises from that multiple objectives can be optimized simultaneously without using heuristics and a set of good non-dominated solutions can be obtained providing additional degrees of freedom for the exploitation of resources of FMSs. Experimental results demonstrate effectiveness of the proposed approach using MOMA for production planning and inspection planning of FMSs.|Jian-Hung Chen","16377|IJCAI|2007|Improving Embeddings by Flexible Exploitation of Side Information|Dimensionality reduction is a much-studied task in machine learning in which high-dimensional data is mapped, possibly via a non-linear transformation, onto a low-dimensional manifold. The resulting embeddings, however, may fail to capture features of interest. One solution is to learn a distance metric which prefers embeddings that capture the salient features. We propose a novel approach to learning a metric from side information to guide the embedding process. Our approach admits the use of two kinds of side information. The first kind is class-equivalence information, where some limited number of pairwise \"samedifferent class\" statements are known. The second form of side information is a limited set of distances between pairs of points in the target metric space. We demonstrate the effectiveness of the method by producing embeddings that capture features of interest.|Ali Ghodsi,Dana F. Wilkinson,Finnegan Southey","16772|IJCAI|2007|Self-Adjusting Ring Modules SARMs for Flexible Gait Pattern Generation|Using the principle of homeostasis, we derive a learning rule for a specific recurrent neural network structure, the so-called Self-Adjusting Ring Module (SARM). Several of these Ring Modules can be plugged together to drive segmented artificial organisms, for example centipede-like robots. Controlling robots of variable morphologies by SARMs has major advantages over using Central Pattern Generators (CPGs). SARMs are able to immediately reconfigure themselves after reassembly of the robot's morphology. In addition, there is no need to decide on a singular place for the robot's control processor, since SARMs represent inherently distributed control structures.|Manfred Hild,Frank Pasemann","65658|AAAI|2006|Robust Execution on Contingent Temporally Flexible Plans|Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and uncertainty. Previous work has produced contingent plan execution systems that provide robustness during their plan extraction phase, by choosing between functionally redundant methods, and during their execution phase, by dispatching temporally flexible plans. Previous contingent execution systems use a centralized architecture in which a single agent conducts planning for the entire group. This can result in a communication bottleneck at the time when plan activities are passed to the other agents for execution, and state information is returned. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Contingent plans are encoded as Temporal Plan Networks (TPNs), which use a non-deterministic choice operator to compose temporally flexible plan fragments into a nested hierarchy of contingencies. To execute a TPN, the TPN is first distributed over multiple agents, by creating a hierarchical ad-hoc network and by mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN using a distributed, parallel algorithm that exploits the structure of the TPN. Third, the temporal consistency of each candidate plan is tested using a distributed Bellman-Ford algorithm. Each stage of plan extraction distributes communication to adjacent agents in the TPN, and in so doing eliminates communication bottlenecks. In addition, the distributed algorithm reduces the computational load on each agent. The algorithm is empirically validated on a range of randomly generated contingent plans.|Stephen A. Block,Andreas F. Wehowsky,Brian C. Williams","16766|IJCAI|2007|Coordination to Avoid Starvation of Bottleneck Agents in a Large Network System|In this paper, we present a multi-agent control method for a large-scale network system. We propose an extension of a token-based coordination technique to improve the tradeoff between two conflicting objectives of the network system reducing the lead time and increasing throughput. In our system, CABS, information about an agent's urgency of jobs to fulfill demanded throughput and to maintain its utilization is passed from downstream agents in the network so that upstream agents can provide necessary and sufficient jobs to bottleneck agents whose loss of capacity degrades the total system performance. We empirically evaluate CABS performance using a benchmark problem of the semiconductor fabrication process, which is a good example of a large-scale network system.|Rajesh Gautam,Kazuo Miyashita"],["57798|GECCO|2006|The effect of crossover on the behavior of the GA in dynamic environments a case study using the shaky ladder hyperplane-defined functions|One argument as to why the hyperplane-defined functions (hdf's) are a good testbed for the genetic algorithm (GA) is that the hdf's are built in the same way that the GA works. In this paper we test that hypothesis in a new setting by exploring the GA on a subset of the hdf's which are dynamic---the shaky ladder hyperplane-defined functions (sl-hdf's). In doing so we gain insight into how the GA makes use of crossover during its traversal of the sl-hdf search space. We begin this paper by explaining the sl-hdf's. We then conduct a series of experiments with various crossover rates and various rates of environmental change. Our results show that the GA performs better with than without crossover in dynamic environments. Though these results have been shown on some static functions in the past, they are re-confirmed and expanded here for a new type of function (the hdf) and a new type of environment (dynamic environments). Moreover we show that crossover is even more beneficial in dynamic environments than it is in static environments. We discuss how these results can be used to develop a richer knowledge about the use of building blocks by the GA.|William Rand,Rick L. Riolo,John H. Holland","66244|AAAI|2007|Automated Online Mechanism Design and Prophet Inequalities|Recent work on online auctions for digital goods has explored the role of optimal stopping theory -- particularly secretary problems -- in the design of approximately optimal online mechanisms. This work generally assumes that the size of the market (number of bidders) is known a priori, but that the mechanism designer has no knowledge of the distribution of bid values. However, in many real-world applications (such as online ticket sales), the opposite is true the seller has distributional knowledge of the bid values (e.g., via the history of past transactions in the market), but there is uncertainty about market size. Adopting the perspective of automated mechanism design, introduced by Conitzer and Sandholm, we develop algorithms that compute an optimal, or approximately optimal, online auction mechanism given access to this distributional knowledge. Our main results are twofold. First, we show that when the seller does not know the market size, no constant-approximation to the optimum efficiency or revenue is achievable in the worst case, even under the very strong assumption that bid values are i.i.d. samples from a distribution known to the seller. Second, we show that when the seller has distributional knowledge of the market size as well as the bid values, one can do well in several senses. Perhaps most interestingly, by combining dynamic programming with prophet inequalities (a technique from optimal stopping theory) we are able to design and analyze online mechanisms which are temporally strategyproof (even with respect to arrival and departure times) and approximately efficiency (revenue)-maximizing. In exploring the interplay between automated mechanism design and prophet inequalities, we prove new prophet inequalities motivated by the auction setting.|Mohammad Taghi Hajiaghayi,Robert D. Kleinberg,Tuomas Sandholm","58004|GECCO|2007|Dimensionality reduction in evolutionary multiobjective design case study|Real-world applications of Pareto-based optimisation commonly involve many objectives. It causes difficulties because of reduced selection pressure for better solutions. Dimensionality Reduction (DR) is a very appealing approach to overcome this problem. A case study of multiobjective Electric Machine (EM) design based on DR of the novel model  is considered.|Piotr Wozniak","16536|IJCAI|2007|Incremental Mechanism Design|Mechanism design has traditionally focused almost exclusively on the design of truthful mechanisms. There are several drawbacks to this . in certain settings (e.g. voting settings), no desirable strategy proof mechanisms exist . truthful mechanisms are unable to take advantage of the fact that computationally bounded agents may not be able to find the best manipulation, and . when designing mechanisms automatically, this approach leads to constrained optimization problems for which current techniques do not scale to very large instances. In this paper, we suggest an entirely different approach we start with a nave (manipulable) mechanism, and incrementally make it more strategy proof over a sequence of iterations. We give examples of mechanisms that (variants of) our approach generate, including the VCG mechanism in general settings with payments, and the plurality-with-runoff voting rule. We also provide several basic algorithms for automatically executing our approach in general settings. Finally, we discuss how computationally hard it is for agents to find any remaining beneficial manipulation.|Vincent Conitzer,Tuomas Sandholm","16486|IJCAI|2007|Mechanism Design with Partial Revelation|Classic direct mechanisms require full utility revelation from agents, which can be very difficult in practical multi-attribute settings. In this work, we study partial revelation within the framework of one-shot mechanisms. Each agent's type space is partitioned into a finite set of partial types and agents (should) report the partial type within which their full type lies. A classic result implies that implementation in dominant strategies is impossible in this model. We first show that a relaxation to Bayes-Nash implementation does not circumvent the problem. We then propose a class of partial revelation mechanisms that achieve approximate dominant strategy implementation, and describe a computationally tractable algorithm for myopically optimizing the partitioning of each agent's type space to reduce manipulability and social welfare loss. This allows for the automated design of one-shot partial revelation mechanisms with worst-case guarantees on both manipulability and efficiency.|Nathanael Hyafil,Craig Boutilier","66095|AAAI|2007|Logic for Automated Mechanism Design - A Progress Report|Over the past half decade, we have been exploring the use of logic in the specification and analysis of computational economic mechanisms. We believe that this approach has the potential to bring the same benefits to the design and analysis of computational economic mechanisms that the use of temporal logics and model checking have brought to the specification and analysis of reactive systems. In this paper, we give a survey of our work. We first discuss the use of cooperation logics such as Alternating-time Temporal Loglc (ATL) for the specification and verification of mechanisms such as social choice procedures. We motivate the approach, and then discuss the work we have done on extensions to ATL to support incomplete information, preferences, and quantification over coalition. We then discuss is the use of ATL-like cooperation logics in the development of social laws.|Michael Wooldridge,Thomas \u2026gotnes,Paul E. Dunne,Wiebe van der Hoek","65964|AAAI|2007|Partial Revelation Automated Mechanism Design|In most mechanism design settings, optimal general-purpose mechanisms are not known. Thus the automated design of mechanisms tailored to specific instances of a decision scenario is an important problem. Existing techniques for automated mechanism design (AMD) require the revelation of full utility information from agents, which can be very difficult in practice. In this work, we study the automated design of mechanisms that only require partial revelation of utilities. Each agent's type space is partitioned into a finite set of partial types, and agents (should) report the partial type within which their full type lies. We provide a set of optimization routines that can be combined to address the trade-offs between the amount of communication, approximation of incentive properties, and objective value achieved by a mechanism. This allows for the automated design of partial revelation mechanisms with worst-case guarantees on incentive properties for any objective function (revenue, social welfare, etc.).|Nathanael Hyafil,Craig Boutilier","57986|GECCO|2007|The defined cliffs variant in dynamic environments a case study using the shaky ladder hyperplane-defined functions|The shaky ladder hyperplane-defined functions (sl-hdfs) are a test suite utilized for exploring the behavior of the genetic algorithm (GA) in dynamic environments. This test suite can generate arbitrary problems with similar levels of difficulty and it provides a platform for systematic controlled observations of the GA in dynamic environments. Previous work has found two factors that contribute to the GA's success on sl-hdfs () short initial building blocks and () significantly changing the reward structure during fitness landscape changes. Therefore a test function that combines these two features should facilitate even better GA performance. This has led to the construction of a new sl-hdf variant, \"Defined Cliffs,\" in which we combine short elementary building blocks with sharp transitions in the environment. We examine this variant with two different levels of dynamics, static and regularly changing, using four different metrics. The results show superior GA performance on the Defined Cliffs over all previous variants (Cliffs, Weight, and Smooth). Our observations and conclusions in this variant further the understanding of the GA in dynamic environments.|Abir Alharbi,William Rand,Rick L. Riolo","16685|IJCAI|2007|Automated Design of Multistage Mechanisms|Mechanism design is the study of preference aggregation protocols that work well in the face of self-interested agents. We present the first general-purpose techniques for automatically designing multistage mechanisms. These can reduce elicitation burden by only querying agents for information that is relevant given their answers to previous queries. We first show how to turn a given (e.g., automatically designed using constrained optimization techniques) single-stage mechanism into the most efficient corresponding multistage mechanism given a specified elicitation tree. We then present greedy and dynamic programming (DP) algorithms that determine the elicitation tree (optimal in the DP case). Next, we show how the query savings inherent in the multistage model can be used to design the underlying single-stage mechanism to maximally take advantage of this approach. Finally, we present negative results on the design of multistage mechanisms that do not correspond to dominant-strategy single-stage mechanisms an optimal multistage mechanism in general has to randomize over queries to hide information from the agents.|Tuomas Sandholm,Vincent Conitzer,Craig Boutilier","57598|GECCO|2006|A comparative study of evolutionary optimization techniques in dynamic environments|Genetic Algorithms have widely been used for solving optimization problems in stationary environments. In recent years, there has been a growing interest for investigating and improving the performance of these algorithms in dynamic environments where the fitness landscape changes. In this study, we present an extensive comparison of several algorithms with different characteristics on a common platform by using the moving peaks benchmark and by varying problem parameters.|Demet Ayvaz,Haluk Topcuoglu,Fikret S. Gürgen"],["57706|GECCO|2006|On the local performance of simulated annealing and the  evolutionary algorithm|Simulated annealing and the (+) EA, a simple evolutionary algorithm, are both general randomized search heuristics that optimize any objective function with probability converging to . But they use very different techniques to achieve this global convergence. The (+) EA applies global mutations than can reach any point in the search space in one step together with an elitist selection mechanism. Simulated annealing restricts its search to a neighborhood but employs a randomized selection scheme where the probability for accepting a move to a new point in the search space depends on the difference in function values as well as on the current time step. Otherwise, the two algorithms are equal. It is known that the different philosophies of search implemented in the two heuristics can lead to exponential performance gaps between the two algorithms with respect to the expected optimization time. Even for very restricted classes of objective functions where the differences in function values between neighboring points are strictly limited the performance differences can be huge. Here, a more local point of view is taken. Considering obstacles in the fitness landscapes it is proven that the local performance of the two algorithms is remarkably similar in spite of their different search behaviors.|Thomas Jansen,Ingo Wegener","58092|GECCO|2007|Guided hyperplane evolutionary algorithm|A new evolutionary technique for multicriteria optimization called Guiding Hyper-plane Evolutionary Algorithm (GHEA) is proposed. The originality of the approach consists in the fact that the fitness assignment is realized by using a guiding hyperplane and a new non Pareto optimality concept. Numerical experiments illustrate the performance of GHEA compared with the popular NSGA-II and SPEA.|Corina Rotar,D. Dumitrescu,Rodica Ioana Lung","57984|GECCO|2007|Discrimination of metabolic flux profiles using a hybrid evolutionary algorithm|Studying metabolic fluxes is a crucial aspect of understanding biological phenotypes. However, it is often not possible to measure these fluxes directly. As an alternative, fluxome profiling provides indirect information about fluxes in a high-throughput setting. In this paper, we consider a scenario where fluxome profiling is used to investigate characteristic differences between a number of bacterial mutant strains. The goal is to identify groups of mutants that show maximally different fluxome profiles. We propose an evolutionary algorithm for this optimization problem and demonstrate that it outperforms alternative methods based on principle component analysis and independent component analysis on both real and synthetic data sets.|Stefan Bleuler,Eckart Zitzler","57821|GECCO|2006|An efficient multi-objective evolutionary algorithm with steady-state replacement model|The generic Multi-objective Evolutionary Algorithm (MOEA) aims to produce Pareto-front approximations with good convergence and diversity property. To achieve convergence, most multi-objective evolutionary algorithms today employ Pareto-ranking as the main criteria for fitness calculation. The computation of Pareto-rank in a population is time consuming, and arguably the most computationally expensive component in an iteration of the said algorithms. This paper proposes a Multi-objective Evolutionary Algorithm which avoids Pareto-ranking altogether by employing the transitivity of the domination relation. The proposed algorithm is an elitist algorithm with explicit diversity preservation procedure. It applies a measure reflecting the degree of domination between solutions in a steady-state replacement strategy to determine which individuals survive to the next iteration. Results on nine standard test functions demonstrated that the algorithm performs favorably compared to the popular NSGA-II in terms of convergence as well as diversity of the Pareto-set approximation, and is computationally more efficient.|Dipti Srinivasan,Lily Rachmawati","57797|GECCO|2006|A multi-objective evolutionary algorithm with weighted-sum niching for convergence on knee regions|A knee region on the Pareto-optimal front of a multi-objective optimization problem consists of solutions with the maximum marginal rates of return, i.e. solutions for which an improvement on one objective is accompanied by a severe degradation in another. The trade-off characteristic renders such solutions of particular interest in practical applications. This paper presents a multi-objective evolutionary algorithm focused on the knee regions. The algorithm facilitates better decision making in contexts where high marginal rates of return are desirable for Decision Makers. The proposed approach computes a transformation of the original objectives based on weighted-sum functions. The transformed functions identify niches which correspond to knee regions in the objective space. The extent and density of coverage of the knee regions are controllable by the niche strength and pool size parameters. Although based on weighted-sums, the algorithm is capable of finding solutions in the non-convex regions of the Pareto-front.|Lily Rachmawati,Dipti Srinivasan","58088|GECCO|2007|Carbon-friendly travel plan construction using an evolutionary algorithm|This paper discusses the use of an evolutionary algorithm to design workplace travel plans, to promote of car sharing and reduce carbon emissions from single-occupancy motor vehicles.|Neil Urquhart","16516|IJCAI|2007|Relevance Estimation and Value Calibration of Evolutionary Algorithm Parameters|The main objective of this paper is to present and evaluate a method that helps to calibrate the parameters of an evolutionary algorithm in a systematic and semi-automated manner. The method for Relevance Estimation and Value Calibration of EA parameters (REVAC) is empirically evaluated in two different ways. First, we use abstract test cases reflecting the typical properties of EA parameter spaces. Here we observe that REVAC is able to approximate the exact (hand-coded) relevance of parameters and it works robustly with measurement noise that is highly variable and not normally distributed. Second, we use REVAC for calibrating GAs for a number of common objective functions. Here we obtain a common sense validation, REVAC finds mutation rate pm much more sensitive than crossover rate pc and it recommends intuitively sound values pm between . and ., and .  pc  ..|Volker Nannen,A. E. Eiben","57841|GECCO|2006|Real-time construction of aircraft landing schedules using an evolutionary algorithm|This poster investigates the use of Evolutionary Algorithms (EAs) to optimise solutions to the Dynamic Aircraft Landing Problem (DALP). The approach adopted here uses an EA to evolve improvements to the solution so as to move towards an optimum solution. By using the EA to evolve gradual improvements to an existing feasible solution, a feasible solution is always maintained, an important consideration in a safety critical area such as air traffic control.|Neil Urquhart","65859|AAAI|2006|A Sequential Covering Evolutionary Algorithm for Expressive Music Performance|In this paper, we describe an evolutionary approach to one of the most challenging problems in computer music modeling the knowledge applied by a musician when performing a score of a piece in order to produce an expressive performance of the piece. We extract a set of acoustic features from Jazz recordings thereby providing a symbolic representation of the musician's expressive performance. By applying a sequential covering evolutionary algorithm to the symbolic representation, we obtain an expressive performance computational model capable of endowing a computer generated music performance with the timing and energy expressiveness that characterizes human generated music.|Rafael Ramirez,Amaury Hazan,Jordi Marine,Esteban Maestre","58167|GECCO|2007|Disburdening the species conservation evolutionary algorithm of arguing with radii|The present paper investigates the hybridization of two well-known multimodal optimization methods, i.e. species conservation and multinational algorithms. The topological species conservation algorithm embraces the vision of the existence of subpopulations around seeds (the best local individuals) and the preservation of these dominating individuals from one generation to another, but detects multimodality by means of the hill-valley mechanism employed by multinational algorithms. The aim is to inherit the strengths of both parent techniques and at the same time overcome their flaws. The species conservation algorithm efficiently keeps track of several good search space regions at once, but is difficult to parametrize without prior problem knowledge. Conversely, the multinational algorithms use many functionevaluations to establish subpopulations, but do not depend onprovided radius parameter values. Experiments with all threealgorithms are made on a wide range of test problems in order toinvestigate their advantages and shortcomings.|Catalin Stoean,Mike Preuss,Ruxandra Stoean,Dumitru Dumitrescu"]]}}