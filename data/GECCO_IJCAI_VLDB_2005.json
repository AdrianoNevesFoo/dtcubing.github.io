{"abstract":{"entropy":6.63581702121603,"topics":["arc consistency, data, applications data, management data, data stream, data mining, applications, xml data, emerging queries, language generation, database data, queries data, increasing data, processing stream, processing data, large number, problem generation, database query, large database, management systems","present novel, neural networks, learning, machine learning, present approach, natural language, learning classifier, classifier systems, learning sequential, describes systems, feature selection, present model, logic programs, learning systems, classifier xcs, best query, different ontologies, systems, introduce called, approach based","genetic algorithms, evolutionary algorithms, algorithms, genetic programming, optimization problem, particle swarm, constraint satisfaction, optimization, present algorithms, problem, evolutionary optimization, fitness function, algorithms optimization, particle pso, evolutionary computation, algorithms problem, web search, constraint problem, search, swarm pso","recent years, consider agents, growing interest, knowledge base, resolution process, belief revision, xml documents, work planning, consider problem, markov decision, problem agents, xml repositories, relational database, analysis tools, problem planning, problem knowledge, planning, knowledge agents, paper knowledge, given problem","arc consistency, due, time, structure, finding, experiments, development, challenging, peer-to-peer, series, temporal","language generation, problem data, problem generation, applications data, applications problem, applications, generation, distributed, important, problem, complexity, web, like, design, challenge, tests, computational, relevant, testing, domain","neural networks, present novel, present model, present approach, approach selection, probabilistic model, present algorithms, present based, present, novel selection, model bayesian, model networks, based theory, proposes novel, model, model based, present networks, based selection, based, present selection","systems, describes systems, model computational, model concept, describes, model systems, systems developed, automatically, developed, concept, development, robot, presented, generic, visual, edge, human, environments, optimal, resources","algorithms search, search, web search, evaluation strategies, evolutionary search, algorithms heuristic, search heuristic, search technique, evolutionary technique, evolutionary algorithms, algorithms based, search based, algorithms technique, technique, use search, well, known, crossover, recently, promising","genetic algorithms, genetic programming, constraint problem, constraint satisfaction, satisfaction problem, distribution algorithms, algorithms gas, estimation algorithms, genetic gas, estimation distribution, algorithms evolve, present genetic, estimation model, model genetic, genetic evolve, present estimation, problem variables, form genetic, genetic problem, algorithms important","given problem, given, study, interaction, world, way, make, schema, describes, result, landscape, continuous, recognition, processes, learning","resolution process, belief revision, information systems, information, belief, provide, process, search, basic, model, form, image, assumed, small, autonomous, properties, related, inference, part, complex"],"ranking":[["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80587|VLDB|2005|Temporal Management of RFID Data|RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.|Fusheng Wang,Peiya Liu","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Ravi Vijay","80504|VLDB|2005|Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling|Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be \"viewed\" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f- (i), which is the number of items that appear i times. While both such \"views\" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and timespace used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.|Graham Cormode,S. Muthukrishnan,Irina Rozenbaum","80523|VLDB|2005|Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures|Synopsis and small space representations are important data analysis tools and have long been used OLAPDSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.|Sudipto Guha,Kyuseok Shim"],["57472|GECCO|2005|Extraction of informative genes from microarray data|Identification of those genes that might anticipate the clinical behavior of different types of cancers is challenging due to availability of a smaller number of patient samples compared to huge number of genes, and the noisy nature of microarray data. After selection of some good genes based on signal-to-noise ratio, unsupervised learning like clustering and supervised learning like k-nearest neighbor (kNN) classifier are widely used in cancer researches to correlate the pathological behavior of cancers with the gene expression levels' differences in cancerous and normal tissues. By applying adaptive searches like Probabilistic Model Building Genetic Algorithm (PMBGA), it may be possible to get a smaller size gene subset that would classify patient samples more accurately than the above methods. In this paper, we propose a new PMBGA based method to extract informative genes from microarray data using Support Vector Machine (SVM) as a classifier. We apply our method to three microarray data sets and present the experimental results. Our method with SVM obtains encouraging results on those data sets as compared with the rank based method using kNN as a classifier.|Topon Kumar Paul,Hitoshi Iba","16301|IJCAI|2005|SVM-based Obstacles Recognition for Road Vehicle Applications|This paper describes an obstacle Recognition System based on SVM and vision. The basic components of the detected objects are first located in the image and then combined with a SVM-based classifier. A distributed learning approach is proposed in order to better deal with objects variability, illumination conditions, partial occlusions and rotations. A large database containing thousands of object examples extracted from real road images has been created for learning purposes. We present and discuss the results achieved up to date.|Miguel √?ngel Sotelo,Jes√∫s Nuevo,David Fern√°ndez,I. Parra,Luis Miguel Bergasa,Manuel Oca√±a,Ram√≥n Flores","16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llor√†,David E. Goldberg","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","16317|IJCAI|2005|Sequential Genetic Search for Ensemble Feature Selection|Ensemble learning constitutes one of the main directions in machine learning and data mining. Ensembles allow us to achieve higher accuracy, which is often not achievable with single models. One technique, which proved to be effective for constructing an ensemble of diverse classifiers, is the use of feature subsets. Among different approaches to ensemble feature selection, genetic search was shown to perform best in many domains. In this paper, a new strategy GAS-SEFS, Genetic Algorithmbased Sequential Search for Ensemble Feature Selection, is introduced. Instead of one genetic process, it employs a series of processes, the goal of each of which is to build one base classifier. Experiments on  data sets are conducted, comparing the new strategy with a previously considered genetic strategy for different ensemble sizes and for five different ensemble integration methods. The experiments show that GAS-SEFS, although being more time-consuming, often builds better ensembles, especially on data sets with larger numbers of features.|Alexey Tsymbal,Mykola Pechenizkiy,Padraig Cunningham","57445|GECCO|2005|A first order logic classifier system|Motivated by the intention to increase the expressive power of learning classifier systems, we developed a new Xcs derivative, Fox-cs, where the classifier and observation languages are a subset of first order logic. We found that Fox-cs was viable at tasks in two relational task domains, poker and blocks world, which cannot be represented easily using traditional bit-string classifiers and inputs. We also found that for these tasks, the level of generality obtained by Fox-cs in the portion of population that produces optimal behaviour is consistent with Wilson's generality hypothesis.|Drew Mellor","57553|GECCO|2005|Hyper-heuristics and classifier systems for solving D-regular cutting stock problems|This paper presents a method for combining concepts of Hyper-heuristics and Learning Classifier Systems for solving D Cutting Stock Problems. The idea behind Hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. In this paper, the Hyper-heuristic is formed using a XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The XCS evolves a behavior model which determines the possible actions (selection and placement heuristics) for given states of the problem. When tested with a collection of different problems, the method finds very competitive results for most of the cases. The testebed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Mar√≠n,E. J. Flores-√?lvarez,Peter Ross","57507|GECCO|2005|Event-driven learning classifier systems for online soccer games|This paper reports on the application of classifier systems to the acquisition of decision-making algorithms for agents in online soccer games. The objective of this research is to support changes in the video-game environment brought on by the Internet and to enable the provision of bug-free programs in a short period of time. To achieve real-time learning during a game, a bucket brigade algorithm is used to reinforce learning by classifiers and a technique for selecting learning targets according to event frequency is adopted. A hybrid system combining an existing strategy algorithm and a classifier system is also employed. In experiments that observed the outcome of , soccer games between this event-driven classifier system and a human-designed algorithm, the proposed system was found to be capable of learning effective decision-making algorithms in real time.|Yuji Sato,Ryutaro Kanno","57413|GECCO|2005|XCS with computed prediction in multistep environments|XCSF extends the typical concept of learning classifier systems through the introduction of computed classifier prediction. Initial results show that XCSF's computed prediction can be used to evolve accurate piecewise linear approximations of simple functions. In this paper, we take XCSF one step further and apply it to typical reinforcement learning problems involving delayed rewards. In essence, we use XCSF as a method of generalized (linear) reinforcement learning to evolve piecewise linear approximations of the payoff surfaces of typical multistep problems. Our results show that XCSF can easily evolve optimal and near optimal solutions for problems introduced in the literature to test linear reinforcement learning methods.|Pier Luca Lanzi,Daniele Loiacono,Stewart W. Wilson,David E. Goldberg"],["57521|GECCO|2005|Breeding swarms a GAPSO hybrid|In this paper we propose a novel hybrid (GAPSO) algorithm, Breeding Swarms, combining the strengths of particle swarm optimization with genetic algorithms. The hybrid algorithm combines the standard velocity and position update rules of PSOs with the ideas of selection, crossover and mutation from GAs. We propose a new crossover operator, Velocity Propelled Averaged Crossover (VPAC), incorporating the PSO velocity vector. The VPAC crossover operator actively disperses the population preventing premature convergence. We compare the hybrid algorithm to both the standard GA and PSO models in evolving solutions to five standard function minimization problems. Results show the algorithm to be highly competitive, often outperforming both the GA and PSO.|Matthew Settles,Terence Soule","57323|GECCO|2005|A modified particle swarm optimization predicted by velocity|In standard particle swarm optimization (PSO), the velocity only provides a position displacement contrast with the longer computational time. To avoid premature convergence, a new modified PSO is proposed in which the velocity considered as a predictor, while the position considered as a corrector. The algorithm gives some balance between global and local search capability, and results the high computational efficiency. The optimization computing of some examples is made to show the new algorithm has better global search capacity and rapid convergence rate.|Zhihua Cui,Jianchao Zeng","57331|GECCO|2005|An efficient evolutionary algorithm applied to the design of two-dimensional IIR filters|This paper presents an efficient technique of designing two-dimensional IIR digital filters using a new algorithm involving the tightly coupled synergism of particle swarm optimization and differential evolution. The design task is reformulated as a constrained minimization problem and is solved by our newly developed PSO-DV (Particle Swarm Optimizer with Differentially perturbed Velocity) algorithm. Numerical results are presented. The paper also demonstrates the superiority of the proposed design method by comparing it with two recently published filter design methods.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57452|GECCO|2005|Bayesian optimization models for particle swarms|We explore the use of information models as a guide for the development of single objective optimization algorithms, giving particular attention to the use of Bayesian models in a PSO context. The use of an explicit information model as the basis for particle motion provides tools for designing successful algorithms. One such algorithm is developed and shown empirically to be effective. Its relationship to other popular PSO algorithms is explored and arguments are presented that those algorithms may be developed from the same model, potentially providing new tools for their analysis and tuning.|Christopher K. Monson,Kevin D. Seppi","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57461|GECCO|2005|Minimum spanning trees made easier via multi-objective optimization|Many real-world problems are multi-objective optimization problems and evolutionary algorithms are quite successful on such problems. Since the task is to compute or approximate the Pareto front, multi-objective optimization problems are considered as more difficult than single-objective problems. One should not forget that the fitness vector with respect to more than one objective contains more information that in principle can direct the search of evolutionary algorithms. Therefore, it is possible that a single-objective problem can be solved more efficiently via a generalized multi-objective model of the problem. That this is indeed the case is proved by investigating the computation of minimum spanning trees.|Frank Neumann,Ingo Wegener","57330|GECCO|2005|Two improved differential evolution schemes for faster global search|Differential evolution (DE) is well known as a simple and efficient scheme for global optimization over continuous spaces. In this paper we present two new, improved variants of DE. Performance comparisons of the two proposed methods are provided against (a) the original DE, (b) the canonical particle swarm optimization (PSO), and (c) two PSO-variants. The new DE-variants are shown to be statistically significantly better on a seven-function test bed for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57483|GECCO|2005|Exploring extended particle swarms a genetic programming approach|Particle Swarm Optimisation (PSO) uses a population of particles that fly over the fitness landscape in search of an optimal solution. The particles are controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best point, while its momentum tries to keep it moving in its current direction.Previous research started exploring the possibility of evolving the force generating equations which control the particles through the use of genetic programming (GP).We independently verify the findings of the previous research and then extend it by considering additional meaningful ingredients for the PSO force-generating equations, such as global measures of dispersion and position of the swarm. We show that, on a range of problems, GP can automatically generate new PSO algorithms that outperform standard human-generated as well as some previously evolved ones.|Riccardo Poli,Cecilia Di Chio,William B. Langdon","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule"],["16251|IJCAI|2005|Automated Composition of Web Services by Planning at the Knowledge Level|In this paper, we address the problem of the automated composition of web services by planning on their \"knowledge level\" models. We start from descriptions of web services in standard process modeling and execution languages, like BPELWS, and automatically translate them into a planning domain that models the interactions among services at the knowledge level. This allows us to avoid the explosion of the search space due to the usually large and possibly infinite ranges of data values that are exchanged among services, and thus to scale up the applicability of state-of-the-art techniques for the automated composition of web services. We present the theoretical framework, implement it, and provide an experimental evaluation that shows the practical advantage of our approach w.r.t. techniques that are not based on a knowledgelevel representation.|Marco Pistore,Annapaola Marconi,Piergiorgio Bertoli,Paolo Traverso","16171|IJCAI|2005|Iterated Belief Revision Revised|The AGM postulates for belief revision, augmented by the DP postulates for iterated belief revision, provide generally accepted criteria for the design of operators by which intelligent agents adapt their beliefs incrementally to new information. These postulates alone, however, are too permissive They support operators by which all newly acquired information is canceled as soon as an agent learns a fact that contradicts some of its current beliefs. In this paper, we present a formal analysis of the deficiency of the DP postulates, and we show how to solve the problem by an additional postulate of independence. We give a representation theorem for this postulate and prove that it is compatible with AGM and DP.|Yi Jin,Michael Thielscher","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","16218|IJCAI|2005|Planning with Continuous Resources in Stochastic Domains|We consider the problem of optimal planning in stochastic domains with resource constraints, where resources are continuous and the choice of action at each step may depend on the current resource level. Our principal contribution is the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. The search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. We claim that this approach is especially effective when resource limitations contribute to reachability constraints. Experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.|Mausam,Emmanuel Benazera,Ronen I. Brafman,Nicolas Meuleau,Eric A. Hansen","16246|IJCAI|2005|Multi-Agent Assumption-Based Planning|The purpose of this poster is to introduce a dialectical theory for plan synthesis based on a multiagent approach. This approach is a promising way to devise systems able to take into account partial knowledge and heterogeneous skills of agents. We propose to consider the planning problem as a defeasible reasoning where agents exchange proposals and counter-proposals and are able to conjecture i.e., formulate plan steps based on hypothetical states of the world.|Damien Pellier,Humbert Fiorino","16182|IJCAI|2005|A Framework for Communication Planning on Mobile Devices|In mobile computing, communicative acts are not free. Costs such as power and bandwidth consumption are prominent issues. In addition, resources vary widely across hardware and operating context. Agents in these settings must account for these costs and adapt to available capabilities. This poster presents a planning optimization formalization of this problem, enabling service-based agents to reason about and conduct communication using local and network accessible resources.|Joseph Kopena,William C. Regli","16078|IJCAI|2005|Attribution of Knowledge to Artificial Agents and their Principals|We consider the problem of attribution of knowledge to artificial agents and their legal principals. When can we say that an artificial agent X knows p and that its principal can be attributed the knowledge of p We offer a pragmatic analysis of knowledge attribution and apply it to the legal theory of artificial agents and their principals.|Samir Chopra,Laurence White","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram","16294|IJCAI|2005|First-Order Logical Filtering|Logical filtering is the process of updating a belief state (set of possible world states) after a sequence of executed actions and perceived observations. In general, it is intractable in dynamic domains that include many objects and relationships. Still, potential applications for such domains (e.g., semantic web, autonomous agents, and partial-knowledge games) encourage research beyond immediate intractability results. In this paper we present polynomial-time algorithms for filtering belief states that are encoded as First-Order Logic (FOL) formulae. We sidestep previous discouraging results, and show that our algorithms are exact in many cases of interest. These algorithms accept belief states in full FOL, which allows natural representation with explicit references to unidentified objects, and partially known relationships. Our algorithms keep the encoding compact for important classes of actions, such as STRIPS actions. These results apply to most expressive modeling languages, such as partial databases and belief revision in FOL.|Afsaneh Shirazi,Eyal Amir","80598|VLDB|2005|Statistical Learning Techniques for Costing XML Queries|Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.|Ning Zhang 0002,Peter J. Haas,Vanja Josifovski,Guy M. Lohman,Chun Zhang"],["16053|IJCAI|2005|The Range and Roots Constraints Specifying Counting and Occurrence Problems|We propose a simple declarative language for specifying a wide range of counting and occurrence constraints. This specification language is executable since it immediately provides a polynomial propagation algorithm. To illustrate the capabilities of this language, we specify a dozen global constraints taken from the literature. We observe one of three outcomes we achieve generalized arc-consistency we do not achieve generalized arc-consistency, but achieving generalized arc-consistency is NP-hard we do not achieve generalized arc-consistency, but specialized propagation algorithms can do so in polynomial time. Experiments demonstrate that this specification language is both efficient and effective in practice.|Christian Bessi√®re,Emmanuel Hebrard,Brahim Hnich,Zeynep Kiziltan,Toby Walsh","16052|IJCAI|2005|Optimal and Suboptimal Singleton Arc Consistency Algorithms|Singleton arc consistency (SAC) enhances the pruning capability of arc consistency by ensuring that the network cannot become arc inconsistent after the assignment of a value to a variable. Algorithms have already been proposed to enforce SAC, but they are far from optimal time complexity. We give a lower bound to the time complexity of enforcing SAC, and we propose an algorithm that achieves this complexity, thus being optimal. However, it can be costly in space on large problems. We then propose another SAC algorithm that trades time optimality for a better space complexity. Nevertheless, this last algorithm has a better worst-case time complexity than previously published SAC algorithms. An experimental study shows the good performance of the new algorithms.|Christian Bessi√®re,Romuald Debruyne","16322|IJCAI|2005|Disjunctive Temporal Planning with Uncertainty|Driven by planning problems with both disjunctive constraints and contingency, we define the Disjunctive Temporal Problem with Uncertainty (DTPU), an extension of the DTP that includes contingent events. Generalizing existing work on Simple Temporal Problems with Uncertainty, we divide the time-points into controllable and uncontrollable classes, and propose varying notions of controllability to replace the notion of consistency.|Kristen Brent Venable,Neil Yorke-Smith","57419|GECCO|2005|GAMM genetic algorithms with meta-models for vision|Recent adaptive image interpretation systems can reach optimal performance for a given domain via machine learning, without human intervention. The policies are learned over an extensive generic image processing operator library. One of the principal weaknesses of the method lies with the large size of such libraries, which can make the machine learning process intractable. We demonstrate how evolutionary algorithms can be used to reduce the size of the operator library, thereby speeding up learning of the policy while still keeping human experts out of the development loop. Experiments in a challenging domain of forestry image interpretation exhibited a % reduction in the average time required to interpret an image, while maintaining the image interpretation accuracy of the full library.|Greg Lee,Vadim Bulitko","57584|GECCO|2005|MRI magnet design search space analysis EDAs and a real-world problem with significant dependencies|This paper introduces the design of superconductive magnet configurations in Magnetic Resonance Imaging (MRI) systems as a challenging real-world problem for Evolutionary Algorithms (EAs). Analysis of the problem structure is conducted using a general statistical method, which could be easily applied to other problems. The results suggest that the problem is highly multimodal and likely to present a significant challenge for many algorithms. Through a series of preliminary experiments, a continuous Estimation of Distribution Algorithm (EDA) is shown to be able to generate promising designs with a small computational effort. The importance of utilizing problem-specific knowledge and the ability of an algorithm to capture dependencies in solving complex real-world problems is also highlighted.|Bo Yuan,Marcus Gallagher,Stuart Crozier","57462|GECCO|2005|Inference of gene regulatory networks using s-system and differential evolution|In this work we present an improved evolutionary method for inferring S-system model of genetic networks from the time series data of gene expression. We employed Differential Evolution (DE) for optimizing the network parameters to capture the dynamics in gene expression data. In a preliminary investigation we ascertain the suitability of DE for a multimodal and strongly non-linear problem like gene network estimation. An extension of the fitness function for attaining the sparse structure of biological networks has been proposed. For estimating the parameter values more accurately an enhancement of the optimization procedure has been also suggested. The effectiveness of the proposed method was justified performing experiments on a genetic network using different numbers of artificially created time series data.|Nasimul Noman,Hitoshi Iba","16142|IJCAI|2005|Real-Time Path Planning for Humanoid Robot Navigation|We present a data structure and an algorithm for real-time path planning of a humanoid robot. Due to the many degrees of freedom, the robots shape and available actions are approximated for finding solutions efficiently. The resulting  dimensional configuration space is searched by the A* algorithm finding solutions in tenths of a second on lowperformance, embedded hardware. Experimental results demonstrate our solution for a robot in a world containing obstacles with different heights, stairs and a higher-level platform.|Jens-Steffen Gutmann,Masaki Fukuchi,Masahiro Fujita","16223|IJCAI|2005|Reducing Checks and Revisions in Coarse-grained MAC Algorithms|Arc consistency algorithms are widely used to prune the search space of Constraint Satisfaction Problems (CSPs). Coarse-grained arc consistency algorithms like AC-, AC-d and AC- are efficient when it comes to transforming a CSP to its arc-consistent equivalent. These algorithms repeatedly carry out revisions. Revisions require support checks for identifying and deleting all unsupported values from the domain of a variable. In revisions for difficult problems most values have some support. Indeed, most revisions are ineffective, i.e. they cannot delete any value and consume a lot of checks and time. We propose two solutions to overcome these problems. First we introduce the notion of a Support Condition (SC) which guarantees that a value has some support. SCs reduce support checks while maintaining arc consistency during search. Second we introduce the notion of a Revision Condition (RC) which guarantees that all values have support. A RC avoids a candidate revision and queue maintenance overhead. For random problems, SCs reduce the checks required by MAC- (MAC-) up to % (%). RCs avoid at least % of the total revisions. Combining the two results in reducing % of the solution time.|Deepak Mehta,Marc R. C. van Dongen","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","16199|IJCAI|2005|A Greedy Approach to Establish Singleton Arc Consistency|In this paper, we propose a new approach to establish Singleton Arc Consistency (SAC) on constraint networks. While the principle of existing SAC algorithms involves performing a breadth-first search up to a depth equal to , the principle of the two algorithms introduced in this paper involves performing several runs of a greedy search (where at each step, arc consistency is maintained). It is then an original illustration of applying inference (i.e. establishing singleton arc consistency) by search. Using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. Further-more, both space and time complexities are quite competitive.|Christophe Lecoutre,St√©phane Cardon"],["80597|VLDB|2005|Light-weight Domain-based Form Assistant Querying Web Databases On the Fly|The Web has been rapidly \"deepened\" by myriad searchable databases online, where data are hidden behind query forms. Helping users query alternative \"deep Web\" sources in the same domain (e.g., Books, Airfares) is an important task with broad applications. As a core component of those applications, dynamic query translation (i.e., translating a user's query across dynamically selected sources) has not been extensively explored. While existing works focus on isolated subproblems (e.g., schema matching, query rewriting) to study, we target at building a complete query translator and thus face new challenges ) To complete the translator, we need to solve the predicate mapping problem (i.e., map a source predicate to target predicates), which is largely unexplored by existing works ) To satisfy our application requirements, we need to design a customizable system architecture to assemble various components addressing respective subproblems (i.e., schema matching, predicate mapping, query rewriting). Tackling these challenges, we develop a light-weight domain-based form assistant, which can generally handle alternative sources in the same domain and is easily customizable to new domains. Our experiment shows the effectiveness of our form assistant in translating queries for real Web sources.|Zhen Zhang,Bin He,Kevin Chen-Chuan Chang","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","16236|IJCAI|2005|Propositional Abduction is Almost Always Hard|Abduction is a fundamental form of nonmonotonic reasoning that aims at finding explanations for observed manifestations. Applications of this process range from car configuration to medical diagnosis. We study here its computational complexity in the case where the application domain is described by a propositional theory built upon a fixed constraint language and the hypotheses and manifestations are described by sets of literals. We show that depending on the language the problem is either polynomial-time solvable, NP-complete, or P-complete. In particular, we show that under the assumption PNP, only languages that are affine of width  have a polynomial algorithm, and we exhibit very weak conditions for NP-hardness.|Gustav Nordh,Bruno Zanuttini","80521|VLDB|2005|Consistency for Web Services Applications|A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.|Paul Greenfield,Dean Kuo,Surya Nepal,Alan Fekete","16091|IJCAI|2005|A Cognitive Model of Visual Analogical Problem-Solving Transfer|Complex problem solving typically involves the generation of a procedure consisting of an ordered sequence of steps. Analogical reasoning is one strategy for solving complex problems, and visual reasoning is another. Visual analogies pertain to analogies based only on visual knowledge. In this paper, we describe the use of Galatea, a computational model of visual analogies in problem solving, to model the problem solving of a human subject (L). L was a given the task of solving a complex problem using analogy in a domain that contained both visual and non-visual knowledge, and was encouraged to use visual analogy. We describe how Galatea models L's use of visual analogy in problem solving.|Jim Davies,Ashok K. Goel,Nancy J. Nersessian","16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando","57411|GECCO|2005|Benefits of software measures for evolutionary white-box testing|White-box testing is an important method for the early detection of errors during software development. In this process test case generation plays a crucial role, defining appropriate and error-sensitive test data. The evolutionary white-box testing is a promising approach for the complete automation of structure-oriented test case generation. Here, test case generation can be completely automated with the help of evolutionary algorithms. However, problem cases exist in which the evolutionary test is not able to find valid test data. Thus, in the case of not achieving a test goal, it is not known whether this is due to non-executable program code or a problem case. This paper will investigate how successfully a software measure can support an evolutionary white-box test if the measure can predict the test effort. Hence, the termination criteria of evolutionary white-box testing can be adapted to test goals with problem cases in such a way that problematic test goals are either excluded from the test in advance or can be covered due to an adequate termination criteria according to a software measure. This could lead to an increase in efficiency and effectiveness of evolutionary white-box testing.|Frank Lammermann,Stefan Wappler","16216|IJCAI|2005|Cognitive Modelling of Event Ordering Reasoning in Imagistic Domains|The inference of temporal information from past event occurences in imagistic domains is relevant in several applications in knowledge engineering. In such applications, the order in which events have happened is imprinted in the domain as visual-spatial relations among its elements. Therefore, the interpretation of the relative ordering in which those events have occured is essential for understanding the domain evolution. We propose a cognitive model for event ordering reasoning within domains whose elements have been modified by past events. From the analysis of cognitive abilities of experts we propose new ontology constructs for knowledge modelling associated to Problem-Solving Methods. We illustrate the effectiveness of the model by means of an applications to an imagistic domain.|Laura S. Mastella,Mara Abel,Lu√≠s C. Lamb,Luis Fernando De Ros","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","16058|IJCAI|2005|Viewing Referring Expression Generation as Search|Almost all natural language generation (NLG) systems are faced with the problem of the generation of referring expressions (GRE) given a symbol corresponding to an intended referent, how do we work out the semantic content of a referring expression that uniquely identifies the entity in question This is now one of the most widely explored problems in NLG over the last  years, a number of algorithms have been proposed for addressing different aspects of this problem, but the different approaches taken make it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we show how viewing the problem of referring expression generation as a search problem allows us to recast existing algorithms in a way that makes their similarities and differences clear.|Bernd Bohnet,Robert Dale"],["16035|IJCAI|2005|A Two-Stage Method for Active Learning of Statistical Grammars|Active learning reduces the amount of manually annotated sentences necessary when training state-of-the-art statistical parsers. One popular method, uncertainty sampling, selects sentences for which the parser exhibits low certainty. However, this method does not quantify confidence about the current statistical model itself. In particular, we should be less confident about selection decisions based on low frequency events. We present a novel two-stage method which first targets sentences which cannot be reliably selected using uncertainty sampling, and then applies standard uncertainty sampling to the remaining sentences. An evaluation shows that this method performs better than pure uncertainty sampling, and better than an ensemble method based on bagged ensemble members only.|Markus Becker,Miles Osborne","57472|GECCO|2005|Extraction of informative genes from microarray data|Identification of those genes that might anticipate the clinical behavior of different types of cancers is challenging due to availability of a smaller number of patient samples compared to huge number of genes, and the noisy nature of microarray data. After selection of some good genes based on signal-to-noise ratio, unsupervised learning like clustering and supervised learning like k-nearest neighbor (kNN) classifier are widely used in cancer researches to correlate the pathological behavior of cancers with the gene expression levels' differences in cancerous and normal tissues. By applying adaptive searches like Probabilistic Model Building Genetic Algorithm (PMBGA), it may be possible to get a smaller size gene subset that would classify patient samples more accurately than the above methods. In this paper, we propose a new PMBGA based method to extract informative genes from microarray data using Support Vector Machine (SVM) as a classifier. We apply our method to three microarray data sets and present the experimental results. Our method with SVM obtains encouraging results on those data sets as compared with the rank based method using kNN as a classifier.|Topon Kumar Paul,Hitoshi Iba","16096|IJCAI|2005|Learning Web Page Scores by Error Back-Propagation|In this paper we present a novel algorithm to learn a score distribution over the nodes of a labeled graph (directed or undirected). Markov Chain theory is used to define the model of a random walker that converges to a score distribution which depends both on the graph connectivity and on the node labels. A supervised learning task is defined on the given graph by assigning a target score for some nodes and a training algorithm based on error backpropagation through the graph is devised to learn the model parameters. The trained model can assign scores to the graph nodes generalizing the criteria provided by the supervisor in the examples. The proposed algorithm has been applied to learn a ranking function for Web pages. The experimental results show the effectiveness of the proposed technique in reorganizing the rank accordingly to the examples provided in the training set.|Michelangelo Diligenti,Marco Gori,Marco Maggini","16086|IJCAI|2005|Feature Selection Based on the Shapley Value|We present and study the Contribution-Selection algorithm (CSA), a novel algorithm for feature selection. The algorithm is based on the Multiperturbation Shapley Analysis, a framework which relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. Empirical comparison with several other existing feature selection methods shows that the backward eliminati-nation variant of CSA leads to the most accurate classification results on an array of datasets.|Shay Cohen,Eytan Ruppin,Gideon Dror","16046|IJCAI|2005|A language for functional interpretation of model based simulation|Functional modeling is in use for the interpretation of the results of model based simulation of engineered systems for design analysis, enabling the automatic generation of a textual design analysis report that expresses the results of the simulation in terms of the system's purpose. We present a novel functional description language that increases the expressiveness of this approach, allowing a system function to be decomposed in terms of subsidiary functions as well as required effects, increasing the range both of systems and design analysis tasks for which the approach can be used.|Jonathan Bell,Neal Snooke,Chris Price","16114|IJCAI|2005|Representing Flexible Temporal Behaviors in the Situation Calculus|In this paper we present an approach to representing and managing temporally-flexible behaviors in the Situation Calculus based on a model of time and concurrent situations. We define a new hybrid framework combining temporal constraint reasoning and reasoning about actions. We show that the Constraint Based Interval Planning approach can be imported into the Situation Calculus by defining a temporal and concurrent extension of the basic action theory. Finally, we provide a version of the Golog interpreter suitable for managing flexible plans on multiple timelines.|Alberto Finzi,Fiora Pirri","16241|IJCAI|2005|Accurate and Low-cost Location Estimation Using Kernels|We present a novel method for indoor-location estimation using a vector-space model based on signals received from a wireless client. Our aim is to obtain an accurate mapping between the signal space and the physical space without incurring too much human calibration effort. This problem has traditionally been tackled through probabilistic models trained on manually labeled data, which are expensive to obtain. In this paper, we present a novel approach to building a mapping between the signalvector space and the physical location space using kernel canonical correlation analysis (KCCA). Its training requires much less human labor. Moreover, unlike traditional location-estimation systems that treat grid points as independent and discrete target classes during training, we use the physical location as a continuous feedback to build a similarity mapping using KCCA. We test our algorithm in a . wireless LAN environment, and demonstrate the advantage of our method in both accuracy and its ability to utilize a much smaller set of labeled training data than previous methods.|Jeffrey Junfeng Pan,James T. Kwok,Qiang Yang,Yiqiang Chen","80586|VLDB|2005|Querying Business Processes with BP-QL|We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (business process execution language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers. We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.|Catriel Beeri,Anat Eyal,Simon Kamenkovich,Tova Milo","57328|GECCO|2005|GATS  a novel GA-based scheduling algorithm for task scheduling on heterogeneous processor nets|We present a novel GA-based scheduling algorithm for heterogeneous processor networks that succeeds in generating task schedules with completion times that are % and .% shorter than those produced by two of the best existing scheduling algorithms for heterogeneous networks of processors HEFT  and DLS . The new algorithm (GATS .) achieves these results by employing an innovative genotype to phenotype encoding scheme and matching crossover and mutation operators. In addition, GATS . uses a simple fitness evaluation function and a small population, which makes it efficient (relative to classic GA implementations), as well as effective.|Mohammad I. Daoud,Nawwaf N. Kharma","57422|GECCO|2005|Evolutionary algorithms for the self-organized evolution of networks|While the evolution of biological networks can be modeled sensefully as a series of mutation and selection, evolution of other networks such as the social network in a city or the network of streets in a country is not determined by selection since there is no alternative network with which these singular networks have to compete. Nonetheless, these singular networks do evolve due to dynamic changes of vertices and edges. In this article we present a formal, analyzable framework for the evolution of singular networks. We show that the careful design of adaptation rules can lead to the emergence of network topologies with satisfying performance in polynomial time while other adaptation rules yield exponential runtime. We further show by example how the framework could be applied to some ad-hoc communication scenarios.|Katharina Anna Lehmann,Michael Kaufmann"],["57368|GECCO|2005|Bias and scalability in evolutionary development|The introduction of a genotype-phenotype map modelled on biological development can potentially improve the scalability of evolutionary algorithms. Previous work by Gordon and Bentley demonstrated that such a model can be used to evolve patterns that map to useful but small phenotypes. This paper uses the same model to generate much larger patterns covering arrays of up to x cells. The results show that the model's performance is generally comparable to similar development-based systems , , and with some measures outperforms them. Additionally the inherent biases of the model are explored, such as the need to use symmetry-breaking initial conditions which some other models do not require. This exploration yields a set of guidelines that suggest what kinds of problem the model is suited to exploring.|Timothy G. W. Gordon,Peter J. Bentley","57351|GECCO|2005|Scale invariant pareto optimality a meta--formalism for characterizing and modeling cooperativity in evolutionary systems|This article describes a mathematical framework for characterizing cooperativity in complex systems subject to evolutionary pressures. This framework uses three foundational components that constitute a meta-formalism that can be utilized in a host of research and development settings to improve the management, control, and understanding of large numbers of interacting systems such as in communication, computer, and sensor networks. A new concept, Scale Invariant Pareto Optimality, provides a mathematical basis for the efficient tradeoffs of efficiency on many scales and the measurement of cooperativity in complex systems. A mathematically oriented definition of self-organized behavior is also described. Discussion and conjectures are offered.|Mark Fleischer","16091|IJCAI|2005|A Cognitive Model of Visual Analogical Problem-Solving Transfer|Complex problem solving typically involves the generation of a procedure consisting of an ordered sequence of steps. Analogical reasoning is one strategy for solving complex problems, and visual reasoning is another. Visual analogies pertain to analogies based only on visual knowledge. In this paper, we describe the use of Galatea, a computational model of visual analogies in problem solving, to model the problem solving of a human subject (L). L was a given the task of solving a complex problem using analogy in a domain that contained both visual and non-visual knowledge, and was encouraged to use visual analogy. We describe how Galatea models L's use of visual analogy in problem solving.|Jim Davies,Ashok K. Goel,Nancy J. Nersessian","57452|GECCO|2005|Bayesian optimization models for particle swarms|We explore the use of information models as a guide for the development of single objective optimization algorithms, giving particular attention to the use of Bayesian models in a PSO context. The use of an explicit information model as the basis for particle motion provides tools for designing successful algorithms. One such algorithm is developed and shown empirically to be effective. Its relationship to other popular PSO algorithms is explored and arguments are presented that those algorithms may be developed from the same model, potentially providing new tools for their analysis and tuning.|Christopher K. Monson,Kevin D. Seppi","16175|IJCAI|2005|Compound Effects of Top-down and Bottom-up Influences on Visual Attention During Action Recognition|The limited visual and computational resources available during the perception of a human action makes a visual attention mechanism essential. In this paper we propose an attention mechanism that combines the saliency of top-down (or goal-directed) elements, based on multiple hypotheses about the demonstrated action, with the saliency of bottom-up (or stimulus-driven) components. Furthermore, we use the bottom-up part to initialise the top-down, hence resulting in a selection of the behaviours that rightly require the limited computational resources. This attention mechanism is then combined with an action understanding model and implemented on a robot, where we examine its performance during the observation of object-directed human actions.|Bassam Khadhouri,Yiannis Demiris","57509|GECCO|2005|Evolving visually guided agents in an ambiguous virtual world|The fundamental challenge faced by any visual system within natural environments is the ambiguity caused by the fact that light that falls on the system's sensors conflates multiple attributes of the physical world. Understanding the computational principles by which natural systems overcome this challenge and generate useful behaviour remains the key objective in neuroscience and machine vision research. In this paper we introduce Mosaic World, an artificial life model that maintains the essential characteristics of natural visual ecologies, and which is populated by virtual agents that - through 'natural' selection - come to resolve stimulus ambiguity by adapting the functional structure of their visual networks according to the statistical structure of their ecological experience. Mosaic World therefore presents us with an important tool for exploring the computational principles by which vision can overcome stimulus ambiguity and usefully guide behaviour.|Ehud Schlessinger,Peter J. Bentley,R. Beau Lotto","57342|GECCO|2005|The emulation of social institutions as a method of coevolution|This paper offers a novel approach to coevolution based on the sociological theory of symbolic interactionism. It provides a multi-agent computational model along with experimental results that suggest improved fitness, robustness, and knowledge due to emergent symbol systems. The main contribution of the symbolic-interactionist approach to coevolution is the concept of the emergence of a system in the abstract, where an interface between agents evolves. The interface is an emergent symbol system that focuses selective pressure among agents in ways that have been beneficial to agents as a whole in the past, creating a coevolving system that takes advantage of epistasis rather than having to prevent it. Global fitness thereby emerges from local, selfish interaction. The assignment of roles in this system is endogenous.|Deborah Vakas Duong,John J. Grefenstette","16043|IJCAI|2005|Analysis and Verification of Qualitative Models of Genetic Regulatory Networks A Model-Checking Approach|Methods developed for the qualitative simulation of dynamical systems have turned out to be powerful tools for studying genetic regulatory networks. A bottleneck in the application of these methods is the analysis of the simulation results. In this paper, we propose a combination of qualitative simulation and model-checking techniques to perform this task systematically and efficiently. We apply our approach to the analysis of the complex network controlling the nutritional stress response in the bacterium Escherichia coli.|Gr√©gory Batt,Delphine Ropers,Hidde de Jong,Johannes Geiselmann,Radu Mateescu,Michel Page,Dominique Schneider","57543|GECCO|2005|Validation of evolutionary activity metrics for long-term evolutionary dynamics|As artificial life systems grow in number and sophistication, it is becoming increasingly important that the field agree on principled metrics for evaluating them. This report describes a series of experiments validating the evolutionary activity statistics developed by Bedau and his colleagues , , . The work described herein was motivated by a feeling that the 'null hypothesis'---that is, that the evolutionary activity statistics fail to exclude intuitively unlifelike systems from Class  dynamics ---had not been sufficiently disproved in the existing literature. We conducted a series of experiments applying the statistics to such systems, attempting to 'break' the scheme by measuring Class  dynamics in an intuitively unlifelike system. The evolutionary activity measurement scheme has so far proved robust to our attempts to break it, but we believe that this work is still valuable in advancing the validity of the scheme, and that this does not mean the scheme is without shortcomings.|Andrew Stout,Lee Spector","57396|GECCO|2005|Evolution of Voronoi based fuzzy recurrent controllers|A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. Among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi (RFV) model, a representation for recurrent fuzzy systems. It is an extension of the FV model proposed by Kavka and Schoenauer that extends the application domain to include temporal problems. The FV model is a representation for fuzzy controllers based on Voronoi diagrams that can represent fuzzy systems with synergistic rules, fulfilling the -completeness property and providing a simple way to introduce a priory knowledge. In the proposed representation, the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs. These internal units act as memory elements. In the RFV model, the semantic of the internal units can be specified together with the a priori rules. The geometric interpretation of the rules allows the use of geometric variational operators during the evolution. The representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics.|Carlos Kavka,Patricia Roggero,Marc Schoenauer"],["57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57439|GECCO|2005|Measuring mobility and the performance of global search algorithms|The global search properties of heuristic search algorithms are not well understood. In this paper, we introduce a new metric, mobility, that quantifies the dispersion of local optima visited during a search. This allows us to explore two questions How disperse are the local optima visited during a search How does mobility relate to algorithm performance We compare local search with two evolutionary algorithms, CHC and CMA-ES, on a set of non-separable, non-symmetric, multi-modal test functions. Given our mobility metric, we show that algorithms visiting more disperse local optima tend to be better optimizers.|Monte Lunacek,L. Darrell Whitley,James N. Knight","57337|GECCO|2005|Some theoretical results about the computation time of evolutionary algorithms|This paper focuses on the computation time of evolutionary algorithms. First, some exact expressions of the mean first hitting times of general evolutionary algorithms in finite search spaces are obtained theoretically by using the properties of Markov chain. Then, by introducing drift analysis and applying Dynkin's Formula, the general upper and lower bounds of the mean first hitting times of evolutionary algorithms are given rigorously under some mild conditions. These results obtained in this paper, and the analytic methods used in this paper, are widely valid for analyzing the computation time of evolutionary algorithms in any search space(finite or infinite)as long as some simple technique processes are introduced.|Lixin X. Ding,Jinghu Yu","57456|GECCO|2005|A comparison study between genetic algorithms and bayesian optimize algorithms by novel indices|Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA's defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization Algorithm (BOA), a well-known Estimation of Distribution Algorithms (EDAs).|Naoki Mori,Masayuki Takeda,Keinosuke Matsumoto","57533|GECCO|2005|Alternative implementations of the Griewangk function|The well-known Griewangk function, used for evaluation of evolutionary algorithms, becomes easier as the number of dimensions grows. This paper suggests three alternative implementations that maintain function complexity for high-dimensional versions of the problem. Diagonal slices of the search landscape and local search are used to demonstrate and evaluate the difficulty of each function.|Artem Sokolov,L. Darrell Whitley,Monte Lunacek","57400|GECCO|2005|Rigorous runtime analysis of a ES for the sphere function|Evolutionary algorithms (EAs) are general, randomized search heuristics applied successfully to optimization problems both in discrete and in continuous search spaces. In recent years, substantial progress has been made in theoretical runtime analysis of EAs, in particular for pseudo-Boolean fitness functions f(,)n  R. Compared to this, little is known about the runtime of simple and, in particular, more complex EAs for continuous functions f Rn  R.In this paper, a first rigorous runtime analysis of a population-based EA in continuous search spaces is presented. A simple (+) evolution strategy ((+)ES) that uses Gaussian mutations adapted by the -rule as its search operator is studied on the well-known Sphere functionand the influence of  and n on its runtime is examined. By generalizing the proof technique of randomized family trees, developed before w.r.t. discrete search spaces, asymptotically upper and lower bounds on the time for the population to make a predefined progress are derived. Furthermore, the utility of the -rule in population-based evolution strategies is shown. Finally, the behavior of the (+)ES on multimodal functions is discussed.|Jens J√§gersk√ºpper,Carsten Witt","16319|IJCAI|2005|Theory of Alignment Generators and Applications to Statistical Machine Translation|Viterbi Alignment and Decoding are two fundamental search problems in Statistical Machine Translation. Both the problems are known to be NP-hard and therefore, it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. In this paper we characterize exponentially large subspaces in the solution space of Viterbi Alignment and Decoding. Each of these subspaces admits polynomial time optimal search algorithms. We propose a local search heuristic using a neighbourhood relation on these subspaces. Experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.|Raghavendra Udupa,Hemanta Kumar Maji","57423|GECCO|2005|ARGEN  AREPO mixing the artificial genetic engineering and artificial evolution of populations to improve the search process|In this paper we analyze the performance of several evolutionary algorithms in the feature and instance selection problem. It is also introduced the ARGEN + AREPO search algorithm which has been tested in the same problem. There is no need to adapt parameters in this genetic algorithm, except the population size. The reported preliminary results show that using this technique in a wrapper model to search data subsets, we can obtain accuracy similar to the obtained with some of the genetic algorithms models here presented, but with less data.|Agust√≠n Le√≥n-Barranco,Sandra E. Barajas,Carlos A. Reyes Garc√≠a","57308|GECCO|2005|Exploiting gradient information in numerical multi--objective evolutionary optimization|Various multi--objective evolutionary algorithms (MOEAs) have obtained promising results on various numerical multi--objective optimization problems. The combination with gradient--based local search operators has however been limited to only a few studies. In the single--objective case it is known that the additional use of gradient information can be beneficial. In this paper we provide an analytical parametric description of the set of all non--dominated (i.e. most promising) directions in which a solution can be moved such that its objectives either improve or remain the same. Moreover, the parameters describing this set can be computed efficiently using only the gradients of the individual objectives. We use this result to hybridize an existing MOEA with a local search operator that moves a solution in a randomly chosen non--dominated improving direction. We test the resulting algorithm on a few well--known benchmark problems and compare the results with the same MOEA without local search and the same MOEA with gradient--based techniques that use only one objective at a time. The results indicate that exploiting gradient information based on the non--dominated improving directions is superior to using the gradients of the objectives separately and that it can furthermore improve the result of MOEAs in which no local search is used, given enough evaluations.|Peter A. N. Bosman,Edwin D. de Jong","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o"],["57339|GECCO|2005|Not all linear functions are equally difficult for the compact genetic algorithm|Estimation of distribution algorithms (EDAs) try to solve an optimization problem by finding a probability distribution focussed around its optima. For this purpose they conduct a sampling-evaluation-adjustment cycle, where search points are sampled with respect to a probability distribution, which is adjusted according to the evaluation of the sampled points. Although there are many successful experiments suggesting the usefulness of EDAs, there are only few rigorous theoretical results apart from convergence results without time bounds. Here we present first rigorous runtime analyses of a simple EDA, the compact genetic algorithm, for linear pseudo-boolean functions on n variables. We prove a number of results showing that not all linear functions have the same asymptotical runtime.|Stefan Droste","57501|GECCO|2005|Real-coded crossover as a role of kernel density estimation|This paper presents a kernel density estimation method by means of real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine the sampling strategy by means of a parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and the sampling strategy is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation.|Jun Sakuma,Shigenobu Kobayashi","57266|GECCO|2005|Goal-oriented preservation of essential genetic information by offspring selection|This contribution proposes an enhanced and generic selection model for Genetic Algorithms (GAs) and Genetic Programming (GP) which is able to preserve the alleles which are part of a high quality solution. Some selected aspects of these enhanced techniques are discussed exemplarily on the basis of standardized benchmark problems.|Michael Affenzeller,Stefan Wagner 0002,Stephan M. Winkler","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llor√†,David E. Goldberg","57456|GECCO|2005|A comparison study between genetic algorithms and bayesian optimize algorithms by novel indices|Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA's defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization Algorithm (BOA), a well-known Estimation of Distribution Algorithms (EDAs).|Naoki Mori,Masayuki Takeda,Keinosuke Matsumoto","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57382|GECCO|2005|Genetic drift in univariate marginal distribution algorithm|Like Darwinian-type genetic algorithms, there also exists genetic drift in Univariate Marginal Distribution Algorithm (UMDA). Since the universal analysis of genetic drift in UMDA is very difficult, in this paper, we just approach a certain kind of problem (WOneMax Problem). For WOneMax Problem, The individual space in UMDA can be denoted as a full binary tree, and the selecting process in UMDA can be considered as a process of cutting branch. We employ this binary tree to calculate the probability change of each variable between two adjacent generations. Comparing this change with our experimental data, we find that when the population size is limited, there exists genetic drift in UMDA. In order to avoid genetic drift, we model the probability of each variable as a signal with noise, and then use smoothing filter to eliminate genetic drift. Numerical results show this method is effective.|Yi Hong,Qingsheng Ren,Jin Zeng","57292|GECCO|2005|Dynamic optimization of migration topology in internet-based distributed genetic algorithms|Distributed Genetic Algorithms (DGAs) designed for the Internet have to take its high communication cost into consideration. For island model GAs, the migration topology has a major impact on DGA performance. This paper describes and evaluates an adaptive migration topology optimizer that keeps the communication load low while maintaining high solution quality. Experiments on benchmark problems show that the optimized topology outperforms static or random topologies of the same degree of connectivity. The applicability of the method on real-world problems is demonstrated on a hard optimization problem in VLSI design.|Johan Berntsson,Maolin Tang","57270|GECCO|2005|On the practical genetic algorithms|This paper offers practical design-guidelines for developing efficient genetic algorithms (GAs) to successfully solve real-world problems. As an important design component, a practical population-sizing model is presented and verified.|Chang Wook Ahn,Sanghoun Oh,Rudrapatna S. Ramakrishna","57435|GECCO|2005|The compact classifier system motivation analysis and first results|This paper presents an initial analysis of how maximally general and accurate rules can be evolved in a Pittsburgh-style classifier system. In order to be able to perform such analysis we introduce a simple bare-bones Pittsburgh classifier systems---the compact classifier system (CCS)---based on estimation of distribution algorithms. Using a common rule encoding scheme of Pittsburgh classifier systems, CCS maintains a dynamic set of probability vectors that compactly describe a rule set. The compact genetic algorithm is used to evolve each of the initially perturbed probability vectors which represents the rules. Results show how CCS is able to evolve in a compact, simple, and elegant manner rule sets composed by maximally general and accurate rules.|Xavier Llor√†,Kumara Sastry,David E. Goldberg"],["16129|IJCAI|2005|A Probabilistic Lexical Approach to Textual Entailment|The textual entailment problem is to determine if a given text entails a given hypothesis. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Nave Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-the-art heuristic scoring approaches.|Oren Glickman,Ido Dagan,Moshe Koppel","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","57419|GECCO|2005|GAMM genetic algorithms with meta-models for vision|Recent adaptive image interpretation systems can reach optimal performance for a given domain via machine learning, without human intervention. The policies are learned over an extensive generic image processing operator library. One of the principal weaknesses of the method lies with the large size of such libraries, which can make the machine learning process intractable. We demonstrate how evolutionary algorithms can be used to reduce the size of the operator library, thereby speeding up learning of the policy while still keeping human experts out of the development loop. Experiments in a challenging domain of forestry image interpretation exhibited a % reduction in the average time required to interpret an image, while maintaining the image interpretation accuracy of the full library.|Greg Lee,Vadim Bulitko","16338|IJCAI|2005|A Motion Closed World Asumption|Yaman et. al. Yaman et al.,  introduce \"go theories\" to reason about moving objects. In this paper, we show that this logic often does not allow us to infer that an object is not present at a given place or region, even though common sense would dictate that this is a reasonable inference to make. We define a class of models of go-theories called coherent models. We use this concept to define a motion closed world assumption (MCWA) and develop a notion of MCWA-entailment. We show that checking if a go-theory has a coherent model is NP-complete. An in atom checks if a given object is present in a given region sometime in a given time interval. We provide sound and complete algorithms to check if a ground in literal (positive or negative in atom) can be inferred from a go-theory using the MCWA. In our experiments our algorithms answer such queries in less than  second when there are up to , go-atoms per object.|Fusun Yaman,Dana S. Nau,V. S. Subrahmanian","57514|GECCO|2005|A study of evolutionary robustness in stochastically tiled polyominos|Given an evolutionary optimization problem with many possible genotypes for each phenotype this study investigates if the evolved genes for a given phenotype are more robust to point mutation than randomly sampled genes for the same phenotype. This question is addressed using a cellular representation for polyominos in the plane. The evolutionary computation system optimizes for shapes which pack well onto the surface of a torus when dropped at random. For the majority of the evolved phenotypes the evolved genes for a given shape proved to be significantly more robust to point mutation than those sampled at random for that same shape. A few evolved genotypes, however, were not significantly more robust than those sampled at random and in some cases were less robust. These observations are placed in the context of the fitness landscape for the representation.|Justin Schonfeld,Daniel A. Ashlock","57572|GECCO|2005|Use of a genetic algorithm in brills transformation-based part-of-speech tagger|The tagging problem in natural language processing is to find a way to label every word in a text as a particular part of speech, e.g., proper noun. An effective way of solving this problem with high accuracy is the transformation-based or \"Brill\" tagger. In Brill's system, a number of transformation templates are specified a priori that are instantiated and ranked during a greedy search-based algorithm. This paper describes a variant of Brill's implementation that instead uses a genetic algorithm to generate the instantiated rules and provide an adaptive ranking. Based on tagging accuracy, the new system provides a better hybrid evolutionary computation solution to the part-of-speech (POS) problem than the previous attempt. Although not able to make up for the use of a priori knowledge utilized by Brill, the method appears to point the way for an improved solution to the tagging problem.|Garnett Carl Wilson,Malcolm I. Heywood","80578|VLDB|2005|Tuning Schema Matching Software using Synthetic Scenarios|Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user must then tune the system select the right component to be executed and correctly adjust their numerous \"knobs\" (e.g., thresholds, formula coefficients). Tuning is skill- and time-intensive, but (as we show) without it the matching accuracy is significantly inferior.We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of matching components. While the tuning process is completely automatic, eTuner can also exploit user assistance (whenever available) to further improve the tuning quality. We employed eTuner to tune four recently developed matching systems on several real-world domains. eTuner produced tuned matching systems that achieve higher accuracy than using the systems with currently possible tuning methods, at virtually no cost to the domain user.|Mayssam Sayyadian,Yoonkyong Lee,AnHai Doan,Arnon Rosenthal","80580|VLDB|2005|Client Assignment in Content Dissemination Networks for Dynamic Data|Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of data item, coherence served by each repository and a set of client, data item, coherence requests, we address the following problem How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximizedIn this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.|Shetal Shah,Krithi Ramamritham,Chinya V. Ravishankar","57576|GECCO|2005|Population-based incremental learning with memory scheme for changing environments|In recent years there has been a growing interest in studying evolutionary algorithms for dynamic optimization problems due to its importance in real world applications. Several approaches have been developed, such as the memory scheme. This paper investigates the application of the memory scheme for population-based incremental learning (PBIL) algorithms, a class of evolutionary algorithms, for dynamic optimization problems. A PBIL-specific memory scheme is proposed to improve its adaptability in dynamic environments. In this memory scheme the working probability vector is stored together with the best sample it creates in the memory and is used to reactivate old environments when change occurs. Experimental study based on a series of dynamic environments shows the efficiency of the memory scheme for PBILs in dynamic environments. In this paper, the relationship between the memory scheme and the multi-population scheme for PBILs in dynamic environments is also investigated. The experimental results indicate a negative interaction of the multi-population scheme on the memory scheme for PBILs in the dynamic test environments.|Shengxiang Yang","16058|IJCAI|2005|Viewing Referring Expression Generation as Search|Almost all natural language generation (NLG) systems are faced with the problem of the generation of referring expressions (GRE) given a symbol corresponding to an intended referent, how do we work out the semantic content of a referring expression that uniquely identifies the entity in question This is now one of the most widely explored problems in NLG over the last  years, a number of algorithms have been proposed for addressing different aspects of this problem, but the different approaches taken make it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we show how viewing the problem of referring expression generation as a search problem allows us to recast existing algorithms in a way that makes their similarities and differences clear.|Bernd Bohnet,Robert Dale"],["16171|IJCAI|2005|Iterated Belief Revision Revised|The AGM postulates for belief revision, augmented by the DP postulates for iterated belief revision, provide generally accepted criteria for the design of operators by which intelligent agents adapt their beliefs incrementally to new information. These postulates alone, however, are too permissive They support operators by which all newly acquired information is canceled as soon as an agent learns a fact that contradicts some of its current beliefs. In this paper, we present a formal analysis of the deficiency of the DP postulates, and we show how to solve the problem by an additional postulate of independence. We give a representation theorem for this postulate and prove that it is compatible with AGM and DP.|Yi Jin,Michael Thielscher","16059|IJCAI|2005|Reconstructing an Agents Epistemic State from Observations|We look at the problem in belief revision of trying to make inferences about what an agent believed - or will believe - at a given moment, based on an observation of how the agent has responded to some sequence of previous belief revision inputs over time. We adopt a \"reverse engineering\" approach to this problem. Assuming a framework for iterated belief revision which is based on sequences, we construct a model of the agent that \"best explains\" the observation. Further considerations on this best-explaining model then allow inferences about the agent's epistemic behaviour to be made. We also provide an algorithm which computes this best explanation.|Richard Booth,Alexander Nittka","16154|IJCAI|2005|Iterated Belief Change A Transition System Approach|We use a transition system approach to reason about the evolution of an agent's beliefs as actions are executed. Some actions cause an agent to perform belief revision and some actions cause an agent to perform belief update, but the interaction between revision and update can be non-elementary. We present a set of basic postulates describing the interaction of revision and update, and we introduce a new belief evolution operator that gives a plausible interpretation to alternating sequences of revisions and updates.|Aaron Hunter,James P. Delgrande","16309|IJCAI|2005|The Ontology Revision|An ontology consists of a set of concepts, a set of constraints imposing on instances of concepts, and the subsumption relation. It is assumed that an ontology is a tree under the subsumption relation between concepts. To preserve structural properties of ontologies, the ontology revision is not only contracting ontologies by discarding statements inconsistent with a revising statement, but also extracting statements consistent with the revising statement and adding some other statements. In the ontology revision, the consistency of a revising statement with the theory of the logical closure of the ontology under the closed world assumption is discussed. The basic postulates of the ontology revision are proposed and a concrete ontology revision is given based on the consistence or inconsistence of an ontology and a revising statement.|Yu Sun,Yuefei Sui","16240|IJCAI|2005|Inverse Resolution as Belief Change|Belief change is concerned with modelling the way in which a rational reasoner maintains its beliefs as it acquires new information. Of particular interest is the way in which new beliefs are acquired and determined and old beliefs are retained or discarded. A parallel can be drawn to symbolic machine learning approaches where examples to be categorised are presented to the learning system and a theory is subsequently derived, usually over a number of iterations. It is therefore not surprising that the term 'theory revision' is used to describe this process Ourston and Mooney, . Viewing a machine learning system as a rational reasoner allows us to begin seeing these seemingly disparate mechanisms in a similar light. In this paper we are concerned with characterising the well known inverse resolution operations Muggleton,   (and more recently, inverse entailment Muggleton, ) as AGM-style belief change operations. In particular, our account is based on the abductive expansion operation Pagnucco et al.,  Pagnucco,  and characterised by using the notion of epistemic entrenchment Grdenfors and Makinson,  extended for this operation. This work provides a basis for reconciling work in symbolic machine learning and belief revision. Moreover, it allows machine learning techniques to be understood as forms of nonmonotonic reasoning.|Maurice Pagnucco,David Rajaratnam","16188|IJCAI|2005|A Unified Framework of Propositional Knowledge Base Revision and Update Based on State Transition Models|Belief revision and belief update are two of the most basic types of belief change operations. We need to select either revision or update when we accept new information into the current belief, however, such decision making has not been considered. In this paper, we propose a unified framework of revision and update based on state transition models that enable us to do such decision making. This framework provides a hybrid operation of revision and update, called acceptance.|Yasuo Kudo,Tetsuya Murai","16049|IJCAI|2005|Encoding formulas with partially constrained weights in a possibilistic-like many-sorted propositional logic|Possibilistic logic offers a convenient tool for handling uncertain or prioritized formulas and coping with inconsistency. Propositional logic formulas are thus associated with weights belonging to a linearly ordered scale. However, especially in case of multiple source information, only partial knowledge may be available about the relative ordering between weights of formulas. In order to cope with this problem, a two-sorted counterpart of possibilistic logic is introduced. Pieces of information are encoded as clauses where special literals refer to the weights. Constraints between weights translate into logical formulas of the corresponding sort and are gathered in a distinct auxiliary knowledge base. An inference relation, which is sound and complete with respect to preferential model semantics, enables us to draw plausible conclusions from the two knowledge bases. The inference process is characterized by using \"forgetting variables\" for handling the symbolic weights, and hence an inference process is obtained by means of a DNF compilation of the two knowledge bases.|Salem Benferhat,Henri Prade","16048|IJCAI|2005|Revision of Partially Ordered Information Axiomatization Semantics and Iteration|This paper deals with iterated revision of partially ordered information. The first part of this paper concerns the Katsuno-Mendelzon's postulates we first point out that these postulates are not fully satisfactory since only a class of partially ordered information can be revised. We then propose a suitable definition of faithful assignment, followed by a new set of postulates and a representation theorem. The second part of this paper investigates additional postulates dedicated to iterated revision operators of partially ordered information. Three extensions of well-known iterated belief revision operations for dealing with partially ordered information are briefly presented.|Salem Benferhat,Sylvain Lagrue,Odile Papini","16294|IJCAI|2005|First-Order Logical Filtering|Logical filtering is the process of updating a belief state (set of possible world states) after a sequence of executed actions and perceived observations. In general, it is intractable in dynamic domains that include many objects and relationships. Still, potential applications for such domains (e.g., semantic web, autonomous agents, and partial-knowledge games) encourage research beyond immediate intractability results. In this paper we present polynomial-time algorithms for filtering belief states that are encoded as First-Order Logic (FOL) formulae. We sidestep previous discouraging results, and show that our algorithms are exact in many cases of interest. These algorithms accept belief states in full FOL, which allows natural representation with explicit references to unidentified objects, and partially known relationships. Our algorithms keep the encoding compact for important classes of actions, such as STRIPS actions. These results apply to most expressive modeling languages, such as partial databases and belief revision in FOL.|Afsaneh Shirazi,Eyal Amir","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"]]},"title":{"entropy":6.014473399205865,"topics":["for data, for xml, system for, planning for, vector machine, data streams, support vector, and system, support machine, logic programs, planning with, query for, for database, system data, and data, management system, database system, database, data mining, and time","genetic algorithm, algorithm for, algorithm the, the problem, for the, for problem, evolutionary for, evolutionary algorithm, the, evolutionary the, search for, genetic the, and algorithm, genetic for, local search, negative selection, algorithm problem, multi-agent system, the search, search","genetic programming, genetic algorithm, particle swarms, using genetic, for optimization, differential evolution, artificial immune, particle optimization, dynamic environments, with genetic, models for, swarms optimization, evolutionary optimization, optimization, optimization with, genetic for, optimization algorithm, neural networks, and programming, evolution strategies","and, the and, for and, constraint satisfaction, learning for, between and, representations and, estimation distribution, models and, and based, statistical learning, reinforcement learning, learning, constraint and, semantic for, the complexity, with and, for satisfaction, combining and, constraint problem","for with, planning for, for reasoning, logic programs, planning with, with, planning and, and reasoning, with and, reasoning with, with logic, planning, reasoning, from, logic, action, domain, temporal, continuous, schema","for xml, efficient for, query database, for robot, and time, efficient and, method for, for pattern, and xml, for web, query for, efficient, pattern, web, time, matching, querying, query, evaluation, using","for problem, the problem, search for, the search, for the, local search, genetic search, search and, and problem, for scheduling, algorithm search, local for, search, algorithm problem, search with, genetic problem, the space, space and, search space, for space","for the, for function, evolutionary for, system for, analysis the, for, learning for, analysis for, classifier system, for and, generation for, for using, for tree, analysis, evolving","for networks, neural networks, based algorithm, genetic networks, approach for, knowledge for, algorithm networks, training networks, networks approach, based for, approach, based, knowledge, networks, testing, analysis, fuzzy, new, markov, parallel","models for, differential evolution, artificial immune, evolution for, using models, using networks, using evolution, and evolution, evolution strategies, gene using, differential for, models, and networks, evolution, models and, for networks, for using, evolving, improving, heuristics","agents and, between and, for agents, state, from, relation, discovery, multiple, variables, role, local, knowledge, the","constraint and, constraint satisfaction, and its, and application, for satisfaction, complexity and, the and, for constraint, the complexity, for and, and problem, constraint problem, for application, application, the, heuristics, structural, search, web, bayesian"],"ranking":[["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","57327|GECCO|2005|DXCS an XCS system for distributed data mining|XCS is a flexible system for data mining due to its ability to deal with environmental changes, learn online with little prior knowledge and evolve accurate and maximally general classifiers. In this paper, we propose DXCS which is an XCS-based distributed data mining system. A MDL metric is proposed to quantify and analyze network load, and study the balance between network load and classifier accuracy in the presence of noise. The DXCS system shows promising results.|Hai Huong Dam,Hussein A. Abbass,Chris Lokan","80556|VLDB|2005|The Integrated Microbial Genomes IMG System A Case Study in Biological Data Management|Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.|Victor M. Markowitz,Frank Korzeniewski,Krishna Palaniappan,Ernest Szeto,Natalia Ivanova,Nikos Kyrpides","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","80561|VLDB|2005|SVM in Oracle Database g Removing the Barriers to Widespread Adoption of Support Vector Machines|Contemporary commercial databases are placing an increased emphasis on analytic capabilities. Data mining technology has become crucial in enabling the analysis of large volumes of data. Modern data mining techniques have been shown to have high accuracy and good generalization to novel data. However, achieving results of good quality often requires high levels of user expertise. Support Vector Machines (SVM) is a powerful state-of-the-art data mining algorithm that can address problems not amenable to traditional statistical analysis. Nevertheless, its adoption remains limited due to methodological complexities, scalability challenges, and scarcity of production quality SVM implementations. This paper describes Oracle's implementation of SVM where the primary focus lies on ease of use and scalability while maintaining high performance accuracy. SVM is fully integrated within the Oracle database framework and thus can be easily leveraged in a variety of deployment scenarios.|Boriana L. Milenova,Joseph Yarmus,Marcos M. Campos","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80517|VLDB|2005|Maximal Vector Computation in Large Data Sets|Finding the maximals in a collection of vectors is relevant to many applications. The maximal set is related to the convex hull---and hence, linear optimization---and nearest neighbors. The maximal vector problem has resurfaced with the advent of skyline queries for relational databases and skyline algorithms that are external and relationally well behaved.The initial algorithms proposed for maximals are based on divide-and-conquer. These established good average and worst case asymptotic running times, showing it to be O(n) average-case. where n is the number of vectors. However, they are not amenable to externalizing. We prove, furthermore, that their performance is quite bad with respect to the dimensionality, k, of the problem. We demonstrate that the more recent external skyline algorithms are actually better behaved, although they do not have as good an apparent asymptotic complexity. We introduce a new external algorithm, LESS, that combines the best features of these. experimentally evaluate its effectiveness and improvement over the field, and prove its average-case running time is O(kn).|Parke Godfrey,Ryan Shipley,Jarek Gryz","16312|IJCAI|2005|Online Support System for Mediator Education|To settle the disputation, ADR (Alternative Disputation Resolution) has been becoming popular instead of the trial. However, to educate the mediation skill, much training is needed. In this paper, we introduce the overview of the online mediator education system. This system navigates the users by providing materials for decision making by referring to old cases. Users can communicate with each other by using avatars, and they can see the status of disputation in the form of several diagrams.|Takahiro Tanaka,Yoshiaki Yasumura,Daisuke Katagami,Katsumi Nitta"],["57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57306|GECCO|2005|A hybrid evolutionary algorithm for the p-median problem|A hybrid evolutionary algorithm (EA) for the p-median problem consist of two stages, each of which is a steady-state hybrid EA. These EAs encode selections of medians as subsets of the candidate sites, apply a recombination operator tailored to the problem, and select symbols in chromosomes to mutate based on an explicit collective memory (named virtual loser). They also apply a sequence of two or three local search procedures to each new solution. Tests e.g. on the benchmark problem instances of ORLIB returned results within .% of the best solutions known.|Istv√°n Borgulya","57405|GECCO|2005|Theoretical analysis of a mutation-based evolutionary algorithm for a tracking problem in the lattice|Evolutionary algorithms are often applied for solving optimization problems that are too complex or different from classical problems so that the application of classical methods is difficult. One example are dynamic problems that change with time. An important class of dynamic problems is the class of tracking problems where an algorithm has to find an approximately optimal solution and insure an almost constant quality in spite of the changing problem. For the application of evolutionary algorithms to static optimization problems, the distribution of the optimization time and most often its expected value are most important. Adopting this perspective a simple tracking problem in the lattice is considered and the performance of a mutation-based evolutionary algorithm is evaluated. For the static case, asymptotically tight upper and lower bounds are proven. These results are applied to derive results on the tracking performance for different rates of change.|Thomas Jansen,Ulf Schellbach","57524|GECCO|2005|An evolutionary algorithm to generate hyper-ellipsoid detectors for negative selection|This paper introduces hyper-ellipsoids as an improvement to hyper-spheres as intrusion detectors in a negative selection problem within an artificial immune system. Since hyper-spheres are a specialization of hyper-ellipsoids, hyper-ellipsoids retain the benefits of hyper-spheres. However, hyper-ellipsoids are much more flexible, mostly in that they can be stretched and reoriented. The viability of using hyper-ellipsoids is established using several pedagogical problems. We conjecture that fewer hyper-ellipsoids than hyper-spheres are needed to achieve similar coverage of nonself space in a negative selection problem. Experimentation validates this conjecture. In pedagogical benchmark problems, the number of hyper-ellipsoids to achieve good results is significantly (%) smaller than the associated number of hyper-spheres.|Joseph M. Shapiro,Gary B. Lamont,Gilbert L. Peterson","57282|GECCO|2005|Local and global order  convergence of a surrogate evolutionary algorithm|A Quasi-Monte-Carlo method based on the computation of a surrogate model of the fitness function is proposed, and its convergence at super-linear rate  is proved under rather mild assumptions on the fitness function -- but assuming that the starting point lies within a small neighborhood of a global maximum. A memetic algorithm is then constructed, that performs both a random exploration of the search space and the exploitation of the best-so-far points using the previous surrogate local algorithm, coupled through selection. Under the same mild hypotheses, the global convergence of the memetic algorithm, at the same  rate, is proved.|Anne Auger,Marc Schoenauer,Olivier Teytaud","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","57474|GECCO|2005|A hybrid genetic algorithm with pattern search for finding heavy atoms in protein crystals|One approach for determining the molecular structure of proteins is a technique called iso-morphous replacement, in which crystallographers dope protein crystals with heavy atoms, such as mercury or platinum. By comparing measured amplitudes of diffracted x-rays through protein crystals with and without the heavy atoms, the locations of the heavy atoms can be estimated. Once the locations of the heavy atoms are known, the phases of the diffracted x-rays through the protein crystal can be estimated, which in turn enables the structure of the protein to be estimated. Unfortunately, the key step in this process is the estimation of the locations of the heavy atoms, and this is a multi-modal, non-linear inverse problem. We report results of a pilot study that show that a -stage hybrid algorithm, using a stochastic genetic algorithm for stage  followed by a deterministic pattern search algorithm for stage , can successfully locate up to  heavy atoms in computer simulated crystals using noise free data. We conclude that the method may be a viable approach for finding heavy atoms in protein crystals, and suggest ways in which the approach can be scaled up to larger problems.|Joshua L. Payne,Margaret J. Eppstein","57298|GECCO|2005|Map-labelling with a multi-objective evolutionary algorithm|We present a multi-objective evolutionary algorithm approach to the map-labelling problem. Map-labelling involves placing labels for sites onto a map such that the result is easy to read and usable for navigation. However, map-users vary in their priorities and capabilities for example, sight-impaired users need to maximise font-size, whereas other users may be willing to accept smaller labels in exchange for increased clarity of bindings of labels to sites. With a multi-objective approach, we evolve a range of labellings from which users can select according to their particular circumstances. We present results from labelling two maps, including a difficult, dense map of Newcastle County in Delaware, which clearly illustrate the advantages of the multi-objective approach.|Lucas Bradstreet,Luigi Barone,R. Lyndon While","57274|GECCO|2005|Heuristic rules embedded genetic algorithm to solve in-core fuel management optimization problem|Because of the large number of possible combinations for the fuel assembly loading in the core, the design of the loading pattern (LP) is a complex optimization problem. It requires finding an optimal fuel arrangement in order to achieve maximum cycle length while satisfying the safety constraints. The objective of this study is to develop a loading pattern optimization code. Generally in-core fuel management codes are written for specific cores and limited fuel inventory. One of the goals of this study is to develop a loading pattern optimization code, which is applicable for all types of Pressurized Water Reactor (PWR) core structures with unlimited number of fuel assembly types in the inventory. To reach this goal an innovative genetic algorithm is developed with modifying the classical representation of the genotype. To obtain the best result in a shorter time not only the representation is changed but also the algorithm is changed to use in-core fuel management heuristics rules. The improved GA code was tested demonstrating the advantages of the introduced enhancements. The core physics code used in this research is Moby-Dick, which was developed to analyze the VVER reactors by SKODA Inc.|Fatih Alim,Kostadin Ivanov","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o"],["57281|GECCO|2005|Optimization with constraints using a cultured differential evolution approach|In this paper we propose a cultural algorithm, where different knowledge sources modify the variation operator of a differential evolution algorithm. Differential evolution is used as a basis for the population, variation and selection processes. The experiments performed show that the cultured differential evolution is able to reduce the number of fitness function evaluations needed to obtain a good aproximation of the optimum value in constrained real-parameter optimization. Comparisons are provided with respect to three techniques that are representative of the state-of-the-art in the area.|Ricardo Landa Becerra,Carlos A. Coello Coello","57452|GECCO|2005|Bayesian optimization models for particle swarms|We explore the use of information models as a guide for the development of single objective optimization algorithms, giving particular attention to the use of Bayesian models in a PSO context. The use of an explicit information model as the basis for particle motion provides tools for designing successful algorithms. One such algorithm is developed and shown empirically to be effective. Its relationship to other popular PSO algorithms is explored and arguments are presented that those algorithms may be developed from the same model, potentially providing new tools for their analysis and tuning.|Christopher K. Monson,Kevin D. Seppi","57354|GECCO|2005|An artificial immune network for multimodal function optimization on dynamic environments|Multimodal optimization algorithms inspired by the immune system are generally characterized by a dynamic control of the population size and by diversity maintenance along the search. One of the most popular proposals is denoted opt-aiNet (artificial immune network for optimization) and is extended here to deal with time-varying fitness functions. Additional procedures are designed to improve the overall performance and the robustness of the immune-inspired approach, giving rise to a version for dynamic optimization, denoted dopt-aiNet. Firstly, challenging benchmark problems in static multimodal optimization are considered to validate the new proposal. No parameter adjustment is necessary to adapt the algorithm according to the peculiarities of each problem. In the sequence, dynamic environments are considered, and usual evaluation indices are adopted to assess the performance of dopt-aiNet and compare with alternative solution procedures available in the literature.|Fabr√≠cio Olivetti de Fran√ßa,Fernando J. Von Zuben,Leandro Nunes de Castro","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57468|GECCO|2005|A hardware pipeline for function optimization using genetic algorithms|Genetic Algorithms (GAs) are very commonly used as function optimizers, basically due to their search capability. A number of different serial and parallel versions of GA exist. In this paper, a pipelined version of the commonly used Genetic Algorithms and a corresponding hardware platform is described. The main idea of achieving pipelined execution of different operations of GA is to use a stochastic selection function which works with the fitness value of the candidate chromosome only. The modified algorithm is termed PLGA (Pipelined Genetic Algorithm). When executed in a CGA (Classical Genetic Algorithm) framework, the stochastic selection gives comparable performances with the roulette-wheel selection. In the pipelined hardware environment, PLGA will be much faster than the CGA. When executed on similar hardware platforms, PLGA may attain a maximum speedup of four over CGA. However, if CGA is executed in a uniprocessor system the speedup is much more. A comparison of PLGA against PGA (Parallel Genetic Algorithms) shows that PLGA may be even more effective than PGAs. A scheme for realizing the hardware pipeline is also presented. Since a general function evaluation unit is essential, a detailed description of one such unit is presented.|Malay Kumar Pakhira,Rajat K. De","57292|GECCO|2005|Dynamic optimization of migration topology in internet-based distributed genetic algorithms|Distributed Genetic Algorithms (DGAs) designed for the Internet have to take its high communication cost into consideration. For island model GAs, the migration topology has a major impact on DGA performance. This paper describes and evaluates an adaptive migration topology optimizer that keeps the communication load low while maintaining high solution quality. Experiments on benchmark problems show that the optimized topology outperforms static or random topologies of the same degree of connectivity. The applicability of the method on real-world problems is demonstrated on a hard optimization problem in VLSI design.|Johan Berntsson,Maolin Tang","57309|GECCO|2005|Optimization of passenger car design for the mitigation of pedestrian head injury using a genetic algorithm|The problem of pedestrian injury is a significant one throughout the world. In , there were  pedestrian fatalities in Europe and  in the US. Significant advances have been made by automotive safety researchers and vehicle manufacturers to address this issue with respect to the design of vehicles, but the complex nature of pedestrian accident scenarios has resulted in great difficulty when using traditional statistical methods. Specifically, problems have been encountered when attempting to study the effects of individual parameters of vehicle front-end geometry on pedestrian head injury. This paper attempts to demonstrate the feasibility of applying the field of evolutionary computation to the problem of pedestrian safety by using a simple genetic algorithm to optimize the centre-line geometry of a car's front-end for the reduction of pedestrian head and thoracic injury. The fitness of each design is assessed by creating a multi-body mathematical model of the vehicle front and simulating impacts with models of different sized pedestrians, and ranking according to the combined injury scores.|Emma Carter,Steve Ebdon,Clive Neal-Sturgess","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens","57483|GECCO|2005|Exploring extended particle swarms a genetic programming approach|Particle Swarm Optimisation (PSO) uses a population of particles that fly over the fitness landscape in search of an optimal solution. The particles are controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best point, while its momentum tries to keep it moving in its current direction.Previous research started exploring the possibility of evolving the force generating equations which control the particles through the use of genetic programming (GP).We independently verify the findings of the previous research and then extend it by considering additional meaningful ingredients for the PSO force-generating equations, such as global measures of dispersion and position of the swarm. We show that, on a range of problems, GP can automatically generate new PSO algorithms that outperform standard human-generated as well as some previously evolved ones.|Riccardo Poli,Cecilia Di Chio,William B. Langdon"],["57343|GECCO|2005|Interactive estimation of agent-based financial markets models modularity and learning|Building upon the interactive inversion method introduced by Ashburn and Bonabeau (), we show how to dramatically improve the results by exploiting modularity and by letting the computer learn user preferences.|M. Ihsan Ecemis,Eric Bonabeau,Trent Ashburn","16328|IJCAI|2005|Learning Subjective Representations for Planning|Planning involves using a model of an agent's actions to find a sequence of decisions which achieve a desired goal. It is usually assumed that the models are given, and such models often require expert knowledge of the domain. This paper explores subjective representations for planning that are learned directly from agent observations and actions (requiring no initial domain knowledge). A non-linear embedding technique called Action Respecting Embedding is used to construct such a representation. It is then shown how to extract the effects of the agent's actions as operators in this learned representation. Finally, the learned representation and operators are combined with search to find sequences of actions that achieve given goals. The efficacy of this technique is demonstrated in a challenging robot-vision-inspired image domain.|Dana F. Wilkinson,Michael H. Bowling,Ali Ghodsi","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","16133|IJCAI|2005|The Complexity of Quantified Constraint Satisfaction Problems under Structural Restrictions|We give a clear picture of the tractabilityintractability frontier for quantified constraint satisfaction problems (QCSPs) under structural restrictions. On the negative side, we prove that checking QCSP satisfiability remains PSPACE-hard for all known structural properties more general than bounded treewidth and for the incomparable hypergraph acyclicity. Moreover, if the domain is not fixed, the problem is PSPACE-hard even for tree-shaped constraint scopes. On the positive side, we identify relevant tractable classes, including QCSPs with prefix  having bounded hypertree width, and QCSPs with a bounded number of guards. The latter are solvable in polynomial time without any bound on domains or quantifier alternations.|Georg Gottlob,Gianluigi Greco,Francesco Scarcello","16261|IJCAI|2005|Using Predictive Representations to Improve Generalization in Reinforcement Learning|The predictive representations hypothesis holds that particularly good generalization will result from representing the state of the world in terms of predictions about possible future experience. This hypothesis has been a central motivation behind recent research in, for example, PSRs and TD networks. In this paper we present the first explicit investigation of this hypothesis. We show in a reinforcement-learning example (a grid-world navigation task) that a predictive representation in tabular form can learn much faster than both the tabular explicit-state representation and a tabular history-based method.|Eddie J. Rafols,Mark B. Ring,Richard S. Sutton,Brian Tanner","16083|IJCAI|2005|A Unified Theory of Structural Tractability for Constraint Satisfaction and Spread Cut Decomposition|In this paper we introduce a generic form of structural decomposition for the constraint satisfaction problem, which we call a guarded decomposition. We show that many existing decomposition methods can be characterized in terms of finding guarded decompositions satisfying certain specified additional conditions. Using the guarded decomposition framework we are also able to define a new form of decomposition, which we call a spread cut. We show that discovery of width k spread-cut decompositions is tractable for each k, and that the spread cut decomposition strongly generalize all existing decompositions except hypertrees. Finally we exhibit a family of hypergraphs Hn, for n  , ,  ..., where the width of the best hypertree decomposition of each Hn is at least n, but the width of the best spreadcut decomposition is at most n.|David A. Cohen,Peter Jeavons,Marc Gyssens","16153|IJCAI|2005|Optimal Refutations for Constraint Satisfaction Problems|Variable ordering heuristics have long been an important component of constraint satisfaction search algorithms. In this paper we study the behaviour of standard variable ordering heuristics when searching an insoluble (sub)problem. We employ the notion of an optimal refutation of an insoluble (sub)problem and describe an algorithm for obtaining it. We propose a novel approach to empirically looking at problem hardness and typical-case complexity by comparing optimal refutations with those generated by standard search heuristics. It is clear from our analysis that the standard variable orderings used to solve CSPs behave very differently on real-world problems than on random problems of comparable size. Our work introduces a potentially useful tool for analysing the causes of the heavy-tailed phenomenon observed in the runtime distributions of backtrack search procedures.|Tudor Hulubei,Barry O'Sullivan","16130|IJCAI|2005|QCSP-Solve A Solver for Quantified Constraint Satisfaction Problems|The Quantified Constraint Satisfaction Problem (QCSP) is a generalization of the CSP in which some variables are universally quantified. It has been shown that a solver based on an encoding of QCSP into QBF can outperform the existing direct QCSP approaches by several orders of magnitude. In this paper we introduce an efficient QCSP solver. We show how knowledge learned from the successful encoding of QCSP into QBF can be utilized to enhance the existing QCSP techniques and speed up search by orders of magnitude. We also show how the performance of the solver can be further enhanced by incorporating advanced look-back techniques such as CBJ and solution-directed pruning. Experiments demonstrate that our solver is several orders of magnitude faster than existing direct approaches to QCSP solving, and significantly outperforms approaches based on encoding QCSPs as QBFs.|Ian P. Gent,Peter Nightingale,Kostas Stergiou","16215|IJCAI|2005|Concurrent Hierarchical Reinforcement Learning|We consider applying hierarchical reinforcement learning techniques to problems in which an agent has several effectors to control simultaneously. We argue that the kind of prior knowledge one typically has about such problems is best expressed using a multithreaded partial program, and present concurrent ALisp, a language for specifying such partial programs. We describe algorithms for learning and acting with concurrent ALisp that can be efficient even when there are exponentially many joint choices at each decision point. Finally, we show results of applying these methods to a complex computer game domain.|Bhaskara Marthi,Stuart J. Russell,David Latham,Carlos Guestrin","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder"],["16322|IJCAI|2005|Disjunctive Temporal Planning with Uncertainty|Driven by planning problems with both disjunctive constraints and contingency, we define the Disjunctive Temporal Problem with Uncertainty (DTPU), an extension of the DTP that includes contingent events. Generalizing existing work on Simple Temporal Problems with Uncertainty, we divide the time-points into controllable and uncontrollable classes, and propose varying notions of controllability to replace the notion of consistency.|Kristen Brent Venable,Neil Yorke-Smith","16132|IJCAI|2005|Integrating Planning and Temporal Reasoning for Domains with Durations and Time Windows|The treatment of exogenous events in planning is practically important in many domains. In this paper we focus on planning with exogenous events that happen at known times, and affect the plan actions by imposing that the execution of certain plan actions must be during some time windows. When actions have durations, handling such constraints adds an extra difficulty to planning, which we address by integrating temporal reasoning into planning. We propose a new approach to planning in domains with durations and time windows, combining graph-based planning and disjunctive constraint-based temporal reasoning. Our techniques are implemented in a planner that took part in the th International Planning Competition showing very good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina","16218|IJCAI|2005|Planning with Continuous Resources in Stochastic Domains|We consider the problem of optimal planning in stochastic domains with resource constraints, where resources are continuous and the choice of action at each step may depend on the current resource level. Our principal contribution is the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables. The search algorithm leverages knowledge of the starting state to focus computational effort on the relevant parts of the state space. We claim that this approach is especially effective when resource limitations contribute to reachability constraints. Experimental results show its effectiveness in the domain that motivates our research - automated planning for planetary exploration rovers.|Mausam,Emmanuel Benazera,Ronen I. Brafman,Nicolas Meuleau,Eric A. Hansen","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando","16202|IJCAI|2005|Planning with Loops|Unlike the case for sequential and conditional planning, much of the work on iterative planning (planning where loops may be needed) leans heavily on theorem-proving. This paper does the following it proposes a different approach where generating plans is decoupled from verifying them describes the implementation of an iterative planner based on the situation calculus presents a few examples illustrating the sorts of plans that can be generated shows some of the strengths and weaknesses of the approach and finally sketches the beginnings of a theory, where validation of plans is done offline.|Hector J. Levesque","16316|IJCAI|2005|Ordering Heuristics for Description Logic Reasoning|We present a new architecture for Description Logic implementations, a range of new optimisation techniques and an empirical analysis of their effectiveness.|Dmitry Tsarkov,Ian Horrocks","16118|IJCAI|2005|Abstraction-based Action Ordering in Planning|Many planning problems contain collections of symmetric objects, actions and structures which render them difficult to solve efficiently. It has been shown that the detection and exploitation of symmetric structure in planning problems can dramatically reduce the size of the search space and the time taken to find a solution. We present the idea of using an abstraction of the problem domain to reveal symmetric structure and guide the navigation of the search space. We show that this is effective even in domains in which there is little accessible symmetric structure available for pruning. Proactive exploitation represents a flexible and powerful alternative to the symmetry-breaking strategies exploited in earlier work in planning and CSPs. The notion of almost symmetry is defined and results are presented showing that proactive exploitation of almost symmetry can improve the performance of a heuristic forward search planner.|Maria Fox,Derek Long,Julie Porteous","16108|IJCAI|2005|Strong Equivalence for Logic Programs with Preferences|Recently, strong equivalence for Answer Set Programming has been studied intensively, and was shown to be beneficial for modular programming and automated optimization. In this paper we define the novel notion of strong equivalence for logic programs with preferences. Based on this definition we give, for several semantics for preference handling, necessary and sufficient conditions for programs to be strongly equivalent. These results provide a clear picture of the relationship of these semantics with respect to strong equivalence, which differs considerably from their relationship with respect to answer sets. Finally, based on these results, we present for the first time simplification methods for logic programs with preferences.|Wolfgang Faber,Kathrin Konczak","16197|IJCAI|2005|From knowledge-based programs to graded belief-based programs part II off-line reasoning|Belief-based programs generalize knowledgebased programs Fagin et al.,  by allowing for incorrect beliefs, unreliable observations, and branching conditions that refer to implicit graded beliefs, such as in \"while my belief about the direction to the railway station is not strong enough do ask someone\". We show how to reason off-line about the possible executions of a belief-based program, which calls for introducing second-order uncertainty in the model.|No√´l Laverny,J√©r√¥me Lang"],["80480|VLDB|2005|Benefits of Path Summaries in an XML Query Optimizer Supporting Multiple Access Methods|We compare several optimization strategies implemented in an XML query evaluation system. The strategies incorporate the use of path summaries into the query optimizer, and rely on heuristics that exploit data statistics.We present experimental results that demonstrate a wide range of performance improvements for the different strategies supported. In addition, we compare the speedups obtained using path summaries with those reported for index-based methods. The comparison shows that low-cost path summaries combined with optimization strategies achieve essentially the same benefits as more expensive index structures.|Attila Barta,Mariano P. Consens,Alberto O. Mendelzon","80568|VLDB|2005|Streaming Pattern Discovery in Multiple Time-Series|In this paper, we introduce SPIRIT (Streaming Pattern dIscoveRy in multIple Time-series). Given n numerical data streams, all of whose values we observe at each time tick t, SPIRIT can incrementally find correlations and hidden variables, which summarise the key trends in the entire stream collection. It can do this quickly, with no buffering of stream values and without comparing pairs of streams. Moreover, it is any-time, single pass, and it dynamically detects changes. The discovered trends can also be used to immediately spot potential anomalies, to do efficient forecasting and, more generally, to dramatically simplify further data processing. Our experimental evaluation and case studies show that SPIRIT can incrementally capture correlations and discover trends, efficiently and effectively.|Spiros Papadimitriou,Jimeng Sun,Christos Faloutsos","80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Sch√§r,Wolfgang Lehner","80562|VLDB|2005|Tree-Pattern Queries on a Lightweight XML Processor|Popular XML languages, like XPath, use \"tree-pattern\" queries to select nodes based on their structural characteristics. While many processing methods have already been proposed for such queries, none of them has found its way to any of the existing \"lightweight\" XML engines (i.e. engines without optimization modules). The main reason is the lack of a systematic comparison of query methods under a common storage model. In this work, we aim to fill this gap and answer two important questions what the relative similarities and important differences among the tree-pattern query methods are, and if there is a prominent method among them in terms of effectiveness and robustness that an XML processor should support. For the first question, we propose a novel classification of the methods according to their matching process. We then describe a common storage model and demonstrate that the access pattern of each class conforms or can be adapted to conform to this model. Finally, we perform an experimental evaluation to compare their relative performance. Based on the evaluation results, we conclude that the family of holistic processing methods, which provides performance guarantees, is the most robust alternative for such an environment.|Mirella Moura Moro,Zografoula Vagena,Vassilis J. Tsotras","80554|VLDB|2005|Query Caching and View Selection for XML Databases|In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath queryview answerability, which allows us to reduce tree operations to string operations for matching a queryview pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.|Bhushan Mandhani,Dan Suciu","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80585|VLDB|2005|An Efficient and Versatile Query Engine for TopX Search|This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in ) the need to consider scores for XML elements while aggregating them at the document level, ) the combination of vague content conditions with XML path conditions, ) the need to relax query conditions if too few results satisfy all conditions, and ) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.|Martin Theobald,Ralf Schenkel,Gerhard Weikum","80553|VLDB|2005|From Region Encoding To Extended Dewey On Efficient Processing of XML Twig Pattern Matching|Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.|Jiaheng Lu,Tok Wang Ling,Chee Yong Chan,Ting Chen","80519|VLDB|2005|Database Change Notifications Primitives for Efficient Database Query Result Caching|Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier , . The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.|C√©sar A. Galindo-Legaria,Torsten Grabs,Christian Kleinerman,Florian Waas"],["57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57584|GECCO|2005|MRI magnet design search space analysis EDAs and a real-world problem with significant dependencies|This paper introduces the design of superconductive magnet configurations in Magnetic Resonance Imaging (MRI) systems as a challenging real-world problem for Evolutionary Algorithms (EAs). Analysis of the problem structure is conducted using a general statistical method, which could be easily applied to other problems. The results suggest that the problem is highly multimodal and likely to present a significant challenge for many algorithms. Through a series of preliminary experiments, a continuous Estimation of Distribution Algorithm (EDA) is shown to be able to generate promising designs with a small computational effort. The importance of utilizing problem-specific knowledge and the ability of an algorithm to capture dependencies in solving complex real-world problems is also highlighted.|Bo Yuan,Marcus Gallagher,Stuart Crozier","16089|IJCAI|2005|Phase Transitions of Dominating Clique Problem and Their Implications to Heuristics in Satisfiability Search|We study a monotone NP decision problem, the dominating clique problem, whose phase transition occurs at a very dense stage of the random graph evolution process. We establish the exact threshold of the phase transition and propose an efficient search algorithm that runs in super-polynomial time with high probability. Our empirical studies reveal two even more intriguing phenomena in its typical-case complexity () the problem is \"uniformly hard\" with a tiny runtime variance on negative instances. () Our algorithm and its CNF-tailored implementation, outperform several SAT solvers by a huge margin on dominating cliques and some other SAT problems with similar structures.|Joseph C. Culberson,Yong Gao,Calin Anton","16345|IJCAI|2005|A Novel Local Search Algorithm for the Traveling Salesman Problem that Exploits Backbones|We present and investigate a new method for the Traveling Salesman Problem (TSP) that incorporates backbone information into the well known and widely applied Lin-Kernighan (LK) local search family of algorithms for the problem. We consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the LK algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. We present extensive experimental results, using large instances from the TSP Challenge suite and real-world instances in TSPLIB, showing the significant improvement that the new method can provide over the original algorithms.|Weixiong Zhang,Moshe Looks","16229|IJCAI|2005|Combination of Local Search Strategies for Rotating Workforce Scheduling Problem|Rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work places (e.g. industrial plants). Solving this problem is of a high practical relevance. In this paper we propose the combination of tabu search with random walk and min conflicts strategy to solve this problem. Computational results for benchmark examples in literature and other real life examples show that combination of tabu search with random walk and min conflicts strategy improves the performance of tabu search for this problem. The methods proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedules.|Nysret Musliu","57408|GECCO|2005|New topologies for genetic search space|We propose three distance measures for genetic search space. One is a distance measure in the population space that is useful for understanding the working mechanism of genetic algorithms. Another is a distance measure in the solution space for K-grouping problems. This can be used for normalization in crossover. The third is a level distance measure for genetic algorithms, which is useful for measuring problem difficulty with respect to genetic algorithms. We show that the proposed measures are metrics and the measures are efficiently computed.|Yong-Hyuk Kim,Byung Ro Moon","57546|GECCO|2005|Applying metaheuristic techniques to search the space of bidding strategies in combinatorial auctions|Many non-cooperative settings that could potentially be studied using game theory are characterized by having very large strategy spaces and payoffs that are costly to compute. Best response dynamics is a method of searching for pure-strategy equilibria in games that is attractive for its simplicity and scalability (relative to more analytical approaches). However, when the cost of determining the outcome of a particular set of joint strategies is high, it is impractical to compute the payoffs of all possible responses to the other players actions. Thus, we study metaheuristic approaches--genetic algorithms and tabu search in particular--to explore the strategy space. We configure the parameters of metaheuristics to adapt to the problem of finding the best response strategy and present how it can be helpful in finding Nash equilibria of combinatorial auctions which is an important solution concept in game theory.|Ashish Sureka,Peter R. Wurman","57267|GECCO|2005|Search space modulation in genetic algorithms evolving the search space by sinusoidal transformations|An experimental form of Modulation (Reinterpretation) of the Search Space is presented. This modulation is developed as a mathematical method that can be implemented directly into existing evolutionary algorithms without writing special operators, changing the program loop etc. The main mathematical principle behind this method is the dynamic sinusoidal envelope of the search space. This method is presented in order to solve some theoretical and practical issues in evolutionary algorithms like numerical bounded variables, dynamic focalized search, dynamic control of diversity, feasible region analysis etc.|Jos√© Antonio Martin H.","16110|IJCAI|2005|Multi-agent Coordination using Local Search|We consider the problem of coordinating the behavior of multiple self-interested agents. It involves constraint optimization problems that often can only be solved by local search algorithms. Using local search poses problems of incentivecompatibility and individual rationality. We thus define a weaker notion of bounded-rational incentive-compatibility where manipulation is made impossible with high probability through computational complexity. We observe that in real life, manipulation of complex situations is often impossible because the effect of the manipulation cannot be predicted with sufficient accuracy. We show how randomization schemes in local search can make predicting its outcome hard and thus form a bounded-rational incentive-compatible coordination algorithm.|Boi Faltings,Quang Huy Nguyen 0003","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o"],["16253|IJCAI|2005|ROCCER An Algorithm for Rule Learning Based on ROC Analysis|We introduce a rule selection algorithm called ROCCER, which operates by selecting classification rules from a larger set of rules - for instance found by Apriori - using ROC analysis. Experimental comparison with rule induction algorithms shows that ROCCER tends to produce considerably smaller rule sets with compatible Area Under the ROC Curve (AUC) values. The individual rules that compose the rule set also have higher support and stronger association indexes.|Ronaldo C. Prati,Peter A. Flach","57405|GECCO|2005|Theoretical analysis of a mutation-based evolutionary algorithm for a tracking problem in the lattice|Evolutionary algorithms are often applied for solving optimization problems that are too complex or different from classical problems so that the application of classical methods is difficult. One example are dynamic problems that change with time. An important class of dynamic problems is the class of tracking problems where an algorithm has to find an approximately optimal solution and insure an almost constant quality in spite of the changing problem. For the application of evolutionary algorithms to static optimization problems, the distribution of the optimization time and most often its expected value are most important. Adopting this perspective a simple tracking problem in the lattice is considered and the performance of a mutation-based evolutionary algorithm is evaluated. For the static case, asymptotically tight upper and lower bounds are proven. These results are applied to derive results on the tracking performance for different rates of change.|Thomas Jansen,Ulf Schellbach","57315|GECCO|2005|Quality-time analysis of multi-objective evolutionary algorithms|A quality-time analysis of multi-objective evolutionary algorithms (MOEAs) based on schema theorem and building blocks hypothesis is developed. A bicriteria OneMax problem, a hypothesis of niche and species, and a definition of dissimilar schemata are introduced for the analysis. In this paper, the convergence time, the first and last hitting time models are constructed for analyzing the performance of MOEAs. Population sizing model is constructed for determining appropriate population sizes. The models are verified using the bicriteria OneMax problem. The theoretical results indicate how the convergence time and population size of a MOEA scale up with the problem size, the dissimilarity of Pareto-optimal solutions, and the number of Pareto-optimal solutions of a multi-objective optimization problem.|Jian-Hung Chen,Shinn-Ying Ho,David E. Goldberg","57369|GECCO|2005|Statistical analysis of heuristics for evolving sorting networks|Designing efficient sorting networks has been a challenging combinatorial optimization problem since the early 's. The application of evolutionary computing to this problem has yielded human-competitive results in recent years. We build on previous work by presenting a genetic algorithm whose parameters and heuristics are tuned on a small instance of the problem, and then scaled up to larger instances. Also presented are positive and negative results regarding the efficacy of several domain-specific heuristics.|Lee K. Graham,Hassan Masum,Franz Oppacher","57335|GECCO|2005|Analysis and mathematical justification of a fitness function used in an intrusion detection system|Convergence to correct solutions in Genetic Algorithms depends largely on the fitness function. A fitness function that captures all goals and constraints can be difficult to find. This paper gives a mathematical justification for a fitness function that has previously been demonstrated experimentally to be effective.|Pedro A. Diaz-Gomez,Dean F. Hougen","57400|GECCO|2005|Rigorous runtime analysis of a ES for the sphere function|Evolutionary algorithms (EAs) are general, randomized search heuristics applied successfully to optimization problems both in discrete and in continuous search spaces. In recent years, substantial progress has been made in theoretical runtime analysis of EAs, in particular for pseudo-Boolean fitness functions f(,)n  R. Compared to this, little is known about the runtime of simple and, in particular, more complex EAs for continuous functions f Rn  R.In this paper, a first rigorous runtime analysis of a population-based EA in continuous search spaces is presented. A simple (+) evolution strategy ((+)ES) that uses Gaussian mutations adapted by the -rule as its search operator is studied on the well-known Sphere functionand the influence of  and n on its runtime is examined. By generalizing the proof technique of randomized family trees, developed before w.r.t. discrete search spaces, asymptotically upper and lower bounds on the time for the population to make a predefined progress are derived. Furthermore, the utility of the -rule in population-based evolution strategies is shown. Finally, the behavior of the (+)ES on multimodal functions is discussed.|Jens J√§gersk√ºpper,Carsten Witt","16138|IJCAI|2005|fMRI Analysis via One-class Machine Learning Techniques|We show how one-class compression Neural Networks and one-class SVM can be applied to fMRI data to learn the classification of brain activity associated with a specific motor activity. For comparison purposes, we use two labeled data and see what degree of classification ability is lost compared with the usual two-class SVM.|David R. Hardoon,Larry M. Manevitz","57293|GECCO|2005|Evolving computer intrusion scripts for vulnerability assessment and log analysis|Evolutionary computation is used to construct undetectable computer attack scripts. Using a simulated operating system, we show that scripts can be evolved to cover their tracks and become difficult to detect from log file analysis.|Julien Budynek,Eric Bonabeau,Ben Shargel","57373|GECCO|2005|On the analysis of the approximation capability of simple evolutionary algorithms for scheduling problems|Two of the major difficulties dealing with real-world problems nowadays are their increasing complexity and the decreasing available timespan to create \"acceptable\" solutions. Due to this and the strongly decreasing costs of CPU-power, non specialized (random) search heuristics gain more and more importance. In this paper we analyze the behavior of two very simple search heuristics on a strongly NP-hard scheduling problem. Although both find feasible solutions in pseudo-polynomial time, at least one of them is not able to present an (+)-approximation for arbitrary  with constant probability. Despite this, one of the two presented search heuristics can even compete with a problem-specific algorithm on a certain class of inputs and deliver solutions convergent to optimality for increasing problem size.|Christian Gunia","57435|GECCO|2005|The compact classifier system motivation analysis and first results|This paper presents an initial analysis of how maximally general and accurate rules can be evolved in a Pittsburgh-style classifier system. In order to be able to perform such analysis we introduce a simple bare-bones Pittsburgh classifier systems---the compact classifier system (CCS)---based on estimation of distribution algorithms. Using a common rule encoding scheme of Pittsburgh classifier systems, CCS maintains a dynamic set of probability vectors that compactly describe a rule set. The compact genetic algorithm is used to evolve each of the initially perturbed probability vectors which represents the rules. Results show how CCS is able to evolve in a compact, simple, and elegant manner rule sets composed by maximally general and accurate rules.|Xavier Llor√†,Kumara Sastry,David E. Goldberg"],["57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","57434|GECCO|2005|A differential evolution based incremental training method for RBF networks|The Differential Evolution (DE) is a floating-point encoded evolutionary strategy for global optimization. It has been demonstrated to be an efficient, effective, and robust optimization method, especially for problems containing continuous variables. This paper concerns applying a DE-based algorithm to training Radial Basis Function (RBF) networks with variables including centres, weights, and widths of RBFs. The proposed algorithm consists of three steps the first step is the initial tuning, which focuses on searching for the center, weight, and width of a one-node RBF network, the second step is the local tuning, which optimizes the three variables of the one-node RBF network --- its centre, weight, and width, and the third step is the global tuning, which optimizes all the parameters of the whole network together. The second step and the third step both use the cycling scheme to find the parameters of RBF network. The Mean Square Error from the desired to actual outputs is applied as the objective function to be minimized. Training the networks is demonstrated by approximating a set of functions, using different strategies of DE. A comparison of the net performances with several approaches reported in the literature is given and shows the resulting network performs better in the tested functions. The results show that proposed method improves the compared approximation results.|Junhong Liu,Jouni Lampinen","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","16071|IJCAI|2005|Sensitivity Analysis in Markov Networks|This paper explores the topic of sensitivity analysis in Markov networks, by tackling questions similar to those arising in the context of Bayesian networks the tuning of parameters to satisfy query constraints, and the bounding of query changes when perturbing network parameters. Even though the distribution induced by a Markov network corresponds to ratios of multi-linear functions, whereas the distribution induced by a Bayesian network corresponds to multi-linear functions, the results we obtain for Markov networks are as effective computationally as those obtained for Bayesian networks. This similarity is due to the fact that conditional probabilities have the same functional form in both Bayesian and Markov networks, which turns out to be the more influential factor. The major difference we found, however, is in how changes in parameter values should be quantified, as such parameters are interpreted differently in Bayesian networks and Markov networks.|Hei Chan,Adnan Darwiche","16231|IJCAI|2005|Generalization Error of Linear Neural Networks in an Empirical Bayes Approach|It is well known that in unidentifiable models, the Bayes estimation has the advantage of generalization performance to the maximum likelihood estimation. However, accurate approximation of the posterior distribution requires huge computational costs. In this paper, we consider an empirical Bayes approach where a part of the parameters are regarded as hyperparameters, which we call a subspace Bayes approach, and theoretically analyze the generalization error of three-layer linear neural networks. We show that a subspace Bayes approach is asymptotically equivalent to a positivepart James-Stein type shrinkage estimation, and behaves similarly to the Bayes estimation in typical cases.|Shinichi Nakajima,Sumio Watanabe","16206|IJCAI|2005|Location-Based Activity Recognition using Relational Markov Networks|In this paper we define a general framework for activity recognition by building upon and extending Relational Markov Networks. Using the example of activity recognition from location data, we show that our model can represent a variety of features including temporal information such as time of day, spatial information extracted from geographic databases, and global constraints such as the number of homes or workplaces of a person. We develop an efficient inference and learning technique based on MCMC. Using GPS location data collected by multiple people we show that the technique can accurately label a person's activity locations. Furthermore, we show that it is possible to learn good models from less data by using priors extracted from other people's data.|Lin Liao,Dieter Fox,Henry A. Kautz","80596|VLDB|2005|AReNA Adaptive Distributed Catalog Infrastructure Based On Relevance Networks|Wide area applications (WAAs) utilize a WAN infrastructure (e.g., the Internet) to connect a federation of hundreds of servers with tens of thousands of clients. Earlier generations of WAA relied on Web accessible sources and the http protocol for data delivery. Recent developments such as the PlanetLab  testbed is now demonstrating an emerging class of data- and compute- intensive wide area applications.|Vladimir Zadorozhny,Avigdor Gal,Louiqa Raschid,Qiang Ye","16220|IJCAI|2005|Training without data Knowledge Insertion into RBF Neural Networks|A major problem when developing neural networks or machine diagnostics situations is that no data or very little data is available for training on fault conditions. However, the domain expert often has a good idea of what to expect in terms of input and output parameter values. If the expert can express these relationships in the form of rules, this would provide a resource too valuable to ignore. Fuzzy logic is used to handle the imprecision and vagueness of natural language and provides this additional advantage to a system. This paper investigates the development of a novel knowledge insertion algorithm that explores the benefits of prestructuring RBF neural networks by using prior fuzzy domain knowledge and previous training experiences. Pre-structuring is accomplished by using fuzzy rules gained from a domain expert and using them to modify existing Radial Basis Function (RBF) networks. The benefits and novel achievements of this work enable RBF neural networks to be trained without actual data but to rely on input to output mappings defined through expert knowledge.|Kenneth McGarry,Stefan Wermter","16043|IJCAI|2005|Analysis and Verification of Qualitative Models of Genetic Regulatory Networks A Model-Checking Approach|Methods developed for the qualitative simulation of dynamical systems have turned out to be powerful tools for studying genetic regulatory networks. A bottleneck in the application of these methods is the analysis of the simulation results. In this paper, we propose a combination of qualitative simulation and model-checking techniques to perform this task systematically and efficiently. We apply our approach to the analysis of the complex network controlling the nutritional stress response in the bacterium Escherichia coli.|Gr√©gory Batt,Delphine Ropers,Hidde de Jong,Johannes Geiselmann,Radu Mateescu,Michel Page,Dominique Schneider","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee"],["57281|GECCO|2005|Optimization with constraints using a cultured differential evolution approach|In this paper we propose a cultural algorithm, where different knowledge sources modify the variation operator of a differential evolution algorithm. Differential evolution is used as a basis for the population, variation and selection processes. The experiments performed show that the cultured differential evolution is able to reduce the number of fitness function evaluations needed to obtain a good aproximation of the optimum value in constrained real-parameter optimization. Comparisons are provided with respect to three techniques that are representative of the state-of-the-art in the area.|Ricardo Landa Becerra,Carlos A. Coello Coello","57434|GECCO|2005|A differential evolution based incremental training method for RBF networks|The Differential Evolution (DE) is a floating-point encoded evolutionary strategy for global optimization. It has been demonstrated to be an efficient, effective, and robust optimization method, especially for problems containing continuous variables. This paper concerns applying a DE-based algorithm to training Radial Basis Function (RBF) networks with variables including centres, weights, and widths of RBFs. The proposed algorithm consists of three steps the first step is the initial tuning, which focuses on searching for the center, weight, and width of a one-node RBF network, the second step is the local tuning, which optimizes the three variables of the one-node RBF network --- its centre, weight, and width, and the third step is the global tuning, which optimizes all the parameters of the whole network together. The second step and the third step both use the cycling scheme to find the parameters of RBF network. The Mean Square Error from the desired to actual outputs is applied as the objective function to be minimized. Training the networks is demonstrated by approximating a set of functions, using different strategies of DE. A comparison of the net performances with several approaches reported in the literature is given and shows the resulting network performs better in the tested functions. The results show that proposed method improves the compared approximation results.|Junhong Liu,Jouni Lampinen","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","57555|GECCO|2005|Using evolutionary computation methods to support analytical models for the evolution and maintenance of conditional strategies in |Biologists have developed models to explain why different environmentally induced morphs of the same organism exist over time. Such conditional strategies are a common form of adaptation to variable environments, whereby an environmental cue allows some individuals to respond to the cue and develop into a morph that is different from the morph of individuals that do not receive the cue. Recently, these efforts have resulted in two different analytical models that give somewhat different predictions. Here we apply evolutionary computation methods to test the two analytical models. The results bear a remarkable similarity to the results of one of the two analytical models. The paper that follows presents the details of a biological application involving snails and barnacles (that occur naturally in two different morphs), moving then to an explanation of two competing mathematical models of the application. Finally, the interdisciplinary paper, which coordinates three separate research projects of a biologist, a mathematician and a computer scientist, describes the evolutionary computation methods used to support one of the two competing analytical models.|Gloria Childress Townsend,Wade N. Hazel,Rick Smock","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,J√∂rn Mehnen,Thomas Michelitsch","57462|GECCO|2005|Inference of gene regulatory networks using s-system and differential evolution|In this work we present an improved evolutionary method for inferring S-system model of genetic networks from the time series data of gene expression. We employed Differential Evolution (DE) for optimizing the network parameters to capture the dynamics in gene expression data. In a preliminary investigation we ascertain the suitability of DE for a multimodal and strongly non-linear problem like gene network estimation. An extension of the fitness function for attaining the sparse structure of biological networks has been proposed. For estimating the parameter values more accurately an enhancement of the optimization procedure has been also suggested. The effectiveness of the proposed method was justified performing experiments on a genetic network using different numbers of artificially created time series data.|Nasimul Noman,Hitoshi Iba","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","16323|IJCAI|2005|Meaning development versus predefined meanings in language evolution models|This paper investigates the effect of predefining semantics in modelling the evolution of compositional languages versus allowing agents to develop these semantics in parallel with the development of language. The study is done using a multi-agent model of language evolution that is based on the Talking Heads experiment. The experiments show that when allowing a co-evolution of semantics with language, compositional languages develop faster than when the semantics are predefined, but compositionality appears more stable in the latter case. The paper concludes that conclusions drawn from simulations with predefined meanings, which most studies use, may need revision.|Paul Vogt","57399|GECCO|2005|Efficient differential evolution using speciation for multimodal function optimization|In this paper differential evolution is extended by using the notion of speciation for solving multimodal optimization problems. The proposed species-based DE (SDE) is able to locate multiple global optima simultaneously through adaptive formation of multiple species (or subpopulations) in an DE population at each iteration step. Each species functions as an DE by itself. Successive local improvements through species formation can eventually transform into global improvements in identifying multiple global optima. In this study the performance of SDE is compared with another recently proposed DE variant CrowdingDE. The computational complexity of SDE, the effect of population size and species radius on SDE are investigated. SDE is found to be more computationally efficient than CrowdingDE over a number of benchmark multimodal test functions.|Xiaodong Li","57422|GECCO|2005|Evolutionary algorithms for the self-organized evolution of networks|While the evolution of biological networks can be modeled sensefully as a series of mutation and selection, evolution of other networks such as the social network in a city or the network of streets in a country is not determined by selection since there is no alternative network with which these singular networks have to compete. Nonetheless, these singular networks do evolve due to dynamic changes of vertices and edges. In this article we present a formal, analyzable framework for the evolution of singular networks. We show that the careful design of adaptation rules can lead to the emergence of network topologies with satisfying performance in polynomial time while other adaptation rules yield exponential runtime. We further show by example how the framework could be applied to some ad-hoc communication scenarios.|Katharina Anna Lehmann,Michael Kaufmann"],["16161|IJCAI|2005|State Abstraction Discovery from Irrelevant State Variables|Abstraction is a powerful form of domain knowledge that allows reinforcement-learning agents to cope with complex environments, but in most cases a human must supply this knowledge. In the absence of such prior knowledge or a given model, we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure. To this end, we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior, and we exhibit statistical methods that detect this condition robustly. Finally, we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error.|Nicholas K. Jong,Peter Stone","16059|IJCAI|2005|Reconstructing an Agents Epistemic State from Observations|We look at the problem in belief revision of trying to make inferences about what an agent believed - or will believe - at a given moment, based on an observation of how the agent has responded to some sequence of previous belief revision inputs over time. We adopt a \"reverse engineering\" approach to this problem. Assuming a framework for iterated belief revision which is based on sequences, we construct a model of the agent that \"best explains\" the observation. Further considerations on this best-explaining model then allow inferences about the agent's epistemic behaviour to be made. We also provide an algorithm which computes this best explanation.|Richard Booth,Alexander Nittka","57280|GECCO|2005|The impact of cellular representation on finite state agents for prisoners dilemma|The iterated prisoner's dilemma is a widely used computational model of cooperation and conflict. Many studies report emergent cooperation in populations of agents trained to play prisoner's dilemma with an evolutionary algorithm. Cellular representation is the practice of evolving a set of instructions for constructing a desired structure. This paper presents a cellular encoding for finite state automata and specializes it to play the iterated prisoner's dilemma. The impact on the character and behavior of finite state agents that results from using the cellular representation is investigated. For the cellular representation presented a statistically significant drop in the level of cooperation is found. Other differences in the character of the automaton generated with a direct and cellular representation are reported. This paper forms part of an ongoing study of the impact of representation on evolved agents for playing prisoner's dilemma.|Daniel A. Ashlock,Eun-Youn Kim","16026|IJCAI|2005|Exploiting Background Knowledge for Knowledge-Intensive Subgroup Discovery|In general, knowledge-intensive data mining methods exploit background knowledge to improve the quality of their results. Then, in knowledge-rich domains often the interestingness of the mined patterns can be increased significantly. In this paper we categorize several classes of background knowledge for subgroup discovery, and present how the necessary knowledge elements can be modelled. Furthermore, we show how subgroup discovery methods benefit from the utilization of background knowledge, and discuss its application in an incremental process-model. The context of our work is to identify interesting diagnostic patterns to supplement a medical documentation and consultation system. We provide a case study in the medical domain, using a case base from a realworld application.|Martin Atzm√ºller,Frank Puppe,Hans-Peter Buscher","57457|GECCO|2005|Genetic network programming with automatically defined groups for assigning proper roles to multiple agents|In this paper, we apply a Genetic Network Programming (GNP) Architecture using Automatically Defined Groups (ADG) to a multi-agent problem where cooperation of agents are required. GNP is a kind of evolutionary methods inspired from Genetic Programming (GP). While GP has a tree architecture, GNP has a network architecture with which an agent works in the virtual world. In GNP with ADG, each agent is assigned to a group according to its role to complete some task of a cooperative problem. We consider two types of problems in this paper one problem is to assign an appropriate role to each agent according to its ability, and the other is to assign a proper role to each agent with the same ability. While the first problem has the specific conditions as for the ability of an agent, the latter is a general problem. We show the effectiveness of GNP with ADG through computer simulations on the two types of load transportation problems.|Tadahiko Murata,Takashi Nakamura","57509|GECCO|2005|Evolving visually guided agents in an ambiguous virtual world|The fundamental challenge faced by any visual system within natural environments is the ambiguity caused by the fact that light that falls on the system's sensors conflates multiple attributes of the physical world. Understanding the computational principles by which natural systems overcome this challenge and generate useful behaviour remains the key objective in neuroscience and machine vision research. In this paper we introduce Mosaic World, an artificial life model that maintains the essential characteristics of natural visual ecologies, and which is populated by virtual agents that - through 'natural' selection - come to resolve stimulus ambiguity by adapting the functional structure of their visual networks according to the statistical structure of their ecological experience. Mosaic World therefore presents us with an important tool for exploring the computational principles by which vision can overcome stimulus ambiguity and usefully guide behaviour.|Ehud Schlessinger,Peter J. Bentley,R. Beau Lotto","16258|IJCAI|2005|PsychSim Modeling Theory of Mind with Decision-Theoretic Agents|Agent-based modeling of human social behavior is an increasingly important research area. A key factor in human social interaction is our beliefs about others, a theory of mind. Whether we believe a message depends not only on its content but also on our model of the communicator. How we act depends not only on the immediate effect but also on how we believe others will react. In this paper, we discuss PsychSim, an implemented multiagent-based simulation tool for modeling interactions and influence. While typical approaches to such modeling have used first-order logic, Psych-Sim agents have their own decision-theoretic model of the world, including beliefs about its environment and recursive models of other agents. Using these quantitative models of uncertainty and preferences, we have translated existing psychological theories into a decision-theoretic semantics that allow the agents to reason about degrees of believability in a novel way. We discuss PsychSim's underlying architecture and describe its application to a school violence scenario for illustration.|David V. Pynadath,Stacy Marsella","16078|IJCAI|2005|Attribution of Knowledge to Artificial Agents and their Principals|We consider the problem of attribution of knowledge to artificial agents and their legal principals. When can we say that an artificial agent X knows p and that its principal can be attributed the knowledge of p We offer a pragmatic analysis of knowledge attribution and apply it to the legal theory of artificial agents and their principals.|Samir Chopra,Laurence White","16219|IJCAI|2005|Topic and Role Discovery in Social Networks|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the direction-sensitive messages sent between entities. The model builds on Latent Dirichlet Allocation (LDA) and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient--steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher's email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people's roles.|Andrew McCallum,Andr√©s Corrada-Emmanuel,Xuerui Wang","16222|IJCAI|2005|Automating the Discovery of Recommendation Knowledge|In case-based reasoning (CBR) systems for product recommendation, the retrieval of acceptable products based on limited information is an important and challenging problem. As we show in this paper, basic retrieval strategies such as nearest neighbor are potentially unreliable when applied to incomplete queries. To address this issue, we present techniques for automating the discovery of recommendation rules that are provably reliable and non-conflicting while requiring minimal information for their application in a rule-based approach to the retrieval of recommended cases.|David McSherry,Christopher Stretch"],["16272|IJCAI|2005|Bounded Search and Symbolic Inference for Constraint Optimization|Constraint optimization underlies many problems in AI. We present a novel algorithm for finite domain constraint optimization that generalizes branch-and-bound search by reasoning about sets of assignments rather than individual assignments. Because in many practical cases, sets of assignments can be represented implicitly and compactly using symbolic techniques such as decision diagrams, the set-based algorithm can compute bounds faster than explicitly searching over individual assignments, while memory explosion can be avoided by limiting the size of the sets. Varying the size of the sets yields a family of algorithms that includes known search and inference algorithms as special cases. Furthermore, experiments on random problems indicate that the approach can lead to significant performance improvements.|Martin Sachenbacher,Brian C. Williams","57388|GECCO|2005|On the complexity of hierarchical problem solving|Competent Genetic Algorithms can efficiently address problems in which the linkage between variables is limited to a small order k. Problems with higher order dependencies can only be addressed efficiently if further problem properties exist that can be exploited. An important class of problems for which this occurs is that of hierarchical problems. Hierarchical problems can contain dependencies between all variables (kn) while being solvable in polynomial time.An open question so far is what precise properties a hierarchical problem must possess in order to be solvable efficiently. We study this question by investigating several features of hierarchical problems and determining their effect on computational complexity, both analytically and empirically. The analyses are based on the Hierarchical Genetic Algorithm (HGA), which is developed as part of this work. The HGA is tested on ranges of hierarchical problems, produced by a generator for hierarchical problems.|Edwin D. de Jong,Richard A. Watson,Dirk Thierens","16119|IJCAI|2005|The Rules of Constraint Modelling|Many and diverse combinatorial problems have been solved successfully using finite-domain constraint programming. However, to apply constraint programming to a particular domain, the problem must first be modelled as a constraint satisfaction or optimisation problem. Since constraints provide a rich language, typically many alternative models exist. Formulating a good model therefore requires a great deal of expertise. This paper describes CONJURE, a system that refines a specification of a problem in the abstract constraint specification language ESSENCE into a set of alternative constraint models. Refinement is compositional alternative constraint models are generated by composing refinements of the components of the specification. Experimental results demonstrate that CONJURE is able to generate a variety of models for practical problems from their ESSENCE specifications.|Alan M. Frisch,Christopher Jefferson,Bernadette Mart√≠nez Hern√°ndez,Ian Miguel","16133|IJCAI|2005|The Complexity of Quantified Constraint Satisfaction Problems under Structural Restrictions|We give a clear picture of the tractabilityintractability frontier for quantified constraint satisfaction problems (QCSPs) under structural restrictions. On the negative side, we prove that checking QCSP satisfiability remains PSPACE-hard for all known structural properties more general than bounded treewidth and for the incomparable hypergraph acyclicity. Moreover, if the domain is not fixed, the problem is PSPACE-hard even for tree-shaped constraint scopes. On the positive side, we identify relevant tractable classes, including QCSPs with prefix  having bounded hypertree width, and QCSPs with a bounded number of guards. The latter are solvable in polynomial time without any bound on domains or quantifier alternations.|Georg Gottlob,Gianluigi Greco,Francesco Scarcello","16083|IJCAI|2005|A Unified Theory of Structural Tractability for Constraint Satisfaction and Spread Cut Decomposition|In this paper we introduce a generic form of structural decomposition for the constraint satisfaction problem, which we call a guarded decomposition. We show that many existing decomposition methods can be characterized in terms of finding guarded decompositions satisfying certain specified additional conditions. Using the guarded decomposition framework we are also able to define a new form of decomposition, which we call a spread cut. We show that discovery of width k spread-cut decompositions is tractable for each k, and that the spread cut decomposition strongly generalize all existing decompositions except hypertrees. Finally we exhibit a family of hypergraphs Hn, for n  , ,  ..., where the width of the best hypertree decomposition of each Hn is at least n, but the width of the best spreadcut decomposition is at most n.|David A. Cohen,Peter Jeavons,Marc Gyssens","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","16153|IJCAI|2005|Optimal Refutations for Constraint Satisfaction Problems|Variable ordering heuristics have long been an important component of constraint satisfaction search algorithms. In this paper we study the behaviour of standard variable ordering heuristics when searching an insoluble (sub)problem. We employ the notion of an optimal refutation of an insoluble (sub)problem and describe an algorithm for obtaining it. We propose a novel approach to empirically looking at problem hardness and typical-case complexity by comparing optimal refutations with those generated by standard search heuristics. It is clear from our analysis that the standard variable orderings used to solve CSPs behave very differently on real-world problems than on random problems of comparable size. Our work introduces a potentially useful tool for analysing the causes of the heavy-tailed phenomenon observed in the runtime distributions of backtrack search procedures.|Tudor Hulubei,Barry O'Sullivan","16130|IJCAI|2005|QCSP-Solve A Solver for Quantified Constraint Satisfaction Problems|The Quantified Constraint Satisfaction Problem (QCSP) is a generalization of the CSP in which some variables are universally quantified. It has been shown that a solver based on an encoding of QCSP into QBF can outperform the existing direct QCSP approaches by several orders of magnitude. In this paper we introduce an efficient QCSP solver. We show how knowledge learned from the successful encoding of QCSP into QBF can be utilized to enhance the existing QCSP techniques and speed up search by orders of magnitude. We also show how the performance of the solver can be further enhanced by incorporating advanced look-back techniques such as CBJ and solution-directed pruning. Experiments demonstrate that our solver is several orders of magnitude faster than existing direct approaches to QCSP solving, and significantly outperforms approaches based on encoding QCSPs as QBFs.|Ian P. Gent,Peter Nightingale,Kostas Stergiou","16239|IJCAI|2005|Corrective Explanation for Interactive Constraint Satisfaction|Interactive tasks such as online configuration can be modeled as constraint satisfaction problems. These can be solved interactively by a user assigning values to variables. Explanations for failure in constraint programming tend to focus on conflict. However, what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem-solving process. This paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real-world configuration problem.|Barry O'Sullivan,Barry O'Callaghan,Eugene C. Freuder","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o"]]}}