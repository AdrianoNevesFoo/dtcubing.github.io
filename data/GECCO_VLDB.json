{"abstract":{"entropy":6.373310685122842,"topics":["real world, recent years, immune systems, time series, artificial immune, last years, area research, wide range, mining web, becoming increasingly, last decade, gene regulatory, gene expression, inspired immune, sensor network, software testing, task classification, detection problem, test cases, testing test","data base, data, data systems, data management, database systems, management systems, database, data model, data stream, relational database, query, xml documents, large data, systems, base systems, data information, query processing, relational data, information systems, xml","genetic programming, genetic algorithms, classifier systems, estimation distribution, learning classifier, learning systems, distribution algorithms, estimation edas, distribution edas, building block, neural network, estimation algorithms, spanning tree, presents approach, learning, presents novel, classifier xcs, machine learning, cartesian programming, symbolic regression","evolutionary algorithms, genetic algorithms, particle swarm, optimization problem, optimization algorithms, algorithms, optimization, evolutionary computation, problem, algorithms problem, swarm optimization, evolutionary optimization, particle optimization, particle pso, swarm pso, multi-objective optimization, multi-objective evolutionary, algorithms search, genetic gas, solving problem","real world, data mining, web search, mining web, becoming increasingly, applications problem, applications, rules mining, web pages, mining classification, web, real applications, important applications, important problem, association mining, become increasingly, data classification, challenge data, world applications, web analyses","immune systems, artificial immune, neural network, inspired immune, artificial systems, software testing, testing test, test cases, artificial network, software engineering, analysis tool, anomaly detection, biological immune, test generation, data generation, test structural, biological systems, resource allocation, artificial neural, evolving research","data base, data management, data, data systems, data stream, information systems, data model, data information, data mining, problem data, base systems, stream processing, presents data, stream systems, use data, base management, important data, information, data integration, data web","database systems, management systems, large data, data systems, relational database, database data, database, systems, database applications, database management, access data, large number, design systems, relational systems, architecture data, design database, data operations, large database, database schema, concurrency control","estimation distribution, distribution algorithms, estimation algorithms, estimation edas, distribution edas, algorithms edas, probabilistic model, use algorithms, hierarchical bayesian, bayesian algorithms, univariate marginal, algorithms based, hierarchical hboa, algorithms probabilistic, model building, introduces called, distribution model, mutual dependency, probabilistic genetic, genetic algorithms","classifier systems, learning systems, learning classifier, learning, xcs systems, machine learning, classifier xcs, evolution strategy, covariance matrix, reinforcement learning, learning xcs, learning algorithms, systems lcs, genetic learning, learning lcs, learning problem, classifier lcs, covariance adaptation, evolutionary learning, support vector","local search, algorithms local, multiobjective algorithms, virtual hashing, fitness function, multiobjective optimization, evolutionary algorithms, evolutionary multiobjective, multimodal optimization, fitness landscape, evolutionary search, cooperative coevolutionary, evolutionary fitness, evolutionary solutions, algorithms solutions, global optimization, multiobjective emo, algorithms well, evolutionary local, evolutionary methods","particle swarm, optimization algorithms, swarm optimization, particle optimization, particle pso, swarm pso, optimization pso, evolutionary eas, swarm algorithms, particle algorithms, algorithms eas, widely used, algorithms dynamic, algorithms used, presents swarm, evolution strategies, evolutionary optimization, use optimization, diversity mechanism, optimization"],"ranking":[["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","57356|GECCO|2005|A comparative analysis of artificial immune network models|This paper presents a review of different artificial immune network models, which have been published during the last years. A general model of artificial immune network is presented, which provides a common notation that allows the comparison of different models. A descriptive and comparative analysis is presented emphasizing similarities, differences and relationships between models. Finally, some conclusions and suggestions for improving existent models are presented.|Juan Carlos Galeano,Angélica Veloza-Suan,Fabio A. González","57791|GECCO|2006|An artificial immune system and its integration into an organic middleware for self-protection|Our human body is well protected by antibodies from our biological immune system. This protection system matured over millions of years and has proven its functionality. In our research we are transfering techniques of a biological immune system to a computer based environment in order to design a self-protecting middleware which isn't vulnerable to malicious events. First off this paper proposes an artificial immune system (AIS) and evaluates optimal parameter settings. As the first research we show up the correlation between the size of a system and the length of the receptors used within antibodies for an efficient detection. Further on we describe the integration of the immune system into our organic middleware AMUN and afterwards we propose optimization techniques to minimize the memory space needed for storing the antibodies and to speedup the time needed for detecting malicious objects.|Andreas Pietzowski,Wolfgang Trumler,Theo Ungerer","57536|GECCO|2005|On the contribution of gene libraries to artificial immune systems|Gene libraries have been added to Artificial Immune Systems in analogy to biological immune systems, but to date no careful study of their effect has been made. This work investigates the contribution of gene libraries to Artificial Immune Systems by reproducing and extending an earlier system that used gene libraries. Performance on a job-shop scheduling problem is evaluated empirically with and without gene libraries, and with many different library configurations. We propose that gene libraries encourage diversity in a population of solutions and that the number of components in the gene library parameterises this effect. The number of gene libraries used is found to affect solution fitness and indeed using larger numbers of libraries (and therefore libraries of smaller components) enables higher fitness to be attained. We conclude that gene libraries are likely to be of use in applications where there is a need to maintain the diversity of solutions.|Peter Spellward,Tim Kovacs","57032|GECCO|2003|Artificial Immune System for Classification of Gene Expression Data|DNA microarray experiments generate thousands of gene expression measurement simultaneously. Analyzing the difference of gene expression in cell and tissue samples is useful in diagnosis of disease. This paper presents an Artificial Immune System for classifying microarray-monitored data. The system evolutionarily selects important features and optimizes their weights to derive classification rules. This system was applied to two datasets of cancerous cells and tissues. The primary result found few classification rules which correctly classified all the test samples and gave some interesting implications for feature selection.|Shin Ando,Hitoshi Iba","80515|VLDB|2005|Scaling and Time Warping in Time Series Querying|The last few years have seen an increasing understanding that Dynamic Time Warping (DTW), a technique that allows local flexibility in aligning time series, is superior to the ubiquitous Euclidean Distance for time series classification, clustering, and indexing. More recently, it has been shown that for some problems, Uniform Scaling (US), a technique that allows global scaling of time series, may just be as important for some problems. In this work, we note that for many real world problems, it is necessary to combine both DTW and US to achieve meaningful results. This is particularly true in domains where we must account for the natural variability of human action, including biometrics, query by humming, motion-captureanimation, and handwriting recognition. We introduce the first technique which can handle both DTW and US simultaneously, and demonstrate its utility and effectiveness on a wide range of problems in industry, medicine, and entertainment.|Ada Wai-Chee Fu,Eamonn J. Keogh,Leo Yung Hang Lau,Chotirat (Ann) Ratanamahatana","58354|GECCO|2008|EIN-WUM an AIS-based algorithm for web usage mining|With the ever expanding Web and the information published on it, effective tools for managing such data and presenting information to users based on their needs are becoming necessary. In this paper, we propose a new algorithm named \"EIN-WUM\" for Web usage mining based on artificial immune system metaphor. This algorithm introduces several novelties such as using danger theory, directed mutation and an enhanced immune network model. Experimental results show that The EIN-WUM algorithm can properly learn the frequent trends in noisy, sparse and huge Web usage data in single pass.|Adel Torkaman Rahmani,B. Hoda Helmi","57465|GECCO|2005|The application of antigenic search techniques to time series forecasting|Time series have been a major topic of interest and analysis for hundreds of years, with forecasting a central problem. A large body of analysis techniques has been developed, particularly from methods in statistics and signal processing. Evolutionary techniques have only recently have been applied to time series problems. To date, applications of artificial immune system (AIS) techniques have been in the area of anomaly detection. In this paper we apply AIS techniques to the forecasting problem. We characterize a class of search algorithms we call antigenic search and show their ability to give a good forecast of next elements in series generated from Mackey-Glass and Lorenz equations.|Ian Nunn,Tony White","57070|GECCO|2003|MILA - Multilevel Immune Learning Algorithm|The biological immune system is an intricate network of specialized tissues, organs, cells, and chemical molecules. T-cell-dependent humoral immune response is one of the complex immunological events, involving interaction of B cells with antigens (Ag) and their proliferation, differentiation and subsequent secretion of antibodies (Ab). Inspired by these immunological principles, we proposed a Multilevel Immune Learning Algorithm (MILA) for novel pattern recognition. It incorporates multiple detection schema, clonal expansion and dynamic detector generation mechanisms in a single framework. Different test problems are studied and experimented with MILA for performance evaluation. Preliminary results show that MILA is flexible and efficient in detecting anomalies and novelties in data patterns.|Dipankar Dasgupta,Senhua Yu,Nivedita Sumi Majumdar","58267|GECCO|2008|The gene regulatory network an application to optimal coverage in sensor networks|This paper proposes a new approach for biologically inspired computing on the basis of Gene Regulatory Networks. These networks are models of genes and dynamic interactions that take place between them. The differential equation representations of such networks resemble neural networks as well as idiotypic networks in immune system. Although several potential applications have been outlined, an example, the problem of placing sensors optimally in a distributed environment is considered in detail. A comparison with NSGA-II suggest that the new method is able to accomplish near-optimal coverage of sensors in a network.|Sanjoy Das,Praveen Koduru,Xinye Cai,Stephen Welch,Venkatesh Sarangan"],["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80638|VLDB|2006|iDM A Unified and Versatile Data Model for Personal Dataspace Management|Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML . Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace  of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) , , . This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80166|VLDB|2002|Optimizing View Queries in ROLEX to Support Navigable Result Trees|An increasing number of applications use XML data published from relational databases. For speed and convenience, such applications routinely cache this XML data locally and access it through standard navigational interfaces such as DOM, sacrificing the consistency and integrity guarantees provided by a DBMS for speed. The ROLEX system is being built to extend the capabilities of relational database systems to deliver fast, consistent and navigable XML views of relational data to an application via a virtual DOM interface. This interface translates navigation operations on a DOM tree into execution-plan actions, allowing a spectrum of possibilities for lazy materialization. The ROLEX query optimizer uses a characterization of the navigation behavior of an application, and optimizes view queries to minimize the expected cost of that navigation. This paper presents the architecture of ROLEX, including its model of query execution and the query optimizer. We demonstrate with a performance study the advantages of the ROLEX approach and the importance of optimizing query execution for navigation.|Philip Bohannon,Sumit Ganguly,Henry F. Korth,P. P. S. Narayan,Pradeep Shenoy","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber"],["57501|GECCO|2005|Real-coded crossover as a role of kernel density estimation|This paper presents a kernel density estimation method by means of real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine the sampling strategy by means of a parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and the sampling strategy is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation.|Jun Sakuma,Shigenobu Kobayashi","57750|GECCO|2006|Does overfitting affect performance in estimation of distribution algorithms|Estimation of Distribution Algorithms (EDAs) are a class of evolutionary algorithms that use machine learning techniques to solve optimization problems. Machine learning is used to learn probabilistic models of the selected population. This model is then used to generate next population via sampling. An important phenomenon in machine learning from data is called overfitting. This occurs when the model is overly adapted to the specifics of the training data so well that even noise is encoded. The purpose of this paper is to investigate whether overfitting happens in EDAs, and to discover its consequences. What is found is overfitting does occur in EDAs overfitting correlates to EDAs performance reduction of overfitting using early stopping can improve EDAs performance.|Hao Wu,Jonathan L. Shapiro","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","58744|GECCO|2009|Approximating the search distribution to the selection distribution in EDAs|In an Estimation of Distribution Algorithm (EDA) with an infinite sized population the selection distribution equals the search distribution. For a finite sized population these distributions are different. In practical EDAs the goal of the search distribution learning algorithm is to approximate the selection distribution. The source data is the selected set, which is derived from the population by applying a selection operator. The new approach described here eliminates the explicit use of the selection operator and the selected set. We rewrite for a finite population the selection distribution equations of four selection operators. The new equation is called the empirical selection distribution. Then we show how to build the search distribution that gives the best approximation to the empirical selection distribution. Our approach gives place to practical EDAs which can be easily and directly implemented from well established theoretical results. This paper also shows how common EDAs with discrete and real variables are adapted to take advantage of the empirical selection distribution. A comparison and discussion of performance is presented.|Sergio Ivvan Valdez Peña,Arturo Hernández Aguirre,Salvador Botello Rionda","58548|GECCO|2008|Improved EDNA estimation of dependency networks algorithm using combining function with bivariate probability distributions|One of the key points in Estimation of Distribution Algorithms (EDAs) is the learning of the probabilistic graphical model used to guide the search the richer the model the more complex the learning task. Dependency networks-based EDAs have been recently introduced. On the contrary of Bayesian networks, dependency networks allow the presence of directed cycles in their structure. In a previous work the authors proposed EDNA, an EDA algorithm in which a multivariate dependency network is used but approximating its structure learning by considering only bivariate statistics. EDNA was compared with other models from the literature with the same computational complexity (e.g., univariate and bivariate models). In this work we propose a modified version of EDNA in which not only the structural learning phase is limited to bivariate statistics, but also the simulation and the parameter learning task. Now, we extend the comparison employing multivariate models based on Bayesian networks (EBNA and hBOA). Our experiments show that the modified EDNA is more accurate than the original one, being its accuracy comparable to EBNA and hBOA, but with the advantage of being faster specially in the more complex cases.|José A. Gámez,Juan L. Mateo,Jose Miguel Puerta","57616|GECCO|2006|Studying XCSBOA learning in Boolean functions structure encoding and random Boolean functions|Recently, studies with the XCS classifier system on Boolean functions have shown that in certain types of functions simple crossover operators can lead to disruption and, consequently, a more effective recombination mechanism is required. Simple crossover operators were replaced by recombination based on estimation of distribution algorithms (EDAs). The combination showed that XCS with such a statistics-based crossover operator can solve challenging hierarchical functions more efficiently. This study elaborates the gained competence further investigating the coding scheme for the EDA component (BOA in our case) of XCS as well as performance in randomly generated Boolean function problems. Results in hierarchical Boolean functions show that the originally used -bit coding scheme induces a certain learning bias that stresses additional diversity in the evolving XCS population. A -bit coding scheme as well as a restricted -bit coding scheme confirm the suspected bias. The alternative encodings decrease the unnecessary bias towards specificity and increase performance robustness. The paper concludes with a discussion on the challenges ahead for XCS in Boolean function problems as well as on the implications of the obtained results for real-valued and multiple-valued classification problems, multi-step problems, and function approximation problems.|Martin V. Butz,Martin Pelikan","57186|GECCO|2003|Reinforcement Learning Estimation of Distribution Algorithm|This paper proposes an algorithm for combinatorial optimizations that uses reinforcement learning and estimation of joint probability distribution of promising solutions to generate a new population of solutions. We call it Reinforcement Learning Estimation of Distribution Algorithm (RELEDA). For the estimation of the joint probability distribution we consider each variable as univariate. Then we update the probability of each variable by applying reinforcement learning method. Though we consider variables independent of one another, the proposed method can solve problems of highly correlated variables. To compare the efficiency of our proposed algorithm with other Estimation of Distribution Algorithms (EDAs) we provide the experimental results of the two problems four peaks problem and bipolar function.|Topon Kumar Paul,Hitoshi Iba","58339|GECCO|2008|Introducing MONEDA scalable multiobjective optimization with a neural estimation of distribution algorithm|In this paper we explore the model-building issue of multiobjective optimization estimation of distribution algorithms. We argue that model-building has some characteristics that differentiate it from other machine learning tasks. A novel algorithm called multiobjective neural estimation of distribution algorithm (MONEDA) is proposed to meet those characteristics. This algorithm uses a custom version of the growing neural gas (GNG) network specially meant for the model-building task. As part of this work, MONEDA is assessed with regard to other classical and state-of-the-art evolutionary multiobjective optimizers when solving some community accepted test problems.|Luis Martí,Jesús García,Antonio Berlanga,José Manuel Molina","57435|GECCO|2005|The compact classifier system motivation analysis and first results|This paper presents an initial analysis of how maximally general and accurate rules can be evolved in a Pittsburgh-style classifier system. In order to be able to perform such analysis we introduce a simple bare-bones Pittsburgh classifier systems---the compact classifier system (CCS)---based on estimation of distribution algorithms. Using a common rule encoding scheme of Pittsburgh classifier systems, CCS maintains a dynamic set of probability vectors that compactly describe a rule set. The compact genetic algorithm is used to evolve each of the initially perturbed probability vectors which represents the rules. Results show how CCS is able to evolve in a compact, simple, and elegant manner rule sets composed by maximally general and accurate rules.|Xavier Llorà,Kumara Sastry,David E. Goldberg","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"],["57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","59071|GECCO|2010|An parallel particle swarm optimization approach for multiobjective optimization problems|This paper proposes a parallel particle swarm optimization (PPSO) to solve the multiobjective optimization problems (MOP). PPSO makes the use of the parallel characteristic of the PSO algorithm to deal with the multiple objectives issue of the MOP. PPSO uses as many swarms as the number of the objectives in the MOP and lets each swarm optimize only one of the objectives. These swarms work in parallel and each swarm can use a standard PSO or any other improved PSO variants to solve a single objective problem. PPSO has advantages on the following two aspects. First, as each swarm focus on optimizing only one objective, PPSO can avoid the difficulty of fitness assignment because the particles can be evaluated like in the single objective optimization problem. Second, as different swarms optimize different objectives, PPSO can maintain the population diversity to make a throughout search along the whole Pareto front to obtain nondominated solutions as many as possible. The performance of PPSO is tested on a set of benchmark problems complicated Pareto sets in CEC. The experimental results compared with those obtained by the state-of-the-art algorithms demonstrate the effectiveness and efficiency of PPSO, showing the good performance of PPSO in solving the MOP with complicated Pareto sets.|Zhi-hui Zhan,Jun Zhang","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58967|GECCO|2010|Development of efficient particle swarm optimizers by using concepts from evolutionary algorithms|Particle swarm optimization (PSO) has been in practice for more than  years now and has gained wide popularity in various optimization tasks. In the context to single objective optimization, this paper studies two aspects of PSO (i) its ability to approach an 'optimal basin', and (ii) to find the optimum with high precision once it enters the region. of interest. We test standard PSO algorithms and discover their inability in handling both aspects efficiently. To address these issues with PSO, we propose an evolutionary algorithm (EA) which is algorithmically similar to PSO, and then borrow different EA-specific operators to enhance the PSO's performance. Our final proposed PSO contains a parent-centric recombination operator instead of usual particle update rule, but maintains PSO's individualistic trait and has a demonstrated performance comparable to a well-known GA (and outperforms the GA in some occasions). Moreover, the modified PSO algorithm is found to scale up to solve as large as -variable problems. This study emphasizes the need for similar such studies in establishing an equivalence between various geneticevolutionary and other bio-inspired algorithms, a process that may lead us to better understand the scope and usefulness of various operators associated with each algorithm.|Kalyanmoy Deb,Nikhil Padhye","58731|GECCO|2009|Using a distance metric to guide PSO algorithms for many-objective optimization|In this paper we propose to use a distance metric based on user-preferences to efficiently find solutions for many-objective problems. We use a particle swarm optimization (PSO) algorithm as a baseline to demonstrate the usefulness of this distance metric, though the metric can be used in conjunction with any evolutionary multi-objective (EMO) algorithm. Existing user-preference based EMO algorithms rely on the use of dominance comparisons to explore the search-space. Unfortunately, this is ineffective and computationally expensive for many-objective problems. In the proposed distance metric based PSO, particles update their positions and velocities according to their closeness to preferred regions in the objective-space, as specified by the decision maker. The proposed distance metric allows an EMO algorithm's search to be more effective especially for many-objective problems, and to be more focused on the preferred regions, saving substantial computational cost. We demonstrate how to use a distance metric with two user-preference based PSO algorithms, which implement the reference point and light beam search methods. These algorithms are compared to a user-preference based PSO algorithm relying on the conventional dominance comparisons. Experimental results suggest that the distance metric based algorithms are more effective and efficient especially for difficult many-objective problems.|Upali K. Wickramasinghe,Xiaodong Li","58287|GECCO|2008|Runtime analysis of binary PSO|We investigate the runtime of the Binary Particle Swarm Optimization (PSO) algorithm introduced by Kennedy and Eberhart (). The Binary PSO maintains a global best solution and a swarm of particles. Each particle consists of a current position, an own best position and a velocity vector used in a probabilistic process to update the particle's position. We present lower bounds for swarms of polynomial size. To prove upper bounds, we transfer a fitness-level argument well-established for evolutionary algorithms (EAs) to PSO. This method is applied to estimate the expected runtime on the class of unimodal functions. A simple variant of the Binary PSO is considered in more detail. The -PSO only maintains one particle, hence own best and global best solutions coincide. Despite its simplicity, the -PSO is surprisingly efficient. A detailed analysis for the function OneMax shows that the -PSO is competitive to EAs.|Dirk Sudholt,Carsten Witt","58870|GECCO|2009|An evaporation mechanism for dynamic and noisy multimodal optimization|Dealing with imprecise information is a common characteristic in real-world problems. Specifically, when the source of the information are physical sensors, a level of noise in the evaluation has to be assumed. Particle Swarm Optimization is a technique that presented a good behavior when dealing with noisy fitness functions. Nevertheless, the problem is still open. In this paper we propose the use of the evaporation mechanism for managing with dynamic multi-modal optimization problems that are subject to noisy fitness functions. We will show how the evaporation mechanism does not require the detection of environment changes and how can be used for improving the performance of PSO algorithms working in noisy environments.|Jose Luis Fernandez-Marquez,Josep Lluís Arcos","58697|GECCO|2009|Estimation of particle swarm distribution algorithms bringing together the strengths of PSO and EDAs|This paper presents a framework of estimation of particle swarm distribution algorithms (EPSDAs). The aim lies in effectively combining particle swarm optimization (PSO) with estimation of distribution algorithms (EDAs) without losing on their unique features. To exhibit its practicability, an extended compact particle swarm optimization (EcPSO) is developed along the lines of the suggested framework. Empirical results have adduced grounds for its effectiveness.|Chang Wook Ahn,Hyun-Tae Kim","58990|GECCO|2010|Biogeography-based optimization with blended migration for constrained optimization problems|Biogeography-based optimization (BBO) is a new evolutionary algorithm based on the science of biogeography. We propose two extensions to BBO. First, we propose blended migration. Second, we modify BBO to solve constrained optimization problems. The constrained BBO algorithm is compared with solutions based on a genetic algorithm (GA) and particle swarm optimization (PSO). Numerical results indicate that BBO generally performs better than GA and PSO in handling constrained single-objective optimization problems.|Haiping Ma,Dan Simon"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","80148|VLDB|1998|Scalable Techniques for Mining Causal Structures|Mining for association rules in market basket data has proved a fruitful area of research. Measures such as conditional probability (confidence) and correlation have been used to infer rules of the form &ldquothe existence of item A implies the existence of item B.&rdquo However, such rules indicate only a statistical relationship between A and B. They do not specify the nature of the relationship whether the presence of A causes the presence of B, or the converse, or some other attribute or phenomenon causes both to appear together. In applications, knowing such causal relationships is extremely useful for enhancing understanding and effecting change. While distinguishing causality from correlation is a truly difficult problem, recent work in statistics and Bayesian learning provide some avenues of attack. In these fields, the goal has generally been to learn complete causal models, which are essentially impossible to learn in large-scale data mining applications with a large number of variables.In this paper, we consider the problem of determining casual relationships, instead of mere associations, when mining market basket data. We identify some problems with the direct application of Bayesian learning ideas to mining large databases, concerning both the scalability of algorithms and the appropriateness of the statistical techniques, and introduce some initial ideas for dealing with these problems. We present experimental results from applying our algorithms on several large, real-world data sets. The results indicate that the approach proposed here is both computationally feasible and successful in identifying interesting causal structures. An interesting outcome is that it is perhaps easier to infer the lack of causality than to infer causality, information that is useful in preventing erroneous decision making.|Craig Silverstein,Sergey Brin,Rajeev Motwani,Jeffrey D. Ullman","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","80258|VLDB|2003|Constructing and integrating data-centric Web Applications Methods Tools and Techniques|This tutorial deals with the construction of data-centric Web applications, focusing on the modelling of processes and on the integration with Web services. The tutorial describes the standards, methods, and tools that are commonly used for building these applications.|Stefano Ceri,Ioana Manolescu","80154|VLDB|2001|Self-similarity in the Web|Algorithmic tools for searching and mining the Web are becoming increasingly sophisticated and vital. In this context, algorithms that use and exploit structural information about the Web perform better than generic methods in both efficiency and reliability.We present an extensive characterization of the graph structure of the Web, with a view to enabling high-performance applications that make use of this structure. In particular, we show that the Web emerges as the outcome of a number of essentially independent stochastic processes that evolve at various scales. A striking consequence of this scale invariance is that the structure of the Web is \"fractal\"---cohesive subregions display the same characteristics as the Web at large. An understanding of this underlying fractal nature is therefore applicable to designing data services across multiple domains and scales.We describe potential applications of this line of research to optimized algorithm design for Web-scale data analysis.|Stephen Dill,Ravi Kumar,Kevin S. McCurley,Sridhar Rajagopalan,D. Sivakumar,Andrew Tomkins","80807|VLDB|2007|PP Authority Analysis for Social Communities|PageRank-style authority analyses of Web graphs are of great importance for Web mining. Such authority analyses also apply to hot \"Web .\" applications that exhibit a natural graph structure, such as social networks (e.g., MySpace, Facebook) or tagging communities (e.g., Flickr, Del.icio.us). Finding the most trustworthy or most important authorities in such a community is a pressing need, given the huge scale and also the anonymity of social networks. Computing global authority measures in a Peer-to-Peer (PP) collaboration of autonomous peers is a hot research topic, in particular because of the incomplete local knowledge of the peers, which typically only know about (arbitrarily overlapping) sub-graphs of the complete graph. We demonstrate a self-organizing PP collaboration that, based on the local sub-graphs, efficiently computes global authority scores. In hand with the loosely-coupled spirit of a PP system, the computation is carried out in a completely asynchronous manner without any central knowledge or coordinating instance. We demonstrate the applicability of authority analyses to large-scale distributed systems.|Josiane Xavier Parreira,Sebastian Michel,Matthias Bender,Tom Crecelius,Gerhard Weikum","80437|VLDB|2004|WIC A General-Purpose Algorithm for Monitoring Web Information Sources|The Web is becoming a universal information dissemination medium, due to a number of factors including its support for content dynamicity. A growing number of Web information providers post near real-time updates in domains such as auctions, stock markets, bulletin boards, news, weather, roadway conditions, sports scores, etc. External parties often wish to capture this information for a wide variety of purposes ranging from online data mining to automated synthesis of information from multiple sources. There has been a great deal of work on the design of systems that can process streams of data from Web sources, but little attention has been paid to how to produce these data streams, given that Web pages generally require \"pull-based\" access. In this paper we introduce a new general-purpose algorithm for monitoring Web information sources, effectively converting pull-based sources into push-based ones. Our algorithm can be used in conjunction with continuous query systems that assume information is fed into the query engine in a push-based fashion. Ideally, a Web monitoring algorithm for this purpose should achieve two objectives () timeliness and () completeness of information captured. However, we demonstrate both analytically and empirically using real-world data that these objectives are fundamentally at odds. When resources available for Web monitoring are limited, and the number of sources to monitor is large, it may be necessary to sacrifice some timeliness to achieve better completeness, or vice versa. To take this fact into account, our algorithm is highly parameterized and targets an application-specified balance between timeliness and completeness. In this paper we formalize the problem of optimizing for a flexible combination of timeliness and completeness, and prove that our parameterized algorithm is a - approximation in all cases, and in certain cases is optimal.|Sandeep Pandey,Kedar Dhamdhere,Christopher Olston","58354|GECCO|2008|EIN-WUM an AIS-based algorithm for web usage mining|With the ever expanding Web and the information published on it, effective tools for managing such data and presenting information to users based on their needs are becoming necessary. In this paper, we propose a new algorithm named \"EIN-WUM\" for Web usage mining based on artificial immune system metaphor. This algorithm introduces several novelties such as using danger theory, directed mutation and an enhanced immune network model. Experimental results show that The EIN-WUM algorithm can properly learn the frequent trends in noisy, sparse and huge Web usage data in single pass.|Adel Torkaman Rahmani,B. Hoda Helmi"],["57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","57029|GECCO|2003|Structural and Functional Sequence Test of Dynamic and State-Based Software with Evolutionary Algorithms|Evolutionary Testing (ET) has been shown to be very successful for testing real world applications . The original ET approach focuses on searching for a high coverage of the test object by generating separate inputs for single function calls. We have identified a large set of real world application for which this approach does not perform well because only sequential calls of the tested function can reach a high structural coverage (white box test) or can check functional behavior (black box tests). Especially, control software which is responsible for controlling and constraining a system cannot be tested successfully with ET. Such software is characterized by storing internal data during a sequence of calls. In this paper we present the Evolutionary Sequence Testing approach for white box and black box tests. For automatic sequence testing, a fitness function for the application of ET will be introduced, which allows the optimization of input sequences that reach a high coverage of the software under test. The authors also present a new compact description for the generation of real-world input sequences for functional testing. A set of objective functions to evaluate the test output of systems under test have been developed. These approaches are currently used for the structural and safety testing of car control systems.|André Baresel,Hartmut Pohlheim,Sadegh Sadeghipour","57563|GECCO|2005|Using evolutionary algorithms for the unit testing of object-oriented software|As the paradigm of object orientation becomes more and more important for modern IT development projects, the demand for an automated test case generation to dynamically test object-oriented software increases. While search-based test case generation strategies, such as evolutionary testing, are well researched for procedural software, relatively little research has been done in the area of evolutionary object-oriented software testing.This paper presents an approach with which to apply evolutionary algorithms for the automatic generation of test cases for the white-box testing of object-oriented software. Test cases for testing object-oriented software include test programs which create and manipulate objects in order to achieve a certain test goal. Strategies for the encoding of test cases to evolvable data structures as well as ideas about how the objective functions could allow for a sophisticated evaluation are proposed. It is expected that the ideas herein can be adapted for other unit testing methods as well.The approach has been implemented by a prototype for empirical validation. In experiments with this prototype, evolutionary testing outperformed random testing. Evolutionary algorithms could be successfully applied for the white-box testing of object-oriented software.|Stefan Wappler,Frank Lammermann","57791|GECCO|2006|An artificial immune system and its integration into an organic middleware for self-protection|Our human body is well protected by antibodies from our biological immune system. This protection system matured over millions of years and has proven its functionality. In our research we are transfering techniques of a biological immune system to a computer based environment in order to design a self-protecting middleware which isn't vulnerable to malicious events. First off this paper proposes an artificial immune system (AIS) and evaluates optimal parameter settings. As the first research we show up the correlation between the size of a system and the length of the receptors used within antibodies for an efficient detection. Further on we describe the integration of the immune system into our organic middleware AMUN and afterwards we propose optimization techniques to minimize the memory space needed for storing the antibodies and to speedup the time needed for detecting malicious objects.|Andreas Pietzowski,Wolfgang Trumler,Theo Ungerer","57536|GECCO|2005|On the contribution of gene libraries to artificial immune systems|Gene libraries have been added to Artificial Immune Systems in analogy to biological immune systems, but to date no careful study of their effect has been made. This work investigates the contribution of gene libraries to Artificial Immune Systems by reproducing and extending an earlier system that used gene libraries. Performance on a job-shop scheduling problem is evaluated empirically with and without gene libraries, and with many different library configurations. We propose that gene libraries encourage diversity in a population of solutions and that the number of components in the gene library parameterises this effect. The number of gene libraries used is found to affect solution fitness and indeed using larger numbers of libraries (and therefore libraries of smaller components) enables higher fitness to be attained. We conclude that gene libraries are likely to be of use in applications where there is a need to maintain the diversity of solutions.|Peter Spellward,Tim Kovacs","58189|GECCO|2007|Extended thymus action for reducing false positives in ais based network intrusion detection systems|One of the major problems faced by anomaly based Network Intrusion Detection (NID) systems is the high number of false positives. False positives refer to the false detection of normal behavior as malicious behavior. Artificial Immune Systems (AISs) also fall under the category of anomaly based-NID systems. AIS presented in this paper is as a victim-end filter, consisting of detectors distributed on the network, which distinguishes normal traffic from malicious traffic. In this work, we focus on TCP-SYN flood based Distributed Denial of Services (DDoS) attacks. Light Weight Intrusion Detection System (LISYS) provides the basic framework for AIS based NID systems. AISs normally utilize the negative selection algorithm in thymus action to tolerize the detectors to normal traffic so they may not detect normal traffic as malicious traffic. We propose and implement extended thymus action' model to improve this characteristic of AIS. Results verify that our model significantly reduces false positives which is a major concern in anomaly-based NID systems.|M. Zubair Shafiq,Mehrin Kiani,Bisma Hashmi,Muddassar Farooq","57411|GECCO|2005|Benefits of software measures for evolutionary white-box testing|White-box testing is an important method for the early detection of errors during software development. In this process test case generation plays a crucial role, defining appropriate and error-sensitive test data. The evolutionary white-box testing is a promising approach for the complete automation of structure-oriented test case generation. Here, test case generation can be completely automated with the help of evolutionary algorithms. However, problem cases exist in which the evolutionary test is not able to find valid test data. Thus, in the case of not achieving a test goal, it is not known whether this is due to non-executable program code or a problem case. This paper will investigate how successfully a software measure can support an evolutionary white-box test if the measure can predict the test effort. Hence, the termination criteria of evolutionary white-box testing can be adapted to test goals with problem cases in such a way that problematic test goals are either excluded from the test in advance or can be covered due to an adequate termination criteria according to a software measure. This could lead to an increase in efficiency and effectiveness of evolutionary white-box testing.|Frank Lammermann,Stefan Wappler","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","57182|GECCO|2003|Developing an Immunity to Spam|Immune systems protect animals from pathogens, so why not apply a similar model to protect computers Several researchers have investigated the use of an artificial immune system to protect computers from viruses and others have looked at using such a system to detect unauthorized computer intrusions. This paper describes the use of an artificial immune system for another kind of protection protection from unsolicited email, or spam.|Terri Oda,Tony White","57070|GECCO|2003|MILA - Multilevel Immune Learning Algorithm|The biological immune system is an intricate network of specialized tissues, organs, cells, and chemical molecules. T-cell-dependent humoral immune response is one of the complex immunological events, involving interaction of B cells with antigens (Ag) and their proliferation, differentiation and subsequent secretion of antibodies (Ab). Inspired by these immunological principles, we proposed a Multilevel Immune Learning Algorithm (MILA) for novel pattern recognition. It incorporates multiple detection schema, clonal expansion and dynamic detector generation mechanisms in a single framework. Different test problems are studied and experimented with MILA for performance evaluation. Preliminary results show that MILA is flexible and efficient in detecting anomalies and novelties in data patterns.|Dipankar Dasgupta,Senhua Yu,Nivedita Sumi Majumdar"],["79888|VLDB|1977|Auditing Large Scale Data Bases|. Data bases and data base auditing are briefly defined. . The roles of external auditors, internal auditors, and corporate managers in data base auditing are explained. . The general approach taken by the auditor in his audit of a data base is presented. . Consideration is given to the characteristics of data base systems which have particular implications for data base auditing. . Attention is given to the nature of audit software packages. . Data base management systems are discussed in relation to the problems they present for data base auditing generally and specifically for audit software packages several alternative solutions to these problems are put forward. . The impact of distributed systems and the data base administration function on data base auditing is explained. . Several specific problems are listed that are matters of concern to data base auditors. . In the Summary and Conclusions a program of cooperation between data base professional associations and professional auditor associations is urged with respect to specific mutual concerns.|George M. Scott","79809|VLDB|1975|Hierarchical Performance Analysis Models for Data Base Systems|This paper presents a comprehensive set of hierarchically organized synthetic models developed for the performance evaluation of data base management systems. The first set of algebraic models for data base management system itself contains a program behavior model, a retrieval model, a logical data base model, a physical data base model, and a data processing model. These models are intended to clarify logical and physical data base structures and essential operations in the data processing on them. Another set of models for performance analysis contains a macroscopic program behavior model, a storage model, a processor model, a user behavior model, and an interactive model. In this set this paper is particularly concerned with the first  models. The macroscopic program behavior model is based on the discussion of data locality and allows us to estimate the frequency of data page loading expected on a given application program and a data base. The storage model enables us to estimate the traverse time of data page loading. Finally the processor model allows us to evaluate the data retrieval processing time of data manipulation commands of a given data base structure, a data base management system and a set of application programs under the multiprogramming environment. These models are to be applied to the optimization of the application programs or the data base structure in order to obtain a higher data retrieval performance.|Isao Miyamoto","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","79820|VLDB|1975|A Data Base Management Facility for Automatic Generation of Data Base Managers|This paper presents a facility for the implementation of data base management systems having high degrees of uhorizontalu data independence, i.e. independence from chosen logical properties of a data base as opposed to uverticalu independence from storage structures. The facility consists of a high level language for the specification of virtual data base managers, a compiler from this language to a pseudo-machine language, and an interpreter for the pseudo-machine language.It is shown how this facility can be used to produce efficient data base management systems with any degree of both horizontal and vertical data independence. Two key features of this tool are the compilation of tailored data base managers from individual schemas and multiple levels of optional binding.|David W. Stemple","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79858|VLDB|1977|Using New Clues to Find Data|Conventional date base management systems use symbolic clues to retrieve data--descriptions of the name, source or contents of data. A new data base management system also allows retrieval of information using spatial clues--where the data is located in the data base management system --and iconic clues--how the data looks. Information is displayed in a simulated space through which users can \"fly\" in search of data, much as people store and retrieve information from particular places in their offices e.g. , specific places on a bookshelf, specific piles of paper on a table, specific places within a filing cabinet. Exploiting the power of human spatial memory, the new system has the significant advantage of allowing the location of information when it cannot be precisely specified e.g., getting near information and browsing through information.|Craig Fields,Nicholas Negroponte","79783|VLDB|1975|Using Large Data Bases for Interactive Problem Solving|Interactive problem solving involves user-machine dialogues to solve unstructured problems, such as selecting marketing strategies and planning mass transit routes. A characteristic of most interactive problem solving systems is that they operate on data bases that are special purpose subsets derived from a large data base. The problem solving system must include code for interfacing with this data base, or the data must be converted to the formats required by the problem solving system. Effective interactive problem solving requires a data management system which provides flexibility in accessing a large data base and fast performance for the problem solving system. This paper describes a method, called data extraction, for interfacing large data bases with interactive problem solving systems. Figure  shows the basic components in the interface. A large data base management system is used to maintain and protect a set of data files. Data extraction is used to aggregate and subset from this set to provide information for problem solving. In addition to an IO interface, data extraction provides interactive data description and presentation functions. For more details on the functional requirements of the data extraction components see .|Eric D. Carlson","79813|VLDB|1975|Data Models for Secondary Storage Representations|Performance modelling of data base management systems, and in particular data base models capable of describing representation features in secondary storage, are discussed. Data structures relevant for secondary storage representations are defined, and a partial characterization of several actual systems is given. The accent is on the use of these structures in the context of a simulation model. We sketch the operation of a generalized modelling program in terms of these structures. Data description and system mechanisms required to generate attribute values are treated. The problem of \"appropriate values\" is discussed in the context of two probabilistic models.|Allen Reiter","80707|VLDB|2006|Load Shedding in Stream Databases A Control-Based Approach|In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.|Yi-Cheng Tu,Song Liu,Sunil Prabhakar,Bin Yao"],["79982|VLDB|1979|Database Program Conversion A Framework for Research|As requirements change, database administrators come under pressure to change the schema which is a description of the database structure. Although writing a new schema is a relatively easy job and transforming the database to match the schema can be accomplished with a modest effort, transforming the numerous programs which operate on the database often requires enormous effort. This interim report describes previous research, defines the problem and proposes a framework for research on the automatic conversion of database programs to match the schema transformations. The approach is based on a precise description of the data structures, integrity constraints, and permissible operations. This work will help designers of manual and computer aided conversion facilities, database administrators who are considering conversions and developers of future database management systems, which will have ease of conversion as a design goal.|Robert W. Taylor,James P. Fry,Ben Shneiderman,Diane C. P. Smith,Stanley Y. W. Su","80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","79862|VLDB|1977|A Scan-Driven Sort Facility for a Relational Database System|In this paper the design and implementation aspects of an integrated sort package are described which was implemented by the author as a part of an experimental relational database system called System R. The sort facility developed for variable length sort keys and tuples is discussed with regard to its anticipated use in relational database systems. It is considered to be a powerful tool to implement complex relational operations like join and projection and to support the dynamic creation of access paths and the loading and reorganizing of data clustered by field values. The input loop of the sort can be driven by various scans which allow the filtering of qualified tuples according to disjunctive normal forms of simple search arguments. Various aspects of the integration with storage and transaction management, locking and logging components are described. A number of design decisions for the sort facility concerning sorting, peerage, and merging techniques are discussed. Finally, some implementation aspects for sorting variable length keys consisting of a number of distributed fields are pointed out.|Theo Härder","80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","79918|VLDB|1978|Current Research Into Specialized Processors For Text Information Retrieval|While there have been a number of projects involved with the design and construction of specialized processors to aid in the efficient operation of large structured database systems, such as RAP or CASSM, very little work has been done on comparable hardware for text information retrieval. This paper summarizes development efforts being carried out to produce backend systems for the efficient searching and retrieval of full text databases. The characteristics of text retrieval, and its special problems when compared to other database systems, are presented. Two representative applications are discussed, one the retrieval of relevant items from a database being updated online from messages originating from a large number of sources, and the second a legal reference system consisting of all court decisions. Processors to scan large amounts of data at speeds comparable to the transfer rate of the disks on which it is stored are presented, along with a network of simple processors to allow rapid merging of directory information for inverted file systems.|Lee A. Hollaar,David C. Roberts","79800|VLDB|1975|Hardware and System Architecture for a Very Large Database|This paper describes practical experience in structuring a large amount of data (about  reels of -track  binary code decimal magnetic tapes of  census data). The hardware and software systems were given. The goal was to structure the database for online devices and to make the retrieval as efficient as possible. This paper explains only the database architecture finally chosen.|Robert Healey,Bradford Heckman","79903|VLDB|1978|Performance Study of a Database Machine in Supporting Relational Databases|Database machines are special-purpose devices that are expected to perform the common data management operations efficiently. In this paper, we attempt to show how a relational database can be supported on a specific database machine, known as the database computer (DBC), with good performance. The DBC employs modified moving-head disks for database storage. To achieve high-volumed accessing, the read-out mechanisms of the moving-head disks are made into tracks-in-parallel. To provide content-addressable search, the disk controller is incorporated with a set of microprocessors, corresponding to the tracks of a cylinder. In this way, not only can an entire cylinder of data be accessed in one disk revolution, but relevant data which satisfies,the user request can also be found and output in the same revolution. To minimize the number of cylinders involved in a database access, some structural information about the database is maintained in a blockoriented content-addressable memory made of charge-coupled devices (CCDs). Furthermore, clustering and security mechanisms are a part of the hardware features provided by the DBC. With cylinder-oriented content-addressable database store, block-oriented content-addressable structure memory and several functionally specialized components, the DBC can achieve one or two orders of magnitude of performance improvement over the conventional computer in database management. Also, a possible twofold increase in database storage requirement as compared to a conventional implementation is adequately offset by one or more orders of magnitude reduction in storage for structural information. The purpose of this paper is to analyze these performance issues. By using the DBC for supporting relational databases, the size of the relational software is considerably reduced. Specifically, the query optimizer of conventional systems is now rendered unnecessary. In comparison with a conventional implementation of a relational system, the DBC has been found to contribute larger performance gains. These gains are tabulated in the paper. All these tend to demonstrate that the DBC in particular and database machines in general can indeed contribute to an appreciable improvement in database management.|Jayanta Banerjee,David K. Hsiao","80407|VLDB|2004|Query and Update Efficient B-Tree Based Indexing of Moving Objects|A number of emerging applications of data management technology involve the monitoring and querying of large quantities of continuous variables, e.g., the positions of mobile service users, termed moving objects. In such applications, large quantities of state samples obtained via sensors are streamed to a database. Indexes for moving objects must support queries efficiently, but must also support frequent updates. Indexes based on minimum bounding regions (MBRs) such as the R-tree exhibit high concurrency overheads during node splitting, and each individual update is known to be quite costly. This motivates the design of a solution that enables the B+ -tree to manage moving objects. We represent moving-object locations as vectors that are timestamped based on their update time. By applying a novel linearization technique to these values, it is possible to index the resulting values using a single B+-tree that partitions values according to their timestamp and otherwise preserves spatial proximity. We develop algorithms for range and k nearest neighbor queries, as well as continuous queries. The proposal can be grafted into existing database systems cost effectively. An extensive experimental study explores the performance characteristics of the proposal and also shows that it is capable of substantially outperforming the R-tree based TPR-tree for both single and concurrent access scenarios.|Christian S. Jensen,Dan Lin,Beng Chin Ooi","79861|VLDB|1977|A Conceptual Design of a Generalized Database Subsystem|A database subsystem (GDS) is proposed. The main objective is to provide a basis for constructing efficient database management systems. A functional interface (GDI), defined for the subsystem, primarily manipulates a single record. Various access functions are provided in order to manipulate a wide variety of logical data structures. Mapping from a logical data structure to a physical data structure is completely maintained in the GDS. Data definition and control functions are also included in the GDI. The subsystem is implemented on a special purpose processor (GDP) which independently operates from a host processor, responsible for the rest of the database management functions. The GDP is composed of special hardware and firmware function modules, local memories and a shared memory which is used for communicating with the host processor. Parallel processing at the host-GDP level and intra-GDP level assures high performance. Furthermore, the interprocessor communication mechanism using the shared memory significantly reduces overhead time by avoiding useless data transfer operations. The subsystem can be applied to various database management system, such as a CODASYL type, a relational type, end user facilities etc.|Katsuya Hakozaki,Takenori Makino,Masayuki Mizuma,Mamoru Umemura,Shigeki Hiyoshi"],["57501|GECCO|2005|Real-coded crossover as a role of kernel density estimation|This paper presents a kernel density estimation method by means of real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine the sampling strategy by means of a parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and the sampling strategy is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation.|Jun Sakuma,Shigenobu Kobayashi","57787|GECCO|2006|Sporadic model building for efficiency enhancement of hierarchical BOA|This paper describes and analyzes sporadic model building, which can be used to enhance the efficiency of the hierarchical Bayesian optimization algorithm (hBOA) and other advanced estimation of distribution algorithms (EDAs) that use complex multivariate probabilistic models. With sporadic model building, the structure of the probabilistic model is updated once every few iterations (generations), whereas in the remaining iterations only model parameters (conditional and marginal probabilities) are updated. Since the time complexity of updating model parameters is much lower than the time complexity of learning the model structure, sporadic model building decreases the overall time complexity of model building. The paper shows that for boundedly difficult nearly decomposable and hierarchical optimization problems, sporadic model building leads to a significant model-building speedup that decreases the asymptotic time complexity of model building in hBOA by a factor of (n .) to (n .), where n is the problem size. On the other hand, sporadic model building also increases the number of evaluations until convergence nonetheless, the evaluation slowdown is insignificant compared to the gains in the asymptotic complexity of model building.|Martin Pelikan,Kumara Sastry,David E. Goldberg","58755|GECCO|2009|Effects of a deterministic hill climber on hBOA|Hybridization of global and local search algorithms is a well-established technique for enhancing the efficiency of search algorithms. Hybridizing estimation of distribution algorithms (EDAs) has been repeatedly shown to produce better performance than either the global or local search algorithm alone. The hierarchical Bayesian optimization algorithm (hBOA) is an advanced EDA which has previously been shown to benefit from hybridization with a local searcher. This paper examines the effects of combining hBOA with a deterministic hill climber (DHC). Experiments reveal that allowing DHC to find the local optima makes model building and decision making much easier for hBOA. This reduces the minimum population size required to find the global optimum, which substantially improves overall performance.|Elizabeth Radetic,Martin Pelikan,David E. Goldberg","58536|GECCO|2008|Using previous models to bias structural learning in the hierarchical BOA|Estimation of distribution algorithms (EDAs) are stochastic optimization techniques that explore the space of potential solutions by building and sampling probabilistic models of promising candidate solutions. While the primary goal of applying EDAs is to discover the global optimum (or an accurate approximation), any EDA also provides us with a sequence of probabilistic models, which hold a great deal of information about the problem. Although using problem-specific knowledge has been shown to significantly improve performance of EDAs and other evolutionary algorithms, this readily available source of information has been largely ignored by the EDA community. This paper takes the first step towards the use of probabilistic models obtained by EDAs to speed up the solution of similar problems in the future. More specifically, we propose two approaches to biasing model building in the hierarchical Bayesian optimization algorithm (hBOA) based on knowledge automatically learned from previous runs on similar problems. We show that the methods lead to substantial speedups and argue that they should work well in other applications that require solving a large number of problems with similar structure.|Mark Hauschild,Martin Pelikan,Kumara Sastry,David E. Goldberg","58726|GECCO|2009|Intelligent bias of network structures in the hierarchical BOA|One of the primary advantages of estimation of distribution algorithms (EDAs) over many other stochastic optimization techniques is that they supply us with a roadmap of how they solve a problem. This roadmap consists of a sequence of probabilistic models of candidate solutions of increasing quality. The first model in this sequence would typically encode the uniform distribution over all admissible solutions whereas the last model would encode a distribution that generates at least one global optimum with high probability. It has been argued that exploiting this knowledge should improve EDA performance when solving similar problems. This paper presents an approach to bias the building of Bayesian network models in the hierarchical Bayesian optimization algorithm (hBOA) using information gathered from models generated during previous hBOA runs on similar problems. The approach is evaluated on trap- and D spin glass problems.|Mark Hauschild,Martin Pelikan","57456|GECCO|2005|A comparison study between genetic algorithms and bayesian optimize algorithms by novel indices|Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA's defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization Algorithm (BOA), a well-known Estimation of Distribution Algorithms (EDAs).|Naoki Mori,Masayuki Takeda,Keinosuke Matsumoto","57750|GECCO|2006|Does overfitting affect performance in estimation of distribution algorithms|Estimation of Distribution Algorithms (EDAs) are a class of evolutionary algorithms that use machine learning techniques to solve optimization problems. Machine learning is used to learn probabilistic models of the selected population. This model is then used to generate next population via sampling. An important phenomenon in machine learning from data is called overfitting. This occurs when the model is overly adapted to the specifics of the training data so well that even noise is encoded. The purpose of this paper is to investigate whether overfitting happens in EDAs, and to discover its consequences. What is found is overfitting does occur in EDAs overfitting correlates to EDAs performance reduction of overfitting using early stopping can improve EDAs performance.|Hao Wu,Jonathan L. Shapiro","57181|GECCO|2003|Design of Multithreaded Estimation of Distribution Algorithms|Estimation of Distribution Algorithms (EDAs) use a probabilistic model of promising solutions found so far to obtain new candidate solutions of an optimization problem. This paper focuses on the design of parallel EDAs. More specifically, the paper describes a method for parallel construction of Bayesian networks with local structures in form of decision trees in the Mixed Bayesian Optimization Algorithm. The proposed Multithreaded Mixed Bayesian Optimization Algorithm (MMBOA) is intended for implementation on a cluster of workstations that communicate by Message Passing Interface (MPI). Communication latencies between workstations are eliminated by multithreaded processing, so in each workstation the high-priority model-building thread, which is communication demanding, can be overlapped by low-priority model sampling thread when necessary. High performance of MMBOA is verified via simulation in TRANSIM tool.|Jiri Ocenasek,Josef Schwarz,Martin Pelikan","58548|GECCO|2008|Improved EDNA estimation of dependency networks algorithm using combining function with bivariate probability distributions|One of the key points in Estimation of Distribution Algorithms (EDAs) is the learning of the probabilistic graphical model used to guide the search the richer the model the more complex the learning task. Dependency networks-based EDAs have been recently introduced. On the contrary of Bayesian networks, dependency networks allow the presence of directed cycles in their structure. In a previous work the authors proposed EDNA, an EDA algorithm in which a multivariate dependency network is used but approximating its structure learning by considering only bivariate statistics. EDNA was compared with other models from the literature with the same computational complexity (e.g., univariate and bivariate models). In this work we propose a modified version of EDNA in which not only the structural learning phase is limited to bivariate statistics, but also the simulation and the parameter learning task. Now, we extend the comparison employing multivariate models based on Bayesian networks (EBNA and hBOA). Our experiments show that the modified EDNA is more accurate than the original one, being its accuracy comparable to EBNA and hBOA, but with the advantage of being faster specially in the more complex cases.|José A. Gámez,Juan L. Mateo,Jose Miguel Puerta","58484|GECCO|2008|On the effectiveness of distributions estimated by probabilistic model building|Estimation of distribution algorithms (EDAs) are a class of evolutionary algorithms that capture the likely structure of promising solutions by explicitly building a probabilistic model and utilize the built model to guide the further search. It is presumed that EDAs can detect the structure of the problem by recognizing the regularities of the promising solutions. However, in certain situations, EDAs are unable to discover the entire structure of the problem because the set of promising solutions on which the model is built contains insufficient information regrading some parts of the problem and renders EDAs incapable of processing those parts accurately. In this work, we firstly propose a general concept that the estimated probabilistic models should be inspected to reveal the effective search directions. Based on that concept, we design a practical approach which utilizes a reserved set of solutions to examine the built model for the fragments that may be inconsistent with the actual problem structure. Furthermore, we provide an implementation of the designed approach on the extended compact genetic algorithm (ECGA) and conduct numerical experiments. The experimental results indicate that the proposed method can significantly assist ECGA to handle problems comprising building blocks of disparate scalings.|Chung-Yao Chuang,Ying-ping Chen"],["57768|GECCO|2006|An anticipatory approach to improve XCSF|XCSF is a novel version of learning classifier systems (LCS) which extends the typical concept of LCS by introducing computable classifier prediction. In XCSF Classifier prediction is computed as a linear combination of classifier inputs and a weight vector associated to each classifier. Learning process takes place using a weight update mechanism. Initial results show that XCSF can be used to evolve accurate approximations of some functions. In this paper, we try to add an anticipatory component to XCSF improving its performance.|Amin Nikanjam,Adel Torkaman Rahmani","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","58612|GECCO|2009|On the scalability of XCSF|Many successful applications have proven the potential of Learning Classifier Systems and the XCS classifier system in particular in datamining, reinforcement learning, and function approximation tasks. Recent research has shown that XCS is a highly flexible system, which can be adapted to the task at hand by adjusting its condition structures, learning operators, and prediction mechanisms. However, fundamental theory concerning the scalability of XCS dependent on these enhancements and problem difficulty is still rather sparse and mainly restricted to boolean function problems. In this article we developed a learning scalability theory for XCSF---the XCS system applied to real-valued function approximation problems. We determine crucial dependencies on functional properties and on the developed solution representation and derive a theoretical scalability model out of these constraints. The theoretical model is verified with empirical evidence. That is, we show that given a particular problem difficulty and particular representational constraints XCSF scales optimally. In consequence, we discuss the importance of appropriate prediction and condition structures regarding a given problem and show that scalability properties can be improved by polynomial orders, given an appropriate, problem-suitable representation.|Patrick O. Stalph,Martin V. Butz,David E. Goldberg,Xavier Llorà","57228|GECCO|2003|Towards Building Block Propagation in XCS A Negative Result and Its Implications|The accuracy-based classifier system XCS is currently the most successful learning classifier system. Several recent studies showed that XCS can produce machine-learning competitive results. Nonetheless, until now the evolutionary mechanisms in XCS remained somewhat ill-understood. This study investigates the selectorecombinative capabilities of the current XCS system. We reveal the accuracy dependence of XCS's evolutionary algorithm and identify a fundamental limitation of the accuracy-based fitness approach in certain problems. Implications and future research directions conclude the paper.|Kurian K. Tharakunnel,Martin Butz,David E. Goldberg","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","58562|GECCO|2009|Discrete dynamical genetic programming in XCS|A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of well-known test problems.|Richard Preen,Larry Bull","59027|GECCO|2010|Adaption of XCS to multi-learner predatorprey scenarios|Learning classifier systems (LCSs) are rule-based evolutionary reinforcement learning systems. Today, especially variants of Wilson's extended classifier system (XCS) are widely applied for machine learning. Despite their widespread application, LCSs have drawbacks, e. g., in multi-learner scennarios, since the Markov property is not fulfilled. In this paper, LCSs are investigated in an instance of the generic homogeneous and non-communicating predatorprey scenario. A group of predators collaboratively observe a (randomly) moving prey as long as possible, where each predator is equipped with a single, independent XCS. Results show that improvements in learning are achieved by cleverly adapting a multi-step approach to the characteristics of the investigated scenario. Firstly, the environmental reward function is expanded to include sensory information. Secondly, the learners are equipped with a memory to store and analyze the history of local actions and given payoffs.|Clemens Lode,Urban Richter,Hartmut Schmeck","57043|GECCO|2003|Tournament Selection Stable Fitness Pressure in XCS|Although it is known from GA literature that proportionate selection is subject to many pitfalls, the LCS community somewhat adhered to proportionate selection. Also in the accuracy-based learning classifier system XCS, introduced by Wilson in , proportionate selection is used. This paper identifies problem properties in which performance of proportionate selection is impaired. Consequently, tournament selection is introduced which makes XCS more parameter independent, noise independent, and more efficient in exploiting fitness guidance.|Martin Butz,Kumara Sastry,David E. Goldberg","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"],["57998|GECCO|2007|Techniques for highly multiobjective optimisation some nondominated points are better than others|The research area of evolutionary multiobjective optimization (EMO) is reaching better understandings of the properties and capabilities of EMO algorithms, and accumulating much evidence of their worth in practical scenarios. An urgent emerging issue is that the favoured EMO algorithms scale poorly when problems have \"many\" (e.g. five or more) objectives. One of the chief reasons for this is believed to be that, in many-objective EMO search, populations are likely to be largely composed of nondominated solutions. In turn, this means that the commonly-used algorithms cannot distinguish between these for selective purposes. However, there are methods that can be used validly to rank points in a nondominated set, and may therefore usefully underpin selection in EMO search. Here we discuss and compare several such methods. Our main finding is that simple variants of the often-overlooked \"Average Ranking\" strategy usually outperform other methods tested, covering problems with - objectives and differing amounts of inter-objective correlation.|David W. Corne,Joshua D. Knowles","58837|GECCO|2009|On the hybridization of SMS-EMOA and local search for continuous multiobjective optimization|In the recent past, hybrid metaheuristics became famous as successful optimization methods. The motivation for the hybridization is a notion of combining the best of two worlds evolutionary black box optimization and local search. Successful hybridizations in large combinatorial solution spaces motivate to transfer the idea of combining the two worlds to continuous domains as well. The question arises Can local search also improve the convergence to the Pareto front in continuous multiobjective solutions spaces We introduce a relay and a concurrent hybridization of the successful multiobjective optimizer SMS-EMOA and local optimization methods like Hooke & Jeeves and the Newton method. The concurrent approach is based on a parameterized probability function to control the local search. Experimental analyses on academic test functions show increased convergence speed as well as improved accuracy of the solution set of the new hybridizations.|Patrick Koch,Oliver Kramer,Günter Rudolph,Nicola Beume","58355|GECCO|2008|Rigorous analyses of fitness-proportional selection for optimizing linear functions|Rigorous runtime analyses of evolutionary algorithms (EAs) mainly investigate algorithms that use elitist selection methods. Two algorithms commonly studied are Randomized Local Search (RLS) and the (+) EA and it is well known that both optimize any linear pseudo-Boolean function on n bits within an expected number of O(n log n) fitness evaluations. In this paper, we analyze variants of these algorithms that use fitness proportional selection. A well-known method in analyzing the local changes in the solutions of RLS is a reduction to the gambler's ruin problem. We extend this method in order to analyze the global changes imposed by the (+) EA. By applying this new technique we show that with high probability using fitness proportional selection leads to an exponential optimization time for any linear pseudo-Boolean function with non-zero weights. Even worse, all solutions of the algorithms during an exponential number of fitness evaluations differ with high probability in linearly many bits from the optimal solution. Our theoretical studies are complemented by experimental investigations which confirm the asymptotic results on realistic input sizes.|Edda Happ,Daniel Johannsen,Christian Klein,Frank Neumann","57395|GECCO|2005|An empirical study on the handling of overlapping solutions in evolutionary multiobjective optimization|We focus on the handling of overlapping solutions in evolutionary multiobjective optimization (EMO) algorithms. First we show that there exist a large number of overlapping solutions in each population when EMO algorithms are applied to multiobjective combinatorial optimization problems with only a few objectives. Next we implement three strategies to handle overlapping solutions. One strategy is the removal of overlapping solutions in the objective space. In this strategy, overlapping solutions in the objective space are removed during the generation update phase except for only a single solution among them. As a result, each solution in the current population has a different location in the objective space. Another strategy is to remove overlapping solutions so that each solution in the current population has a different location in the decision space. The other strategy is the modification of Pareto ranking where overlapping solutions in the objective space are allocated to different fronts. As a result, each solution in each front has a different location in the objective space. Effects of each strategy on the performance of the NSGA-II algorithm are examined through computational experiments on multiobjective  knapsack problems, multiobjective flowshop scheduling problems, and multiobjective fuzzy rule selection problems.|Hisao Ishibuchi,Kaname Narukawa,Yusuke Nojima","59104|GECCO|2010|Simultaneous use of different scalarizing functions in MOEAD|The use of Pareto dominance for fitness evaluation has been the mainstream in evolutionary multiobjective optimization for the last two decades. Recently, it has been pointed out in some studies that Pareto dominance-based algorithms do not always work well on multiobjective problems with many objectives. Scalarizing function-based fitness evaluation is a promising alternative to Pareto dominance especially for the case of many objectives. A representative scalarizing function-based algorithm is MOEAD (multiobjective evolutionary algorithm based on decomposition) of Zhang & Li (). Its high search ability has already been shown for various problems. One important implementation issue of MOEAD is a choice of a scalarizing function because its search ability strongly depends on this choice. It is, however, not easy to choose an appropriate scalarizing function for each multiobjective problem. In this paper, we propose an idea of using different types of scalarizing functions simultaneously. For example, both the weighted Tchebycheff (Chebyshev) and the weighted sum are used for fitness evaluation. We examine two methods for implementing our idea. One is to use multiple grids of weight vectors and the other is to assign a different scalarizing function alternately to each weight vector in a single grid.|Hisao Ishibuchi,Yuji Sakane,Noritaka Tsukamoto,Yusuke Nojima","58460|GECCO|2008|Effectiveness of scalability improvement attempts on the performance of NSGA-II for many-objective problems|Recently a number of approaches have been proposed to improve the scalability of evolutionary multiobjective optimization (EMO) algorithms to many-objective problems. In this paper, we examine the effectiveness of those approaches through computational experiments on multiobjective knapsack problems with two, four, six, and eight objectives. First we briefly review related studies on evolutionary many-objective optimization. Next we explain why Pareto dominance-based EMO algorithms do not work well on many-objective optimization problems. Then we explain various scalability improvement approaches. We examine their effects on the performance of NSGA-II through computational experiments. Experimental results clearly show that the diversity of solutions is decreased by most scalability improvement approaches while the convergence of solutions to the Pareto front is improved. Finally we conclude this paper by pointing out future research directions.|Hisao Ishibuchi,Noritaka Tsukamoto,Yasuhiro Hitotsuyanagi,Yusuke Nojima","57702|GECCO|2006|Incorporation of decision makers preference into evolutionary multiobjective optimization algorithms|The main characteristic feature of evolutionary multiobjective optimization (EMO) is that no a priori information about the decision maker's preference is utilized in the search phase. EMO algorithms try to find a set of well-distributed Pareto-optimal solutions with a wide range of objective values. It is, however, very difficult for EMO algorithms to find a good solution set of a multiobjective combinatorial optimization problem with many decision variables andor many objectives. In this paper, we propose an idea of incorporating the decision maker's preference into EMO algorithms to efficiently search for Pareto-optimal solutions of such a hard multiobjective optimization problem.|Hisao Ishibuchi,Yusuke Nojima,Kaname Narukawa,Tsutomu Doi","58904|GECCO|2010|Indicator-based evolutionary algorithm with hypervolume approximation by achievement scalarizing functions|Pareto dominance-based algorithms have been the main stream in the field of evolutionary multiobjective optimization (EMO) for the last two decades. It is, however, well-known that Pareto-dominance-based algorithms do not always work well on many-objective problems with more than three objectives. Currently alternative frameworks are studied in the EMO community very actively. One promising framework is the use of an indicator function to find a good solution set of a multiobjective problem. EMO algorithms with this framework are called indicator-based evolutionary algorithms (IBEAs) where the hypervolume measure is frequently used as an indicator. IBEAs with the hypervolume measure have strong theoretical support and high search ability. One practical difficult of such an IBEA is that the hypervolume calculation needs long computation time especially when we have many objectives. In this paper, we propose an idea of using a scalarizing function-based hypervolume approximation method in IBEAs. We explain how the proposed idea can be implemented in IBEAs. We also demonstrate through computational experiments that the proposed idea can drastically decrease the computation time of IBEAs without severe performance deterioration.|Hisao Ishibuchi,Noritaka Tsukamoto,Yuji Sakane,Yusuke Nojima","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang","58972|GECCO|2010|Integrating decision space diversity into hypervolume-based multiobjective search|Multiobjective optimization in general aims at learning about the problem at hand. Usually the focus lies on objective space properties such as the front shape and the distribution of optimal solutions. However, structural characteristics in the decision space can also provide valuable insights. In certain applications, it may even be more important to find a structurally diverse set of close-to-optimal solutions than to identify a set of optimal but structurally similar solutions. Accordingly, multiobjective optimizers are required that are capable of considering both the objective space quality of a Pareto-set approximation and its diversity in the decision space. Although NSGA, one of the first multiobjective evolutionary algorithms, explicitly considered decision space diversity, only a few other studies address that issue. It therefore is an open research question how modern multiobjective evolutionary algorithms can be adapted to search for structurally diverse high-quality Pareto-set approximations. To this end we propose an approach to integrate decision space diversity into hypervolume-based multiobjective search. We present a modified hypervolume indicator and integrate it into an evolutionary algorithm. The proof-of-principle results show the potential of the approach and indicate further research directions for structure-oriented multiobjective search.|Tamara Ulrich,Johannes Bader,Eckart Zitzler"],["58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","57120|GECCO|2003|A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective Optimization|This paper introduces a modified PSO, Non-dominated Sorting Particle Swarm Optimizer (NSPSO), for better multiobjective optimization. NSPSO extends the basic form of PSO by making a better use of particles' personal bests and offspring for more effective nondomination comparisons. Instead of a single comparison between a particle's personal best and its offspring, NSPSO compares all particles' personal bests and their offspring in the entire population. This proves to be effective in providing an appropriate selection pressure to propel the swarm population towards the Pareto-optimal front. By using the non-dominated sorting concept and two parameter-free niching methods, NSPSO and its variants have shown remarkable performance against a set of well-known difficult test functions (ZDT series). Our results and comparison with NSGA II show that NSPSO is highly competitive with existing evolutionary and PSO multiobjective algorithms.|Xiaodong Li","59071|GECCO|2010|An parallel particle swarm optimization approach for multiobjective optimization problems|This paper proposes a parallel particle swarm optimization (PPSO) to solve the multiobjective optimization problems (MOP). PPSO makes the use of the parallel characteristic of the PSO algorithm to deal with the multiple objectives issue of the MOP. PPSO uses as many swarms as the number of the objectives in the MOP and lets each swarm optimize only one of the objectives. These swarms work in parallel and each swarm can use a standard PSO or any other improved PSO variants to solve a single objective problem. PPSO has advantages on the following two aspects. First, as each swarm focus on optimizing only one objective, PPSO can avoid the difficulty of fitness assignment because the particles can be evaluated like in the single objective optimization problem. Second, as different swarms optimize different objectives, PPSO can maintain the population diversity to make a throughout search along the whole Pareto front to obtain nondominated solutions as many as possible. The performance of PPSO is tested on a set of benchmark problems complicated Pareto sets in CEC. The experimental results compared with those obtained by the state-of-the-art algorithms demonstrate the effectiveness and efficiency of PPSO, showing the good performance of PPSO in solving the MOP with complicated Pareto sets.|Zhi-hui Zhan,Jun Zhang","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58287|GECCO|2008|Runtime analysis of binary PSO|We investigate the runtime of the Binary Particle Swarm Optimization (PSO) algorithm introduced by Kennedy and Eberhart (). The Binary PSO maintains a global best solution and a swarm of particles. Each particle consists of a current position, an own best position and a velocity vector used in a probabilistic process to update the particle's position. We present lower bounds for swarms of polynomial size. To prove upper bounds, we transfer a fitness-level argument well-established for evolutionary algorithms (EAs) to PSO. This method is applied to estimate the expected runtime on the class of unimodal functions. A simple variant of the Binary PSO is considered in more detail. The -PSO only maintains one particle, hence own best and global best solutions coincide. Despite its simplicity, the -PSO is surprisingly efficient. A detailed analysis for the function OneMax shows that the -PSO is competitive to EAs.|Dirk Sudholt,Carsten Witt","58870|GECCO|2009|An evaporation mechanism for dynamic and noisy multimodal optimization|Dealing with imprecise information is a common characteristic in real-world problems. Specifically, when the source of the information are physical sensors, a level of noise in the evaluation has to be assumed. Particle Swarm Optimization is a technique that presented a good behavior when dealing with noisy fitness functions. Nevertheless, the problem is still open. In this paper we propose the use of the evaporation mechanism for managing with dynamic multi-modal optimization problems that are subject to noisy fitness functions. We will show how the evaporation mechanism does not require the detection of environment changes and how can be used for improving the performance of PSO algorithms working in noisy environments.|Jose Luis Fernandez-Marquez,Josep Lluís Arcos","58697|GECCO|2009|Estimation of particle swarm distribution algorithms bringing together the strengths of PSO and EDAs|This paper presents a framework of estimation of particle swarm distribution algorithms (EPSDAs). The aim lies in effectively combining particle swarm optimization (PSO) with estimation of distribution algorithms (EDAs) without losing on their unique features. To exhibit its practicability, an extended compact particle swarm optimization (EcPSO) is developed along the lines of the suggested framework. Empirical results have adduced grounds for its effectiveness.|Chang Wook Ahn,Hyun-Tae Kim","58857|GECCO|2009|Particle swarm optimization in the presence of multiple global optima|Dynamic analyses of canonical particle swarm optimization (PSO) have indicated that parameter values of phimax  . and constriction coefficient chi  . provide adequate exploration and prevent swarm explosion. This paper shows by example that these values do not prevent swarm explosion in some cases. In other examples it is shown that even when the swarm does not explode, the canonical PSO algorithm with these parameter values can still fail to converge indefinitely. A satisfactory analysis of PSO has yet to be made, and will require abandoning certain assumptions that oversimplify particle behavior.|Sunny Choi,Blayne E. Mayfield"]]},"title":{"entropy":5.6594736957009495,"topics":["genetic programming, using genetic, genetic algorithm, genetic for, using programming, using, programming for, using algorithm, and genetic, and programming, evolution strategies, genetic with, using and, artificial immune, for using, evolution and, evolving for, time series, cellular automata, building blocks","data base, for data, data, and data, for systems, database systems, database, the data, for database, data systems, data management, query for, management systems, and database, data streams, data model, base systems, relational database, for xml, and for","estimation distribution, ant colony, neural networks, for optimization, algorithm with, algorithm for, evolutionary for, for networks, for problem, for with, multi-objective optimization, for learning, estimation algorithm, solving problem, with, method for, for multi-objective, dynamic environments, for dynamic, optimization with","genetic algorithm, algorithm for, particle swarm, the problem, for the, algorithm the, the, evolutionary algorithm, the and, for problem, particle optimization, swarm optimization, algorithm problem, genetic for, the genetic, for optimization, genetic problem, algorithm optimization, evolutionary for, and algorithm","using evolution, evolution strategies, evolution and, evolution for, for detection, strategies for, time series, the evolution, differential evolution, algorithm detection, grammatical evolution, classifier systems, series forecasting, evolution, evolution genetic, detection using, learning classifier, and strategies, for game, time forecasting","for design, genetic design, and design, selection and, analysis for, for selection, selection using, analysis genetic, design using, analysis and, genetic selection, evolutionary design, algorithm design, evolutionary testing, design, selection algorithm, analysis using, analysis the, analysis programming, for testing","for database, and database, relational database, the database, for web, database, database systems, the web, and web, for information, information and, search web, keyword search, support vector, the relational, data web, web services, approach database, performance database, relational data","data base, for systems, data systems, database systems, data management, and systems, for base, management systems, systems, the systems, for data, base systems, large data, for large, for management, distributed systems, framework for, relational base, data design, base design","evolutionary for, method for, evolutionary algorithm, evolutionary approach, approach for, evolutionary computation, for time, for routing, the time, evolutionary method, approach the, based approach, the vehicle, algorithm for, approach, evolutionary, covariance adaptation, genetic approach, for vehicle, approach problem","for learning, algorithm for, new for, differential evolution, for scheduling, for functions, learning classifier, reinforcement learning, classifier systems, learning with, for systems, for optimization, evolutionary for, the learning, learning systems, functions optimization, evolutionary learning, for the, classifier for, for multimodal","the evolutionary, evolutionary for, evolutionary algorithm, crossover for, evolutionary and, evolutionary computation, the crossover, the computation, crossover and, crossover operator, crossover genetic, evolutionary, crossover problem, for the, crossover, evolutionary design, free lunch, evolutionary problem, mechanisms for, synthesis algorithm","the search, the performance, search for, and search, the effects, local search, the case, study the, the functions, performance algorithm, local and, performance for, search, case study, the using, search space, genetic search, for the, the and, for multiobjective"],"ranking":[["57632|GECCO|2006|Using a genetic algorithm to evolve cellular automata for DD computational development|Form generation or morphogenesis is one of the main stages of both artificial and natural development. This paper provides results from experiments in which a genetic algorithm (GA) was used to evolve cellular automata (CA) to produce predefined D and D shapes. The GA worked by evolving the CA rule table and the number of iterations that the model was to run. After the final chromosomes were obtained for all shapes, the CA model was allowed to run starting with a single cell in the middle of the lattice until the allowed number of iterations was reached and a shape was formed. In all cases, mean fitness values of evolved chromosomes were above .|Arturo Chavoya,Yves Duthen","57703|GECCO|2006|On evolving buffer overflow attacks using genetic programming|In this work, we employed genetic programming to evolve a \"white hat\" attacker that is to say, we evolve variants of an attack with the objective of providing better detectors. Assuming a generic buffer overflow exploit, we evolve variants of the generic attack, with the objective of evading detection by signature-based methods. To do so, we pay particular attention to the formulation of an appropriate fitness function and partnering instruction set. Moreover, by making use of the intron behavior inherent in the genetic programming paradigm, we are able to explicitly obfuscate the true intent of the code. All the resulting attacks defeat the widely used 'Snort' Intrusion Detection System.|Hilmi Günes Kayacik,Malcolm I. Heywood,A. Nur Zincir-Heywood","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","58992|GECCO|2010|Improved forecasting of time series data of real system using genetic programming|A study is made to improve short term forecasting of time series data of real system using Genetic Programming (GP) under the framework of time delayed embedding technique. GP based approach is used to make analytical model of time series data of real system using embedded vectors that help reconstruct the phase space. The map equations, involving non-linear symbolic expressions in the form of binary trees comprising of time delayed components in the immediate past, are first obtained by carrying out single-step GP fit for the training data set and usually they are found to give good fitness as well as single-step predictions. However while forecasting the time series based on multi-step predictions in the out-of-sample region in an iterative manner, these solutions often show rapid deterioration as we dynamically forward the solution in future time. With a view to improve on this limitation, it is shown that if the multi-step aspect is incorporated while making the GP fit itself, the corresponding GP solutions give multi-step predictions that are improved to a fairly good extent for around those many multi-steps as incorporated during the multi-step GP fit. Two different methods for multi-step fit are introduced, and the corresponding prediction results are presented. The modified method is shown to make better forecast for out-of-sample multi-step predictions for the time series of a real system, namely Electroencephelograph (EEG) signals.|Dilip P. Ahalpara","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","58830|GECCO|2009|Evolution development and learning using self-modifying cartesian genetic programming|Self-Modifying Cartesian Genetic Programming (SMCGP) is a form of genetic programming that integrates developmental (self-modifying) features as a genotype-phenotype mapping. This paper asks Is it possible to evolve a learning algorithm using SMCGP|Simon Harding,Julian Francis Miller,Wolfgang Banzhaf","58748|GECCO|2009|Evolution of hyperheuristics for the biobjective graph coloring problem using multiobjective genetic programming|We consider a formulation of the biobjective soft graph coloring problem so as to simultaneously minimize the number of colors used as well as the number of edges that connect vertices of the same color. We aim to evolve hyperheuristics for this class of problem using multiobjective genetic programming (MOGP). The major advantage being that these hyperheuristics can then be applied to any instance of this problem. We test the hyperheuristics on benchmark graph coloring problems, and in the absence of an actual Pareto-front, we compare the solutions obtained with existing heuristics. We then further improve the quality of hyperheuristics evolved, and try to make them closer to human-designed heuristics.|Paresh Tolay,Rajeev Kumar","57898|GECCO|2007|Evolving controllers for simulated car racing using object oriented genetic programming|Several different controller representations are compared on anon-trivial problem in simulated car racing, with respect tolearning speed and final fitness. The controller representations arebased either on Neural Networks or Genetic Programming, and alsodiffer in regards to whether they allow for stateful controllers orjust reactive ones. Evolved GP trees are analysed, and attempts aremade at explaining the performance differences observed.|Alexandros Agapitos,Julian Togelius,Simon M. Lucas","57161|GECCO|2003|Building Decision Tree Software Quality Classification Models Using Genetic Programming|Predicting the quality of software modules prior to testing or system operations allows a focused software quality improvement en-deavor. Decision trees are very attractive for classification problems, because of their comprehensibility and white box modeling features. However, optimizing the classification accuracy and the tree size is a difficult problem, and to our knowledge very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for calibrating software quality classification models. The proposed technique is based on multi-objective optimization using strongly typed gp. Two fitness functions are used to optimize the classification accuracy and tree size of the classification models calibrated for a real-world high-assurance software system. The performances of the classification models are compared with those obtained by standard gp. It is shown that the gp-based decision tree technique yielded better classification models.|Yi Liu,Taghi M. Khoshgoftaar","58582|GECCO|2009|Evolving stochastic processes using feature tests and genetic programming|The synthesis of stochastic processes using genetic programming is investigated. Stochastic process behaviours take the form of time series data, in which quantities of interest vary over time in a probabilistic, and often noisy, manner. A suite of statistical feature tests are performed on time series plots from example processes, and the resulting feature values are used as targets during evolutionary search. A process algebra, the stochastic -calculus, is used to denote processes. Investigations consider variations of GP representations for a subset of the stochastic -calculus, for example, the use of channel unification, and various grammatical constraints. Target processes of varying complexity are studied. Results show that the use of grammatical GP with statistical feature tests can successfully synthesize stochastic processes. Success depends upon a selection of appropriate feature tests for characterizing the target behaviour, and the complexity of the target process.|Brian J. Ross,Janine H. Imada"],["79879|VLDB|1977|A Survey The Application of Data Base Management Computers in Distributed Systems|Data Base Management Computers (DBMC) are special purpose computers in distributed systems. The dedicated role of the DBMC is to control the devices upon which data bases reside and to provide data base management services for other computers in the system. and how it relates to other computers, Depending upon how it operates a given DBMC can be classified either as an intelligent controller, a backend or a datacomputer. Prototypes of each class have been proposed and implemented. In addition there is a growing body of literature devoted to DBMC technology -- objectives, requirements, problems, performance issues, etc. The purpose of this paper is to present a survey of recent research and then to examine special DBMC requirements in the increasingly important areas of very large data bases and distributed data bases.|Eugene I. Lowenthal","80022|VLDB|1980|Status Report on ISOTCSCWG - Data Base Management Systems|This paper discussed the current status of the work of the ISOTCSCWG data base management systems working group. In existence for over five years, this working group has devoted it energies to the elaboration and extension of the notion of a \"conceptual schema\" as initially defined by the ANSIXSPARCDBMS Study Group. The focus of the ISO work has been the construction of a report on conceptual schema concepts, identification of various methodologies for their construction and the application of such methodologies to an example \"world\" in such a fashion that comparisons can be made between the different approaches. Underlying the work of this group has been the identification of fundamental philosophical differences in the various methodologies specifically that some assume the conceptual schema models the real world and others assume the conceptual schema models the data in the information base that holds information about the real world. This difference leads to considerable confusion in discussion when it is not explicitly recognized and one of the principal objectives of the ISOTCSCWG report is to expurgate that difference.|Thomas B. Steel Jr.","79899|VLDB|1978|Issues in Distributed Data Base Management Systems A Technical Overview|Distributed Data Base Management Systems (D-DBMS) are the converging point of apparently contrasting areas networking and Data Bases, representative of distribution and integration respectively. Users should benefit from this new dimension when trying to make data processing architectures conform to managerial and organizational philosophies. This paper is a technical overview of research and development in D-DMBS. In the first section, various architectures and design strategies are examined, followed by a description of functional layers. The second section is dedicated to Data Base design and mappings between levels of representations. The last section addresses Integrity control and Recovery.|Michel E. Adiba,Jean-Claude Chupin,Robert Demolombe,Georges Gardarin,Jean Le Bihan","80290|VLDB|2003|Grid Data Management Systems  Services|The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities. Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies. The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.|Arun Jagatheesan,Reagan Moore,Norman W. Paton,Paul Watson","79929|VLDB|1978|An Alternative Structure for Data Base Management Systems|The ANSIXSPARC proposal of standardization for architecture of Data Base Management Systems is particularly concerned with the achievement of iogical data independence. This proposal has almost universally been recognized as valid. In this paper we argue that this proposal is not a standardization of Data Base Management Systems features, but rather an implementation proposal, based on schemas mappings and transforms. A more general architecture is proposed and an implementation other than the one of the ANSIXSPARC is examined. Tradeoffs between the two implementation strategies are outlined.|Paolo Paolini","79923|VLDB|1978|A Search Processor for Data Base Management Systems|A special processor for performing the selection and restriction operation of data base management systems by linearly searching large files with very high speed is described. It includes the reasoning, leading to the architecture and the structure of the search processor and a report on the status and the plans of a research project.|Hans-Otto Leilich,Günther Stiege,Hans Christoph Zeidler","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79814|VLDB|1975|A Multi-Level Architecture for Relational Data Base Systems|Most of the literature on implementation of relations has been directed toward user features, with little attention paid to an overall conceptual view of underlying structures. Performance oriented considerations have been treated only for isolated problems. Toward a solution to these problems we describe a multi-level architecture for relational data base systems. This architecture distinguishes clearly between user oriented features, access path structures, data structures and file organization. It also allows efficiency problems to be isolated within levels and solved independently of each other, without impacting the logical structure of the user's virtual machine. Specific problems considered here in the context of this architecture include the mapping of relations into files, the implementation of fast access paths, and some file level optimizations that are particularly useful in relational systems.|Hans Albrecht Schmid,Philip A. Bernstein","80070|VLDB|1981|IML-Inscribed Nets for Modeling Text Processing and Data Base Management Systems|Predicatetransition-nets with a suitable inscription allow building practice-oriented and expressive models of distributed information systems with concurrent processes as found in office information systems or decentralized database management systems. The paper presents the first version of an inscription language called IML (Information Management Language) which has been derived from the results of some long range database research activities. The abstract objects \"construct\" and \"form\" are introduced and basic operations are defined on them. It is shown how to apply these language elements to inscribe net symbols for high-fidelity modeling of text processing and data management.|Gernot Richter","79840|VLDB|1976|An Approach to Data Communication between Different Generalized Data Base Management Systems|A number of data base management systems (DBMS) are now in operation in industry, government and the academic world. However, since most of these DBMSs have different conceptual and internal mo- dels, they cannot interact and exchange information in a network environment. This report discusses some of the problems and provides a methodology for communicating and exchanging information between hetrogeneous models of data or DBMSs that are commercially available.|E. Nahouraii,L. O. Brooks,Alfonso F. Cardenas"],["58157|GECCO|2007|Towards billion-bit optimization via a parallel estimation of distribution algorithm|This paper presents a highly efficient, fully parallelized implementation of the compact genetic algorithm (cGA) to solve very large scale problems with millions to billions of variables. The paper presents principled results demonstrating the scalable solution of a difficult test function on instances over a billion variables using a parallel implementation of cGA. The problem addressed is a noisy, blind problem over a vector of binary decision variables. Noise is added equaling up to a tenth of the deterministic objective function variance of the problem, thereby making it difficult for simple hillclimbers to find the optimal solution. The compact GA, on the other hand, is able to find the optimum in the presence of noise quickly, reliably, and accurately, and the solution scalability follows known convergence theories. These results on noisy problem together with other results on problems involving varying modularity, hierarchy, and overlap foreshadow routine solution of billion-variable problems across the landscape of search problems.|Kumara Sastry,David E. Goldberg,Xavier Llorà","58328|GECCO|2008|Hybridizing an evolutionary algorithm with mathematical programming techniques for multi-objective optimization|In recent years, the development of multi-objective evolutionary algorithms (MOEAs) hybridized with mathematical programming techniques has significantly increased. However, most of these hybrid approaches are gradient-based, and tend to require a high number of extra objective function evaluations to estimate the gradient information required. The use of nonlinear optimization approaches taken from the mathematical programming literature has been, however, less popular (although such approaches have been used with single-objective evolutionary algorithms). This paper precisely focuses on the design of a hybrid between a well-known MOEA (the NSGA-II) and two direct search methods taken from the mathematical programming literature (Nelder and Mead.s method and the golden section algorithm). The idea is to combine the explorative power of the evolutionary algorithm with the exploitative power of the direct search methods previously indicated (one is used for unidimensional functions and the other for multidimensional functions). Clearly, these mathematical programming techniques act as local search engines, whose goal is to refine the search performed by the MOEA. Our preliminary results indicate that this sort of hybridization is quite promising.|Saúl Zapotecas Martínez,Carlos A. Coello Coello","58522|GECCO|2008|A multi-objective ant colony approach for pareto-optimization using dynamic programming|This paper covers a multi-objective Ant Colony Optimization, which is applied to the NP-complete multi-objective shortest path problem in order to approximate Pareto-fronts. The efficient single-objective solvability of the problem is used to improve the results of the ant algorithm significantly. A dynamic program is developed which generates local heuristic values on the edges of the problem graph. These heuristic values are used by the artificial ants.|Sascha Häckel,Marco Fischer,David Zechel,Tobias Teich","59076|GECCO|2010|An archived-based stochastic ranking evolutionary algorithm asrea for multi-objective optimization|In this paper, we propose a new multi-objective optimization algorithm called Archived-based Stochastic Ranking Evolutionary Algorithm (ASREA) that ranks the population by comparing individuals with members of an archive. The stochastic comparison breaks the usual O(mn) complexity into O(man) (m being the number of objectives, a the size of the archive and n the population size), whereas updating the archive with distinct and well-spread non-dominated solutions and developed selection strategy retain the quality of state of the art deterministic multi-objective evolutionary algorithms (MOEAs). Comparison on ZDT and -objective DTLZ functions shows that ASREA converges on the Pareto-optimal front at least as well as NSGA-II and SPEA while reaching it much faster, and being cheaper on ranking comparisons.|Deepak Sharma,Pierre Collet","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","58714|GECCO|2009|Solving complex high-dimensional problems with the multi-objective neural estimation of distribution algorithm|The multi-objective optimization neural estimation of distribution algorithm (MONEDA) was devised with the purpose of dealing with the model-building issues of MOEDAs and, therefore address their scalability. In this paper we put forward a comprehensive set of experiments that intends to compare MONEDA with similar approaches when solving complex community accepted MOPs. In particular, we deal with the Walking Fish Group scalable test problem set (WFG). These tests aim to establish the optimizing capacity of MONEDA and the consistency as an optimization method. The fundamental conclusion of these assessment is that we provide strong evidences of the viability of MONEDA for handling hard and complex high-dimensional problems and its superior performance when compared to similar approaches. In spite of the fact that obviously further studies are necessary, these extensive experiments have provided solid ground for the use of MONEDA in more ambitious real-world applications.|Luis Martí,Jesús García,Antonio Berlanga,José M. Molina","59013|GECCO|2010|A sphere-dominance based preference immune-inspired algorithm for dynamic multi-objective optimization|Real-world optimization involving multiple objectives in changing environment known as dynamic multi-objective optimization (DMO) is a challenging task, especially special regions are preferred by decision maker (DM). Based on a novel preference dominance concept called sphere-dominance and the theory of artificial immune system (AIS), a sphere-dominance preference immune-inspired algorithm (SPIA) is proposed for DMO in this paper. The main contributions of SPIA are its preference mechanism and its sampling study, which are based on the novel sphere-dominance and probability statistics, respectively. Besides, SPIA introduces two hypermutation strategies based on history information and Gaussian mutation, respectively. In each generation, which way to do hypermutation is automatically determined by a sampling study for accelerating the search process. Furthermore, The interactive scheme of SPIA enables DM to include hisher preference without modifying the main structure of the algorithm. The results show that SPIA can obtain a well distributed solution set efficiently converging into the DM's preferred region for DMO.|Ruochen Liu,Wei Zhang,Licheng Jiao,Fang Liu,Jingjing Ma","57186|GECCO|2003|Reinforcement Learning Estimation of Distribution Algorithm|This paper proposes an algorithm for combinatorial optimizations that uses reinforcement learning and estimation of joint probability distribution of promising solutions to generate a new population of solutions. We call it Reinforcement Learning Estimation of Distribution Algorithm (RELEDA). For the estimation of the joint probability distribution we consider each variable as univariate. Then we update the probability of each variable by applying reinforcement learning method. Though we consider variables independent of one another, the proposed method can solve problems of highly correlated variables. To compare the efficiency of our proposed algorithm with other Estimation of Distribution Algorithms (EDAs) we provide the experimental results of the two problems four peaks problem and bipolar function.|Topon Kumar Paul,Hitoshi Iba","58339|GECCO|2008|Introducing MONEDA scalable multiobjective optimization with a neural estimation of distribution algorithm|In this paper we explore the model-building issue of multiobjective optimization estimation of distribution algorithms. We argue that model-building has some characteristics that differentiate it from other machine learning tasks. A novel algorithm called multiobjective neural estimation of distribution algorithm (MONEDA) is proposed to meet those characteristics. This algorithm uses a custom version of the growing neural gas (GNG) network specially meant for the model-building task. As part of this work, MONEDA is assessed with regard to other classical and state-of-the-art evolutionary multiobjective optimizers when solving some community accepted test problems.|Luis Martí,Jesús García,Antonio Berlanga,José Manuel Molina","58010|GECCO|2007|Global multiobjective optimization via estimation of distribution algorithm with biased initialization and crossover|Multiobjective optimization problems with many local Pareto fronts is a big challenge to evolutionary algorithms. In this paper, two operators, biased initialization and biased crossover, are proposed to improve the global search ability of RM-MEDA, a recently proposed multiobjective estimation of distribution algorithm. Biased initialization inserts several globally Pareto optimal solutions into the initial population biased crossover combines the location information of some best solutions found so far and globally statistical information extracted from current population. Experiments have been conducted to study the effects of these two operators.|Aimin Zhou,Qingfu Zhang,Yaochu Jin,Bernhard Sendhoff,Edward P. K. Tsang"],["57057|GECCO|2003|A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem|We propose a hybrid genetic algorithm for the hexagonal tortoise problem. We combined the genetic algorithm with an efficient local heuristic and aging mechanism. Another search heuristic which focuses on the space around existing solutions is also incorporated into the genetic algorithm. With the proposed algorithm, we could find the optimal solutions of up to a fairly large problem.|Heemahn Choe,Sung-Soon Choi,Byung Ro Moon","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58491|GECCO|2008|Convergence behavior of the fully informed particle swarm optimization algorithm|The fully informed particle swarm optimization algorithm (FIPS) is very sensitive to changes in the population topology. The velocity update rule used in FIPS considers all the neighbors of a particle to update its velocity instead of just the best one as it is done in most variants. It has been argued that this rule induces a random behavior of the particle swarm when a fully connected topology is used. This argument could explain the often observed poor performance of the algorithm under that circumstance. In this paper we study experimentally the convergence behavior of the particles in FIPS when using topologies with different levels of connectivity. We show that the particles tend to search a region whose size decreases as the connectivity of the population topology increases. We therefore put forward the idea that spatial convergence, and not a random behavior, is the cause of the poor performance of FIPS with a fully connected topology. The practical implications of this result are explored.|Marco Antonio Montes de Oca,Thomas Stützle","57274|GECCO|2005|Heuristic rules embedded genetic algorithm to solve in-core fuel management optimization problem|Because of the large number of possible combinations for the fuel assembly loading in the core, the design of the loading pattern (LP) is a complex optimization problem. It requires finding an optimal fuel arrangement in order to achieve maximum cycle length while satisfying the safety constraints. The objective of this study is to develop a loading pattern optimization code. Generally in-core fuel management codes are written for specific cores and limited fuel inventory. One of the goals of this study is to develop a loading pattern optimization code, which is applicable for all types of Pressurized Water Reactor (PWR) core structures with unlimited number of fuel assembly types in the inventory. To reach this goal an innovative genetic algorithm is developed with modifying the classical representation of the genotype. To obtain the best result in a shorter time not only the representation is changed but also the algorithm is changed to use in-core fuel management heuristics rules. The improved GA code was tested demonstrating the advantages of the introduced enhancements. The core physics code used in this research is Moby-Dick, which was developed to analyze the VVER reactors by SKODA Inc.|Fatih Alim,Kostadin Ivanov","57037|GECCO|2003|A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem|Recently proved successful for variants of the vehicle routing problem (VRP) involving time windows, genetic algorithms have not yet shown to compete or challenge current best search techniques in solving the classical capacitated VRP. In this paper, a hybrid genetic algorithm to address the capacitated vehicle routing problem is proposed. The basic scheme consists in concurrently evolving two populations of solutions to minimize total traveled distance using genetic operators combining variations of key concepts inspired from routing techniques and search strategies used for a time-variant of the problem to further provide search guidance while balancing intensification and diversification. Results from a computational experiment over common benchmark problems report the proposed approach to be very competitive with the best-known methods.|Jean Berger,Mohamed Barkaoui","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens","58759|GECCO|2009|The singly-linked ring topology for the particle swarm optimization algorithm|This paper introduces a new neighborhood structure for Particle Swarm Optimization, called Singly-Linked Ring. The approach proposes a neighborhood whose members share the information at a different rate. The objective is to avoid the premature convergence of the flock and stagnation into local optimal. The approach is applied at a set of global optimization problems commonly used in the literature. The singly-linked structure is compared against the state-of-the-art neighborhoods structures. The proposal is easy to implement, and its results and its convergence performance are better than other structures.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce"],["58889|GECCO|2010|Fitting multi-planet transit models to photometric time-data series by evolution strategies|In this paper we present the application of an evolution strategy to the problem of detecting multi-planet transit events in photometric time-data series. Planetary transits occur when a planet regularly eclipses its host star, reducing stellar luminosity. The transit method is amongst the most successful detection methods for exoplanet and is presently performed by space telescope missions. The goal of the presented algorithm is to find high quality fits of multi-planet transit models to observational data, which is a challenging computational task. In particular we present a method for an effective objective function evaluation and show how the algorithm can be implemented on graphics processing units. Results on artificial test data with three artificial planets are reported.|Andreas M. Chwatal,Günther R. Raidl,Michael Zöch","58066|GECCO|2007|Automatic generation of benchmarks for plagiarism detection tools using grammatical evolution|Student plagiarism is a mayor problem in universities worldwide. In this paper,we focus on plagiarism in answers to computer programming assignments,where student mix andor modify one or more original solutions to obtain counterfeits. Although several software tools have been implemented to help the tedious and time consuming task of detecting plagiarism, little has been done to assess their quality, because, in fact, determining the original subset of the whole solutionset is practically impossible for graders. In this article we present a Grammatical Evolution technique which generates benchmarks. Given a programming language, our technique generates a set of original solutions to an assignment, together with a set of plagiarisms of the former set which mimic the way in which students act. The phylogeny of the coded solutions is predefined, providing a base for the evaluationof the performance of copy-catching tools. We give empirical evidence of the suitability of our approach by studying the behavior of one state-of-the-art detection tool (AC) on four benchmarks coded in APL, generated with this technique.|Manuel Cebrián,Manuel Alfonseca,Alfonso Ortega","57150|GECCO|2003|Evolution Strategies with Exclusion-Based Selection Operators and a Fourier Series Auxiliary Function|To improve the efficiency of the currently known evolutionary algorithms, we have proposed two complementary efficiency speed-up strategies in our previous research work respectively the exclusion-based selection operators and the Fourier series auxiliary function. In this paper, we combine these two strategies together to search the global optima in parallel, one for optima in large attraction basins and the other for optima in very narrow attraction basins respectively. They can compliment each other to improve evolutionary algorithms (EAs) on efficiency and safety. In a case study, the two strategies have been incorporated into evolution strategies (ES), yielding a new type of accelerated exclusion and Fourier series auxiliary function ES the EFES. The EFES is experimentally tested with a test suite containing  complex multimodal function optimization problems and compared against the standard ES (SES). The experiments all demonstrate that the EFES consistently and significantly outperforms the SES in efficiency and solution quality.|Kwong-Sak Leung,Yong Liang","58770|GECCO|2009|Efficient natural evolution strategies|Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.|Yi Sun,Daan Wierstra,Tom Schaul,Jürgen Schmidhuber","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,Jörn Mehnen,Thomas Michelitsch","57728|GECCO|2006|Mixed-integer optimization of coronary vessel image analysis using evolution strategies|In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.|Rui Li,Michael Emmerich,Jeroen Eggermont,Ernst G. P. Bovenkamp","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","57525|GECCO|2005|Niching in evolution strategies|EAs have the tendency to converge quickly into a single solution. Niching methods, the extension of EAs to address this issue, have been investigated up to date mainly within the field of Genetic Algorithms (GAs). In our study we investigate the basis for niching methods within Evolution Strategies (ES), and propose the first ES niching method. Results show that this method can reliably find and maintain multiple niches even for high-dimensional problems.|Ofer M. Shir,Thomas Bäck","58961|GECCO|2010|Fast parallelization of differential evolution algorithm using MapReduce|MapReduce is a promising programming model for developing distributed applications due to its superb simplicity, scalability and fault tolerance. This paper demonstrates how to apply MapReduce and the open source Hadoop framework for a quick and easy parallelization of the Differential Evolution algorithm. Instead of parallelizing the whole evolution process, our simple solution is to only apply the MR model to the fitness evaluation part, which usually consumes most of the running time. Two alternative approaches are investigated, i.e., population based and data based. Experimental results reveal that even though the population based approach is a better way, the extra cost of Hadoop DFS IO operations and system bookkeeping overhead significantly reduces the benefits of parallelism.|Chi Zhou"],["59094|GECCO|2010|Knowledge mining with genetic programming methods for variable selection in flavor design|This paper presents a novel approach for knowledge mining from a sparse and repeated measures dataset. Genetic programming based symbolic regression is employed to generate multiple models that provide alternate explanations of the data. This set of models, called an ensemble, is generated for each of the repeated measures separately. These multiple ensembles are then utilized to generate information about, (a) which variables are important in each ensemble, (b) cluster the ensembles into different groups that have similar variables that drive their response variable, and (c) measure sensitivity of response with respect to the important variables. We apply our methodology to a sensory science dataset. The data contains hedonic evaluations (liking scores), assigned by a diverse set of human testers, for a small set of flavors composed from seven ingredients. Our approach () identifies the important ingredients that drive the liking score of a panelist and () segments the panelists into groups that are driven by the same ingredient, and () enables flavor scientists to perform the sensitivity analysis of liking scores relative to changes in the levels of important ingredients.|Katya Vladislavleva,Kalyan Veeramachaneni,Matt Burland,Jason Parcon,Una-May O'Reilly","57968|GECCO|2007|An analysis of constructive crossover and selection pressure in genetic programming|A common problem in genetic programming search algorithms is destructive crossover in which the offspring of good parents generally has worse performance than the parents. Designing constructive crossover operators and integrating some local search techniques into the breeding process have been suggested as solutions. This paper reports on experiments demonstrating that premature convergence may happen more often when using these techniques in combination with standard parent selection. It shows that modifying the selection pressure in the parent selection process is necessary to obtain a significant performance improvement.|Huayang Xie,Mengjie Zhang,Peter Andreae","79802|VLDB|1975|The Use of Cluster Analysis in Physical Data Base Design|The physical structure and relative placement of information elements within a data base is critical for the efficient design of a computerized information system which is shared by a community of users. Traditionally the selection among alternative structural designs has been handled largely via heuristics. Recent research has shown that a number of significant design problems can be stated mathematically as nonlinear, integer, zero-one programming problems. In concept, therefore, mathematical programming algorithms can be used to determine \"optimal\" data base designs. In practice, one finds that realistic problems of even modest size are computationally infeasible. This paper presents a means for overcoming this difficulty in the design of data base records. A metric with which to measure the similarity of usage among data items is developed and used by a clustering algorithm to reduce the space of alternative designs to a point where solution is economically feasible.|Jeffrey A. Hoffer,Dennis G. Severance","57627|GECCO|2006|Pareto front genetic programming parameter selection based on design of experiments and industrial data|Symbolic regression based on Pareto Front GP is the key approach for generating high-performance parsimonious empirical models acceptable for industrial applications. The paper addresses the issue of finding the optimal parameter settings of Pareto Front GP which direct the simulated evolution toward simple models with acceptable prediction error. A generic methodology based on statistical design of experiments is proposed. It includes statistical determination of the number of replicates by half-width confidence intervals, determination of the significant inputs by fractional factorial design of experiments, approaching the optimum by steepest ascentdescent, and local exploration around the optimum by Box Behnken or by central composite design of experiments. The results from implementing the proposed methodology to a small-sized industrial data set show that the statistically significant factors for symbolic regression, based on Pareto Front GP, are the number of cascades, the number of generations, and the population size. A second order regression model with high R of . includes the three parameters and their optimal values have been defined. The optimal parameter settings were validated with a separate small sized industrial data set. The optimal settings are recommended for symbolic regression applications using data sets with up to  inputs and up to  data points.|Flor A. Castillo,Arthur K. Kordon,Guido Smits,Ben Christenson,Dee Dickerson","57149|GECCO|2003|Multicriteria Network Design Using Evolutionary Algorithm|In this paper, we revisit a general class of multicriteria multi-constrained network design problems and attempt to solve, in a novel way, with Evolutionary Algorithms (EAs). A major challenge to solving such problems is to capture possibly all the (representative) equivalent and diverse solutions. In this work, we formulate, without loss of generality, a bi-criteria bi- constrained communication network topological design problem. Two of the primary objectives to be optimized are network delay and cost subject to satisfaction of reliability and flowconstraints. This is a NP-hard problem so we use a hybrid approach (for initialization of the population) along with EA. Furthermore, the twoobjective optimal solution front is not known a priori. Therefore, we use a multiobjective EA which produces diverse solution space and monitors convergence the EA has been demonstrated to work effectively across complex problems of unknown nature. We tested this approach for designing networks of different sizes and found that the approach scales well with larger networks. Results thus obtained are compared with those obtained by two traditional approaches namely, the exhaustive search and branch exchange heuristics.|Rajeev Kumar,Nilanjan Banerjee","58301|GECCO|2008|Pareto analysis for the selection of classifier ensembles|The overproduce-and-choose strategy involves the generation of an initial large pool of candidate classifiers and it is intended to test different candidate ensembles in order to select the best performing solution. The ensemble's error rate, ensemble size and diversity measures are the most frequent search criteria employed to guide this selection. By applying the error rate, we may accomplish the main objective in Pattern Recognition and Machine Learning, which is to find high-performance predictors. In terms of ensemble size, the hope is to increase the recognition rate while minimizing the number of classifiers in order to meet both the performance and low ensemble size requirements. Finally, ensembles can be more accurate than individual classifiers only when classifier members present diversity among themselves. In this paper we apply two Pareto front spread quality measures to analyze the relationship between the three main search criteria used in the overproduce-and-choose strategy. Experimental results conducted demonstrate that the combination of ensemble size and diversity does not produce conflicting multi-objective optimization problems. Moreover, we cannot decrease the generalization error rate by combining this pair of search criteria. However, when the error rate is combined with diversity or the ensemble size, we found that these measures are conflicting objective functions and that the performances of the solutions are much higher.|Eulanda Miranda dos Santos,Robert Sabourin,Patrick Maupin","57428|GECCO|2005|Multiplex PCR primer design for gene family using genetic algorithm|The multiplex PCR experiment is to amplify multiple regions of a DNA sequence at the same time by using different primer pairs. Designing feasible primer pairs for multiplex PCR is a tedious task since there are too many constraints to be satisfied. In this paper, a new method for multiplex PCR primer design strategy using genetic algorithm is proposed. The proposed algorithm is able to find a set of suitable primer pairs more efficient and uses a MAP model to speed up the examination of the specificity constraint that is important for gene family sequences. The dry-dock experiment shows that the proposed algorithm finds several sets of primer pairs of gene family sequences for multiplex PCR that not only obey the design properties, but also have specificity.|Hong-Long Liang,Chungnan Lee,Jain-Shing Wu","59109|GECCO|2010|Mixed-integer evolution strategy using multiobjective selection applied to warehouse design optimization|This paper reports about the application of a new variant of multiobjective Mixed-Integer Evolution Strategy to a warehouse design optimization problem. The algorithm is able to deal with real-valued, integer, and discrete nominal input variables in a multiobjective problem, and utilizes a multiobjective selection procedure based on either crowding distance or hypervolume contribution (also called S metric). The warehouse design optimization problem investigated in this study is represented by a warehouse simulator (provided by our collaboration partner) which calculates four warehouse performance measures total handling time, crate fill rate, order latency, and investment cost. Two of those are treated as objectives, while the other two are represented as constraints. As demonstrated by the results, the algorithm generates solutions which cover the whole Pareto front, as opposed to the expert-generated solutions. Moreover, the algorithm is able to find solutions which improve on the expert-generated solutions with respect to both objectives.|Edgar Reehuis,Thomas Bäck","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","57432|GECCO|2005|Primer design for multiplex PCR using a genetic algorithm|Multiplex Polymerase Chain Reaction (PCR) experiments are used for amplifying several segments of the target DNA simultaneously and thereby to conserve template DNA, reduce the experimental time, and minimize the experimental expense. The success of the experiment is dependent on primer design. However, this can be a dreary task as there are many constrains such as melting temperatures, primer length, GC content and complementarity that need to be optimized to obtain a good PCR product. Motivated by the lack of primer design tools for multiplex PCR genotypic assay, we propose a multiplex PCR primer design tool using a genetic algorithm, which is a stochastic approach based on the concept of biological evolution, biological genetics and genetic operations on chromosomes, to find an optimal selection of primer pairs for multiplex PCR experiments. The presented experimental results indicate that the proposed algorithm is capable of finding a series of primer pairs that obeies the design properties in the same tube.|Feng-Mao Lin,Hsien-Da Huang,Hsi-Yuan Huang,Jorng-Tzong Horng"],["79856|VLDB|1977|The Decomposition Versus Synthetic Approach to Relational Database Design|Two of the competing approaches to the logical design of relational databases are the third normal form decomposition approach of Codd and the synthetic approach of Bernstein and others. The synthetic approach seems on the surface to be the more powerful unfortunately, to avoid serious problems, a nonintuitive constraint (the \"uniqueness\" of functional dependencies) must be assumed. We demonstrate the fourth normal form approach, which not only can deal with this difficulty, but which is also more powerful than either of the earlier approaches. The input of the new method includes attributes (potential column names), along with semantic information in the form of functional and multivalued dependencies the output is a \"good\" (fourth normal form) logical design. The new method is semi-automatic, which is especially helpful in the case of a very large database with many attributes that interrelate in complex ways.|Ronald Fagin","79892|VLDB|1977|Decomposition and Composition of a Relational Database|New algorithms for the decomposition and the composition of a relational data base are proposed in this paper. Given a data base specified by a set of attributes and a set of functional dependencies, the decomposition algorithm removes redundancy and abnormalities to generate a nondecomposable schema and a D-tree. After the data base is constructed following this schema, the D-tree is used by the composition algorithm to construct the simplest relational expression that specifies a desired subrelation over an arbitrary set of attributes given as a query by a user.|Yuzuru Tanaka,Takao Tsuda","80012|VLDB|1980|Some Analytic Tools for the Design of Relational Database Systems|Making use of arguments from information theory it is shown that a boolean function can represent multivalued dependencies. A method is described by which a hypergraph can be constructed to represent dependencies in a relation. A new normal form called generalized Boyce-Codd normal form is defined. An explicit formula is derived for representing dependencies that would remain in a projection of a relation. A definition of join is given which makes the derivation of many theoretical results easy. Another definition given is that of information in a relation. The information gets conserved whenever lossless decompositions are involved. It is shown that the use of null elements is important in handling data.|K. K. Nambiar","80385|VLDB|2004|Simlarity Search for Web Services|Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem locating desired web services. Traditional keyword search is insufficient in this context the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over  web-service operations that shows the high recall and precision of our algorithms.|Xin Dong,Alon Y. Halevy,Jayant Madhavan,Ema Nemes,Jun Zhang","79898|VLDB|1978|The Information Preserving Properties of Relational Database Transformations|Several areas of relational database research rely on techniques for transforming the schema (intention) and underlying occurrence structures (extension) of a database. In each of these areas, it is imperative to know what information from the original database is preserved by the restructuring operation. This paper examines the extent to which an important class of relational database transformations preserves information. This is accomplished in two steps. First, a formal characterization of \"information content\" is presented based on the set of functional and nonfunctional associations among attributes of a relational database. Second, formal conditions are presented indicating the extent to which these associations are preserved. Based on these conditions, the information preserving properties of several well-known relational transformations are described. Some problems in maintaining these properties under updates are also discussed.|Adarsh K. Arora,C. Robert Carlson","79903|VLDB|1978|Performance Study of a Database Machine in Supporting Relational Databases|Database machines are special-purpose devices that are expected to perform the common data management operations efficiently. In this paper, we attempt to show how a relational database can be supported on a specific database machine, known as the database computer (DBC), with good performance. The DBC employs modified moving-head disks for database storage. To achieve high-volumed accessing, the read-out mechanisms of the moving-head disks are made into tracks-in-parallel. To provide content-addressable search, the disk controller is incorporated with a set of microprocessors, corresponding to the tracks of a cylinder. In this way, not only can an entire cylinder of data be accessed in one disk revolution, but relevant data which satisfies,the user request can also be found and output in the same revolution. To minimize the number of cylinders involved in a database access, some structural information about the database is maintained in a blockoriented content-addressable memory made of charge-coupled devices (CCDs). Furthermore, clustering and security mechanisms are a part of the hardware features provided by the DBC. With cylinder-oriented content-addressable database store, block-oriented content-addressable structure memory and several functionally specialized components, the DBC can achieve one or two orders of magnitude of performance improvement over the conventional computer in database management. Also, a possible twofold increase in database storage requirement as compared to a conventional implementation is adequately offset by one or more orders of magnitude reduction in storage for structural information. The purpose of this paper is to analyze these performance issues. By using the DBC for supporting relational databases, the size of the relational software is considerably reduced. Specifically, the query optimizer of conventional systems is now rendered unnecessary. In comparison with a conventional implementation of a relational system, the DBC has been found to contribute larger performance gains. These gains are tabulated in the paper. All these tend to demonstrate that the DBC in particular and database machines in general can indeed contribute to an appreciable improvement in database management.|Jayanta Banerjee,David K. Hsiao","80095|VLDB|1985|An Evaluation of Buffer Management Strategies for Relational Database Systems|In this paper we present a new algorithm, DBMIN, for managing the buffer pool of a relational database manaegememt system. DBMIN is based on a new model of relational query behavior, the query locality set model (QLSM). Like the hot set model, the QLSM has an advantage over the stochastic models due to its ability to predict future reference hehavior. However, the QLSM avoids the potential problems of the hot set model by separating the modeling of referr-ence bahavior from any particular buffer management algorithm. After introducing the QLSM and describing the DBMIN algorithm, we present a performance evaluation methodology for evaluating buffer manage-ment algorithms in a multiuser environment. This methodology employed a hybrid model that comhines features of both trace driven and distribution driven simulation models. Using this model the performance of the DBMIN algorithm in a multiuser environment is compared with that of the hot set algorithm and four more traditional buffer replacement algorithms.|Hong-Tai Chou,David J. DeWitt","80300|VLDB|2003|Balancing Performance and Data Freshness in Web Database Servers|Personalization, advertising, and the sheer volume of online data generate a staggering amount of dynamic web content. In addition to web caching, View Materialization has been shown to accelerate the generation of dynamic web content. View materialization is an attractive solution as it decouples the serving of access requests from the handling of updates. In the context of the Web, selecting which views to materialize must be decided online and needs to consider both performance and data freshness, which we refer to as the Online View Selection problem. In this paper, we define data freshness metrics, provide an adaptive algorithm for the online view selection problem, and present experimental results.|Alexandros Labrinidis,Nick Roussopoulos","80435|VLDB|2004|Indexing XML Data Stored in a Relational Database|As XML usage grows for both data-centric and document-centric applications, introducing native support for XML data in relational databases brings significant benefits. It provides a more mature platform for the XML data model and serves as the basis for interoperability between relational and XML data. Whereas query processing on XML data shredded into one or more relational tables is well understood, it provides limited support for the XML data model. XML data can be persisted as a byte sequence (BLOB) in columns of tables to support the XML model more faithfully. This introduces new challenges for query processing such as the ability to index the XML blob for good query performance. This paper reports novel techniques for indexing XML data in the upcoming version of Microsoft SQL ServerTM, and how it ties into the relational framework for query processing.|Shankar Pal,Istvan Cseri,Gideon Schaller,Oliver Seeliger,Leo Giakoumakis,Vasili Vasili Zolotov","80199|VLDB|2002|Distributed Search over the Hidden Web Hierarchical Database Sampling and Selection|Many valuable text databases on the web have non-crawlable contents that are \"hidden\" behind search interfaces. Metasearchers are helpful tools for searching over many such databases at once through a unified query interface. A critical task for a metasearcher to process a query efficiently and effectively is the selection of the most promising databases for the query, a task that typically relies on statistical summaries of the database contents. Unfortunately, web-accessible text databases do not generally export content summaries. In this paper, we present an algorithm to derive content summaries from \"uncooperative\" databases by using \"focused query probes,\" which adaptively zoom in on and extract documents that are representative of the topic coverage of the databases. Our content summaries are the first to include absolute document frequency estimates for the database words. We also present a novel database selection algorithm that exploits both the extracted content summaries and a hierarchical classification of the databases, automatically derived during probing, to compensate for potentially incomplete content summaries. Finally, we evaluate our techniques thoroughly using a variety of databases, including  real web-accessible text databases. Our experiments indicate that our new content-summary construction technique is efficient and produces more accurate summaries than those from previously proposed strategies. Also, our hierarchical database selection algorithm exhibits significantly higher precision than its flat counterparts.|Panagiotis G. Ipeirotis,Luis Gravano"],["79879|VLDB|1977|A Survey The Application of Data Base Management Computers in Distributed Systems|Data Base Management Computers (DBMC) are special purpose computers in distributed systems. The dedicated role of the DBMC is to control the devices upon which data bases reside and to provide data base management services for other computers in the system. and how it relates to other computers, Depending upon how it operates a given DBMC can be classified either as an intelligent controller, a backend or a datacomputer. Prototypes of each class have been proposed and implemented. In addition there is a growing body of literature devoted to DBMC technology -- objectives, requirements, problems, performance issues, etc. The purpose of this paper is to present a survey of recent research and then to examine special DBMC requirements in the increasingly important areas of very large data bases and distributed data bases.|Eugene I. Lowenthal","80022|VLDB|1980|Status Report on ISOTCSCWG - Data Base Management Systems|This paper discussed the current status of the work of the ISOTCSCWG data base management systems working group. In existence for over five years, this working group has devoted it energies to the elaboration and extension of the notion of a \"conceptual schema\" as initially defined by the ANSIXSPARCDBMS Study Group. The focus of the ISO work has been the construction of a report on conceptual schema concepts, identification of various methodologies for their construction and the application of such methodologies to an example \"world\" in such a fashion that comparisons can be made between the different approaches. Underlying the work of this group has been the identification of fundamental philosophical differences in the various methodologies specifically that some assume the conceptual schema models the real world and others assume the conceptual schema models the data in the information base that holds information about the real world. This difference leads to considerable confusion in discussion when it is not explicitly recognized and one of the principal objectives of the ISOTCSCWG report is to expurgate that difference.|Thomas B. Steel Jr.","79899|VLDB|1978|Issues in Distributed Data Base Management Systems A Technical Overview|Distributed Data Base Management Systems (D-DBMS) are the converging point of apparently contrasting areas networking and Data Bases, representative of distribution and integration respectively. Users should benefit from this new dimension when trying to make data processing architectures conform to managerial and organizational philosophies. This paper is a technical overview of research and development in D-DMBS. In the first section, various architectures and design strategies are examined, followed by a description of functional layers. The second section is dedicated to Data Base design and mappings between levels of representations. The last section addresses Integrity control and Recovery.|Michel E. Adiba,Jean-Claude Chupin,Robert Demolombe,Georges Gardarin,Jean Le Bihan","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","79929|VLDB|1978|An Alternative Structure for Data Base Management Systems|The ANSIXSPARC proposal of standardization for architecture of Data Base Management Systems is particularly concerned with the achievement of iogical data independence. This proposal has almost universally been recognized as valid. In this paper we argue that this proposal is not a standardization of Data Base Management Systems features, but rather an implementation proposal, based on schemas mappings and transforms. A more general architecture is proposed and an implementation other than the one of the ANSIXSPARC is examined. Tradeoffs between the two implementation strategies are outlined.|Paolo Paolini","79923|VLDB|1978|A Search Processor for Data Base Management Systems|A special processor for performing the selection and restriction operation of data base management systems by linearly searching large files with very high speed is described. It includes the reasoning, leading to the architecture and the structure of the search processor and a report on the status and the plans of a research project.|Hans-Otto Leilich,Günther Stiege,Hans Christoph Zeidler","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","80070|VLDB|1981|IML-Inscribed Nets for Modeling Text Processing and Data Base Management Systems|Predicatetransition-nets with a suitable inscription allow building practice-oriented and expressive models of distributed information systems with concurrent processes as found in office information systems or decentralized database management systems. The paper presents the first version of an inscription language called IML (Information Management Language) which has been derived from the results of some long range database research activities. The abstract objects \"construct\" and \"form\" are introduced and basic operations are defined on them. It is shown how to apply these language elements to inscribe net symbols for high-fidelity modeling of text processing and data management.|Gernot Richter","79814|VLDB|1975|A Multi-Level Architecture for Relational Data Base Systems|Most of the literature on implementation of relations has been directed toward user features, with little attention paid to an overall conceptual view of underlying structures. Performance oriented considerations have been treated only for isolated problems. Toward a solution to these problems we describe a multi-level architecture for relational data base systems. This architecture distinguishes clearly between user oriented features, access path structures, data structures and file organization. It also allows efficiency problems to be isolated within levels and solved independently of each other, without impacting the logical structure of the user's virtual machine. Specific problems considered here in the context of this architecture include the mapping of relations into files, the implementation of fast access paths, and some file level optimizations that are particularly useful in relational systems.|Hans Albrecht Schmid,Philip A. Bernstein","79840|VLDB|1976|An Approach to Data Communication between Different Generalized Data Base Management Systems|A number of data base management systems (DBMS) are now in operation in industry, government and the academic world. However, since most of these DBMSs have different conceptual and internal mo- dels, they cannot interact and exchange information in a network environment. This report discusses some of the problems and provides a methodology for communicating and exchanging information between hetrogeneous models of data or DBMSs that are commercially available.|E. Nahouraii,L. O. Brooks,Alfonso F. Cardenas"],["58674|GECCO|2009|The bee colony-inspired algorithm BCiA a two-stage approach for solving the vehicle routing problem with time windows|The paper presents a new optimization algorithm, which adapts the behavior of honey bees during their search for nectar. In addition to the established ant algorithms, bee-inspired algorithms represent a relatively young form of solution procedures, whose applicability to the solution of complex optimization problems has already been shown. The herein presented two-stage approach belongs to the class of metaheuristics to control a construction heuristic and has been applied successfully to the NP-hard Vehicle Routing Problem with Time Windows (VRPTW). Within the paper, evaluation results are presented, which compare the developed algorithm to some of the most successful procedures for the solution of benchmark problems. The pursued approach gives the best results so far for a metaheuristic to control a construction heuristic.|Sascha Häckel,Patrick Dippold","59075|GECCO|2010|A neuro-evolutionary approach to micro aerial vehicle control|Applying classical control methods to Micro Aerial Vehicles (MAVs) is a difficult process due to the complexity of the control laws with fast and highly non-linear dynamics. Such methods rely heavily on difficult to obtain models and are particularly ill-suited to the stochastic and dynamic environments in which MAVs operate. Instead, in this paper, we focus on a neuro-evolutionary method that learns to map MAV states (position, velocity) to MAV actions (e.g., actuator position). Our results show significant improvements in response times to minor altitude and heading corrections over a traditional PID controller. In addition, we show that the MAV response to maintaining altitude in the presence of wind gusts improves by a factor of five. Similarly, we show that the MAV response to maintaining heading in the presence of turbulence improves by factors of three.|Max Salichon,Kagan Tumer","57256|GECCO|2003|An Evolutionary Approach for Molecular Docking|We have developed an evolutionary approach for the flexible docking that is now an important component of a rational drug design. This automatic docking tool, referred to as the GEMDOCK (Generic Evolutionary Method for DOCKing molecules), combines both global and local search strategies search mechanisms. GEMDOCKused a simple scoring function to recognize compounds by minimizing the energy of molecular interactions. The interactive types of atoms between ligands and proteins of our linear scoring function consist only hydrogen-bonding and steric terms. GEMDOCK has been tested on a diverse dataset of  protein-ligand complexes from Protein Data Bank. In total % of these complexes, it obtained docked ligand conformations with root mean square derivations (RMSD) to the crystal ligand structures less than .  when the ligand was docked back into the binding site. Experiments shows that the scoring function is simple and efficiently discriminates between native and non-native docked conformations. This study suggests that GEMDOCK is a useful tool for molecular recognition and is a potential docking tool for protein structure variations.|Jinn-Moon Yang","58193|GECCO|2007|An evolutionary approach to collective communication scheduling|In this paper, we describe two evolutionary algorithms aimed at scheduling collective communications on interconnection networks of parallel computers. To avoid contention for links and associated delays, collective communications proceed in synchronized steps. Minimum number of steps is sought for the given network topology, wormhole (pipelined) switching, minimum routing and given sets of sender andor receiver nodes. Used algorithms are able not only re-invent optimum schedules for known symmetric topologies like hyper-cubes, but they can find schedules even for any asymmetric or irregular topologies in case of general many-to-many collective communications. In most cases does the number of steps reach the theoretical lower bound for the given type of collective communication if it does not, non-minimum routing can provide further improvement. Optimum schedules may serve for writing high-performance communication routines for application-specific networks on chip or for development of communication libraries in case of general-purpose interconnection networ.|Jirí Jaros,Milos Ohlídal,Vaclav Dvorak","58812|GECCO|2009|An evolutionary approach to underwater sensor deployment|Underwater acoustic sensor deployment for military surveillance is a significant challenge due to the inherent difficulties posed by the underwater channel in terms of sensing and communications between sensors, as well as the exorbitant cost of the sensors. Thus, these sensors must be deployed as efficiently as possible. The proposed Underwater Sensor Deployment Evolutionary Algorithm (USDEA) considers six important factors that have not yet been simultaneously considered due to the ensuing complexity of the problem. Sensing capability tradeoffs are shown through simulation of two sensor network topologies, mesh and cluster.|Erik F. Golen,Bo Yuan,Nirmala Shenoy","58523|GECCO|2008|An enhanced statistical approach for evolutionary algorithm comparison|This paper presents an enhanced approach for comparing evolutionary algorithm. This approach is based on three statistical techniques (a) Principal Component Analysis, which is used to make the data uncorrelated (b) Bootstrapping, which is employed to build the probability distribution function of the merit functions and (c) Stochastic Dominance Analysis, that is employed to make possible the comparison between two or more probability distribution functions. Since the approach proposed here is not based on parametric properties, it can be applied to compare any kind of quantity, regardless the probability distribution function. The results achieved by the proposed approach have provided more supported decisions than former approaches, when applied to the same problems.|Eduardo G. Carrano,Ricardo H. C. Takahashi,Elizabeth F. Wanner","57592|GECCO|2006|A neural evolutionary approach to financial modeling|This paper presents an approach to the joint optimization of neural network structure and weights which can take advantage of backpropagation as a specialized decoder. The approach has been applied to a financial problem, whereby a factor model capturing the mutual relationships among several financial instruments is sought for. A sample application of such a model to statistical arbitrage is also presented.|Antonia Azzini,Andrea Tettamanzi","58595|GECCO|2009|An evolutionary approach to planning IEEE  networks|Efficient and effective deployment of IEEE . networks to service an area of users with certain traffic demands is an important network planning problem. We resort to an evolutionary approach in order to yield good approximation solutions. In our method, novel genetic variation operations are proposed to incorporate the feature of this real-world application of evolutionary algorithm.|Ting Hu,Yuanzhu Peter Chen,Wolfgang Banzhaf,Robert Benkoczi","57779|GECCO|2006|Adaption in distributed systems an evolutionary approach|There is a trend towards networked and distributed systems, complicating the design process of self-adaptive software. Logistics networks can be seen as a distributed system that have to adapt to requirements of companies and customers in a flexible and fast manner. When constructing and planning logistic networks different aspects of complexity have to be considered the number of stores, intermediate stores and transport entities that are required at every stage in a supply chain as well as the sufficient size of every store or transport entity. This paper presents an approach that simulates adaptive logistic networks using a multi-agent system (MAS) based on Evolutionary Computation (EC). Our approach uses fully decentralized operators for reproduction like mutation, recombination and selection, regulated by market mechanisms. The novelty of this approach lies in the decentralized bottom-up adaption method for decentralized systems and we use a logistic scenario as an example. Our proposed method is based on a formal model explaining how adaption occurs in the number and strategies of agents and thus of logistic networks. The implementation and experimental results are given to illustrate the expected outcomes.|Stephan Otto,Stefan Kirn","58099|GECCO|2007|Automatic analog IC layout generation based on a evolutionary computation approach|This paper describes an innovative analog IC layout generation approach based on evolutionary computation techniques.|Nuno C. Lourenço,Nuno C. G. Horta"],["58941|GECCO|2010|On the problems of using learning classifier systems for fraud detection|Fraud detection problems have some uniquely challenging properties which make them difficult. In this paper, we investigate the fraud detection problem by describing the common properties of electronic fraud and examining how learning classifier systems (LCSs) can be applied to it. Also, we introduce \"random Boolean function\" (RBF) an abstract problem with high level of controllability which can be tuned to exhibit those characteristics individually, and report the results of using XCSR (a continuous variant of LCS) on RBF problem and also on a real-world problem. Results from our experiments demonstrate that XCSR can overcome most of the difficulties inherent to the fraud detection problem and can achieve good performance in case of the real-world problem.|Mohammad Behdad,Tim French,Luigi Barone,Mohammed Bennamoun","57747|GECCO|2006|Multi-step environment learning classifier systems applied to hyper-heuristics|Heuristic Algorithms (HA) are very widely used to tackle practical problems in operations research. They are simple, easy to understand and inspire confidence. Many of these HAs are good for some problem instances while very poor for other cases. While Meta-Heuristics try to find which is the best heuristic andor parameters to apply for a given problem instance Hyper-Heuristics (HH) try to combine several heuristics in the same solution searching process, switching among them whenever the circumstances vary. Besides, instead to solve a single problem instance it tries to find a general algorithm to apply to whole families of problems. HH use evolutionary methods to search for such a problem-solving algorithm and, once produced, to apply it to any new problem instance desired. Learning Classifier Systems (LCS), and in particular XCS, represents an elegant and simple way to try to fabricate such a composite algorithm. This represents a different kind of problem to those already studied by the LCS community. Previous work, using single step environments, already showed the usefulness of the approach. This paper goes further and studies the novel use of multi-step environments for HH and an alternate way to consider states to see if chains of actions can be learnt. A non-trivial, NP-hard family of problems, the Bin Packing one, is used as benchmark for the procedure. Results of the approach are very encouraging, showing outperformance over all HAs used individually and over previously reported work by the authors, including non-LCS (a GA based approach used for the same BP set of problems) and LCS (using single step environments).|Javier G. Marín-Blázquez,Sonia Schulenburg","57595|GECCO|2006|Coordination number prediction using learning classifier systems performance and interpretability|The prediction of the coordination number (CN) of an amino acid in a protein structure has recently received renewed attention. In a recent paper, Kinjo et al. proposed a real-valued definition of CN and a criterion to map it onto a finite set of classes, in order to predict it using classification approaches. The literature reports several kinds of input information used for CN prediction. The aim of this paper is to assess the performance of a state-of-the-art learning method, Learning Classifier Systems (LCS) on this CN definition, with various degrees of precision, based on several combinations of input attributes. Moreover, we will compare the LCS performance to other well-known learning techniques. Our experiments are also intended to determinethe minimum set of input information needed to achieve good predictive performance, so as to generate competent yet simple and interpretable classification rules. Thus, the generated predictors (rule sets) are analyzed for their interpretability.|Jaume Bacardit,Michael Stout,Natalio Krasnogor,Jonathan D. Hirst,Jacek Blazewicz","57218|GECCO|2003|Towards Learning Classifier Systems for Continuous-Valued Online Environments|Previous work has studied the use of interval representations in XCS to allow its use in continuous-valued environments. Here we compare the speed of learning of continuous-valued versions of ZCS and XCS with a simple model of an online environment.|Christopher Stone,Larry Bull","57733|GECCO|2006|Fast rule matching for learning classifier systems via vector instructions|Over the last ten years XCS has become the standard for Michigan-style learning classifier systems (LCS). Since the initial CS- work conceived by Holland, classifiers (rules) have widely used a ternary condition alphabet ,, for binary input problems. Most of the freely available implementations of this ternary alphabet in XCS rely on character-based encodings---easy to implement, not memory efficient, and expensive to compute. Profiling of freely available XCS implementations shows that most of their execution time is spent determining whether a rule is match or not, posing a serious threat to XCS scalability. In the last decade, multimedia and scientific applications have pushed CPU manufactures to include native support for vector instruction sets. This paper presents how to implement efficient condition encoding and fast rule matching strategies using vector instructions. The paper elaborates on Altivec (PowerPC G, G) and SSE (Intel PXeon and AMD Opteron) instruction sets producing speedups of XCS matching process beyond ninety times. Moreover, such a vectorized matching code will allow to easily scale beyond tens of thousands of conditions in a reasonable time. The proposed fast matching scheme also fits in any other LCS other than XCS.|Xavier Llorà,Kumara Sastry","57507|GECCO|2005|Event-driven learning classifier systems for online soccer games|This paper reports on the application of classifier systems to the acquisition of decision-making algorithms for agents in online soccer games. The objective of this research is to support changes in the video-game environment brought on by the Internet and to enable the provision of bug-free programs in a short period of time. To achieve real-time learning during a game, a bucket brigade algorithm is used to reinforce learning by classifiers and a technique for selecting learning targets according to event frequency is adopted. A hybrid system combining an existing strategy algorithm and a classifier system is also employed. In experiments that observed the outcome of , soccer games between this event-driven classifier system and a human-designed algorithm, the proposed system was found to be capable of learning effective decision-making algorithms in real time.|Yuji Sato,Ryutaro Kanno","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov","57748|GECCO|2006|A representational ecology for learning classifier systems|The representation used by a learning algorithm introduces a bias which is more or less well-suited to any given learning problem. It is well known that, across all possible problems, one algorithm is no better than any other. Accordingly, the traditional approach in machine learning is to choose an appropriate representation making use of some domain-specific knowledge, and this representation is then used exclusively during the learning process.To reduce reliance on domain-knowledge and its appropriate use it would be desirable for the learning algorithm to select its own representation for the problem.We investigate this with XCS, a Michigan-style Learning Classifier System.We begin with an analysis of two representations from the literature hyperplanes and hyperspheres. We then apply XCS with either one or the other representation to two Boolean functions, the well-known multiplexer function and a function defined by hyperspheres, and confirm that planes are better suited to the multiplexer and spheres to the sphere-based function.Finally, we allow both representations to compete within XCS, which learns the most appropriate representation for problem thanks to the pressure against overlapping rules which its niche GA supplies. The result is an ecology in which the representations are species.|James A. R. Marshall,Tim Kovacs","58397|GECCO|2008|An analysis of matching in learning classifier systems|We investigate rule matching in learning classifier systems for problems involving binary and real inputs. We consider three rule encodings the widely used character-based encoding, a specificity-based encoding, and a binary encoding used in Alecsys. We compare the performance of the three algorithms both on matching alone and on typical test problems. The results on matching alone show that the population generality influences the performance of the matching algorithms based on string representations in different ways. Character-based encoding becomes slower and slower as generality increases, specificity-based encoding becomes faster and faster as generality increases. The results on typical test problems show that the specificity-based representation can halve the time required for matching but also that binary encoding is about ten times faster on the most difficult problems. Moreover, we extend specificity-based encoding to real-inputs and propose an algorithm that can halve the time require for matching real inputs using an interval-based representation.|Martin V. Butz,Pier Luca Lanzi,Xavier Llorà,Daniele Loiacono","57601|GECCO|2006|A Bayesian approach to learning classifier systems in uncertain environments|In this paper we propose a Bayesian framework for XCS , called BXCS. Following , we use probability distributions to represent the uncertainty over the classifier estimates of payoff. A novel interpretation of classifier and an extension of the accuracy concept are presented. The probabilistic approach is aimed at increasing XCS learning capabilities and tendency to evolve accurate, maximally general classifiers, especially when uncertainty affects the environment or the reward function. We show that BXCS can approximate optimal solutions in stochastic environments with a high level of uncertainty.|Davide Aliprandi,Alex Mancastroppa,Matteo Matteucci"],["58381|GECCO|2008|Feasibility-preserving crossover for maximum k-coverage problem|The maximum k-coverage problem is a generalized version of covering problems. We introduce the problem formally and analyze its property in relation to the operators of genetic algorithm. Based on the analysis, we propose a new crossover tailored to the maximum k-coverage problem. While traditional n-point crossovers have a problem of requiring repair steps, the proposed crossover has an additional advantage of always producing feasible solutions. We give a comparative analysis of the proposed crossover through experiments.|Yourim Yoon,Yong-Hyuk Kim,Byung Ro Moon","57106|GECCO|2003|Multi-FPGA Systems Synthesis by Means of Evolutionary Computation|Multi-FPGA systems (MFS) are used for a great variety of applications, for instance, dynamically re-configurable hardware applications, digital circuit emulation, and numerical computation. There are a great variety of boards for MFS implementation. In this paper a methodology for MFS design is presented. The techniques used are evolutionary programs and they solve all of the design tasks (partitioning placement and routing). Firstly a hybrid compact genetic algorithm solves the partitioning problem and then genetic programming is used to obtain a solution for the two other tasks.|José Ignacio Hidalgo,Francisco Fernández de Vega,Juan Lanchares,Juan Manuel Sánchez-Pérez,Román Hermida,Marco Tomassini,Ranieri Baraglia,Raffaele Perego,Oscar Garnica","58334|GECCO|2008|A swarm-based crossover operator for genetic programming|A swarm-based improvement to Genetic Programming (GP) is described and tested on the domain of symbolic regression in this paper. The motivating idea is to keep all of the benefits of genetic programming such as crossover and fitness proportional selection within a population of candidate solutions. The improvement comes in using swarm-based ideas similar to Ant Colony Optimization (ACO) to improve the operation of the crossover operator. Statistically significant results are reported in support of the hypothesis that ACO-inspired crossover can improve GP.|Tony White,Amirali Salehi-Abari","58245|GECCO|2007|Effects of the use of non-geometric binary crossover on evolutionary multiobjective optimization|In the design of evolutionary multiobjective optimization (EMO) algorithms, it is important to strike a balance between diversity and convergence. Traditional mask-based crossover operators for binary strings (e.g., one-point and uniform) tend to decrease the diversity of solutions in EMO algorithms while they improve the convergence to the Pareto front. This is because such a crossover operator, which is called geometric crossover, always generates an offspring in the segment between its two parents under the Hamming distance in the genotype space. That is, the sum of the distances from the generated offspring to its two parents is always equal to the distance between the parents. In this paper, first we propose a non-geometric binary crossover operator to generate an offspring outside the segment between its parents. Next we examine the effect of the use of non-geometric binary crossover on single-objective genetic algorithms. Experimental results show that non-geometric binary crossover improves their search ability. Then we examine its effect on EMO algorithms. Experimental results show that non-geometric binary crossover drastically increases the diversity of solutions while it slightly degrades their convergence to the Pareto front. As a result, some performance measures such as hypervolume are clearly improved.|Hisao Ishibuchi,Yusuke Nojima,Noritaka Tsukamoto,Ken Ohara","58269|GECCO|2008|A Tabu history driven crossover operator design for memetic algorithm applied to Max-SAT-problems|The solution for the Max-SAT is the starting point for a selection of these strategies by a brief review. Moreover, a memetic algorithm for Max-SAT problems based on a specific crossover operator and an improved tabu search stage is presented. Simulation performed on several instances of Max-SAT reference problems are used to evaluate the different memetic algorithm strategies applied in our approach and to compare it to the computational complexity of existing local search solutions.|M. Borschbach,A. Exeler","57232|GECCO|2003|The Structure of Evolutionary Exploration On Crossover Buildings Blocks and Estimation-Of-Distribution Algorithms|Correlations between alleles after selection are an important source of information. Such correlations should be exploited for further search and thereby constitute the building blocks of evolutionary exploration. With this background we analyze the structure of the offspring probability distribution, or exploration distribution, for a simple GA with mutation only and a crossover GA and compare them to Estimation-Of-Distribution Algorithms (EDAs). This will allow a precise characterization of the structure of exploration w.r.t. correlations in the search distribution for these algorithms. We find that crossover transforms, depending on the crossover mask, mutual information between loci into entropy. In total, it can only decrease such mutual information. In contrast, the objective of EDAs is to estimate the correlations between loci and exploit this information during exploration. This may lead to an effective increase of mutual information in the exploration distribution, what we define correlated exploration.|Marc Toussaint","59036|GECCO|2010|A probabilistic functional crossover operator for genetic programming|The original mechanism by which evolutionary algorithms were to solve problems was to allow for the gradual discovery of sub-solutions to sub-problems, and the automated combination of these sub-solutions into larger solutions. This latter property is particularly challenging when recombination is performed on genomes encoded as trees, as crossover events tend to greatly alter the original genomes and therefore greatly reduce the chance of the crossover event being beneficial. A number of crossover operators designed for tree-based genetic encodings have been proposed, but most consider crossing genetic components based on their structural similarity. In this work we introduce a tree-based crossover operator that probabilistically crosses branches based on the behavioral similarity between the branches. It is shown that this method outperforms genetic programming without crossover, random crossover, and a deterministic form of the crossover operator in the symbolic regression domain.|Josh C. Bongard","58156|GECCO|2007|A genetic algorithm with exon shuffling crossover for hard bin packing problems|A novel evolutionary approach for the bin packing problem (BPP) is presented. A simple steady-state genetic algorithm is developed that produces results comparable to other approaches in the literature, without the need for any additional heuristics. The algorithm's design makes maximum use of the principle of natural selection to evolve valid solutions without the explicit need to verify constraint violations. Our algorithm is based upon a biologically inspired group encoding which allows for a modularisation of the search space in which individual sub-solutions may be assigned independent cost values. These values are subsequently utilised in a crossover event modelled on the theory of exon shuffling to produce a single offspring that inherits the most promising segments from its parents. The algorithm is tested on a set of hard benchmark problems and the results indicate that the method has a very high degree of accuracy and reliability compared to other approaches in the literature.|Philipp Rohlfshagen,John A. Bullinaria","57741|GECCO|2006|A crossover operator for the anonymity problem|Recent dissemination of personal data has created an important optimization problem what is the minimal transformation of a dataset that is needed to guarantee the anonymity of the underlying individuals One natural representation for this problem is a bit-string, which makes a genetic algorithm a logical choice for optimization. Unfortunately, under certain realistic conditions, not all bit combinations will represent valid solutions. This means that in many instances, useful solutions are sparse in the search space. We implement a new crossover operator that preserves valid solutions under this representation. Our results show that this reproductive strategy is more efficient, effective, and robust than previous work. We also investigate how the population size and uniqueness can affect the performance of genetic search on this application.|Monte Lunacek,Darrell Whitley,Indrakshi Ray","58449|GECCO|2008|Crossover can provably be useful in evolutionary computation|We show that the natural evolutionary algorithm for the all-pairs shortest path problem is significantly faster with a crossover operator than without. This is the first theoretical analysis proving the usefulness of crossover for a non-artificial problem.|Benjamin Doerr,Edda Happ,Christian Klein"],["57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57439|GECCO|2005|Measuring mobility and the performance of global search algorithms|The global search properties of heuristic search algorithms are not well understood. In this paper, we introduce a new metric, mobility, that quantifies the dispersion of local optima visited during a search. This allows us to explore two questions How disperse are the local optima visited during a search How does mobility relate to algorithm performance We compare local search with two evolutionary algorithms, CHC and CMA-ES, on a set of non-separable, non-symmetric, multi-modal test functions. Given our mobility metric, we show that algorithms visiting more disperse local optima tend to be better optimizers.|Monte Lunacek,L. Darrell Whitley,James N. Knight","58259|GECCO|2008|Comparing genetic algorithm and guided local search methods by symmetric TSP instances|This paper aims at comparing Genetic Algorithm (GA) and Guided Local Search (GLS) methods so as to scrutinize their behaviors. Authors apply the GLS program with the Fast Local Search (FLS), developed at University of Essex, and implement a genetic algorithm with partially-mapped and order crossovers, reciprocal and inversion mutations, and rank and tournament selections in order to experiment with various Travelling Salesman Problems. The paper then ends up with two prominent conclusions regarding the performance of these meta-heuristic techniques over wide range of symmetric-TSP instances. First, the GLS-FLS strategy on the s-TSP instances yields the most promising performance in terms of the near-optimality and the mean CPU time. Second, the GA results are comparable to GLS-FLS outcomes on the same s-TSP instances. In the other word, the GA is able to generate near optimal solutions with some compromise in the CPU time.|Mehrdad Nojoumian,Divya K. Nair","58415|GECCO|2008|Precision local search and unimodal functions|We investigate the effects of precision on the efficiency of various local search algorithms on -D unimodal functions. We present a (+)-EA with adaptive step size which finds the optimum in O(log n) steps, where n is the number of points used. We then consider binary and Gray representations with single bit mutations. The standard binary method does not guarantee locating the optimum, whereas using Gray code does so in O((log n)) steps. A (+)-EA with a fixed mutation probability distribution is then presented which also runs in O((log n)). Moreover, a recent result shows that this is optimal (up to some constant scaling factor), in that there exist unimodal functions for which a lower bound of ((log n)) holds regardless of the choice of mutation distribution. Finally, we show that it is not possible for a black box algorithms to efficiently optimise unimodal functions for two or more dimensions (in terms of the precision used).|Martin Dietzfelbinger,Jonathan E. Rowe,Ingo Wegener,Philipp Woelfel","57463|GECCO|2005|Enhancing differential evolution performance with local search for high dimensional function optimization|In this paper, we proposed Fittest Individual Refinement (FIR), a crossover based local search method for Differential Evolution (DE). The FIR scheme accelerates DE by enhancing its search capability through exploration of the neighborhood of the best solution in successive generations. The proposed memetic version of DE (augmented by FIR) is expected to obtain an acceptable solution with a lower number of evaluations particularly for higher dimensional functions. Using two different implementations DEfirDE and DEfirSPX we showed that proposed FIR increases the convergence velocity of DE for well known benchmark functions as well as improves the robustness of DE against variation of population. Experiments using multimodal landscape generator showed our proposed algorithms consistently outperformed their parent algorithms. A performance comparison with reported results of well known real coded memetic algorithms is also presented.|Nasimul Noman,Hitoshi Iba","58488|GECCO|2008|Genetic local search for rule learning|The performance of Evolutionary Algorithms for combinatorial problems can be significantly improved by adding Local Search, thus obtaining a Genetic Local Search (GLS) also called Memetic Algorithm. In this work, we adapt a previous Stochastic Local Search (SLS) algorithm and embed it into a GBML system. The adapted SLS algorithm works as a module of the system that tries to improve a random individual in the population. We perform experiments to evaluate this adapted SLS procedure and results show that this new GLS system is very effective, not losing in any of the  UCI datasets tested when compared to the system without the SLS procedure. The system either obtained significantly more accurate concepts using lower number of rules and features or it achieved the same accuracy as the system without the SLS procedure, but reduced the number of rules and features, and also the time taken to develop the solution.|Cristiano Grijó Pitangui,Gerson Zaverucha","57869|GECCO|2006|Genetic local search for multicast routing|We describe a population-based search algorithm for cost minimization of multicast routing. The algorithm utilizes the partially mixed crossover operation (PMX) and a landscape analysis in a pre-processing step. The aim of the landscape analysis is to estimate the depth  of the deepest local minima in the landscape generated by the routing tasks and the objective function. The analysis employs simulated annealing with a logarithmic cooling schedule (LSA). The local search performs alternating sequences of descending and ascending steps for each individual of the population, where the length of a sequence with uniform direction is controlled by the estimated value of . We present results from computational experiments on a synthetic routing tasks, and we provide experimental evidence that our genetic local search procedure, that combines LSA and PMX, performs better than algorithms using either LSA or PMX only.|Mohammed S. Zahrani,Martin J. Loomes,James A. Malcolm,Andreas Alexander Albrecht","58365|GECCO|2008|Analysis of the performance of genetic multi-step search in interpolation and extrapolation domain|In this paper, we examine overall performances and behaviors of deterministic multi-step search in interpolation  extrapolation domain, dMSXF and dMSMF, using NK model that is one of appropriate models for analyzing fundamental search mechanisms in combinatorial problems. We focus on the local property of landscape, such as epistasis that is comprehended as ruggedness in fitness function, and investigate the efficacy of dMSXF and dMSMF and the behavior observed by tuning the level of epistasis.|Yoshiko Hanada,Tomoyuki Hiroyasu,Mitsunori Miki","57681|GECCO|2006|Hybridization of genetic algorithm and local search in multiobjective function optimization recommendation of GA then LS|Hybridization with local search (LS) is known to enhance the performance of genetic algorithms (GA) in single objective optimization and have also been studied in the multiobjective combinatorial optimization literature. In most such studies, LS is applied to the solutions of each generation of GA, which is the scheme called \"GA with LS\" herein. Another scheme, in which LS is applied to the solutions obtained with GA, has also been studied, which is called \"GA then LS\" herein. It seems there is no consensus in the literature as to which scheme is better, let alone the reasoning for it. The situation in the multiobjective function optimization literature is even more unclear since the number of such studies in the field has been small.This paper, assuming that objective functions are differentiable, reveals the reasons why GA is not suitable for obtaining solutions of high precision, thereby justifying hybridization of GA and LS. It also suggests that the hybridization scheme which maximally exploits both GA and LS is GA then LS. Experiments conducted on many benchmark problems verified our claims.|Ken Harada,Kokolo Ikeda,Shigenobu Kobayashi","59066|GECCO|2010|Incremental evolution of local search heuristics|In evolutionary computation, incremental evolution refers to the process of employing an evolutionary environment that becomes increasingly complex over time. We present an implementation of this approach to develop randomised local search heuristics for constraint satisfaction problems, combining research on incremental evolution with local search heuristics evolution. A population of local search heuristics is evolved using a genetic programming framework on a simple problem for a short period and is then allowed to evolve on a more complex problem. Experiments compare the performance of this population with that of a randomly initialised population evolving directly on the more complex problem. The results obtained show that incremental evolution can represent a significant improvement in terms of optimisation speed, solution quality and solution structure.|Dara Curran,Eugene C. Freuder,Thomas Jansen"]]}}