{
  "sentence": {
    "entropy": 5.2790288722768,
    "topics": [
      "problem learning, learning model, machine learning, approach learning, paper learning, propose learning, learning system, neural networks, methods learning, learning based, data qualitative, inductive learning, learning process, problem solving, framework learning, learning search, learning networks, induction qualitative, reinforcement learning, bayesian networks",
      "learning data, learning algorithm, learning task, results learning, learning examples, learning domain, training data, show learning, learning training, experimental results, methods learning, results show, machine data, visual cues, supervised learning, machine learning, data sets, learning classifiers, learning concept, learning features",
      "machine learning, paper learning, learning knowledge, learning system, general learning, explanation-based learning, paper describe, natural language, describe learning, present learning, work learning, learning representation, paper present, methods learning, learning language, research learning, use learning, moore neural, learning discussed, learning information",
      "motives emotions, units links, processes emotions, processes motives, acoustic expressive, monophonic expressive, melody expressive, expressive transformation, learning component, acoustic monophonic, acoustic melody, monophonic melody, nature emotions, nature motives, acoustic transformation, monophonic transformation, melody transformation, learning using, learning emotions, learning motives",
      "learning model, problem learning, learning networks, neural networks, reinforcement learning, methods learning, learning mechanism, vector machine, bayesian networks, machine learning, learning based, learning bayesian, neural learning, learning rate, learning system, learned learning, learning using, learning markov, learning parameters, probabilistic learning",
      "approach learning, paper learning, data qualitative, learning search, learning data, learning system, induction qualitative, behaviour qualitative, system's qualitative, machine learning, qualitative system, learning qualitative, study learning, learning complex, machine qualitative, paper approach, learning highly, describe learning, recognition learning, qualitative model",
      "machine learning, machine data, learning environment, way learning, learning called, order learning, manifold learning, learning reduction, learning data, formation learning, learning concept, help learning, learning various, hypotheses learning, learning features, techniques learning, multiple data, induction data, use learning, learning induction",
      "learning large, experimental results, learning data, experimental learning, show learning, real-world learning, empirical learning, results show, learning accuracy, results learning, demonstrate learning, learning domain, learning number, show significantly, large data, large built, datasets learning, results demonstrate, significantly learning, learning points",
      "learning knowledge, learning representation, work learning, learning concept, learning system, learning user, machine translation, machine learning, knowledge representation, probably learning, approaches learning, learning elementary, system knowledge, paper learning, translation system, probably approximately, means knowledge, probably correct, contrast learning, symbolic machine",
      "machine learning, research learning, learning computer, research machine, learning language, problem learning, important learning, learning processing, learning memory, learning capabilities, machine machine, processing machine, natural language, learning presented, human learning, current learning, learning learn, learning system, machine machine, learning task",
      "used learning, learning using, techniques learning, general learning, used machine, learning knowledge, learning scheme, learning environment, describe learning, knowledge base, take learning, learning automatically, learning experience, implementation machine, using machine, learning procedures, used system, feedback selection, machine techniques, knowledge structure",
      "acoustic expressive, monophonic expressive, melody expressive, expressive transformation, learning expressive, acoustic monophonic, acoustic melody, monophonic melody, acoustic transformation, monophonic transformation, melody transformation, expressive audio, component expressive, machine expressive, features expressive, description expressive, learning component, expressive model, recordings expressive, induce expressive"
    ],
    "ranking": [
      [
        "16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton",
        "16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao",
        "17086|IJCAI|2009|Autonomously Learning an Action Hierarchy Using a Learned Qualitative State Representation|There has been intense interest in hierarchical reinforcement learning as a way to make Markov decision process planning more tractable, but there has been relatively little work on autonomously learning the hierarchy, especially in continuous domains. In this paper we present a method for learning a hierarchy of actions in a continuous environment. Our approach is to learn a qualitative representation of the continuous environment and then to define actions to reach qualitative states. Our method learns one or more options to perform each action. Each option is learned by first learning a dynamic Bayesian network (DBN). We approach this problem from a developmental robotics perspective. The agent receives no extrinsic reward and has no external direction for what to learn. We evaluate our work using a simulation with realistic physics that consists of a robot playing with blocks at a table.|Jonathan Mugan,Benjamin Kuipers",
        "15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik",
        "15987|IJCAI|2003|Qualitatively Faithful Quantitative Prediction|In this paper we describe a case study in which we applied an approach to qualitative machine learning to induce, from system's behaviour data, a qualitative model of a complex, industrially relevant mechanical system (a car wheel suspension system). The induced qualitative model enables nice causal interpretation of the relations in the modelled system. Moreover, we also show that the qualitative model can be used to guide the quantitative modelling process leading to numerical predictions that may be considerably more accurate than those obtained by state-of-the-art numerical modelling methods. This idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages () induction of qualitative constraints from system's behaviour data, and () induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. We call this approach Q learning, which stands for Qualitatively faithful Quantitative learning.|Dorian Suc,Daniel Vladusic,Ivan Bratko",
        "17066|IJCAI|2009|Learning Probabilistic Hierarchical Task Networks to Capture User Preferences|While much work on learning in planning focused on learning domain physics (i.e., action models), and search control knowledge, little attention has been paid towards learning user preferences on desirable plans. Hierarchical task networks (HTN) are known to provide an effective way to encode user prescriptions about what constitute good plans. However, manual construction of these methods is complex and error prone. In this paper, we propose a novel approach to learning probabilistic hierarchical task networks that capture user preferences by examining user-produced plans given no prior information about the methods (in contrast, most prior work on learning within the HTN framework focused on learning \"method preconditions\"--i.e., domain physics--assuming that the structure of the methods is given as input). We will show that this problem has close parallels to the problem of probabilistic grammar induction, and describe how grammar induction methods can be adapted to learn task networks. We will empirically demonstrate the effectiveness of our approach by showing that task networks we learn are able to generate plans with a distribution close to the distribution of the user-preferred plans.|Nan Li,Subbarao Kambhampati,Sung Wook Yoon",
        "14961|IJCAI|1991|Qualitative Model Evolution|A genetic algorithm is used for learning qualitative model* based on the QSIM formalism. Hierarchical representation enables formation of \"submodels\" relevant for induction of domain explanation. Daring the search for better coding of the candidates, in parallel with the search for better solutions, the sise and shape of candidate solutions are dynamically created. Optimisation is based on the maximisation of the number of examples covered by a candidate solution combined with the minimisation of the number of constraints used in the solution. The result of learning is a set of models of different specificity that explain all given examples. An experiment in learning a qualitative model of the connected container system (U-TUBE) is described in detail. Several solutions, equivalent to the original model, were discovered.|Alen Varsek",
        "15762|IJCAI|2001|Genetic Algorithm based Selective Neural Network Ensemble|Neural network ensemble is a learning paradigm where several neural networks are jointly used to solve a problem. In this paper, the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural networks is analyzed, which reveals that ensembling a selective subset of individual networks is superior to ensembling all the individual networks in some cases. Therefore an approach named GASEN is proposed, which trains several individual neural networks and then employs genetic algorithm to select an optimum subset of individual networks to constitute an ensemble. Experimental results show that, comparing with a popular ensemble approach, i.e. averaging all, and a theoretically optimum selective ensemble approach, i.e. enumerating, GASEN has preferable performance in generating ensembles with strong generalization ability in relatively small computational cost.|Zhi-Hua Zhou,Jianxin Wu,Yuan Jiang,Shifu Chen",
        "15275|IJCAI|1995|Local Learning in Probabilistic Networks with Hidden Variables|Probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks Because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process, thereby significantly improving the learning rate Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.|Stuart J. Russell,John Binder,Daphne Koller,Keiji Kanazawa"
      ],
      [
        "16660|IJCAI|2007|Selective Supervision Guiding Supervised Learning with Decision-Theoretic Active Learning|An inescapable bottleneck with learning from large data sets is the high cost of labeling training data. Unsupervised learning methods have promised to lower the cost of tagging by leveraging notions of similarity among data points to assign tags. However, unsupervised and semi-supervised learning techniques often provide poor results due to errors in estimation. We look at methods that guide the allocation of human effort for labeling data so as to get the greatest boosts in discriminatory power with increasing amounts of work. We focus on the application of value of information to Gaussian Process classifiers and explore the effectiveness of the method on the task of classifying voice messages.|Ashish Kapoor,Eric Horvitz,Sumit Basu",
        "14986|IJCAI|1993|Estimating the Accuracy of Learned Concepts|This paper investigates alternative estimators of the accuracy of concepts learned from examples. In particular, the cross-validation and  bootstrap estimators are studied, using synthetic training data and the FOIL learning algorithm. Our experimental results contradict previous papers in statistics, which advocate the  bootstrap method as superior to cross-validation. Nevertheless, our results also suggest that conclusions based on cross-validation in previous machine learning papers are unreliable. Specifically, our observations are that (i) the true error of the concept learned by FOIL from independently drawn sets of examples of the same concept varies widely, (ii) the estimate of true error provided by cross-validation has high variability but is approximately unbiased, and (iii) the  bootstrap estimator has lower variability than cross-validation, but is systematically biased.|Timothy L. Bailey,Charles Elkan",
        "14707|IJCAI|1989|An Experimental Comparison of Symbolic and Connectionist Learning Algorithms|Despite the fact that many symbolic and connectionist (neural net) learning algorithms are addressing the same problem of learning from classified examples, very little Is known regarding their comparative strengths and weaknesses. This paper presents the results of experiments comparing the ID symbolic learning algorithm with the perceptron and back-propagation connectionist learning algorithms on several large real-world data sets. The results show that ID and perceptron run significantly faster than does backpropagation, both during learning and during classification of novel examples. However, the probability of correctly classifying new examples is about the same for the three systems. On noisy data sets there is some indication that backpropagation classifies more accurately.|Raymond J. Mooney,Jude W. Shavlik,Geoffrey G. Towell,Alan Gove",
        "16346|IJCAI|2005|Semi-Supervised Regression with Co-Training|In many practical machine learning and data mining applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. Previous research mainly focuses on semi-supervised classification. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two k-nearest neighbor regressors with different distance metrics, each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates.|Zhi-Hua Zhou,Ming Li",
        "15723|IJCAI|2001|Active Learning for Class Probability Estimation and Ranking|For many supervised learning tasks it is very costly to produce training data with class labels. Active learning acquires data incrementally, at each stage using the model learned so far to help identify especially useful additional data for labeling. Existing empirical active learning approaches have focused on learning classifiers. However, many applications require estimations of the probability of class membership, or scores that can be used to rank new cases. We present a new active learning method for class probability estimation (CPE) and ranking. BOOTSTRAP-LV selects new data for labeling based on the variance in probability estimates, as determined by learning multiple models from bootstrap samples of the existing labeled data. We show empirically that the method reduces the number of data items that must be labeled, across a wide variety of data sets. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV dominates for CPE. Surprisingly it also often is preferable for accelerating simple accuracy maximization.|Maytal Saar-Tsechansky,Foster J. Provost",
        "16895|IJCAI|2009|Semi-Supervised Learning of Visual Classifiers from Web Images and Text|The web holds tremendous potential as a source of training data for visual classification. However, web images must be correctly indexed and labeled before this potential can be realized. Accordingly, there has been considerable recent interest in collecting imagery from the web using image search engines to build databases for object and scene recognition research. While search engines can provide rough sets of image data, results are noisy and this leads to problems when training classifiers. In this paper we propose a semi-supervised model for automatically collecting clean example imagery from the web. Our approach includes both visual and textual web data in a unified framework. Minimal supervision is enabled by the selective use of generative and discriminative elements in a probabilistic model and a novel learning algorithm. We show through experiments that our model discovers good training images from the web with minimal manual work. Classifiers trained using our method significantly outperform analogous baseline approaches on the Caltech- dataset.|Nicholas Morsillo,Christopher Pal,Randal Nelson",
        "16958|IJCAI|2009|Semi-Supervised Metric Learning Using Pairwise Constraints|Distance metric has an important role in many machine learning algorithms. Recently, metric learning for semi-supervised algorithms has received much attention. For semi-supervised clustering, usually a set of pairwise similarity and dissimilarity constraints is provided as supervisory information. Until now, various metric learning methods utilizing pairwise constraints have been proposed. The existing methods that can consider both positive (must-link) and negative (cannot-link) constraints find linear transformations or equivalently global Mahalanobis metrics. Additionally, they find metrics only according to the data points appearing in constraints (without considering other data points). In this paper, we consider the topological structure of data along with both positive and negative constraints. We propose a kernel-based metric learning method that provides a non-linear transformation. Experimental results on synthetic and real-world data sets show the effectiveness of our metric learning method.|Mahdieh Soleymani Baghshah,Saeed Bagheri Shouraki",
        "17011|IJCAI|2009|Robust Distance Metric Learning with Auxiliary Knowledge|Most of the existing metric learning methods are accomplished by exploiting pairwise constraints over the labeled data and frequently suffer from the insufficiency of training examples. To learn a robust distance metric from few labeled examples, prior knowledge from unlabeled examples as well as the metrics previously derived from auxiliary data sets can be useful. In this paper, we propose to leverage such auxiliary knowledge to assist distance metric learning, which is formulated following the regularized loss minimization principle. Two algorithms are derived on the basis of manifold regularization and log-determinant divergence regularization technique, respectively, which can simultaneously exploit label information (i.e., the pairwise constraints over labeled data), unlabeled examples, and the metrics derived from auxiliary data sets. The proposed methods directly manipulate the auxiliary metrics and require no raw examples from the auxiliary data sets, which make them efficient and flexible. We conduct extensive evaluations to compare our approaches with a number of competing approaches on face recognition task. The experimental results show that our approaches can derive reliable distance metrics from limited training examples and thus are superior in terms of accuracy and labeling efforts.|Zheng-Jun Zha,Tao Mei,Meng Wang,Zengfu Wang,Xian-Sheng Hua"
      ],
      [
        "14792|IJCAI|1989|Improving Efficiency by Learning Intermediate Concepts|One goal of explanation-based learning is to transform knowledge into an operational form for efficient use. Typically, this involves rewriting concept descriptions in terms of the predicates used to describe examples. In this paper we present RINCON, a system that extends domain theories from examples with the goal of maximizing classification efficiency. RINCON'S basic learning operator involves the introduction of new intermediate concepts into a domain theory, which can be viewed as the inverse of the operationalization process. We discuss the system's learning algorithm and its relation to work on explanation-based learning, incremental concept formation, representation change, and pattern matching. We also present experimental evidence from two natural domains that indicates the addition of intermediate concepts can improve classification efficiency.|James Wogulis,Pat Langley",
        "14420|IJCAI|1987|Machine Learning for Software Reuse|Recent work on learning apprentice systems suggests new approaches for using interactive programming environments to promote software reuse. Methodologies for software specification and validation yield natural domains of application for explanation-based learning techniques. This paper develops a relation between data abstractions in software and explanation-based generalization problems and shows how explanation-based learning can be used to generalize program abstractions to promote their reuse. This method is applied in the design of a system called LASR (Learning Apprentice for Software Reuse) which will acquire programming knowledge by capturing and generalizing interconnections between abstract data type theories. The technical role of theories in defining learned concepts in this application suggests their more general use in representing problems in explanation-based learning.|Walter L. Hill",
        "16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins",
        "14251|IJCAI|1985|A Prototypical Approach to Machine Learning|This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization. A structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.|R. I. Phelps,Peter B. Musgrove",
        "13654|IJCAI|1977|How to LearnWhat to Learn|This paper discusses the kind of information that must be present in a computer program that models the linguistic development of a child. A three stage model is presented that characterizes the development of a natural language parser in a child of ages one, one and a half, and two. Some data from children of these ages is presented. General problems with respect to computer learning are also discussed.|Roger C. Schank,Mallory Selfridge",
        "15525|IJCAI|1999|Learning in Natural Language|Statistics-based classifiers in natural language are developed typically by assuming a generative model for the data, estimating its parameters from training data and then using Bayes rule to obtain a classifier. For many problems the assumptions made by the generative models are evidently wrong, leaving open the question of why these approaches work. This paper presents a learning theory account of the major statistical approaches to learning in natural language. A class of Linear Statistical Queries (LSQ) hypotheses is defined and learning with it is shown to exhibit some robustness properties. Many statistical learners used in natural language, including naive Bayes, Markov Models and Maximum Entropy models are shown to be LSQ hypotheses, explaining the robustness of these predictors even when the underlying probabilistic assumptions do not hold. This coherent view of when and why learning approaches work in this context may help to develop better learning methods and an understanding of the role of learning in natural language inferences.|Dan Roth",
        "14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi",
        "15375|IJCAI|1997|PRISM A Language for Symbolic-Statistical Modeling|We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs.|Taisuke Sato,Yoshitaka Kameya",
        "14940|IJCAI|1991|Quantitative Evaluation of Explanation-Based Learning as an Optimisation Tool for a Large-Scale Natural Language System|This paper describes the application of explanation-based learning, a machine learning technique, to the SRI Core Language Engine, a large scale general purpose natural language analysis system. The idea is to bypass normal morphological, syntactic and (partly) semantic processing, for most input sentences, instead using a set of learned rules. Explanation-based learning is used to extract the learned rules automatically from sample sentences submitted by a user and thus tune the system for that particular user. By indexing the learned rules efficiently, it is possible to achieve dramatic speedups. Performance measurements were carried out using a training set of  sentences and a separate test set of  sentences, all from the ATIS corpus. A set of  learned rules was derived from the training set. These rules covered  percent of the test sentences and reduced the total processing time to a third. An overall speed-up of  percent was accomplished using a set of only  learned rules.|Christer Samuelsson,Manny Rayner"
      ],
      [
        "13716|IJCAI|1981|A Computational Model of Analogical Problem Solving|This paper outlines a theory of analogical reasoning based on a process-model of problem solving by analogy and the hypothesis that problem solving and learning are inalienable, concurrent processes in the human cognitive system. The analogical problem solver exploits prior experience in solving similar problems, and, in the process, augments a hierarchically-structured epsiodic long term memory. An analogical transformation process is developed based on a modified version of Means-Ends Analysis in order to map past solutions from similar problems into solutions satisfying the requirements of the new problem.|Jaime G. Carbonell",
        "16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe",
        "14573|IJCAI|1989|Explanation Based Program Transformation|Fold-unfold is a well known program transformation technique. Its major drawback is that folding requires an Eureka step to invent new procedures. In the context of logic programming, we present a technique where the folding is driven by an example. The transformation is aimed at programs suffering from inefficiencies due to the repetition of identical subcomputations. The execution of an example is analysed to locate repeated subcomputations. Then the structure of the example is used to control a fold-unfold-transformation of the program. The transformation can be automated. The method can be regarded as an extension of explanation based learning.|Maurice Bruynooghe,Luc De Raedt,Danny De Schreye",
        "13499|IJCAI|1975|The Vocal Speech Understanding System|This paper describes the VOCAL (Voice Operated CALculator) speech understanding system. VOCAL is a software package that lets its user program a computer to perform numerical calculations by speaking to It in English-like sentences. To accomplish this, VOCAL uses processes for acoustic, grammatical, and semantic analysis. These individual procedures, which are relatively simple, are embedded in a control structure that uses the information from each component to arrive at a meaningful interpretation of spoken sentences. One unique feature of VOCAL, which is essential to the development of speech understanding systems, is that it is complete and self-contained. Coded in standard FORTRAN, it is compact enough to run on many minicomputers and can be used in a real-time, on-line environment on slightly more powerful machines. Testing has shown that despite a correct word identification rate of leas that % the VOCAL system usually correctly interprets even very long sentences.|Stephen E. Levinson",
        "16371|IJCAI|2007|Cooperating Reasoning Processes More than Just the Sum of Their Parts|Using the achievements of my research group over the last + years, I provide evidence to support the following hypothesis By complementing each other, cooperating reasoning process can achieve much more than they could if they only acted individually. Most of the work of my group has been on processes for mathematical reasoning and its applications, e.g. to formal methods. The reasoning processes we have studied include Proof Search by meta-level inference, proof planning, abstraction, analogy, symmetry, and reasoning with diagrams. Representation Discovery, Formation and Evolution by analysing, diagnosing and repairing failed proof and planning attempts, forming and repairing new concepts and conjectures, and forming logical representations of informally stated problems. Other learning of new proof methods from example proofs, finding counter-examples, reasoning under uncertainty, the presentation of and interaction with proofs, the automation of informal argument. In particular, we have studied how these different kinds of process can complement each other, and cooperate to achieve complex goals. We have applied this work to the following areas proof by mathematical induction and co-induction analysis equation solving, mechanics problems the building of ecological models the synthesis, verification, transformation and editing of both hardware and software, including logic, functional and imperative programs, security protocols and process algebras the configuration of hardware game playing and cognitive modelling.|Alan Bundy",
        "16264|IJCAI|2005|A Learning Scheme for Generating Expressive Music Performances of Jazz Standards|We describe our approach for generating expressive music performances of monophonic Jazz melodies. It consists of three components (a) a melodic transcription component which extracts a set of acoustic features from monophonic recordings, (b) a machine learning component which induces an expressive transformation model from the set of extracted acoustic features, and (c) a melody synthesis component which generates expressive monophonic output (MIDI or audio) from inexpressive melody descriptions using the induced expressive transformation model. In this paper we concentrate on the machine learning component, in particular, on the learning scheme we use for generating expressive audio from a score.|Rafael Ramirez,Amaury Hazan",
        "13717|IJCAI|1981|A Computational Model of Analogical Problem Solving|This paper outlines a theory of analogical reasoning based on a process-model of problem solving by analogy and the hypothesis that problem solving and learning are inalienable, concurrent processes in the human cognitive system. The analogical problem solver exploits prior experience in solving similar problems, and, in the process, augments a hierarchically-structured epsiodic long term memory. An analogical transformation process is developed based on a modified version of Means-Ends Analysis in order to map past solutions from similar problems into solutions satisfying the requirements of the new problem.|Jaime G. Carbonell",
        "14065|IJCAI|1983|Motives and Emotions in a General Learning System|The relationship between motives, emotions and learning in the design of intelligent systems has received relatively little consideration to date. Clarification of the relationship involves tackling fundamental questions such as the survival value or adaptive function of motives and emotions in intelligent systems, the nature of motivational and emotional processes which are features of 'innate' endowment, the learning of additional motives and emotions as a result of environmental interaction, and the results of the influence of motives and emotions on learning processes and, conversely, the effect of learning processes on the nature of motives and emotions.|J. G. (Iain) Wallace",
        "16314|IJCAI|2005|Learning to Play Like the Great Pianists|An application of relational instance-based learning to the complex task of expressive music performance is presented. We investigate to what extent a machine can automatically build 'expressive profiles' of famous pianists using only minimal performance information extracted from audio CD recordings by pianists and the printed score of the played music. It turns out that the machine-generated expressive performances on unseen pieces are substantially closer to the real performances of the 'trainer' pianist than those of all others. Two other interesting applications of the work are discussed recognizing pianists from their style of playing, and automatic style replication.|Asmir Tobudic,Gerhard Widmer"
      ],
      [
        "15442|IJCAI|1999|Learning Probabilistic Relational Models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.|Nir Friedman,Lise Getoor,Daphne Koller,Avi Pfeffer",
        "16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton",
        "16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao",
        "14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine",
        "15095|IJCAI|1993|Integrating Inductive Neural Network Learning and Explanation-Based Learning|Many researchers have noted the importance of combining inductive and analytical learning, yet we still lack combined learning methods that are effective in practice. We present here a learning method that combines explanation-based learning from a previously learned approximate domain theory, together with inductive learning from observations. This method, called explanation-based neural network learning (EBNN), is based on a neural network representation of domain knowledge. Explanations are constructed by chaining together inferences from multiple neural networks. In contrast with symbolic approaches to explanation-based learning which extract weakest preconditions from the explanation, EBNN extracts the derivatives of the target concept with respect to the training example features. These derivatives summarize the dependencies within the explanation, and are used to bias the inductive learning of the target concept. Experimental results on a simulated robot control task show that EBNN requires significantly fewer training examples than standard inductive learning. Furthermore, the method is shown to be robust to errors in the domain theory, operating effectively over a broad spectrum from very strong to very weak domain theories.|Sebastian Thrun,Tom M. Mitchell",
        "16474|IJCAI|2007|Kernel Carpentry for Online Regression Using Randomly Varying Coefficient Model|We present a Bayesian formulation of locally weighted learning (LWL) using the novel concept of a randomly varying coefficient model. Based on this, we propose a mechanism for multivariate non-linear regression using spatially localised linear models that learns completely independent of each other, uses only local information and adapts the local model complexity in a data driven fashion. We derive online updates for the model parameters based on variational Bayesian EM. The evaluation of the proposed algorithm against other state-of-the-art methods reveal the excellent, robust generalization performance beside surprisingly efficient time and space complexity properties. This paper, for the first time, brings together the computational efficiency and the adaptability of 'non-competitive' locally weighted learning schemes and the modelling guarantees of the Bayesian formulation.|Narayanan U. Edakunni,Stefan Schaal,Sethu Vijayakumar",
        "15375|IJCAI|1997|PRISM A Language for Symbolic-Statistical Modeling|We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs.|Taisuke Sato,Yoshitaka Kameya",
        "15762|IJCAI|2001|Genetic Algorithm based Selective Neural Network Ensemble|Neural network ensemble is a learning paradigm where several neural networks are jointly used to solve a problem. In this paper, the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural networks is analyzed, which reveals that ensembling a selective subset of individual networks is superior to ensembling all the individual networks in some cases. Therefore an approach named GASEN is proposed, which trains several individual neural networks and then employs genetic algorithm to select an optimum subset of individual networks to constitute an ensemble. Experimental results show that, comparing with a popular ensemble approach, i.e. averaging all, and a theoretically optimum selective ensemble approach, i.e. enumerating, GASEN has preferable performance in generating ensembles with strong generalization ability in relatively small computational cost.|Zhi-Hua Zhou,Jianxin Wu,Yuan Jiang,Shifu Chen",
        "15275|IJCAI|1995|Local Learning in Probabilistic Networks with Hidden Variables|Probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks Because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process, thereby significantly improving the learning rate Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.|Stuart J. Russell,John Binder,Daphne Koller,Keiji Kanazawa",
        "17079|IJCAI|2009|Large Margin Boltzmann Machines|Boltzmann Machines are a powerful class of undirected graphical models. Originally proposed as artificial neural networks, they can be regarded as a type of Markov Random Field in which the connection weights between nodes are symmetric and learned from data. They are also closely related to recent models such as Markov logic networks and Conditional Random Fields. A major challenge for Boltzmann machines (as well as other graphical models) is speeding up learning for large-scale problems. The heart of the problem lies in efficiently and effectively approximating the partition function. In this paper, we propose a new efficient learning algorithm for Boltzmann machines that allows them to be applied to problems with large numbers of random variables. We introduce a new large-margin variational approximation to the partition function that allows Boltzmann machines to be trained using a support vector machine (SVM) style learning algorithm. For discriminative learning tasks, these large margin Boltzmann machines provide an alternative approach to structural SVMs. We show that these machines have low sample complexity and derive a generalization bound. Our results demonstrate that on multilabel classification problems, large margin Boltzmann machines achieve orders of magnitude faster performance than structural SVMs and also outperform structural SVMs on problems with large numbers of labels.|Xu Miao,Rajesh P. N. Rao"
      ],
      [
        "14341|IJCAI|1987|Approximation in Mathematical Domains|Explanation-based learning is accomplished through the generalization of an explanation produced by analysis of a single example. A theory of the domain is utilized in generating the explanation. However, problems arise when the domain theory is intractable. Simplifications must be made in order to make the problem tractable. Well-founded simplifications based on our real world knowledge are termed approximations. This paper discusses how approximations can be used to deal with the intractable domain problem in mathematical domains. The approximation method strongly supports the use of a mix of quantitative and qualitative reasoning over either a purely quantitative or qualitative approach. The approximation technique is demonstrated on one of the examples which has been implemented in the chemistry domain.|Scott Bennett",
        "17086|IJCAI|2009|Autonomously Learning an Action Hierarchy Using a Learned Qualitative State Representation|There has been intense interest in hierarchical reinforcement learning as a way to make Markov decision process planning more tractable, but there has been relatively little work on autonomously learning the hierarchy, especially in continuous domains. In this paper we present a method for learning a hierarchy of actions in a continuous environment. Our approach is to learn a qualitative representation of the continuous environment and then to define actions to reach qualitative states. Our method learns one or more options to perform each action. Each option is learned by first learning a dynamic Bayesian network (DBN). We approach this problem from a developmental robotics perspective. The agent receives no extrinsic reward and has no external direction for what to learn. We evaluate our work using a simulation with realistic physics that consists of a robot playing with blocks at a table.|Jonathan Mugan,Benjamin Kuipers",
        "14483|IJCAI|1987|Towards an Integrated Discovery System|Previous research on machine discovery has focused on limited parts of the empirical discovery task. In this paper we describe IDS, an integrated system that addresses both qualitative and quantitative discovery. The program represents its knowledge in terms of qualitative schema, which it discovers by interacting with a simulated physical environment. Once IDS has formulated a qualitative schema, it uses that schema to design experiments and to constrain the search for quantitative laws. We have carried out preliminary tests in the domain of heat phenomena. In this context the system has discovered both intrinsic properties, such as the melting point of substances, and numeric laws, such as the conservation of mass for objects going through a phase change.|Bernd Nordhausen,Pat Langley",
        "15987|IJCAI|2003|Qualitatively Faithful Quantitative Prediction|In this paper we describe a case study in which we applied an approach to qualitative machine learning to induce, from system's behaviour data, a qualitative model of a complex, industrially relevant mechanical system (a car wheel suspension system). The induced qualitative model enables nice causal interpretation of the relations in the modelled system. Moreover, we also show that the qualitative model can be used to guide the quantitative modelling process leading to numerical predictions that may be considerably more accurate than those obtained by state-of-the-art numerical modelling methods. This idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages () induction of qualitative constraints from system's behaviour data, and () induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. We call this approach Q learning, which stands for Qualitatively faithful Quantitative learning.|Dorian Suc,Daniel Vladusic,Ivan Bratko",
        "16745|IJCAI|2007|Analogical Learning in a Turn-Based Strategy Game|A key problem in playing strategy games is learning how to allocate resources effectively. This can be a difficult task for machine learning when the connections between actions and goal outputs are indirect and complex. We show how a combination of structural analogy, experimentation, and qualitative modeling can be used to improve performance in optimizing food production in a strategy game. Experimentation bootstraps a case library and drives variation, while analogical reasoning supports retrieval and transfer. A qualitative model serves as a partial domain theory to support adaptation and credit assignment. Together, these techniques can enable a system to learn the effects of its actions, the ranges of quantities, and to apply training in one city to other, structurally different cities. We describe experiments demonstrating this transfer of learning.|Thomas R. Hinrichs,Kenneth D. Forbus",
        "14961|IJCAI|1991|Qualitative Model Evolution|A genetic algorithm is used for learning qualitative model* based on the QSIM formalism. Hierarchical representation enables formation of \"submodels\" relevant for induction of domain explanation. Daring the search for better coding of the candidates, in parallel with the search for better solutions, the sise and shape of candidate solutions are dynamically created. Optimisation is based on the maximisation of the number of examples covered by a candidate solution combined with the minimisation of the number of constraints used in the solution. The result of learning is a set of models of different specificity that explain all given examples. An experiment in learning a qualitative model of the connected container system (U-TUBE) is described in detail. Several solutions, equivalent to the original model, were discovered.|Alen Varsek",
        "16407|IJCAI|2007|Incremental Learning of Perceptual Categories for Open-Domain Sketch Recognition|Most existing sketch understanding systems require a closed domain to achieve recognition. This paper describes an incremental learning technique for open-domain recognition. Our system builds generalizations for categories of objects based upon previous sketches of those objects and uses those generalizations to classify new sketches. We represent sketches qualitatively because we believe qualitative information provides a level of description that abstracts away details that distract from classification, such as exact dimensions. Bayesian reasoning is used in building representations to deal with the inherent uncertainty in perception. Qualitative representations are compared using SME, a computational model of analogy and similarity that is supported by psychological evidence, including studies of perceptual similarity. We use SEQL to produce generalizations based on the common structure found by SME in different sketches of the same object. We report on the results of testing the system on a corpus of sketches of everyday objects, drawn by ten different people.|Andrew M. Lovett,Morteza Dehghani,Kenneth D. Forbus",
        "14383|IJCAI|1987|An Examination of the Third Stage in the Analogy Process Verification-based Analogical Learning|Many studies of analogy in Artificial Intelligence have focused on analogy as a heuristic mechanism to guide search and simplify problem solving or as a basis for forming generalizations. This paper examines analogical learning, where analogy is used to conjecture new knowledge about some domain. A theory of Verification-Based Analogical Learning is presented which addresses the tenuous nature of analogically inferred concepts and describes procedures that can be used to increase confidence in the inferred knowledge. The theory describes how analogy may be used to discover and refine scientific models of the physical world. Examples are taken from an implemented system, which discovers qualitative models of processes such as liquid flow and heat flow.|Brian Falkenhainer"
      ],
      [
        "14786|IJCAI|1989|An Empirical Comparison of Pattern Recognition Neural Nets and Machine Learning Classification Methods|Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.|Sholom M. Weiss,Ioannis Kapouleas",
        "14499|IJCAI|1987|Layered Concept-Learning and Dynamically Variable Bias Management|Concept learning is inherently complex. Without severe constraint or inductive \"bias,\" the general problem is intractable. While most learning systems have been designed with built-in biases, these systems typically work well only in narrowly circumscribed problem domains. Here we present a model of concept formation that views learning as a simultaneous optimization problem at three different levels, with dynamically chosen biases guiding the search for satisfactory hypotheses. In this model, the partitioning of events into classes occurs through dynamic interactions among three layers event space, hypothesis space, and bias space. This view of the induction process may help clarify the problem of learning and lead to more general and efficient induction systems. To test this model of meta-knowledge, a variable bias management system (VBMS) has been designed and partly implemented. The system will dynamically alter evolving hypotheses, concept representation languages, and concept formation algorithms by monitoring progress and selecting biases based on characteristics of the particular induction problems presented. VBMS is designed to learn the best biases for different types of induction problems. Thus it is robust (effective and efficient in many domains). The system can learn incrementally despite noisy data at any level.|Larry A. Rendell,Raj Sheshu,David K. Tcheng",
        "14818|IJCAI|1991|The Problem of Induction and Machine Learning|Are we justified in inferring a general rule from observations that frequently confirm it This is the usual statement of the problem of induction. The present paper argues that this question is relevant for the understanding of Machine Learning, but insufficient. Research in Machine Learning has prompted another, more fundamental question the number of possible rules grows exponentially with the size of the examples, and many of them are somehow confirmed by the data - how are we to choose effectively some rules that have good chances of being predictive We analyze if and how this problem is approached in standard accounts of induction and show the difficulties that are present. Finally, we suggest that the Explanation-based Learning approach and related methods of knowledge intensive induction could be a partial solution to some of these problems, and help understanding the question of valid induction from a new perspective.|Francesco Bergadano",
        "15931|IJCAI|2003|Constructing Diverse Classifier Ensembles using Artificial Training Examples|Ensemble methods like bagging and boosting that combine the decisions of multiple hypotheses are some of the strongest existing machine learning methods. The diversity of the members of an ensemble is known to be an important factor in determining its generalization error. This paper presents a new method for generating ensembles that directly constructs diverse hypotheses using additional artificially-constructed training examples. The technique is a simple, general metalearner that can use any strong learner as a base classifier to build diverse committees. Experimental results using decision-tree induction as a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging (whereas boosting can occasionally decrease accuracy), and also obtains higher accuracy than boosting early in the learning curve when training data is limited.|Prem Melville,Raymond J. Mooney",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "14938|IJCAI|1991|Towards a Model of Grounded Concept Formation|In most research on concept formation within machine learning and cognitive psychology, the features from which concepts are built are assumed to be provided as elementary vocabulary. In this paper, we argue that this is an unnecessarily limited paradigm within which to examine concept formation. Based on evidence from psychology and machine learning, we contend that a principled account of the origin of features can only be given with a grounded model of concept formation, i.e., with a model that incorporates direct access to the world via sensors and manipulators. We discuss the domain of process control as a suitable framework for research into such models, and present a first approach to the problem of developing elementary vocabularies from perceptual sensor data.|Stefan Wrobel"
      ],
      [
        "15442|IJCAI|1999|Learning Probabilistic Relational Models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.|Nir Friedman,Lise Getoor,Daphne Koller,Avi Pfeffer",
        "16903|IJCAI|2009|Learning Optimal Subsets with Implicit User Preferences|We study the problem of learning an optimal subset from a larger ground set of items, where the optimality criterion is defined by an unknown preference function. We model the problem as a discriminative structural learning problem and solve it using a Structural Support Vector Machine (SSVM) that optimizes a \"set accuracy\" performance measure representing set similarities. Our approach departs from previous approaches since we do not explicitly learn a pre-defined preference function. Experimental results on both a synthetic problem domain and a real-world face image subset selection problem show that our method significantly outperforms previous learning approaches for such problems.|Yunsong Guo,Carla P. Gomes",
        "16860|IJCAI|2009|Efficient Skill Learning using Abstraction Selection|We present an algorithm for selecting an appropriate abstraction when learning a new skill. We show empirically that it can consistently select an appropriate abstraction using very little sample data, and that it significantly improves skill learning performance in a reasonably large real-valued reinforcement learning domain.|George Konidaris,Andrew G. Barto",
        "14707|IJCAI|1989|An Experimental Comparison of Symbolic and Connectionist Learning Algorithms|Despite the fact that many symbolic and connectionist (neural net) learning algorithms are addressing the same problem of learning from classified examples, very little Is known regarding their comparative strengths and weaknesses. This paper presents the results of experiments comparing the ID symbolic learning algorithm with the perceptron and back-propagation connectionist learning algorithms on several large real-world data sets. The results show that ID and perceptron run significantly faster than does backpropagation, both during learning and during classification of novel examples. However, the probability of correctly classifying new examples is about the same for the three systems. On noisy data sets there is some indication that backpropagation classifies more accurately.|Raymond J. Mooney,Jude W. Shavlik,Geoffrey G. Towell,Alan Gove",
        "15874|IJCAI|2003|Use of Off-line Dynamic Programming for Efficient Image Interpretation|An interpretation system finds the likely mappings from portions of an image to real-world objects. An interpretation policy specifies when to apply which imaging operator, to which portion of the image, during every stage of interpretation. Earlier results compared a number of policies, and demonstrated that policies that select operators which maximize the information gain per cost, worked most effectively. However, those policies are myopic -- they rank the operators based only on their immediate rewards. This can lead to inferior overall results it may be better to use a relatively expensive operator first, if that operator provides information that will significantly reduce the cost of the subsequent operators. This suggests using some lookahead process to compute the quality for operators non-myopically. Unfortunately, this is prohibitively expensive for most domains, especially for domains that have a large number of complex states. We therefore use ideas from reinforcement learning to compute the utility of each operator sequence. In particular, our system first uses dynamic programming, over abstract simplifications of interpretation states, to precompute the utility of each relevant sequence. It does this off-line, over a training sample of images. At run time, our interpretation system uses these estimates to decide when to use which imaging operator. Our empirical results, in the challenging real-world domain of face recognition, demonstrate that this approach works more effectively than myopic approaches.|Ramana Isukapalli,Russell Greiner",
        "16958|IJCAI|2009|Semi-Supervised Metric Learning Using Pairwise Constraints|Distance metric has an important role in many machine learning algorithms. Recently, metric learning for semi-supervised algorithms has received much attention. For semi-supervised clustering, usually a set of pairwise similarity and dissimilarity constraints is provided as supervisory information. Until now, various metric learning methods utilizing pairwise constraints have been proposed. The existing methods that can consider both positive (must-link) and negative (cannot-link) constraints find linear transformations or equivalently global Mahalanobis metrics. Additionally, they find metrics only according to the data points appearing in constraints (without considering other data points). In this paper, we consider the topological structure of data along with both positive and negative constraints. We propose a kernel-based metric learning method that provides a non-linear transformation. Experimental results on synthetic and real-world data sets show the effectiveness of our metric learning method.|Mahdieh Soleymani Baghshah,Saeed Bagheri Shouraki",
        "15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller",
        "14631|IJCAI|1989|An Integrated Characterization and Discrimination Scheme to Improve Learning Efficiency in Large Data Sets|This work proposes a learning scheme which integrates Characterization and Discrimination activities with the aim of improving learning efficiency in large data sets. Characterization is considered to be a process which builds up a rough concept description using only positive examples. This description already excludes most of the extreme negative examples. Discrimination is considered to be an incremental learning process which begins with the characteristic description and refines it so as to make it consistent with the negative examples (near misses) which are still covered. During this phase learning efficiency is greatly improved by considering only near misses as counterexamples. Finally, the description is simplified by dropping some characterizing but not discriminant parts of the description. This learning scheme is discussed and compared with the traditional data reduction techniques. Some experimental results are reported which show the gain in efficiency obtained, particularly on real applicative domains.|Roberto Gemello,Franco Mana",
        "17079|IJCAI|2009|Large Margin Boltzmann Machines|Boltzmann Machines are a powerful class of undirected graphical models. Originally proposed as artificial neural networks, they can be regarded as a type of Markov Random Field in which the connection weights between nodes are symmetric and learned from data. They are also closely related to recent models such as Markov logic networks and Conditional Random Fields. A major challenge for Boltzmann machines (as well as other graphical models) is speeding up learning for large-scale problems. The heart of the problem lies in efficiently and effectively approximating the partition function. In this paper, we propose a new efficient learning algorithm for Boltzmann machines that allows them to be applied to problems with large numbers of random variables. We introduce a new large-margin variational approximation to the partition function that allows Boltzmann machines to be trained using a support vector machine (SVM) style learning algorithm. For discriminative learning tasks, these large margin Boltzmann machines provide an alternative approach to structural SVMs. We show that these machines have low sample complexity and derive a generalization bound. Our results demonstrate that on multilabel classification problems, large margin Boltzmann machines achieve orders of magnitude faster performance than structural SVMs and also outperform structural SVMs on problems with large numbers of labels.|Xu Miao,Rajesh P. N. Rao"
      ],
      [
        "16592|IJCAI|2007|Learning from Partial Observations|We present a general machine learning framework for modelling the phenomenon of missing information in data. We propose a masking process model to capture the stochastic nature of information loss. Learning in this context is employed as a means to recover as much of the missing information as is recoverable. We extend the Probably Approximately Correct semantics to the case of learning from partial observations with arbitrarily hidden attributes. We establish that simply requiring learned hypotheses to be consistent with observed values suffices to guarantee that hidden values are recoverable to a certain accuracy we also show that, in some sense, this is an optimal strategy for achieving accurate recovery. We then establish that a number of natural concept classes, including all the classes of monotone formulas that are PAC learnable by monotone formulas, and the classes of conjunctions, disjunctions, k-CNF, k-DNF, and linear thresholds, are consistently learnable from partial observations. We finally show that the concept classes of parities and monotone term -decision lists are not properly consistently learnable from partial observations, if RP  NP. This implies a separation of what is consistently learnable from partial observations versus what is learnable in the complete or noisy setting.|Loizos Michael",
        "14045|IJCAI|1983|Knowledge Oriented Learning|Two alternative learning strategies are defined and discussed Task-oriented learning in which a system tries to improve its performance at a specified task and knowledge-oriented learning in which a system builds an organised representation of experience. It is argued that the relationship between these two strategies is analagous to the relationship between technology and science. Hence it is concluded that the development of knowledge-oriented learning systems, which has been largely overlooked by the AI community, is a worth while research goal. We then proceed to describe how such a system may be constructed by building a machine which continually tries to reduce its own uncertainty regarding the outcome of its actions. An implementation which learns to perform a multiple concept learning task in the presence of noisy data is briefly described.|Paul D. Scott,Robert C. Vogt",
        "14385|IJCAI|1987|Interactive Vocabulary Acquisition in XTRA|This paper describes a practical solution to on-line dictionary update in XTRA, a machine translation system developed by Xiuming Huang at the Computing Research Laboratory of New Mexico State University. The focus of the discussion in on IVES - an Interactive Vocabulary Enrichment System built by this writer for XTRA, It reflects an on-going effort at the laboratory to build embedded learning mechanisms in machine translation systems. Two types of learning are discussed, word learning and word sense learning. Each type of learning undergoes three routine processes detection, acquisition, and evaluation. The emphasis of this paper is on the use of semantic preference violations in the detection of the need to learn new word senses.|Cheng-ming Guo",
        "14562|IJCAI|1989|Coping With Uncertainty in Map Learning|In many applications in mobile robotics, it is important for a robot to explore its environment in order to construct a representation of space useful for guiding movement. We refer to such a representation as a map, and the process of constructing a map from a set of measurements as map learning. In this paper, we develop a framework for describing map-learning problems in which the measurements taken by the robot are subject to known errors. We investigate two approaches to learning maps under such conditions one based on Valiant's probably approximately correct learning model, and a second based on Rivest Sz Sloan's reliable and probably nearly almost always useful learning model. Both methods deal with the problem of accumulated error in combining local measurements to make global inferences. In the first approach, the effects of accumulated error are eliminated by the use of reliable and probably useful methods for discerning the local properties of space. In the second, the effects of accumulated error are reduced to acceptable levels by repeated exploration of the area to be learned. Finally, we suggest some insights into why certain existing techniques for map learning perform as well as they do.|Kenneth Basye,Thomas Dean,Jeffrey Scott Vitter",
        "15708|IJCAI|2001|Deriving a multi-domain information extraction system from a rough ontology|This paper presents a multi-domain information extraction system. In order to decrease the time spent on the elaboration of resources for the IE system and guide the end-user in a new domain, we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with the end-user to rapidly develop a local ontology giving an accurate image of the content of the text. The system is finally evaluated using classical indicators.|Thierry Poibeau",
        "14955|IJCAI|1991|A Formalization of Explanation-Based Macro-operator Learning|In spite of the popularity of Explanation-Based Learning (EBL), its theoretical basis is not well-understood. Using a generalization of Probably Approximately Correct (PAC) learning to problem solving domains, this paper formalizes two forms of Explanation-Based Learning of macro-operators and proves the sufficient conditions for their success. These two forms of EBL, called \"Macro Caching\" and \"Serial Parsing,\" respectively exhibit two distinct sources of power or \"bias\" the sparseness of the solution space and the decomposability of the problem-space. The analysis shows that exponential speedup can be achieved when either of these biases is suitable for a domain. Somewhat surprisingly, it also shows that computing the preconditions of the macro-operators is not necessary to obtain these speedups. The the oretical results are confirmed by experiments in the domain of Eight Puzzle. Our work suggests that the best way to address the utility problem in EBL is to implement a bias which exploits the problem-space structure of the set of domains that one is interested in learning.|Prasad Tadepalli",
        "15279|IJCAI|1995|Practical PAC Learning|We present new strategies for \"probably approximately correct\" (par) learning that use fewer training examples than previous approaches. The idea is to observe training examples one-at-a-time and decide \"on-line\" when to return a hypothesis, rather than collect a large fixed-size training sample. This yields sequential learning procedures that par-learn by observing a small random number of examples. We provide theoretical bounds on the expected training sample size of our procedure -- but establish its efficiency primarily by a scries of experiments which show sequential learning actually uses many times fewer training examples in practice. These results demonstrate that pac-learning can be far more efficiently achieved in practice than previously thought.|Dale Schuurmans,Russell Greiner",
        "16674|IJCAI|2007|Effective Control Knowledge Transfer through Learning Skill and Representation Hierarchies|Learning capabilities of computer systems still lag far behind biological systems. One of the reasons can be seen in the inefficient re-use of control knowledge acquired over the lifetime of the artificial learning system. To address this deficiency, this paper presents a learning architecture which transfers control knowledge in the form of behavioral skills and corresponding representation concepts from one task to subsequent learning tasks. The presented system uses this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy. Experimental results show that the presented method can significantly outperform learning on a flat state space representation and the MAXQ method for hierarchical reinforcement learning.|Mehran Asadi,Manfred Huber"
      ],
      [
        "14251|IJCAI|1985|A Prototypical Approach to Machine Learning|This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization. A structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.|R. I. Phelps,Peter B. Musgrove",
        "14787|IJCAI|1989|Integration of Semantic and Syntactic Constraints for Structural Noun Phrase Disambiguation|A fundamental problem in Natural Language Processing is the integration of syntactic and semantic constraints. In this paper we describe a new approach for the integration of syntactic and semantic constraints which takes advantage of a learned memory model. Our model combines localist representations for the integration of constraints and distributed representations for learning semantic constraints. We apply this model to the problem of structural disambiguation of noun phrases and show that a learned connectionist model can scale up the underlying memory of a Natural Language Processing system.|Stefan Wermter",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "15217|IJCAI|1995|Determining What to Learn Through Component-Task Modeling|Research in machine learning has typically addressed the problem of how and when to learn, and ignored the problem of formulating learning tasks in the first place. This paper addresses this issue in the context of the CASTLE system, that dynamically formulates learning tasks for a given situation. Our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. CASTLE can then focus the learning process on the issues involved in improving the performance of the particular component.|Bruce Krulwich,Lawrence Birnbaum,Gregg Collins",
        "14940|IJCAI|1991|Quantitative Evaluation of Explanation-Based Learning as an Optimisation Tool for a Large-Scale Natural Language System|This paper describes the application of explanation-based learning, a machine learning technique, to the SRI Core Language Engine, a large scale general purpose natural language analysis system. The idea is to bypass normal morphological, syntactic and (partly) semantic processing, for most input sentences, instead using a set of learned rules. Explanation-based learning is used to extract the learned rules automatically from sample sentences submitted by a user and thus tune the system for that particular user. By indexing the learned rules efficiently, it is possible to achieve dramatic speedups. Performance measurements were carried out using a training set of  sentences and a separate test set of  sentences, all from the ATIS corpus. A set of  learned rules was derived from the training set. These rules covered  percent of the test sentences and reduced the total processing time to a third. An overall speed-up of  percent was accomplished using a set of only  learned rules.|Christer Samuelsson,Manny Rayner",
        "17116|IJCAI|2009|Learning to Follow Navigational Route Instructions|We have developed a simulation model that accepts instructions in unconstrained natural language, and then guides a robot to the correct destination. The instructions are segmented on the basis of the actions to be taken, and each segment is labeled with the required action. This flat formulation reduces the problem to a sequential labeling task, to which machine learning methods are applied. We propose an innovative machine learning method for explicitly modeling the actions described in instructions and integrating learning and inference about the physical environment. We obtained a corpus of  route instructions that experimenters verified as follow-able, given by people in building navigation situations. Using the four-fold cross validation, our experiments showed that the simulated robot reached the correct destination % of the time.|Nobuyuki Shimizu,Andrew R. Haas"
      ],
      [
        "14786|IJCAI|1989|An Empirical Comparison of Pattern Recognition Neural Nets and Machine Learning Classification Methods|Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.|Sholom M. Weiss,Ioannis Kapouleas",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14699|IJCAI|1989|Constructive Induction On Decision Trees|Selective induction techniques perform poorly when the features are inappropriate for the target concept. One solution is to have the learning system construct new features automatically unfortunately feature construction is a difficult and poorly understood problem. In this paper we present a definition of feature construction in concept learning, and offer a framework for its study based on four aspects detection, selection, generalization, and evaluation. This framework is used in the analysis of existing learning systems and as the basis for the design of a new system, CITRE. CITRE performs feature construction using decision trees and simple domain knowledge as constructive biases. Initial results on a set of spatial-dependent problems suggest the importance of domain knowledge and feature generalization, i.e., constructive induction.|Christopher J. Matheus,Larry A. Rendell",
        "16340|IJCAI|2005|Question Classification by Structure Induction|In this article we introduce a new approach (and several implementations) to the task of question classification. The approach extracts structural information using machine learning techniques and the patterns found are used to classify the questions. The approach fits in between the machine learning and handcrafting of regular expressions (as it was done in the past) and combines the best of both classifiers can be generated automatically and the output can be investigated and manually optimised if needed.|Menno van Zaanen,Luiz Augusto Sangoi Pizzato,Diego Moll\u00e1",
        "16647|IJCAI|2007|A Hybrid Ontology Directed Feedback Selection Algorithm for Supporting Creative Problem Solving Dialogues|We evaluate a new hybrid language processing approach designed for interactive applications that maintain an interaction with users over multiple turns. Specifically, we describe a method for using a simple topic hierarchy in combination with a standard information retrieval measure of semantic similarity to reason about the selection of appropriate feedback in response to extended language inputs in the context of an interactive tutorial system designed to support creative problem solving. Our evaluation demonstrates the value of using a machine learning approach that takes feedback from experts into account for optimizing the hierarchy based feedback selection strategy.|Hao-Chuan Wang,Rohit Kumar,Carolyn Penstein Ros\u00e9,Tsai-Yen Li,Chun-Yen Chang",
        "13858|IJCAI|1981|Developing Microprocessor Based Expert Models for Instrument Interpretation|We describe a scheme for developing expert interpretive systems and transferring them to a microprocessor environment The scheme has been successfully implemented and tested by producing a program for interpreting results from a widely used medical laboratory instrument a scanning densitometer Specialists in the field of serum protein electrophoresis analysis provided the knowledge needed to build an interpretive model using EXPERT, a general production rule system for developing consultation models By constraining a few of the structures used in the general model it was possible to develop procedures for automatically translating the model to a specialized application program and then to a microprocessor assembly language program Thus, the model development can take place on a large machine, using established techniques for capturing and conveniently updating expert knowledge structures, while the final interpretive program can be targeted to a microprocessor depending on the application Our experience in carrying out the complete process illustrates many of the requirements involved in taking an expert system from its early development phase to actual implementation and use in a real world application.|Sholom M. Weiss,Casimir A. Kulikowski,Robert S. Galen"
      ],
      [
        "16731|IJCAI|2007|Online Learning and Exploiting Relational Models in Reinforcement Learning|In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits thesemodels by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally.|Tom Croonenborghs,Jan Ramon,Hendrik Blockeel,Maurice Bruynooghe",
        "17120|IJCAI|2009|A Tableaux-Based Method for Computing Least Common Subsumers for Expressive Description Logics|Least Common Subsumers (LCS) have been proposed in Description Logics (DL) to capture the commonalities between two or more concepts. Since its introduction in , LCS have been successfully employed as a logical tool for a variety of applications, spanning from inductive learning, to bottom-up construction of knowledge bases, information retrieval, to name a few. The best known algorithm for computing LCS uses structural comparison on normal forms, and the most expressive DL it is applied to is ALEN. We provide a general tableau-based calculus for computing LCS, via substitutions on concept terms containing concept variables. We show the applicability of our method to an expressive DL (but without disjunction and full negation), discuss complexity issues, and show the generality of our proposal.|Francesco M. Donini,Simona Colucci,Tommaso Di Noia,Eugenio Di Sciascio",
        "13480|IJCAI|1975|The Representation of Concepts in OWL|This paper discusses the theoretical basis of the formalist used as a foundation for OWL, a system for representing and processing conceptual knowledge. This foraallsa attempts to capture the expressive power of natural language by adopting the underlying representational conventions and \"conceptual models\" of English. The foraallsa Is built around specialization, which is perceived to be the key organizing principle of English at a deep conceptual level. The use of specialization In combination with another low-level structural device, reference, leads to a slaple but powerful structure, the concept, which is ideal both for the representation of a broad spectrum of conceptual knowledge and for computation on existing machines equipped with large, high-speed, random-access areas.|L. Hawkinson",
        "16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins",
        "14479|IJCAI|1987|Multiple Convergence An Approach to Disjunctive Concept Acquisition|Multiple convergence is proposed as a method for acquiring disjunctive concept descriptions. Disjunctive descriptions are necessary when the concept representation language is insufficiently expressive to satisfy the completeness and consistency requirements of inductive learning with a single conjunction of generalized features. Multiple convergence overcomes this insufficiency by allowing the disjuncts of a complex concept to be acquired independently. By summarizing correlations among features in the training data, disjunctive concepts can provide rich extensions to the representation language which may enhance subsequent learning. This paper presents the benefits of disjunctive concept descriptions and advocates multiple convergence as an approach to their acquisition. Multiple convergence has been implemented in the learning system HYDRA, and a detailed example of its execution is presented.|K. S. Murray",
        "16441|IJCAI|2007|Explanation-Based Feature Construction|Choosing good features to represent objects can be crucial to the success of supervised machine learning algorithms. Good high-level features are those that concentrate information about the classification task. Such features can often be constructed as non-linear combinations of raw or native input features such as the pixels of an image. Using many nonlinear combinations, as do SVMs, can dilute the classification information necessitating many training examples. On the other hand, searching even a modestly-expressive space of nonlinear functions for high-information ones can be intractable. We describe an approach to feature construction where task-relevant discriminative features are automatically constructed, guided by an explanation-based interaction of training examples and prior domain knowledge. We show that in the challenging task of distinguishing handwritten Chinese characters, our automatic feature-construction approach performs particularly well on the most difficult and complex character pairs.|Shiau Hong Lim,Li-Lun Wang,Gerald DeJong",
        "16264|IJCAI|2005|A Learning Scheme for Generating Expressive Music Performances of Jazz Standards|We describe our approach for generating expressive music performances of monophonic Jazz melodies. It consists of three components (a) a melodic transcription component which extracts a set of acoustic features from monophonic recordings, (b) a machine learning component which induces an expressive transformation model from the set of extracted acoustic features, and (c) a melody synthesis component which generates expressive monophonic output (MIDI or audio) from inexpressive melody descriptions using the induced expressive transformation model. In this paper we concentrate on the machine learning component, in particular, on the learning scheme we use for generating expressive audio from a score.|Rafael Ramirez,Amaury Hazan",
        "16314|IJCAI|2005|Learning to Play Like the Great Pianists|An application of relational instance-based learning to the complex task of expressive music performance is presented. We investigate to what extent a machine can automatically build 'expressive profiles' of famous pianists using only minimal performance information extracted from audio CD recordings by pianists and the printed score of the played music. It turns out that the machine-generated expressive performances on unseen pieces are substantially closer to the real performances of the 'trainer' pianist than those of all others. Two other interesting applications of the work are discussed recognizing pianists from their style of playing, and automatic style replication.|Asmir Tobudic,Gerhard Widmer"
      ]
    ]
  },
  "abstract": {
    "entropy": 4.5891124895239,
    "topics": [
      "learning data, learning approach, learning knowledge, paper learning, knowledge acquisition, learning using, conceptual clustering, based learning, problem learning, used learning, learning method, information learning, learning algorithm, training learning, learning show, learning results, learned learning, search learning, learning model, different learning",
      "learning data, learning approach, learning knowledge, paper learning, learning using, knowledge acquisition, conceptual clustering, problem learning, based learning, information learning, learning algorithm, learning method, used learning, learning show, learning results, training learning, search learning, learned learning, textual entailment, different learning",
      "learning data, learning approach, learning knowledge, paper learning, learning using, knowledge acquisition, problem learning, based learning, conceptual clustering, used learning, information learning, learning method, learning show, learning algorithm, learning results, training learning, search learning, textual entailment, learned learning, different learning",
      "learning data, learning approach, learning knowledge, paper learning, knowledge acquisition, learning using, conceptual clustering, problem learning, based learning, used learning, learning method, information learning, learning results, learning algorithm, search learning, learning show, training learning, learned learning, different learning, learning model",
      "learning knowledge, knowledge acquisition, paper learning, problem learning, learning approach, learning system, learning domain, learning model, learning show, learning algorithm, learning results, task learning, used learning, learning method, learn learning, present learning, use learning, learning data, search knowledge, based learning",
      "learning data, learning sets, learning using, learning approach, data sets, information learning, based learning, learned learning, mining data, unlabeled data, training learning, labeled learning, using data, labeled data, information data, labeled unlabeled, approach data, based data, learning labeling, using approach",
      "learning approach, hypotheses learning, explanation-based learning, paper learning, analogical planning, learning planning, learning knowledge, framework learning, planning approach, learning induction, learning using, based learning, hypotheses formed, information learning, derived learning, general learning, search learning, inductive learning, planning search, learning theory",
      "learning data, search distributed, distributed using, learning using, similarity data, using data, learning approach, distributed simulated, data sets, learning sets, matrix data, search learning, dissimilarity learning, unlabeled data, information data, labeled data, labeled learning, multiple clustering, proposed data, semi-supervised data",
      "learning knowledge, knowledge acquisition, paper learning, problem learning, search learning, learning approach, learning system, learning results, learning using, learning domain, used learning, learning method, learning show, learning model, based learning, learning algorithm, learned learning, search knowledge, use learning, task learning",
      "search distributed, distributed using, decision-making multi-agent, distributed simulated, entity news, search simulated, access news, learning decision-making, learning distributed, using simulated, using search, search news, simulated learning, underlying distributed, learning using, search learning, demonstration search, demonstration news, underlying search, learning drawn",
      "knowledge acquisition, learning knowledge, learned learning, learning using, learning approach, specific knowledge, different learning, search knowledge, analysis knowledge, different knowledge, approach knowledge, using approach, knowledge using, using learned, knowledge system, acquisition learning, learning data, knowledge used, information learning, based learning",
      "learning qualitative, textual entailment, qualitative quantitative, learning approach, analogical planning, partial learnable, classes learnable, information learning, learning planning, numerical qualitative, planning approach, consistently learnable, learning process, qualitative data, partial learning, classes partial, selection analogical, learning problem-solving, analogy analogical, classes learning"
    ],
    "ranking": [
      [
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "14518|IJCAI|1987|Concepts in Conceptual Clustering|Although it has a relatively short history, conceptual clustering is an especially active area of research in machine learning. There are a variety of ways in which conceptual patterns (the AI contribution to clustering) play a role in the clustering process. Two distinct conceptual clustering paradigms (conceptual sorting of exemplars and concept discovery) are described briefly. Then six types of conceptual clustering algorithms are characterized, attempting to cover the present spectrum of mechanisms used to conceptualize the clustering process.|Robert E. Stepp",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney",
        "14706|IJCAI|1989|The Effect of Rule Use on the Utility of Explanation-Based Learning|The utility problem in explanation-based learning concerns the ability of learned rules or plans to actually improve the performance of a problem solving system. Previous research on this problem has focused on the amount, content, or form of learned information. This paper examines the effect of the use of learned information on performance. Experiments and informal analysis show that unconstrained use of learned rules eventually leads to degraded performance. However, constraining the use of learned rules helps avoid the negative effect of learning and lead to overall performance improvement. Search strategy is also shown to have a substantial effect on the contribution of learning to performance by affecting the manner in which learned rules arc used. These effects help explain why previous experiments have obtained a variety of different results concerning the impact of explanation-based learning on performance.|Raymond J. Mooney"
      ],
      [
        "16932|IJCAI|2009|Reading Between the Lines|Reading involves, among others, identifying what is implied but not expressed in text. This task, known as textual entailment, offers a natural abstraction for many NLP tasks, and has been recognized as a central tool for the new area of Machine Reading. Important in the study of textual entailment is making precise the sense in which something is implied by text. The operational definition often employed is a subjective one something is implied if humans are more likely to believe it given the truth of the text, than otherwise. In this work we propose a natural objective definition for textual entailment. Our approach is to view text as a partial depiction of some underlying hidden reality. Reality is mapped into text through a possibly stochastic process, the author of the text. Textual entailment is then formalized as the task of accurately, in a defined sense, recovering information about this hidden reality. We show how existing machine learning work can be applied to this information recovery setting, and discuss the implications for the construction of machines that autonomously engage in textual entailment. We then investigate the role of using multiple inference rules for this task. We establish that such rules cannot be learned and applied in parallel, but that layered learning and reasoning are necessary.|Loizos Michael",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "14518|IJCAI|1987|Concepts in Conceptual Clustering|Although it has a relatively short history, conceptual clustering is an especially active area of research in machine learning. There are a variety of ways in which conceptual patterns (the AI contribution to clustering) play a role in the clustering process. Two distinct conceptual clustering paradigms (conceptual sorting of exemplars and concept discovery) are described briefly. Then six types of conceptual clustering algorithms are characterized, attempting to cover the present spectrum of mechanisms used to conceptualize the clustering process.|Robert E. Stepp",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney"
      ],
      [
        "16932|IJCAI|2009|Reading Between the Lines|Reading involves, among others, identifying what is implied but not expressed in text. This task, known as textual entailment, offers a natural abstraction for many NLP tasks, and has been recognized as a central tool for the new area of Machine Reading. Important in the study of textual entailment is making precise the sense in which something is implied by text. The operational definition often employed is a subjective one something is implied if humans are more likely to believe it given the truth of the text, than otherwise. In this work we propose a natural objective definition for textual entailment. Our approach is to view text as a partial depiction of some underlying hidden reality. Reality is mapped into text through a possibly stochastic process, the author of the text. Textual entailment is then formalized as the task of accurately, in a defined sense, recovering information about this hidden reality. We show how existing machine learning work can be applied to this information recovery setting, and discuss the implications for the construction of machines that autonomously engage in textual entailment. We then investigate the role of using multiple inference rules for this task. We establish that such rules cannot be learned and applied in parallel, but that layered learning and reasoning are necessary.|Loizos Michael",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "14518|IJCAI|1987|Concepts in Conceptual Clustering|Although it has a relatively short history, conceptual clustering is an especially active area of research in machine learning. There are a variety of ways in which conceptual patterns (the AI contribution to clustering) play a role in the clustering process. Two distinct conceptual clustering paradigms (conceptual sorting of exemplars and concept discovery) are described briefly. Then six types of conceptual clustering algorithms are characterized, attempting to cover the present spectrum of mechanisms used to conceptualize the clustering process.|Robert E. Stepp",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney"
      ],
      [
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine",
        "14646|IJCAI|1989|Concept Formation by Incremental Conceptual Clustering|Incremental conceptual clustering is an important area of machine learning. It is concerned with summarizing data in a form of concept hierarchies, which will eventually ease the problem of knowledge acquisition for knowledge-based systems. In this paper we have described INC, a program that generates a hierarchy of concept descriptions incrementally. INC searches a space of classification hierarchies in both top-down and bottom-up fashion. The system was evaluated along four dimensions and tested in two domains universities and countries.|Mirsad Hadzikadic,David Y. Y. Yun",
        "14518|IJCAI|1987|Concepts in Conceptual Clustering|Although it has a relatively short history, conceptual clustering is an especially active area of research in machine learning. There are a variety of ways in which conceptual patterns (the AI contribution to clustering) play a role in the clustering process. Two distinct conceptual clustering paradigms (conceptual sorting of exemplars and concept discovery) are described briefly. Then six types of conceptual clustering algorithms are characterized, attempting to cover the present spectrum of mechanisms used to conceptualize the clustering process.|Robert E. Stepp",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney",
        "14706|IJCAI|1989|The Effect of Rule Use on the Utility of Explanation-Based Learning|The utility problem in explanation-based learning concerns the ability of learned rules or plans to actually improve the performance of a problem solving system. Previous research on this problem has focused on the amount, content, or form of learned information. This paper examines the effect of the use of learned information on performance. Experiments and informal analysis show that unconstrained use of learned rules eventually leads to degraded performance. However, constraining the use of learned rules helps avoid the negative effect of learning and lead to overall performance improvement. Search strategy is also shown to have a substantial effect on the contribution of learning to performance by affecting the manner in which learned rules arc used. These effects help explain why previous experiments have obtained a variety of different results concerning the impact of explanation-based learning on performance.|Raymond J. Mooney"
      ],
      [
        "16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir",
        "15720|IJCAI|2001|Robot Weightlifting By Direct Policy Search|This paper describes a method for structuring a robot motor learning task. By designing a suitably parameterized policy, we show that a simple search algorithm, along with biologically motivated constraints, offers an effective means for motor skill acquisition. The framework makes use of the robot counterparts to several elements found in human motor learning imitation, equilibrium-point control, motor programs, and synergies. We demonstrate that through learning, coordinated behavior emerges from initial, crude knowledge about a difficult robot weightlifting task.|Michael T. Rosenstein,Andrew G. Barto",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "14226|IJCAI|1985|LEAP A Learning Apprentice for VLSl Design|It is by now well-recognised that a major impediment to developing know ledge-based systems is the knowledge acquisition bottleneck the task of building up a complete enough and correct enough knowledge base to provide high-level performance. This paper proposes a new class of knowledge-based systems designed to address this knowledge-acquisition bottleneck by incorporating a learning component to acquire new knowledge through experience. In particular, we define Learning Apprentice Systems as the class of interactive knowledge-based consultants that directly assimilate new knowledge by observing and analyzing the problem solving steps contributed by their users through their normal use of the system. This paper describes a specific Learning Apprentice System, called LEAP, which is presently being developed in the domain of VLSI design We also discuss design issues for Learning Apprentice Systems more generally, as well as restrictions on the generality of our current approach.|Tom M. Mitchell,Sridhar Mahadevan,Louis I. Steinberg",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo"
      ],
      [
        "16169|IJCAI|2005|Learning with Labeled Sessions|Traditional supervised learning deals with labeled instances. In many applications such as physiological data modeling and speaker identification, however, training examples are often labeled objects and each of the labeled objects consists of multiple unlabeled instances. When classifying a new object, its class is determined by the majority of its instance classes. As a consequence of this decision rule, one challenge to learning with labeled objects (or sessions) is to determine during training which subset of the instances inside an object should belong to the class of the object. We call this type of learning 'session-based learning' to distinguish it from the traditional supervised learning. In this paper, we introduce session-based learning problems, give a formal description of session-based learning in the context of related work, and propose an approach that is particularly designed for session-based learning. Empirical studies with UCI datasets and real-world data show that the proposed approach is effective for session-based learning.|Rong Jin,Huan Liu",
        "16711|IJCAI|2007|Semi-Supervised Gaussian Process Classifiers|In this paper, we propose a graph-based construction of semi-supervised Gaussian process classifiers. Our method is based on recently proposed techniques for incorporating the geometric properties of unlabeled data within globally defined kernel functions. The full machinery for standard supervised Gaussian process inference is brought to bear on the problem of learning from labeled and unlabeled data. This approach provides a natural probabilistic extension to unseen test examples. We employ Expectation Propagation procedures for evidence-based model selection. In the presence of few labeled examples, this approach is found to significantly outperform cross-validation techniques. We present empirical results demonstrating the strengths of our approach.|Vikas Sindhwani,Wei Chu,S. Sathiya Keerthi",
        "16577|IJCAI|2007|Optimistic Active-Learning Using Mutual Information|An \"active learning system\" will sequentially decide which unlabeled instance to label, with the goal of efficiently gathering the information necessary to produce a good classifier. Some such systems greedily select the next instance based only on properties of that instance and the few currently labeled points -- e.g., selecting the one closest to the current classification boundary. Unfortunately, these approaches ignore the valuable information contained in the other unlabeled instances, which can help identify a good classifier much faster. For the previous approaches that do exploit this unlabeled data, this information is mostly used in a conservative way. One common property of the approaches in the literature is that the active learner sticks to one single query selection criterion in the whole process. We propose a system, MM+M, that selects the query instance that is able to provide the maximum conditional mutual information about the labels of the unlabeled instances, given the labeled data, in an optimistic way. This approach implicitly exploits the discriminative partition information contained in the unlabeled data. Instead of using one selection criterion, MM+M also employs a simple on-line method that changes its selection rule when it encounters an \"unexpected label\". Our empirical results demonstrate that this new approach works effectively.|Yuhong Guo,Russell Greiner",
        "16870|IJCAI|2009|Knowledge Transfer on Hybrid Graph|In machine learning problems, labeled data are often in short supply. One of the feasible solution for this problem is transfer learning. It can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain. In this paper, we propose a transfer learning framework based on similarity matrix approximation to tackle such problems. Two practical algorithms are proposed, which are the label propagation and the similarity propagation. In these methods, we build a hybrid graph based on all available data. Then the information is transferred cross domains through alternatively constructing the similarity matrix for different part of the graph. Among all related methods, similarity propagation approach can make maximum use of all available similarity information across domains. This leads to more efficient transfer and better learning result. The experiment on real world text mining applications demonstrates the promise and effectiveness of our algorithms.|Zheng Wang,Yangqiu Song,Changshui Zhang",
        "16346|IJCAI|2005|Semi-Supervised Regression with Co-Training|In many practical machine learning and data mining applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. Previous research mainly focuses on semi-supervised classification. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two k-nearest neighbor regressors with different distance metrics, each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates.|Zhi-Hua Zhou,Ming Li",
        "15878|IJCAI|2003|Spectral Learning|We present a simple, easily implemented spectral learning algorithm which applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the  Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data. By using normalized affinity matrices which are both symmetric and stochastic, we also obtain both a probabilistic interpretation of our method and certain guarantees of performance.|Sepandar D. Kamvar,Dan Klein,Christopher D. Manning",
        "15723|IJCAI|2001|Active Learning for Class Probability Estimation and Ranking|For many supervised learning tasks it is very costly to produce training data with class labels. Active learning acquires data incrementally, at each stage using the model learned so far to help identify especially useful additional data for labeling. Existing empirical active learning approaches have focused on learning classifiers. However, many applications require estimations of the probability of class membership, or scores that can be used to rank new cases. We present a new active learning method for class probability estimation (CPE) and ranking. BOOTSTRAP-LV selects new data for labeling based on the variance in probability estimates, as determined by learning multiple models from bootstrap samples of the existing labeled data. We show empirically that the method reduces the number of data items that must be labeled, across a wide variety of data sets. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV dominates for CPE. Surprisingly it also often is preferable for accelerating simple accuracy maximization.|Maytal Saar-Tsechansky,Foster J. Provost",
        "17011|IJCAI|2009|Robust Distance Metric Learning with Auxiliary Knowledge|Most of the existing metric learning methods are accomplished by exploiting pairwise constraints over the labeled data and frequently suffer from the insufficiency of training examples. To learn a robust distance metric from few labeled examples, prior knowledge from unlabeled examples as well as the metrics previously derived from auxiliary data sets can be useful. In this paper, we propose to leverage such auxiliary knowledge to assist distance metric learning, which is formulated following the regularized loss minimization principle. Two algorithms are derived on the basis of manifold regularization and log-determinant divergence regularization technique, respectively, which can simultaneously exploit label information (i.e., the pairwise constraints over labeled data), unlabeled examples, and the metrics derived from auxiliary data sets. The proposed methods directly manipulate the auxiliary metrics and require no raw examples from the auxiliary data sets, which make them efficient and flexible. We conduct extensive evaluations to compare our approaches with a number of competing approaches on face recognition task. The experimental results show that our approaches can derive reliable distance metrics from limited training examples and thus are superior in terms of accuracy and labeling efforts.|Zheng-Jun Zha,Tao Mei,Meng Wang,Zengfu Wang,Xian-Sheng Hua"
      ],
      [
        "14370|IJCAI|1987|A Formal Approach to Learning From Examples|This paper presents a formal, foundational approach to learning from examples in machine learning. It is assumed that a learning system is presented with a stream of facts describing a domain of application. The task of the system is to form and modify hypotheses characterising the relations in the domain, based on this information. Presumably the set of hypotheses that may be so formed will require continual revision as further information is received. The emphasis in this paper is to characterise those hypotheses that may potentially be formed, rather than to specify the subset of the hypotheses that, for whatever reason, should be held. To this end. formal systems are derived from which the set of potential hypotheses that may be formed is precisely specified. A procedure is also derived for restoring the consistency of a set of hypotheses after conflicting evidence is encountered. In addition, this work is extended to where a learning system may be \"told\" arbitrary sentences concerning a domain The approach is intended to provide a basic framework lor the development of systems that learn from examples, as well as a neutral point from which such systems may be viewed and compared.|James P. Delgrande",
        "15145|IJCAI|1995|Extending Classical Planning to Real-World Execution with Machine Learning|In previous work (Bennett  DeJong and Bennetl ) we proposed a machine learning approach called permissive planning to extend classical planning into the realm of real world plan execution. Our prior results have been favorable but empirical (Bennett and DeJong ). Here we examine the analytic foundations of our empirical success. We advance a formal account of realworld planning adequacy. We prove that permissive planning does what it claims to do it probabilistically achieves adequate real-world performance or guarantees that no adequate real-world planning behavior is possible within the flexibility allowed. We prove that the approach scales tractably We prove that restrictions are necessary without them permissive planning is impossible. We also show how these restrictions can be quite naturally met through schema based planning and explanation-based learning.|Gerald DeJong,Scott Bennett",
        "16562|IJCAI|2007|Real-Time Heuristic Search with a Priority Queue|Learning real-time search, which interleaves planning and acting, allows agents to learn from multiple trials and respond quickly. Such algorithms require no prior knowledge of the environment and can be deployed without pre-processing. We introduce Prioritized-LRTA* (P-LRTA*), a learning real-time search algorithm based on Prioritized Sweeping. P-LRTA* focuses learning on important areas of the search space, where the importance of a state is determined by the magnitude of the updates made to neighboring states. Empirical tests on path-planning in commercial game maps show a substantial learning speed-up over state-of-the-art real-time search algorithms.|D. Chris Rayner,Katherine Davison,Vadim Bulitko,Kenneth Anderson,Jieshan Lu",
        "14844|IJCAI|1991|The Base Selection Task in Analogical Planning|Analogical planning provides a means of solving problems where other machine learning methods fail, because it does not require numerous previous examples or a rich domain theory. Given a problem in an unfamiliar domain (the target case), an analogical planning system locates a successful plan in a similar domain (the bast case), and uses the similarities to generate the target plan. Unfortunately, the analogical planning process is expensive and inflexible Many of the limiting factors reside in the base selection step, which drives the analogy formation process. This paper describes two ways of increasing the effectiveness and efficiency of analogical planning. First, a parallel graph-match base selection algorithm is presented. A parallel implementation on the Connection Machine is described and shown to substantially decrease the complexity of base selection. Second, a base-case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefits of several base cases when no single plan contributes enough information to the analogy. The effectiveness of this approach is demonstrated with examples from the domain of automatic programming.|Diane J. Cook",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "15335|IJCAI|1997|Learning to Improve both Efficiency and Quality of Planning|Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the SCOPE learning system to acquire control knowledge that improves on both of these metrics. Since SCOPE uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. Experimental results show that SCOPE can significantly improve both the quality of final plans and overall planning efficiency.|Tara A. Estlin,Raymond J. Mooney",
        "16551|IJCAI|2007|Discriminative Learning of Beam-Search Heuristics for Planning|We consider the problem of learning heuristics for controlling forward state-space beam search in AI planning domains. We draw on a recent framework for \"structured output classification\" (e.g. syntactic parsing) known as learning as search optimization (LaSO). The LaSO approach uses discriminative learning to optimize heuristic functions for search-based computation of structured outputs and has shown promising results in a number of domains. However, the search problems that arise in AI planning tend to be qualitatively very different from those considered in structured classification, which raises a number of potential difficulties in directly applying LaSO to planning. In this paper, we discuss these issues and describe a LaSO-based approach for discriminative learning of beam-search heuristics in AI planning domains. We give convergence results for this approach and present experiments in several benchmark domains. The results show that the discriminatively trained heuristic can outperform the one used by the planner FF and another recent non-discriminative learning approach.|Yuehua Xu,Alan Fern,Sung Wook Yoon"
      ],
      [
        "16711|IJCAI|2007|Semi-Supervised Gaussian Process Classifiers|In this paper, we propose a graph-based construction of semi-supervised Gaussian process classifiers. Our method is based on recently proposed techniques for incorporating the geometric properties of unlabeled data within globally defined kernel functions. The full machinery for standard supervised Gaussian process inference is brought to bear on the problem of learning from labeled and unlabeled data. This approach provides a natural probabilistic extension to unseen test examples. We employ Expectation Propagation procedures for evidence-based model selection. In the presence of few labeled examples, this approach is found to significantly outperform cross-validation techniques. We present empirical results demonstrating the strengths of our approach.|Vikas Sindhwani,Wei Chu,S. Sathiya Keerthi",
        "16870|IJCAI|2009|Knowledge Transfer on Hybrid Graph|In machine learning problems, labeled data are often in short supply. One of the feasible solution for this problem is transfer learning. It can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain. In this paper, we propose a transfer learning framework based on similarity matrix approximation to tackle such problems. Two practical algorithms are proposed, which are the label propagation and the similarity propagation. In these methods, we build a hybrid graph based on all available data. Then the information is transferred cross domains through alternatively constructing the similarity matrix for different part of the graph. Among all related methods, similarity propagation approach can make maximum use of all available similarity information across domains. This leads to more efficient transfer and better learning result. The experiment on real world text mining applications demonstrates the promise and effectiveness of our algorithms.|Zheng Wang,Yangqiu Song,Changshui Zhang",
        "16346|IJCAI|2005|Semi-Supervised Regression with Co-Training|In many practical machine learning and data mining applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. Previous research mainly focuses on semi-supervised classification. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two k-nearest neighbor regressors with different distance metrics, each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates.|Zhi-Hua Zhou,Ming Li",
        "15878|IJCAI|2003|Spectral Learning|We present a simple, easily implemented spectral learning algorithm which applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the  Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data. By using normalized affinity matrices which are both symmetric and stochastic, we also obtain both a probabilistic interpretation of our method and certain guarantees of performance.|Sepandar D. Kamvar,Dan Klein,Christopher D. Manning",
        "15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer",
        "16958|IJCAI|2009|Semi-Supervised Metric Learning Using Pairwise Constraints|Distance metric has an important role in many machine learning algorithms. Recently, metric learning for semi-supervised algorithms has received much attention. For semi-supervised clustering, usually a set of pairwise similarity and dissimilarity constraints is provided as supervisory information. Until now, various metric learning methods utilizing pairwise constraints have been proposed. The existing methods that can consider both positive (must-link) and negative (cannot-link) constraints find linear transformations or equivalently global Mahalanobis metrics. Additionally, they find metrics only according to the data points appearing in constraints (without considering other data points). In this paper, we consider the topological structure of data along with both positive and negative constraints. We propose a kernel-based metric learning method that provides a non-linear transformation. Experimental results on synthetic and real-world data sets show the effectiveness of our metric learning method.|Mahdieh Soleymani Baghshah,Saeed Bagheri Shouraki",
        "16515|IJCAI|2007|A Scalable Kernel-Based Algorithm for Semi-Supervised Metric Learning|In recent years, metric learning in the semisupervised setting has aroused a lot of research interests. One type of semi-supervised metric learning utilizes supervisory information in the form of pairwise similarity or dissimilarity constraints. However, most methods proposed so far are either limited to linear metric learning or unable to scale up well with the data set size. In this paper, we propose a nonlinear metric learning method based on the kernel approach. By applying low-rank approximation to the kernel matrix, our method can handle significantly larger data sets. Moreover, our low-rank approximation scheme can naturally lead to out-of-sample generalization. Experiments performed on both artificial and real-world data show very promising results.|Dit-Yan Yeung,Hong Chang,Guang Dai"
      ],
      [
        "15720|IJCAI|2001|Robot Weightlifting By Direct Policy Search|This paper describes a method for structuring a robot motor learning task. By designing a suitably parameterized policy, we show that a simple search algorithm, along with biologically motivated constraints, offers an effective means for motor skill acquisition. The framework makes use of the robot counterparts to several elements found in human motor learning imitation, equilibrium-point control, motor programs, and synergies. We demonstrate that through learning, coordinated behavior emerges from initial, crude knowledge about a difficult robot weightlifting task.|Michael T. Rosenstein,Andrew G. Barto",
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14828|IJCAI|1991|Classifiers A Theoretical and Empirical Study|This paper describes how a competitive tree learning algorithm can be derived from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and statistical families of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed.|Wray L. Buntine",
        "15296|IJCAI|1995|Learning One More Thing|Most research on machine learning has focused on scenarios in which a learner faces a single isolated learning task. The lifelong learning framework assume, that the learner encounters a multitude of related learning tasks over Us lifetime providing the opportunity for the transfer of knowledge among these. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach in which knowledge is transferred via a learned model of the invariances of the domain Results on learning to recognize objects from color images demonstrate superior generalization capabilities of invariances are learned and used to bias subsequent learning.|Sebastian Thrun,Tom M. Mitchell",
        "14226|IJCAI|1985|LEAP A Learning Apprentice for VLSl Design|It is by now well-recognised that a major impediment to developing know ledge-based systems is the knowledge acquisition bottleneck the task of building up a complete enough and correct enough knowledge base to provide high-level performance. This paper proposes a new class of knowledge-based systems designed to address this knowledge-acquisition bottleneck by incorporating a learning component to acquire new knowledge through experience. In particular, we define Learning Apprentice Systems as the class of interactive knowledge-based consultants that directly assimilate new knowledge by observing and analyzing the problem solving steps contributed by their users through their normal use of the system. This paper describes a specific Learning Apprentice System, called LEAP, which is presently being developed in the domain of VLSI design We also discuss design issues for Learning Apprentice Systems more generally, as well as restrictions on the generality of our current approach.|Tom M. Mitchell,Sridhar Mahadevan,Louis I. Steinberg",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "14697|IJCAI|1989|Utilization Filtering A Method for Reducing the Inherent Harmfulness of Deductively Learned Knowledge|This paper highlights a phenomenon that causes deductively learned knowledge to be harmful when used for problem solving. The problem occurs when deductive problem solvers encounter a failure branch of the search tree. The backtracking mechanism of such problem solvers will force the program to traverse the whole subtree thus visiting many nodes twice - once by using the deductively learned rule and once by using the rules that generated the learned rule in the first place. We suggest an approach called utilization filtering to solve that problem. Learners that use this approach submit to the problem solver a filter function together with the knowledge that was acquired. The function decides for each problem whether to use the learned knowledge and what part of it to use. We have tested the idea in the context of a lemma learning system, where the filter uses the probability of a subgoal failing to decide whether to turn lemma usage off. Experiments show an improvement of performance by a factor of .|Shaul Markovitch,Paul D. Scott"
      ],
      [
        "15138|IJCAI|1995|On Bootstrapping Local Search with Trail-Markers|We study a simple, general framework for search called bootstrap search, which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. We present a simple algorithm for bootstrap search, and provide some initial theory on its performance. In our theoretical analysis, we develop a random digraph problem model and use it to make some performance predictions and comparisons. We also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. We validate our theoretical results with empirical demonstration on the -puzzle. We show how to reduce the cost of a global search by  orders of magnitude using bootstrap search. We also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution.|Pang C. Chen",
        "15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore",
        "15501|IJCAI|1999|Towards Flexible Multi-Agent Decision-Making Under Time Pressure|To perform rational decision-making, autonomous agents need considerable computational resources. In multi-agent settings, when other agents are present in the environment, these demands are even more severe. We investigate ways in which the agent's knowledge and the results of deliberative decision-making can be compiled to reduce the complexity of decision-making procedures and to save time in urgent situations. We use machine learning algorithms to compile decision-theoretic deliberations into condition-action rules on how to coordinate in a multi-agent environment. Using different learning algorithms, we endow a resource-bounded agent with a tapestry of decision making tools, ranging from purely reactive to fully deliberative ones. The agent can then select a method depending on the time constraints of the particular situation. We also propose combining the decision-making tools, so that, for example, more reactive methods serve as a pre-processing stage to the more accurate but slower deliberative decision-making ones. We validate our framework with experimental results in simulated coordinated defense. The experiments show that compiling the results of decision-making saves deliberation time while offering good performance in our multi-agent domain.|Sanguk Noh,Piotr J. Gmytrasiewicz",
        "16637|IJCAI|2007|An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource Allocation Mechanism|Multiagent distributed resource allocation requires that agents act on limited, localized information with minimum communication overhead in order to optimize the distribution of available resources. When requirements and constraints are dynamic, learning agents may be needed to allow for adaptation. One way of accomplishing learning is to observe past outcomes, using such information to improve future decisions. When limits in agents' memory or observation capabilities are assumed, one must decide on how large should the observation window be. We investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems, namely dispersion games. We show by numerical experiments over a specific dispersion game (the Minority Game) that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. We then provide an information-theoretic explanation for the observed behaviors, showing that a downward causation effect takes place.|Ricardo M. Araujo,Lu\u00eds C. Lamb",
        "15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer",
        "15928|IJCAI|2003|Broadcast News Navigator BNN Demonstration|The Broadcast News Navigator (BNN) is a fully implemented system that incorporates image, speech, and language processing together with visualization and user preference modeling to support intelligent, personalized access to broadcast news video. The demonstration will illustrate the use of the system's underlying machine learning enabled story segmentation and processing, called the Broadcast News Editor (BNE). A live, scenario-based demonstration will illustrate the use of named entity search, temporal visualization of entities, story clustering and geospatial story visualization, discovery of entity relations, and personalized multimedia summary generation. By transforming access from sequential to direct search and providing hierarchical hyperlinked summaries, we will demonstrate how users can access topics and entity specific news clusters nearly three times as fast as direct search of digital video. In short, we will demonstrate intelligent news on demand enabled by a suite of AI technologies.|Mark T. Maybury"
      ],
      [
        "14372|IJCAI|1987|Knowledge-based Knowledge Elicitation|A method for using the advantages of domain-specific knowledge acquisition for a general purpose knowledge acquisition tool is introduced. To adapt the knowledge acquisition tool for a specific application and a specific problem solving strategy (e.g. heuristic classification, such diagnostic strategies as establish and refine), acquisition knowledge bases (AKBs) are integrated in the system to guide the employment of different knowledge elicitation methods (interview techniques, protocol analysis, semantic text analysis and learning mechanisms). Acquisition knowledge bases are predefined deep models, consisting of structured objects to represent important concepts of a domain. These knowledge bases are used in addition to the already acquired knowledge to trigger specific elicitation methods by an analysis of incompleteness and inconsistency of the existing knowledge in the system. Furthermore, methods for integrating these kinds of knowledge acquisition tools with machine learning approaches are discussed.|Joachim Diederich",
        "14226|IJCAI|1985|LEAP A Learning Apprentice for VLSl Design|It is by now well-recognised that a major impediment to developing know ledge-based systems is the knowledge acquisition bottleneck the task of building up a complete enough and correct enough knowledge base to provide high-level performance. This paper proposes a new class of knowledge-based systems designed to address this knowledge-acquisition bottleneck by incorporating a learning component to acquire new knowledge through experience. In particular, we define Learning Apprentice Systems as the class of interactive knowledge-based consultants that directly assimilate new knowledge by observing and analyzing the problem solving steps contributed by their users through their normal use of the system. This paper describes a specific Learning Apprentice System, called LEAP, which is presently being developed in the domain of VLSI design We also discuss design issues for Learning Apprentice Systems more generally, as well as restrictions on the generality of our current approach.|Tom M. Mitchell,Sridhar Mahadevan,Louis I. Steinberg",
        "13666|IJCAI|1977|Levels of Pattern Description in Learning|A learning system in a complex, real-world domain will require a significant amount of knowledge to be used in order to () deal with large numbers of features, most of which are irrelevant, and () find similarities between the concepts that are inferred from the observed data. Use of knowledge-free, syntactic approaches to generalization in complex environments will result in a combinatorial explosion in the number of possible generalizations. Moreover, the important semantic features are not \"in\" the data rather they must be hypothesized using prior knowledge. The learning system described in this paper uses a multi-level knowledge-directed approach in order to cope with these problems. This paradigm is explored in the action-oriented game of baseball. The system attempts to interpret observed activity in terms of general knowledge provided about competitive games. This approach to learning can be viewed as a type of recognition, where the level of initial knowledge is general and where the specific observations mold a particular structure from the general knowledge. The system is organized into multiple levels of pattern descriptions, processing, and knowledge, reflecting the logical structure of the problem. In moving through those levels of description, the system filters out irrelevant features, hypothesizes additional semantic features (goals and relationships) and forms a hierarchy of generalized classes that extract the similarities in the descriptions. Examples of learning by a working computer program are presented.|Elliot Soloway,Edward M. Riseman",
        "15542|IJCAI|1999|Combining Weak Knowledge Sources for Sense Disambiguation|There has been a tradition of combining different knowledge sources in Artificial Intelligence research. We apply this methodology to word sense disambiguation (WSD), a long-standing problem in Computational Linguistics. We report on an implemented sense tagger which uses a machine readable dictionary to provide both a set of senses and associated forms of information on which to base disambiguation decisions. The system is based on an architecture which makes use of different sources of lexical knowledge in two ways and optimises their combination using a learning algorithm. Tested accuracy of our approach on a general corpus exceeds %, demonstrating the viability of allword disambiguation as opposed to restricting oneself to a small sample.|Mark Stevenson,Yorick Wilks",
        "16490|IJCAI|2007|General Game Learning Using Knowledge Transfer|We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.|Bikramjit Banerjee,Peter Stone",
        "15886|IJCAI|2003|Proactive Dialogue for Interactive Knowledge Capture|Current tools for interactive knowledge capture have little or no learning aptitude. They are mostly oblivious to the process or strategy that the user may be following in entering new knowledge, unaware of their progress during a session, and ignorant of typical skills expected from a good student. We present an approach to make acquisition interfaces more proactive by extending them with ) goals that represent what remains to be learned, ) strategies to achieve these goals and acquire further knowledge, and ) awareness of the current status of the body of knowledge learned. The resulting interaction shows that the system is aware of its progress towards acquiring the new knowledge, and moves forward by understanding what acquisition goals and strategies to pursue.|Jihie Kim,Yolanda Gil",
        "16547|IJCAI|2007|Transferring Learned Control-Knowledge between Planners|As any other problem solving task that employs search, AI Planning needs heuristics to efficiently guide the problem-space exploration. Machine learning (ML) provides several techniques for automatically acquiring those heuristics. Usually, a planner solves a problem, and a ML technique generates knowledge from the search episode in terms of complete plans (macro-operators or cases), or heuristics (also named control knowledge in planning). In this paper, we present a novel way of generating planning heuristics we learn heuristics in one planner and transfer them to another planner. This approach is based on the fact that different planners employ different search bias. We want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. The goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. We employ a deductive learning method (EBL) that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a Graphplan-based planner. Then, we transform the learned knowledge so that it can be used by a bidirectional planner.|Susana Fern\u00e1ndez,Ricardo Aler,Daniel Borrajo",
        "14697|IJCAI|1989|Utilization Filtering A Method for Reducing the Inherent Harmfulness of Deductively Learned Knowledge|This paper highlights a phenomenon that causes deductively learned knowledge to be harmful when used for problem solving. The problem occurs when deductive problem solvers encounter a failure branch of the search tree. The backtracking mechanism of such problem solvers will force the program to traverse the whole subtree thus visiting many nodes twice - once by using the deductively learned rule and once by using the rules that generated the learned rule in the first place. We suggest an approach called utilization filtering to solve that problem. Learners that use this approach submit to the problem solver a filter function together with the knowledge that was acquired. The function decides for each problem whether to use the learned knowledge and what part of it to use. We have tested the idea in the context of a lemma learning system, where the filter uses the probability of a subgoal failing to decide whether to turn lemma usage off. Experiments show an improvement of performance by a factor of .|Shaul Markovitch,Paul D. Scott"
      ],
      [
        "16592|IJCAI|2007|Learning from Partial Observations|We present a general machine learning framework for modelling the phenomenon of missing information in data. We propose a masking process model to capture the stochastic nature of information loss. Learning in this context is employed as a means to recover as much of the missing information as is recoverable. We extend the Probably Approximately Correct semantics to the case of learning from partial observations with arbitrarily hidden attributes. We establish that simply requiring learned hypotheses to be consistent with observed values suffices to guarantee that hidden values are recoverable to a certain accuracy we also show that, in some sense, this is an optimal strategy for achieving accurate recovery. We then establish that a number of natural concept classes, including all the classes of monotone formulas that are PAC learnable by monotone formulas, and the classes of conjunctions, disjunctions, k-CNF, k-DNF, and linear thresholds, are consistently learnable from partial observations. We finally show that the concept classes of parities and monotone term -decision lists are not properly consistently learnable from partial observations, if RP  NP. This implies a separation of what is consistently learnable from partial observations versus what is learnable in the complete or noisy setting.|Loizos Michael",
        "17086|IJCAI|2009|Autonomously Learning an Action Hierarchy Using a Learned Qualitative State Representation|There has been intense interest in hierarchical reinforcement learning as a way to make Markov decision process planning more tractable, but there has been relatively little work on autonomously learning the hierarchy, especially in continuous domains. In this paper we present a method for learning a hierarchy of actions in a continuous environment. Our approach is to learn a qualitative representation of the continuous environment and then to define actions to reach qualitative states. Our method learns one or more options to perform each action. Each option is learned by first learning a dynamic Bayesian network (DBN). We approach this problem from a developmental robotics perspective. The agent receives no extrinsic reward and has no external direction for what to learn. We evaluate our work using a simulation with realistic physics that consists of a robot playing with blocks at a table.|Jonathan Mugan,Benjamin Kuipers",
        "14844|IJCAI|1991|The Base Selection Task in Analogical Planning|Analogical planning provides a means of solving problems where other machine learning methods fail, because it does not require numerous previous examples or a rich domain theory. Given a problem in an unfamiliar domain (the target case), an analogical planning system locates a successful plan in a similar domain (the bast case), and uses the similarities to generate the target plan. Unfortunately, the analogical planning process is expensive and inflexible Many of the limiting factors reside in the base selection step, which drives the analogy formation process. This paper describes two ways of increasing the effectiveness and efficiency of analogical planning. First, a parallel graph-match base selection algorithm is presented. A parallel implementation on the Connection Machine is described and shown to substantially decrease the complexity of base selection. Second, a base-case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefits of several base cases when no single plan contributes enough information to the analogy. The effectiveness of this approach is demonstrated with examples from the domain of automatic programming.|Diane J. Cook",
        "15987|IJCAI|2003|Qualitatively Faithful Quantitative Prediction|In this paper we describe a case study in which we applied an approach to qualitative machine learning to induce, from system's behaviour data, a qualitative model of a complex, industrially relevant mechanical system (a car wheel suspension system). The induced qualitative model enables nice causal interpretation of the relations in the modelled system. Moreover, we also show that the qualitative model can be used to guide the quantitative modelling process leading to numerical predictions that may be considerably more accurate than those obtained by state-of-the-art numerical modelling methods. This idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages () induction of qualitative constraints from system's behaviour data, and () induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. We call this approach Q learning, which stands for Qualitatively faithful Quantitative learning.|Dorian Suc,Daniel Vladusic,Ivan Bratko",
        "16745|IJCAI|2007|Analogical Learning in a Turn-Based Strategy Game|A key problem in playing strategy games is learning how to allocate resources effectively. This can be a difficult task for machine learning when the connections between actions and goal outputs are indirect and complex. We show how a combination of structural analogy, experimentation, and qualitative modeling can be used to improve performance in optimizing food production in a strategy game. Experimentation bootstraps a case library and drives variation, while analogical reasoning supports retrieval and transfer. A qualitative model serves as a partial domain theory to support adaptation and credit assignment. Together, these techniques can enable a system to learn the effects of its actions, the ranges of quantities, and to apply training in one city to other, structurally different cities. We describe experiments demonstrating this transfer of learning.|Thomas R. Hinrichs,Kenneth D. Forbus",
        "14383|IJCAI|1987|An Examination of the Third Stage in the Analogy Process Verification-based Analogical Learning|Many studies of analogy in Artificial Intelligence have focused on analogy as a heuristic mechanism to guide search and simplify problem solving or as a basis for forming generalizations. This paper examines analogical learning, where analogy is used to conjecture new knowledge about some domain. A theory of Verification-Based Analogical Learning is presented which addresses the tenuous nature of analogically inferred concepts and describes procedures that can be used to increase confidence in the inferred knowledge. The theory describes how analogy may be used to discover and refine scientific models of the physical world. Examples are taken from an implemented system, which discovers qualitative models of processes such as liquid flow and heat flow.|Brian Falkenhainer"
      ]
    ]
  },
  "title": {
    "entropy": 6.6158701488723,
    "topics": [
      "learning, learning approach, reinforcement learning, neural networks, constructive induction, machine learning, empirical comparison, theoretical framework, active learning, learning using, adaptation analysis, knowledge transfer, artificial intelligence, virtual demonstration, mobile robot, inverse belief, learning rule, utility back-propagation, version space, learning networks",
      "text categorization, question answering, building domain-specific, weighting categorization, sense disambiguation, domain-specific search, building search, weighting text, support vector, web search, word sense, word disambiguation, margin clustering, conditional graphical, recognition graphical, integration answering, predictive evaluation, simple gaussian, simple weighting, improve coordination",
      "information extraction, decision trees, real boosting, hidden markov, boosting decision, metric learning, logic programming, inductive logic, unsupervised estimation, inductive programming, strategy inferring, generalized inferring, generalized strategy, knowledge graph, hierarchical hidden, flexible multi-agent, hierarchical markov, learning strategy, hidden extraction, case policy",
      "conceptual clustering, natural language, problem solving, dimensionality reduction, evaluation tool, learning natural, task planning, local matrix, feature selection, analyzing complexity, induction complexity, continuous dimensionality, reduction mechanism, quantitative evaluation, selection task, explanation-based learning, hybrid ontology, continuous reduction, sequential genetic, base task",
      "semi-supervised learning, transfer learning, learning policies, adaptation analysis, virtual demonstration, policies demonstration, policies virtual, semi-supervised regression, learning demonstration, learning virtual, adaptation transfer, transfer analysis, semi-supervised using, learning agents, theory, detection, process, sparse, distributed, control",
      "learning approach, constructive induction, learning rule, learning examples, learning identification, relational learning, machine approach, identification resolution, mobile robot, induction learning, induction approach, approach resolution, approach examples, approach constructive, learning robot, statistical learning, learning mobile, formal learning, approach identification, formal approach",
      "question answering, building domain-specific, domain-specific search, building search, web search, integration answering, learning domain-specific, integration question, learning web, adaptive answering, domain-specific web, learning building, adaptive question, building web, learning search, adaptive, terms, application, intelligent, processes",
      "text categorization, weighting categorization, weighting text, simple weighting, simple approach, simple gaussian, world knowledge, simple categorization, using knowledge, categorization world, approach categorization, gaussian categorization, gaussian weighting, using world, generation world, simple text, weighting approach, text world, categorization using, approach text",
      "learning constrained, flexible multi-agent, flexible time, case policy, learning reasoning, multi-agent time, learning case, temporal reasoning, policy reasoning, case reasoning, reasoning, system, optimal, combining",
      "metric learning, logic programming, inductive logic, inductive programming, learning instructions, domains, dynamic, inference, descriptions, program, state",
      "learning search, task planning, feature selection, selection task, analogical learning, sequential genetic, base task, base planning, base analogical, sequential selection, analogical planning, task analogical, via linear, base selection, selection via, sequential search, selection linear, genetic selection, selection planning, learning heuristics",
      "local matrix, learning multiagent, learning matrix, theories"
    ],
    "ranking": [
      [
        "14786|IJCAI|1989|An Empirical Comparison of Pattern Recognition Neural Nets and Machine Learning Classification Methods|Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.|Sholom M. Weiss,Ioannis Kapouleas",
        "16533|IJCAI|2007|Bayesian Inverse Reinforcement Learning|Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.|Deepak Ramachandran,Eyal Amir",
        "16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan",
        "16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao",
        "16379|IJCAI|2007|Learning Policies for Embodied Virtual Agents through Demonstration|Although many powerful AI and machine learning techniques exist, it remains difficult to quickly create AI for embodied virtual agents that produces visually lifelike behavior. This is important for applications (e.g., games, simulators, interactive displays) where an agent must behave in a manner that appears human-like. We present a novel technique for learning reactive policies that mimic demonstrated human behavior. The user demonstrates the desired behavior by dictating the agent's actions during an interactive animation. Later, when the agent is to behave autonomously, the recorded data is generalized to form a continuous state-to-action mapping. Combined with an appropriate animation algorithm (e.g., motion capture), the learned policies realize stylized and natural-looking agent behavior. We empirically demonstrate the efficacy of our technique for quickly producing policies which result in lifelike virtual agent behavior.|Jonathan Dinerstein,Parris K. Egbert,Dan Ventura",
        "15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik",
        "15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller",
        "13635|IJCAI|1977|Version Spaces A Candidate Elimination Approach to Rule Learning|An important research problem in artificial intelligence is the study of methods for learning general concepts or rules from a set of training instances. An approach to this problem is presented which is guaranteed to find, without backtracing, all rule versions consistent with a set of positive and negative training instances. The algorithm put forth uses a representation of the space of those rules consistent with the observed training data. This \"rule version space\" is modified in response to new training instances by eliminating candidate rule versions found to conflict with each new instance. The use of version spaces is discussed in the context of Meta-DENDRAL, a program which learns rules in the domain of chemical spectroscopy.|Tom M. Mitchell"
      ],
      [
        "15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore",
        "16303|IJCAI|2005|Beyond TFIDF Weighting for Text Categorization in the Vector Space Model|KNN and SVM are two machine learning approaches to Text Categorization (TC) based on the Vector Space Model. In this model, borrowed from Information Retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary. Traditionally, each component value is assigned using the information retrieval TFIDF measure. While this weighting method seems very appropriate for IR, it is not clear that it is the best choice for TC problems. Actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents. In this paper, we introduce a new weighting method based on statistical estimation of the importance of a word for a specific categorization problem. This method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight. Extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks.|Pascal Soucy,Guy W. Mineau",
        "15700|IJCAI|2001|Keyword Spices A New Method for Building Domain-Specific Web Search Engines|This paper presents a new method for building domain-specific web search engines. Previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. Accordingly, they are hard to build and can not be applied to other domains. The keyword spice method, in contrast, improves search performance by adding domain-specific keywords, called keyword spices, to the user's input query the modified query is then forwarded to a general-purpose search engine. Keyword spices can be effectively discovered automatically from web documents allowing us to build high quality domain-specific search engines in various domains without requiring the collection of heuristic knowledge. We describe a machine learning algorithm, which is a type of decision-tree learning algorithm, that can extract keyword spices. To demonstrate the value of the proposed approach, we conduct experiments in the domain of cooking. The results confirm the excellent performance of our method in terms of both precision and recall.|Satoshi Oyama,Takashi Kokubo,Toru Ishida,Teruhiro Yamada,Yasuhiko Kitamura",
        "16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli",
        "15946|IJCAI|2003|Does a New Simple Gaussian Weighting Approach Perform Well in Text Categorization|A new approach to the Text Categorization problem is here presented. It is called Gaussian Weighting and it is a supervised learning algorithm that, during the training phase, estimates two very simple and easily computable statistics which are the Presence P, how much a term is present in a category c in the Expressiveness E, how much is present outside c in the rest of the domain. Once the system has learned this information, a Gaussian function is shaped for each term of a category, in order to assign the term a weight that estimates the level of its importance for that particular category. We tested our learning method on the task of single-label classification using the Reuters- benchmark. The outcome of the result was quite impressive in different experimental setups, we reached a micro-averaged Fl-measure of ., with a peak of .. Moreover, a macro-averaged Recall and Precision was calculated the former reported a ., the latter a .. These results reach most of the state-of-the-art techniques of machine learning applied to Text Categorization, demonstrating that this new weighting scheme does perform well on this particular task.|Giorgio Maria Di Nunzio,Alessandro Micarelli"
      ],
      [
        "16686|IJCAI|2007|Hierarchical Multi-channel Hidden Semi Markov Models|Many interesting human actions involve multiple interacting agents and also have typical durations. Further, there is an inherent hierarchical organization of these activities. In order to model these we introduce a new family of hidden Markov models (HMMs) that provide compositional state representations in both space and time and also a recursive hierarchical structure for inference at higher levels of abstraction. In particular, we focus on two possible -layer structures - the Hierarchical-Semi Parallel Hidden Markov Model (HSPaHMM) and the Hierarchical Parallel Hidden Semi-Markov Model (HPaHSMM). The lower layer of HSPaHMM consists of multiple HMMs for each agent while the top layer consists of a single HSMM. HPaHSMM on the other hand has multiple HSMMs at the lower layer and a Markov chain at the top layer. We present efficient learning and decoding algorithms for these models and then demonstrate them first on synthetic time series data and then in an application for sign language recognition.|Pradeep Natarajan,Ramakant Nevatia",
        "15978|IJCAI|2003|Hierarchical Hidden Markov Models for Information Extraction|Information extraction can be defined as the task of automatically extracting instances of specified classes or relations from text. We consider the case of using machine learning methods to induce models for extracting relation instances from biomedical articles. We propose and evaluate an approach that is based on using hierarchical hidden Markov models to represent the grammatical structure of the sentences being processed. Our approach first uses a shallow parser to construct a multi-level representation of each sentence being processed. Then we train hierarchical HMMs to capture the regularities of the parses for both positive and negative sentences. We evaluate our method by inducing models to extract binary relations in three biomedical domains. Our experiments indicate that our approach results in more accurate models than several baseline HMM approaches.|Marios Skounakis,Mark Craven,Soumya Ray",
        "15713|IJCAI|2001|Representing Sentence Structure in Hidden Markov Models for Information Extraction|We study the application of Hidden Markov Models (HMMs) to learning information extractors for n-ary relations from free text. We propose an approach to representing the grammatical structure of sentences in the states of the model. We also investigate using an objective function during HMM training which maximizes the ability of the learned models to identify the phrases of interest. We evaluate our methods by deriving extractors for two binary relations in biomedical domains. Our experiments indicate that our approach learns more accurate models than several baseline approaches.|Soumya Ray,Mark Craven",
        "14933|IJCAI|1991|Determinate Literals in Inductive Logic Programming|A recent system, FOIL, constructs Horn clause programs from numerous examples. Computational efficiency is achieved by using greedy search guided by an information-based heuristic. Greedy search tends to be myopic but determinate terms, an adaptation of an idea introduced by another new system (GOLEM), has been found to provide many of the benefits of lookahead without substantial increases in computation. This paper sketches key ideas from FOIL and GOLEM and discusses the use of determinate literals in a greedy search context. The efficacy of this approach is illustrated on the task of learning the quicksort procedure and other small but non-trivial list-manipulation functions.|J. Ross Quinlan",
        "15109|IJCAI|1995|AILP Abductive Inductive Logic Programming|Inductive Logic Programming (ILP) is often situated as a research area emerging at the intersection of Machine Learning and Logic Programming (LP). This paper makes the link more clear between ILP and LP, in particular, between ILP and Abductive Logic Programming (ALP), i e, LP extended with abductive reasoning. We formulate a generic framework for handling incomplete knowledge. This framework can be instantiated both to ALP and ILP approaches. By doing so more light is shed on the relationship between abduction and induction. As an example we consider the abductive procedure SLDNFA, and modify it into an inductive procedure which we call SLDNFAI.|Hilde Ad\u00e9,Marc Denecker",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "14213|IJCAI|1985|Verification-based Learning A Generalized Strategy for Inferring Problem-Reduction Methods|A major impediment to the development of high-performance knowledge-based systems arises from the prohibitive effort involved in equipping these systems with a sufficient set of problem-solving methods. Thus, one important research problem in Machine Learning has been the study of techniques for inferring problem-solving methods from examples. Although a number of techniques for learning problem-solving methods have been described in the literature, all of them assume a state-space model of problem-solving. In this paper we describe a new technique for learning problem-reduction methods, Verification-Based Learning (VBL), which extends the earlier techniques to the problem-reduction formulation of problem-solving. We illustrate the VBL technique with examples drawn from circuit design and symbolic integration.|Sridhar Mahadevan"
      ],
      [
        "15799|IJCAI|2003|Continuous nonlinear dimensionality reduction by kernel Eigenmaps|We equate nonlinear dimensionality reduction (NLDR) to graph embedding with side information about the vertices, and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space. Unlike most spectral NLDR methods, the central eigenproblem can be made relatively small, and the result is a continuous mapping defined over the entire space, not just the datapoints. A demonstration is made to visualizing the distribution of word usages (as a proxy to word meanings) in a sample of the machine learning literature.|Matthew Brand",
        "16802|IJCAI|2007|Representations for Action Selection Learning from Real-Time Observation of Task Experts|The association of perception and action is key to learning by observation in general, and to program-level task imitation in particular. The question is how to structure this information such that learning is tractable for resource-bounded agents. By introducing a combination of symbolic representation with Bayesian reasoning, we demonstrate both theoretical and empirical improvements to a general-purpose imitation system originally based on a model of infant social learning. We also show how prior task knowledge and selective attention can be rigorously incorporated via loss matrices and Automatic Relevance Determination respectively.|Mark A. Wood,Joanna Bryson",
        "14844|IJCAI|1991|The Base Selection Task in Analogical Planning|Analogical planning provides a means of solving problems where other machine learning methods fail, because it does not require numerous previous examples or a rich domain theory. Given a problem in an unfamiliar domain (the target case), an analogical planning system locates a successful plan in a similar domain (the bast case), and uses the similarities to generate the target plan. Unfortunately, the analogical planning process is expensive and inflexible Many of the limiting factors reside in the base selection step, which drives the analogy formation process. This paper describes two ways of increasing the effectiveness and efficiency of analogical planning. First, a parallel graph-match base selection algorithm is presented. A parallel implementation on the Connection Machine is described and shown to substantially decrease the complexity of base selection. Second, a base-case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefits of several base cases when no single plan contributes enough information to the analogy. The effectiveness of this approach is demonstrated with examples from the domain of automatic programming.|Diane J. Cook",
        "16952|IJCAI|2009|Linear Dimensionality Reduction for Multi-label Classification|Dimensionality reduction is an essential step in high-dimensional data analysis. Many dimensionality reduction algorithms have been applied successfully to multi-class and multi-label problems. They are commonly applied as a separate data preprocessing step before classification algorithms. In this paper, we study a joint learning framework in which we perform dimensionality reduction and multi-label classification simultaneously. We show that when the least squares loss is used in classification, this joint learning decouples into two separate components, i.e., dimensionality reduction followed by multi-label classification. This analysis partially justifies the current practice of a separate application of dimensionality reduction for classification problems. We extend our analysis using other loss functions, including the hinge loss and the squared hinge loss. We further extend the formulation to the more general case where the input data for different class labels may differ, overcoming the limitation of traditional dimensionality reduction algorithms. Experiments on benchmark data sets have been conducted to evaluate the proposed joint formulations.|Shuiwang Ji,Jieping Ye",
        "14940|IJCAI|1991|Quantitative Evaluation of Explanation-Based Learning as an Optimisation Tool for a Large-Scale Natural Language System|This paper describes the application of explanation-based learning, a machine learning technique, to the SRI Core Language Engine, a large scale general purpose natural language analysis system. The idea is to bypass normal morphological, syntactic and (partly) semantic processing, for most input sentences, instead using a set of learned rules. Explanation-based learning is used to extract the learned rules automatically from sample sentences submitted by a user and thus tune the system for that particular user. By indexing the learned rules efficiently, it is possible to achieve dramatic speedups. Performance measurements were carried out using a training set of  sentences and a separate test set of  sentences, all from the ATIS corpus. A set of  learned rules was derived from the training set. These rules covered  percent of the test sentences and reduced the total processing time to a third. An overall speed-up of  percent was accomplished using a set of only  learned rules.|Christer Samuelsson,Manny Rayner",
        "14213|IJCAI|1985|Verification-based Learning A Generalized Strategy for Inferring Problem-Reduction Methods|A major impediment to the development of high-performance knowledge-based systems arises from the prohibitive effort involved in equipping these systems with a sufficient set of problem-solving methods. Thus, one important research problem in Machine Learning has been the study of techniques for inferring problem-solving methods from examples. Although a number of techniques for learning problem-solving methods have been described in the literature, all of them assume a state-space model of problem-solving. In this paper we describe a new technique for learning problem-reduction methods, Verification-Based Learning (VBL), which extends the earlier techniques to the problem-reduction formulation of problem-solving. We illustrate the VBL technique with examples drawn from circuit design and symbolic integration.|Sridhar Mahadevan"
      ],
      [
        "16887|IJCAI|2009|Semi-Supervised Classification Using Sparse Gaussian Process Regression|Gaussian Processes (GPs) are promising Bayesian methods for classification and regression problems. They have also been used for semi-supervised learning tasks. In this paper, we propose a new algorithm for solving semi-supervised binary classification problem using sparse GP regression (GPR) models. It is closely related to semi-supervised learning based on support vector regression (SVR) and maximum margin clustering. The proposed algorithm is simple and easy to implement. It gives a sparse solution directly unlike the SVR based algorithm. Also, the hyperparameters are estimated easily without resorting to expensive cross-validation technique. Use of sparse GPR model helps in making the proposed algorithm scalable. Preliminary results on synthetic and real-world data sets demonstrate the efficacy of the new algorithm.|Amrish Patel,S. Sundararajan,Shirish Krishnaj Shevade",
        "16467|IJCAI|2007|Graph-Based Semi-Supervised Learning as a Generative Model|This paper proposes and develops a new graph-based semi-supervised learning method. Different from previous graph-based methods that are based on discriminative models, our method is essentially a generative model in that the class conditional probabilities are estimated by graph propagation and the class priors are estimated by linear regression. Experimental results on various datasets show that the proposed method is superior to existing graph-based semi-supervised learning methods, especially when the labeled subset alone proves insufficient to estimate meaningful class priors.|Jingrui He,Jaime G. Carbonell,Yan Liu 0002",
        "17118|IJCAI|2009|Spectral Kernel Learning for Semi-Supervised Classification|Typical graph-theoretic approaches for semi-supervised classification infer labels of unlabeled instances with the help of graph Laplacians. Founded on the spectral decomposition of the graph Laplacian, this paper learns a kernel matrix via minimizing the leave-one-out classification error on the labeled instances. To this end, an efficient algorithm is presented based on linear programming, resulting in a transductive spectral kernel. The idea of our algorithm stems from regularization methodology and also has a nice interpretation in terms of spectral clustering. A simple classifier can be readily built upon the learned kernel, which suffices to give prediction for any data point aside from those in the available dataset. Besides this usage, the spectral kernel can be effectively used in tandem with conventional kernel machines such as SVMs. We demonstrate the efficacy of the proposed algorithm through experiments carried out on challenging classification tasks.|Wei Liu,Buyue Qian,Jingyu Cui,Jianzhuang Liu",
        "16379|IJCAI|2007|Learning Policies for Embodied Virtual Agents through Demonstration|Although many powerful AI and machine learning techniques exist, it remains difficult to quickly create AI for embodied virtual agents that produces visually lifelike behavior. This is important for applications (e.g., games, simulators, interactive displays) where an agent must behave in a manner that appears human-like. We present a novel technique for learning reactive policies that mimic demonstrated human behavior. The user demonstrates the desired behavior by dictating the agent's actions during an interactive animation. Later, when the agent is to behave autonomously, the recorded data is generalized to form a continuous state-to-action mapping. Combined with an appropriate animation algorithm (e.g., motion capture), the learned policies realize stylized and natural-looking agent behavior. We empirically demonstrate the efficacy of our technique for quickly producing policies which result in lifelike virtual agent behavior.|Jonathan Dinerstein,Parris K. Egbert,Dan Ventura",
        "16848|IJCAI|2009|Domain Adaptation via Transfer Component Analysis|Domain adaptation solves a learning problem in a target domain by utilizing the training data in a different but related source domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). In the subspace spanned by these transfer components, data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. The main contribution of our work is that we propose a novel feature representation in which to perform domain adaptation via a new parametric kernel using feature extraction methods, which can dramatically minimize the distance between domain distributions by projecting data onto the learned transfer components. Furthermore, our approach can handle large datsets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach in are verified by experiments on two real-world applications cross-domain indoor WiFi localization and cross-domain text classification.|Sinno Jialin Pan,Ivor W. Tsang,James T. Kwok,Qiang Yang",
        "16958|IJCAI|2009|Semi-Supervised Metric Learning Using Pairwise Constraints|Distance metric has an important role in many machine learning algorithms. Recently, metric learning for semi-supervised algorithms has received much attention. For semi-supervised clustering, usually a set of pairwise similarity and dissimilarity constraints is provided as supervisory information. Until now, various metric learning methods utilizing pairwise constraints have been proposed. The existing methods that can consider both positive (must-link) and negative (cannot-link) constraints find linear transformations or equivalently global Mahalanobis metrics. Additionally, they find metrics only according to the data points appearing in constraints (without considering other data points). In this paper, we consider the topological structure of data along with both positive and negative constraints. We propose a kernel-based metric learning method that provides a non-linear transformation. Experimental results on synthetic and real-world data sets show the effectiveness of our metric learning method.|Mahdieh Soleymani Baghshah,Saeed Bagheri Shouraki",
        "16947|IJCAI|2009|Selecting Informative Universum Sample for Semi-Supervised Learning|The Universum sample, which is defined as the sample that doesn't belong to any of the classes the learning task concerns, has been proved to be helpful in both supervised and semi-supervised settings. The former works treat the Universum samples equally. Our research found that not all the Universum samples are helpful, and we propose a method to pick the informative ones, i.e., in-between Universum samples. We also set up a new semi-supervised framework to incorporate the in-between Universum samples. Empirical experiments show that our method outperforms the former ones.|Shuo Chen,Changshui Zhang"
      ],
      [
        "14370|IJCAI|1987|A Formal Approach to Learning From Examples|This paper presents a formal, foundational approach to learning from examples in machine learning. It is assumed that a learning system is presented with a stream of facts describing a domain of application. The task of the system is to form and modify hypotheses characterising the relations in the domain, based on this information. Presumably the set of hypotheses that may be so formed will require continual revision as further information is received. The emphasis in this paper is to characterise those hypotheses that may potentially be formed, rather than to specify the subset of the hypotheses that, for whatever reason, should be held. To this end. formal systems are derived from which the set of potential hypotheses that may be formed is precisely specified. A procedure is also derived for restoring the consistency of a set of hypotheses after conflicting evidence is encountered. In addition, this work is extended to where a learning system may be \"told\" arbitrary sentences concerning a domain The approach is intended to provide a basic framework lor the development of systems that learn from examples, as well as a neutral point from which such systems may be viewed and compared.|James P. Delgrande",
        "13864|IJCAI|1981|Concept Learning by Structured Examples - An Algebraic Approach|A system learning concepts from training samples consisting of structured objects is described. It is based on descriptions invariant under isomorphism. In order to get a unified mathematical formalism recent graph theoretic results are used- The structures are transformed into feature vectors and after that a concept learning algorithm developing decision trees is applied which is an extension of algorithms found in psychological experiments. It corresponds to a general-to-specific depth-first search with reexamination of past events. The generalization ability is demonstrated by means of the blocks world example and it is shown that the algorithm can successfully handle practical problems with samples of about one hundred of relatively complicated structures in a reasonable time. Additionally, the problem of representation and learning context dependent concepts is discussed in the paper.|Fritz Wysotzki,Werner Kolbe,Joachim Selbig",
        "14251|IJCAI|1985|A Prototypical Approach to Machine Learning|This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization. A structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.|R. I. Phelps,Peter B. Musgrove",
        "16233|IJCAI|2005|A Machine Learning Approach to Identification and Resolution of One-Anaphora|We present a machine learning approach to identifying and resolving one-anaphora. In this approach, the system first learns to distinguish different uses of instances of the word one in the second stage, the antecedents of those instances of one that are classified as anaphoric are then determined. We evaluated our approach on written texts drawn from the informative domains of the British National Corpus (BNC), and achieved encouraging results. To our knowledge, this is the first learning-based system for the identification and resolution of one-anaphora.|Hwee Tou Ng,Yu Zhou,Robert Dale,Mary Gardiner",
        "14534|IJCAI|1987|Guiding Constructive Induction for Incremental Learning from Examples|LAIR is a system that incrementally learns conjunctive concept descriptions from positive and negative examples. These concept descriptions are used to create and extend a domain theory that is applied, by means of constructive induction, to later learning tasks. Important issues for constructive induction are when to do it and how to control it LAIR demonstrates how constructive induction can be controlled by () reducing it to simpler operations, () constraining the simpler operations to preserve relative correctness, () doing deductive inference on an as-needed basis to meet specific information requirements of learning subtasks, and () constraining the search space by subtask-dependent constraints.|Larry Watanabe,Renee Elio",
        "15151|IJCAI|1995|Rule Induction and Instance-Based Learning A Unified Approach|This paper presents a new approach to inductive learning that combines aspects of instancebased learning and rule induction in a single simple algorithm. The RISE system searches for rules in a specific-to-general fashion, starting with one rule per training example, and avoids some of the difficulties of separate-and-eonquer approaches by evaluating each proposed induction step globally, i e, through an efficient procedure that is equivalent to checking the accuracy of the rule set as a whole on every training example. Classification is performed using a best-match strategy, and reduces to nearest-neighbor if all generalizations of instances were rejected. An extensive empirical study shows that RISE consistently achieves higher accuracies than state-of-the-art representatives of its \"parent\" paradigms (PEBLS and CN), and also outperforms a decision-tree learner (C ) in  out of  test domains (in  with % confidence).|Pedro Domingos",
        "13635|IJCAI|1977|Version Spaces A Candidate Elimination Approach to Rule Learning|An important research problem in artificial intelligence is the study of methods for learning general concepts or rules from a set of training instances. An approach to this problem is presented which is guaranteed to find, without backtracing, all rule versions consistent with a set of positive and negative training instances. The algorithm put forth uses a representation of the space of those rules consistent with the observed training data. This \"rule version space\" is modified in response to new training instances by eliminating candidate rule versions found to conflict with each new instance. The use of version spaces is discussed in the context of Meta-DENDRAL, a program which learns rules in the domain of chemical spectroscopy.|Tom M. Mitchell"
      ],
      [
        "16373|IJCAI|2007|Learning User Clicks in Web Search|Machine learning for predicting user clicks in Web-based search offers automated explanation of user activity. We address click prediction in the Web search scenario by introducing a method for click prediction based on observations of past queries and the clicked documents. Due to the sparsity of the problem space, commonly encountered when learning for Web search, new approaches to learn the probabilistic relationship between documents and queries are proposed. Two probabilistic models are developed, which differ in the interpretation of the query-document co-occurrences. A novel technique, namely, conditional probability hierarchy, flexibly adjusts the level of granularity in parsing queries, and, as a result, leverages the advantages of both models.|Ding Zhou,Levent Bolelli,Jia Li,C. Lee Giles,Hongyuan Zha",
        "15489|IJCAI|1999|A Machine Learning Approach to Building Domain-Specific Search Engines|Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system a search engine for computer science research papers available at www.cora.justrcsettrch.com.|Andrew McCallum,Kamal Nigam,Jason Rennie,Kristie Seymore",
        "16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins",
        "16907|IJCAI|2009|Introspection and Adaptable Model Integration for Dialogue-based Question Answering|Dialogue-based Question Answering (QA) is a highly complex task that brings together a QA system including various natural language processing components (i.e., components for question classification, information extraction, and retrieval) with dialogue systems for effective and natural communication. The dialogue-based access is difficult to establish when the QA system in use is complex and combines many different answer services with different quality and access characteristics. For example, some questions are processed by opendomain QA services with a broad coverage. Others should be processed by using a domain-specific instance ontology for more reliable answers. Different answer services may change their characteristics over time and the dialogue reaction models have to be updated according to that. To solve this problem, we developed introspective methods to integrate adaptable models of the answer services. We evaluated the impact of the learned models on the dialogue performance, i.e., whether the adaptable models can be used for a more convenient dialogue formulation process. We show significant effectiveness improvements in the resulting dialogues when using the machine learning (ML) models. Examples are provided in the context of the generation of system-initiative feedback to user questions and answers, as provided by heterogeneous information services.|Daniel Sonntag",
        "15700|IJCAI|2001|Keyword Spices A New Method for Building Domain-Specific Web Search Engines|This paper presents a new method for building domain-specific web search engines. Previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. Accordingly, they are hard to build and can not be applied to other domains. The keyword spice method, in contrast, improves search performance by adding domain-specific keywords, called keyword spices, to the user's input query the modified query is then forwarded to a general-purpose search engine. Keyword spices can be effectively discovered automatically from web documents allowing us to build high quality domain-specific search engines in various domains without requiring the collection of heuristic knowledge. We describe a machine learning algorithm, which is a type of decision-tree learning algorithm, that can extract keyword spices. To demonstrate the value of the proposed approach, we conduct experiments in the domain of cooking. The results confirm the excellent performance of our method in terms of both precision and recall.|Satoshi Oyama,Takashi Kokubo,Toru Ishida,Teruhiro Yamada,Yasuhiko Kitamura",
        "16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli"
      ],
      [
        "16122|IJCAI|2005|Feature Generation for Text Categorization Using World Knowledge|We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.|Evgeniy Gabrilovich,Shaul Markovitch",
        "16699|IJCAI|2007|Avoidance of Model Re-Induction in SVM-Based Feature Selection for Text Categorization|Searching the feature space for a subset yielding optimum performance tends to be expensive, especially in applications where the cardinality of the feature space is high (e.g., text categorization). This is particularly true for massive datasets and learning algorithms with worse than linear scaling factors. Linear Support Vector Machines (SVMs) are among the top performers in the text classification domain and often work best with very rich feature representations. Even they however benefit from reducing the number of features, sometimes to a large extent. In this work we propose alternatives to exact re-induction of SVM models during the search for the optimum feature subset. The approximations offer substantial benefits in terms of computational efficiency. We are able to demonstrate that no significant compromises in terms of model quality are made and, moreover, in some cases gains in accuracy can be achieved.|Aleksander Kolcz,Abdur Chowdhury",
        "15682|IJCAI|2001|Using Text Classifiers for Numerical Classification|Consider a supervised learning problem in which examples contain both numerical- and text-valued features. To use traditional feature-vector-based learning methods, one could treat the presence or absence of a word as a Boolean feature and use these binary-valued features together with the numerical features. However, the use of a text-classification system on this is a bit more problematic --in the most straight-forward approach each number would be considered a distinct token and treated as a word. This paper presents an alternative approach for the use of text classification methods for supervised learning problems with numerical-valued features in which the numerical features are converted into bag-of-words features, thereby making them directly usable by text classification methods. We show that even on purely numerical-valued data the results of text-classification on the derived text-like representation outperforms the more naive numbers-as-tokens representation and, more importantly, is competitive with mature numerical classification methods such as C. and Ripper.|Sofus A. Macskassy,Haym Hirsh,Arunava Banerjee,Aynur A. Dayanik",
        "16303|IJCAI|2005|Beyond TFIDF Weighting for Text Categorization in the Vector Space Model|KNN and SVM are two machine learning approaches to Text Categorization (TC) based on the Vector Space Model. In this model, borrowed from Information Retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary. Traditionally, each component value is assigned using the information retrieval TFIDF measure. While this weighting method seems very appropriate for IR, it is not clear that it is the best choice for TC problems. Actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents. In this paper, we introduce a new weighting method based on statistical estimation of the importance of a word for a specific categorization problem. This method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight. Extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks.|Pascal Soucy,Guy W. Mineau",
        "15818|IJCAI|2003|Hierarchical Semantic Classification Word Sense Disambiguation with World Knowledge|We present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general \"world knowledge\". The model compiles knowledge contained in a dictionary-ontology into additional training data, and integrates task-specific and background data through a novel hierarchical learning architecture. Experiments on a word sense disambiguation task provide empirical evidence that this \"hierarchical classifier\" outperforms a state-of-the-art standard \"flat\" one.|Massimiliano Ciaramita,Thomas Hofmann,Mark Johnson",
        "16112|IJCAI|2005|A Simple-Transition Model for Relational Sequences|We use \"nearly sound\" logical constraints to infer hidden states of relational processes. We introduce a simple-transition cost model, which is parameterized by weighted constraints and a statetransition cost. Inference for this model, i.e. finding a minimum-cost state sequence, reduces to a single-state minimization (SSM) problem. For relational Horn constraints, we give a practical approach to SSM based on logical reasoning and bounded search. We present a learning method that discovers relational constraints using CLAUDIEN De Raedt and Dehaspe,  and then tunes their weights using perceptron updates. Experiments in relational video interpretation show that our learned models improve on a variety of competitors.|Alan Fern",
        "15946|IJCAI|2003|Does a New Simple Gaussian Weighting Approach Perform Well in Text Categorization|A new approach to the Text Categorization problem is here presented. It is called Gaussian Weighting and it is a supervised learning algorithm that, during the training phase, estimates two very simple and easily computable statistics which are the Presence P, how much a term is present in a category c in the Expressiveness E, how much is present outside c in the rest of the domain. Once the system has learned this information, a Gaussian function is shaped for each term of a category, in order to assign the term a weight that estimates the level of its importance for that particular category. We tested our learning method on the task of single-label classification using the Reuters- benchmark. The outcome of the result was quite impressive in different experimental setups, we reached a micro-averaged Fl-measure of ., with a peak of .. Moreover, a macro-averaged Recall and Precision was calculated the former reported a ., the latter a .. These results reach most of the state-of-the-art techniques of machine learning applied to Text Categorization, demonstrating that this new weighting scheme does perform well on this particular task.|Giorgio Maria Di Nunzio,Alessandro Micarelli"
      ],
      [
        "16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone",
        "15501|IJCAI|1999|Towards Flexible Multi-Agent Decision-Making Under Time Pressure|To perform rational decision-making, autonomous agents need considerable computational resources. In multi-agent settings, when other agents are present in the environment, these demands are even more severe. We investigate ways in which the agent's knowledge and the results of deliberative decision-making can be compiled to reduce the complexity of decision-making procedures and to save time in urgent situations. We use machine learning algorithms to compile decision-theoretic deliberations into condition-action rules on how to coordinate in a multi-agent environment. Using different learning algorithms, we endow a resource-bounded agent with a tapestry of decision making tools, ranging from purely reactive to fully deliberative ones. The agent can then select a method depending on the time constraints of the particular situation. We also propose combining the decision-making tools, so that, for example, more reactive methods serve as a pre-processing stage to the more accurate but slower deliberative decision-making ones. We validate our framework with experimental results in simulated coordinated defense. The experiments show that compiling the results of decision-making saves deliberation time while offering good performance in our multi-agent domain.|Sanguk Noh,Piotr J. Gmytrasiewicz",
        "16057|IJCAI|2005|TimeML-Compliant Text Analysis for Temporal Reasoning|Reasoning with time needs more than just a list of temporal expressions. TimeML--an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text--is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeML-compliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.|Branimir Boguraev,Rie Kubota Ando",
        "14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi",
        "14340|IJCAI|1987|Learning Strategies by Reasoning about Rules|One of the major 'weaknesses of current automated reasoning systems is that they lack the ability to control inference in a sophisticated, context-directed fashion. General strategies such as the set-of-support strategy are useful, but have proven inadequate for many individual problems. A strategy component is needed that possesses knowledge about many particular domains and problems. Such a body of knowledge would require a prohibitive amount of time to construct by hand. This leads us to consider means of automatically acquiring control knowledge from example proofs. One particular means of learning is explanation-based learning. This paper analyzes the basis of explanations -- finding weakest preconditions that enable a particular rule to fire -- to derive a representation within which explanations can be extracted from examples, generalized and used to guide the actions of a problem-solving system.|D. Paul Benjamin",
        "15284|IJCAI|1995|Remembering To Forget A Competence-Preserving Case Deletion Policy for Case-Based Reasoning Systems|The utility problem occurs when the cost associated with searching for relevant knowledge outweighs the benefit of applying this knowledge. One common machine learning strategy for coping with this problem ensures that stored knowledge is genuinely useful, deleting any structures that do not contribute to performance in a positive sense, and essentially limiting the size of the knowledge-base. We will examine this deletion strategy in the context of case-based reasoning (CBR) systems. In CBR the impact of the utility problem is very much dependant on the size and growth of the case-base larger case-bases mean more expensive retrieval stages, an expensive overhead in CBR systems. Traditional deletion strategies will keep performance in check (and thereby control the classical utility problem) but they may cause problems for CBR system competence. This effect is demonstrated experimentally and in reply two new deletion strategies are proposed that can take both competence and performance into consideration during deletion.|Barry Smyth,Mark T. Keane"
      ],
      [
        "13624|IJCAI|1977|A Proof-Checker for Dynamic Logic|We consider the problem of getting a computer to follow reasoning conducted in dynamic logic. This is a recently developed logic of programs that subsumes most existing first-order logics of programs that manipulate their environment, including Floyd's and Hoare's logics of partial correctness and Manna and Waldinger's logic of total correctness. Dynamic logic is more closely related to classical first-order logic than any other proposed logic of programs. This simplifies the design of a proof-checker for dynamic logic. Work in progress on the implementation of such a program is reported on, and an example machine-checked proof is exhibited.|Steven D. Litvintchouk,Vaughan R. Pratt",
        "14257|IJCAI|1985|Dynamic Student Modelling in an Intelligent Tutor for LISP Programming|We describe an intelligent tutor for LISP programming. This tutor achieves a set of pedagogical objectives derived from Anderson's () learning theory provide instruction in the context of problem-solving, have the student generate as much of each solution as possible, provide immediate feedback on errors, and represent the goal structure of the problem-solving. The tutorial interface facilitates communication and prevents distracting low-level errors Field tests of the tutor in college classes demonstrate that it is more effective than conventional classroom instruction.|Brian J. Reiser,John R. Anderson,Robert G. Farrell",
        "15740|IJCAI|2001|Neural Logic Network Learning using Genetic Programming|Neural Logic Network or Neulonet is a hybrid of neural network expert systems. Its strength lies in its ability to learn and to represent human logic in decision making using component net rules. The technique originally employed in neulonet learning is backpropagation. However, the resulting weight adjustments will lead to a loss in the logic of the net rules. A new technique is now developed that allows the neulonet to learn by composing net rules using genetic programming. This paper presents experimental results to demonstrate this new and exciting capability in capturing human decision logic from examples. Comparisons will also be made between the use of net rules, and the use of standard boolean logic of negation, disjunction and conjunction in evolutionary computation.|Chew Lim Tan,Henry Wai Kit Chia",
        "14264|IJCAI|1985|Heuristics for Inductive Learning|A number of heuristics have been developed which greatly reduce the search space a learning program must consider in its attempt to construct hypotheses about why a failure occurred. These heuristics have been implemented in the HANDICAPPER system Salzberg , Atkinson & Salzberg , in which they significantly improved predictive ability while demonstrating a remarkable learning curve The rationalization process has been developed as a verification system for the hypotheses suggested by the heuristics. Rationalization uses the causal knowledge of the system to ascertain whether or not a hypothesis is reasonable. If the hypothesis is not supported by causal knowledge, it is discarded and another hypothesis must be generated by the heuristics. The resulting learning system, by integrating causal knowledge w i th heuristic search, has quickly gone from essentially random predictive accuracy to a system which consistently outperforms the experts at predicting events in its problem domain.|Steven Salzberg",
        "14933|IJCAI|1991|Determinate Literals in Inductive Logic Programming|A recent system, FOIL, constructs Horn clause programs from numerous examples. Computational efficiency is achieved by using greedy search guided by an information-based heuristic. Greedy search tends to be myopic but determinate terms, an adaptation of an idea introduced by another new system (GOLEM), has been found to provide many of the benefits of lookahead without substantial increases in computation. This paper sketches key ideas from FOIL and GOLEM and discusses the use of determinate literals in a greedy search context. The efficacy of this approach is illustrated on the task of learning the quicksort procedure and other small but non-trivial list-manipulation functions.|J. Ross Quinlan",
        "14422|IJCAI|1987|Explanation-based Generalization in a Logic- Programming Environment|This paper describes a domain-independent implementation of explanation-based generalization (EBG) within a logic-programming environment. Explanation is interleaved with generalization, so that as the training instance is proven to be a positive example of the goal concept, the generalization is simultaneously created. All aspects of the EBG task are viewed in logic, which provides a clear semantics for EBG, and allows its integration into the logic-programming system. In this light operationally becomes a property requiring explicit reasoning. Additionally, viewing EBG in logic clarifies the relation of learning search-control to EBG, and suggests solutions for dealing with imperfect domain theories.|Haym Hirsh",
        "15109|IJCAI|1995|AILP Abductive Inductive Logic Programming|Inductive Logic Programming (ILP) is often situated as a research area emerging at the intersection of Machine Learning and Logic Programming (LP). This paper makes the link more clear between ILP and LP, in particular, between ILP and Abductive Logic Programming (ALP), i e, LP extended with abductive reasoning. We formulate a generic framework for handling incomplete knowledge. This framework can be instantiated both to ALP and ILP approaches. By doing so more light is shed on the relationship between abduction and induction. As an example we consider the abductive procedure SLDNFA, and modify it into an inductive procedure which we call SLDNFAI.|Hilde Ad\u00e9,Marc Denecker",
        "16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce",
        "16384|IJCAI|2007|Efficiently Exploiting Symmetries in Real Time Dynamic Programming|Current approaches to solving Markov Decision Processes (MDPs) are sensitive to the size of the MDP. When applied to real world problems though, MDPs exhibit considerable implicit redundancy, especially in the form of symmetries. Existing model minimization methods do not exploit this redundancy due to symmetries well. In this work, given such symmetries, we present a time-efficient algorithm to construct a functionally equivalent reduced model of the MDP. Further, we present a Real Time Dynamic Programming (RTDP) algorithm which obviates an explicit construction of the reduced model by integrating the given symmetries into it. The RTDP algorithm solves the reduced model, while working with parameters of the original model and the given symmetries. As RTDP uses its experience to determine which states to backup, it focuses on parts of the reduced state set that are most relevant. This results in significantly faster learning and a reduced overall execution time. The algorithms proposed are particularly effective in the case of structured automorphisms even when the reduced model does not have fewer features. We demonstrate the results empirically on several domains.|Shravan Matthur Narayanamurthy,Balaraman Ravindran"
      ],
      [
        "16563|IJCAI|2007|Heuristic Selection of Actions in Multiagent Reinforcement Learning|This work presents a new algorithm, called Heuristically Accelerated Minimax-Q (HAMMQ), that allows the use of heuristics to speed up the well-known Multiagent Reinforcement Learning algorithm Minimax-Q. A heuristic function H that influences the choice of the actions characterises the HAMMQ algorithm. This function is associated with a preference policy that indicates that a certain action must be taken instead of another. A set of empirical evaluations were conducted for the proposed algorithm in a simplified simulator for the robot soccer domain, and experimental results show that even very simple heuristics enhances significantly the performance of the multiagent reinforcement learning algorithm.|Reinaldo A. C. Bianchi,Carlos H. C. Ribeiro,Anna Helena Reali Costa",
        "16802|IJCAI|2007|Representations for Action Selection Learning from Real-Time Observation of Task Experts|The association of perception and action is key to learning by observation in general, and to program-level task imitation in particular. The question is how to structure this information such that learning is tractable for resource-bounded agents. By introducing a combination of symbolic representation with Bayesian reasoning, we demonstrate both theoretical and empirical improvements to a general-purpose imitation system originally based on a model of infant social learning. We also show how prior task knowledge and selective attention can be rigorously incorporated via loss matrices and Automatic Relevance Determination respectively.|Mark A. Wood,Joanna Bryson",
        "14844|IJCAI|1991|The Base Selection Task in Analogical Planning|Analogical planning provides a means of solving problems where other machine learning methods fail, because it does not require numerous previous examples or a rich domain theory. Given a problem in an unfamiliar domain (the target case), an analogical planning system locates a successful plan in a similar domain (the bast case), and uses the similarities to generate the target plan. Unfortunately, the analogical planning process is expensive and inflexible Many of the limiting factors reside in the base selection step, which drives the analogy formation process. This paper describes two ways of increasing the effectiveness and efficiency of analogical planning. First, a parallel graph-match base selection algorithm is presented. A parallel implementation on the Connection Machine is described and shown to substantially decrease the complexity of base selection. Second, a base-case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefits of several base cases when no single plan contributes enough information to the analogy. The effectiveness of this approach is demonstrated with examples from the domain of automatic programming.|Diane J. Cook",
        "16262|IJCAI|2005|InterActive Feature Selection|We study the effects of feature selection and human feedback on features in active learning settings. Our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features. Consistent with previous findings, we find that feature selection based on the labeled training set has little effect. But our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (%) of the most relevant features. Furthermore, these experiments show that feature labeling takes much less (about th) time than document labeling. We propose an algorithm that interleaves labeling features and documents which significantly accelerates active learning.|Hema Raghavan,Omid Madani,Rosie Jones",
        "16855|IJCAI|2009|Learning Hierarchical Task Networks for Nondeterministic Planning Domains|This paper describes how to learn Hierarchical Task Networks (HTNs) in nondeterministic planning domains, where actions may have multiple possible outcomes. We discuss several desired properties that guarantee that the resulting HTNs will correctly handle the nondeterminism in the domain. We developed a new learning algorithm, called HTN-MAKERND, that exploits these properties. We implemented HTN-MAKERND in the recently-proposed HTN-MAKER system, a goal-regression based HTN learning approach. In our theoretical study, we show that HTN-MAKERND soundly produces HTN planning knowledge in low-order polynomial times, despite the nondeterminism. In our experiments with two nondeterministic planning domains, ND-SHOP, a well-known HTN planning algorithm for nondeterministic domains, significantly outperformed (in some cases, by about  orders of magnitude) the well-known planner MBP using the learned HTNs.|Chad Hogg,Ugur Kuter,H\u00e9ctor Mu\u00f1oz-Avila",
        "16317|IJCAI|2005|Sequential Genetic Search for Ensemble Feature Selection|Ensemble learning constitutes one of the main directions in machine learning and data mining. Ensembles allow us to achieve higher accuracy, which is often not achievable with single models. One technique, which proved to be effective for constructing an ensemble of diverse classifiers, is the use of feature subsets. Among different approaches to ensemble feature selection, genetic search was shown to perform best in many domains. In this paper, a new strategy GAS-SEFS, Genetic Algorithmbased Sequential Search for Ensemble Feature Selection, is introduced. Instead of one genetic process, it employs a series of processes, the goal of each of which is to build one base classifier. Experiments on  data sets are conducted, comparing the new strategy with a previously considered genetic strategy for different ensemble sizes and for five different ensemble integration methods. The experiments show that GAS-SEFS, although being more time-consuming, often builds better ensembles, especially on data sets with larger numbers of features.|Alexey Tsymbal,Mykola Pechenizkiy,Padraig Cunningham",
        "16386|IJCAI|2007|Feature Selection and Kernel Design via Linear Programming|The definition of object (e.g., data point) similarity is critical to the performance of many machine learning algorithms, both in terms of accuracy and computational efficiency. However, it is often the case that a similarity function is unknown or chosen by hand. This paper introduces a formulation that given relative similarity comparisons among triples of points of the form object i is more like object j than object k, it constructs a kernel function that preserves the given relationships. Our approach is based on learning a kernel that is a combination of functions taken from a set of base functions (these could be kernels as well). The formulation is based on defining an optimization problem that can be solved using linear programming instead of a semidefinite program usually required for kernel learning. We show how to construct a convex problem from the given set of similarity comparisons and then arrive to a linear programming formulation by employing a subset of the positive definite matrices. We extend this formulation to consider representationevaluation efficiency based on formulating a novel form of feature selection using kernels (that is not much more expensive to solve). Using publicly available data, we experimentally demonstrate how the formulation introduced in this paper shows excellent performance in practice by comparing it with a baseline method and a related state-of-the art approach, in addition of being much more efficient computationally.|Glenn Fung,R\u00f3mer Rosales,R. Bharat Rao"
      ],
      [
        "16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone",
        "17104|IJCAI|2009|Simultaneous Discovery of Conservation Laws and Hidden Particles with Smith Matrix Decomposition|Particle physics experiments, like the Large Hadron Collider in Geneva, can generate thousands of data points listing detected particle reactions. An important learning task is to analyze the reaction data for evidence of conserved quantities and hidden particles. This task involves latent structure in two ways first, hypothesizing hidden quantities whose conservation determines which reactions occur, and second, hypothesizing the presence of hidden particles. We model this problem in the classic linear algebra framework of automated scientific discovery due to Valds-Prez, Zytkow and Simon, where both reaction data and conservation laws are represented as matrices. We introduce a new criterion for selecting a matrix model for reaction data find hidden particles and conserved quantities that rule out as many interactions among the nonhidden particles as possible. A polynomial-time algorithm for optimizing this criterion is based on the new theorem that hidden particles are required if and only if the Smith Normal Form of the reaction matrix R contains entries other than  or . To our knowledge this is the first application of Smith matrix decomposition to a problem in AI. Using data from particle accelerators, we compare our algorithm to the main model of particles in physics, known as the Standard Model our algorithm discovers conservation laws that are equivalent to those in the Standard Model, and indicates the presence of a hidden particle (the electron antineutrino) in accordance with the Standard Model.|Oliver Schulte",
        "16563|IJCAI|2007|Heuristic Selection of Actions in Multiagent Reinforcement Learning|This work presents a new algorithm, called Heuristically Accelerated Minimax-Q (HAMMQ), that allows the use of heuristics to speed up the well-known Multiagent Reinforcement Learning algorithm Minimax-Q. A heuristic function H that influences the choice of the actions characterises the HAMMQ algorithm. This function is associated with a preference policy that indicates that a certain action must be taken instead of another. A set of empirical evaluations were conducted for the proposed algorithm in a simplified simulator for the robot soccer domain, and experimental results show that even very simple heuristics enhances significantly the performance of the multiagent reinforcement learning algorithm.|Reinaldo A. C. Bianchi,Carlos H. C. Ribeiro,Anna Helena Reali Costa",
        "17106|IJCAI|2009|Local Learning Regularized Nonnegative Matrix Factorization|Nonnegative Matrix Factorization (NMF) has been widely used in machine learning and data mining. It aims to find two nonnegative matrices whose product can well approximate the nonnegative data matrix, which naturally lead to parts-based representation. In this paper, we present a local learning regularized nonnegative matrix factorization (LLNMF) for clustering. It imposes an additional constraint on NMF that the cluster label of each point can be predicted by the points in its neighborhood. This constraint encodes both the discriminative information and the geometric structure, and is good at clustering data on manifold. An iterative multiplicative updating algorithm is proposed to optimize the objective, and its convergence is guaranteed theoretically. Experiments on many benchmark data sets demonstrate that the proposed method outperforms NMF as well as many state of the art clustering methods.|Quanquan Gu,Jie Zhou",
        "17005|IJCAI|2009|Relation Regularized Matrix Factorization|In many applications, the data, such as web pages and research papers, contain relation (link) structure among entities in addition to textual content information. Matrix factorization (MF) methods, such as latent semantic indexing (LSI), have been successfully used to map either content information or relation information into a lower-dimensional latent space for subsequent processing. However, how to simultaneously model both the relation information and the content information effectively with an MF framework is still an open research problem. In this paper, we propose a novel MF method called relation regularized matrix factorization (RRMF) for relational data analysis. By using relation information to regularize the content MF procedure, RRMF seamlessly integrates both the relation information and the content information into a principled framework. We propose a linear-time learning algorithm with convergence guarantee to learn the parameters of RRMF. Extensive experiments on real data sets show that RRMF can achieve state-of-the-art performance.|Wu-Jun Li,Dit-Yan Yeung",
        "14487|IJCAI|1987|Using Prior Learning to Facilitate the Learning of New Causal Theories|We present an approach to learning causal knowledge which lies in between two extremely different approaches to learning  empirical methods (e.g., ,) which detect similarities and differences between between examples to reveal regularities.  explanation-based methods (e.g., ,) which derive a causal explanation for a single event from existing causal knowledge. The event and the causal explanation are generalized to create a new \"chunk\" of causal knowledge by retaining only those features of the event which were needed to produce the explanation. In the approach to learning presented in this paper and implemented in a program called OCCAM, prior knowledge indicating what sort of distinctions have proven useful in the past influences the search for causal hypotheses. Our approach to learning snares a goal with explanation-based learning to allow existing knowledge to facilitate future learning so that fewer examples are required. However, it does not share one shortcoming of explanation-based learning since it can create causal theories which are not implications of existing causal theories.|Michael J. Pazzani,Michael G. Dyer,Margot Flowers",
        "15981|IJCAI|2003|An Integrated Multilevel Learning Approach to Multiagent Coalition Formation|In this paper we describe an integrated multilevel learning approach to multiagent coalition formation in a real-time environment. In our domain, agents negotiate to form teams to solve joint problems. The agent that initiates a coalition shoulders the responsibility of overseeing and managing the formation process. A coalition formation process consists of two stages. During the initialization stage, the initiating agent identifies the candidates of its coalition, i.e., known neighbors that could help. The initiating agent negotiates with these candidates during the finalization stage to determine the neighbors that are willing to help. Since our domain is dynamic, noisy, and time-constrained, the coalitions are not optimal. However, our approach employs learning mechanisms at several levels to improve the quality of the coalition formation process. At a tactical level, we use reinforcement learning to identify viable candidates based on their potential utility to the coalition, and case-based learning to refine negotiation strategies. At a strategic level, we use distributed, cooperative case-based learning to improve general negotiation strategies. We have implemented the above three learning components and conducted experiments in multisensor target tracking and CPU re-allocation applications.|Leen-Kiat Soh,Xin Li",
        "15275|IJCAI|1995|Local Learning in Probabilistic Networks with Hidden Variables|Probabilistic networks which provide compact descriptions of complex stochastic relationships among several random variables are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks Because probabilistic networks provide explicit representations of causal structure human experts can easily contribute pnor knowledge to the training process, thereby significantly improving the learning rate Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.|Stuart J. Russell,John Binder,Daphne Koller,Keiji Kanazawa"
      ]
    ]
  }
}