{"abstract":{"entropy":6.625807215740544,"topics":["genetic algorithms, evolutionary algorithms, genetic programming, algorithms, particle swarm, optimization problem, optimization algorithms, evolutionary computation, optimization, algorithms problem, algorithms search, present algorithms, solving problem, evolutionary optimization, swarm optimization, particle optimization, particle pso, multi-objective optimization, swarm pso, neural network","artificial intelligence, real world, play role, agents, description logic, constraint satisfaction, wide range, mobile robot, job shop, belief revision, reasoning, computer game, cartesian programming, association rules, logic, agents environment, logic programming, previous work, autonomous agents, cooperative coevolutionary","data base, data, data management, data systems, database, systems, management systems, database systems, data stream, relational database, information, large data, web, relational data, xml documents, data mining, information systems, applications, query processing, base systems","machine learning, classifier systems, learning systems, estimation distribution, learning classifier, learning, present approach, present novel, consider problem, reinforcement learning, markov decision, present based, estimation edas, partially observable, markov processes, building block, distribution edas, decision processes, support vector, time series","optimization problem, solving problem, optimization algorithms, widely used, ant colony, algorithms used, algorithms problem, algorithms solving, applied problem, algorithms applied, combinatorial optimization, combinatorial problem, evolutionary applied, performance algorithms, last years, colony aco, used problem, applied optimization, algorithms eas, ant aco","evolutionary algorithms, particle swarm, evolutionary computation, swarm optimization, particle optimization, evolutionary optimization, multi-objective evolutionary, multi-objective optimization, optimization algorithms, particle pso, multi-objective algorithms, swarm pso, present evolutionary, recent years, optimization pso, differential evolution, evolutionary approach, evolutionary problem, paper optimization, evolution strategies","real world, wide range, computer game, social network, provides, lower bound, activity recognition, strategy game, cognitive architecture, work, game player, hyper-heuristics straightforward, agents goal, work present, discover straightforward, world wide, combination straightforward, idea straightforward, domain-independent planning, problem activity","play role, previous work, scheduling problem, recent work, problem allocation, resource allocation, game playing, propositional satisfiability, problem different, different, teaching students, different model, knowledge task, task, general playing, allocation agents, task allocation, distributed constraint, approach analog, diagnostic systems","data base, data management, management systems, database systems, base systems, data systems, applications data, database applications, control systems, base management, applications require, database data, database, database management, knowledge base, applications, relational database, access data, recent advances, applications systems","data information, web search, search engine, data web, semantic web, web pages, web, information retrieval, information, web services, web information, sensor network, web applications, information systems, mining web, semantic data, conceptual data, current data, schema data, computer science","markov decision, partially observable, markov processes, decision processes, time series, decision making, data model, bayesian network, present model, systems model, observable markov, model, decision mdp, partially markov, observable decision, decision support, problem model, protein structure, markov mdp, partially decision","classifier systems, learning systems, learning classifier, reinforcement learning, learning problem, learning, machine learning, xcs systems, learning data, classifier xcs, task classification, present learning, virtual hashing, systems lcs, discriminant analysis, learning algorithms, classifier lcs, learning xcs, classification problem, learning task"],"ranking":[["66621|AAAI|2010|Goal-Driven Autonomy in a Navy Strategy Simulation|This paper introduces a novel optimization algorithm, group search optimization (GSO) algorithm and its implementation method is presented in detail. The GSO is used to investigate the planar and space truss structures with continuous variables and is tested by two truss optimization problems. The optimization results are compared with that of the particle swarm optimization (PSO) algorithm, the particle swarm optimization with passive congregation (PSOPC) and the heuristic particle swarm optimizer (HPSO) algorithm. Results from the two tested cases illustrate the ability of the GSO algorithm to find the optimal results, which are better than that of the PSO and PSOPC, while are at the same level of that of HPSO optimization method. Meanwhile, the results also show that the GSO algorithm maintains a preferable convergence accuracy among these four algorithms.|Matthew Molineaux,Matthew Klenk,David W. Aha","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","59071|GECCO|2010|An parallel particle swarm optimization approach for multiobjective optimization problems|This paper proposes a parallel particle swarm optimization (PPSO) to solve the multiobjective optimization problems (MOP). PPSO makes the use of the parallel characteristic of the PSO algorithm to deal with the multiple objectives issue of the MOP. PPSO uses as many swarms as the number of the objectives in the MOP and lets each swarm optimize only one of the objectives. These swarms work in parallel and each swarm can use a standard PSO or any other improved PSO variants to solve a single objective problem. PPSO has advantages on the following two aspects. First, as each swarm focus on optimizing only one objective, PPSO can avoid the difficulty of fitness assignment because the particles can be evaluated like in the single objective optimization problem. Second, as different swarms optimize different objectives, PPSO can maintain the population diversity to make a throughout search along the whole Pareto front to obtain nondominated solutions as many as possible. The performance of PPSO is tested on a set of benchmark problems complicated Pareto sets in CEC. The experimental results compared with those obtained by the state-of-the-art algorithms demonstrate the effectiveness and efficiency of PPSO, showing the good performance of PPSO in solving the MOP with complicated Pareto sets.|Zhi-hui Zhan,Jun Zhang","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58967|GECCO|2010|Development of efficient particle swarm optimizers by using concepts from evolutionary algorithms|Particle swarm optimization (PSO) has been in practice for more than  years now and has gained wide popularity in various optimization tasks. In the context to single objective optimization, this paper studies two aspects of PSO (i) its ability to approach an 'optimal basin', and (ii) to find the optimum with high precision once it enters the region. of interest. We test standard PSO algorithms and discover their inability in handling both aspects efficiently. To address these issues with PSO, we propose an evolutionary algorithm (EA) which is algorithmically similar to PSO, and then borrow different EA-specific operators to enhance the PSO's performance. Our final proposed PSO contains a parent-centric recombination operator instead of usual particle update rule, but maintains PSO's individualistic trait and has a demonstrated performance comparable to a well-known GA (and outperforms the GA in some occasions). Moreover, the modified PSO algorithm is found to scale up to solve as large as -variable problems. This study emphasizes the need for similar such studies in establishing an equivalence between various geneticevolutionary and other bio-inspired algorithms, a process that may lead us to better understand the scope and usefulness of various operators associated with each algorithm.|Kalyanmoy Deb,Nikhil Padhye","58287|GECCO|2008|Runtime analysis of binary PSO|We investigate the runtime of the Binary Particle Swarm Optimization (PSO) algorithm introduced by Kennedy and Eberhart (). The Binary PSO maintains a global best solution and a swarm of particles. Each particle consists of a current position, an own best position and a velocity vector used in a probabilistic process to update the particle's position. We present lower bounds for swarms of polynomial size. To prove upper bounds, we transfer a fitness-level argument well-established for evolutionary algorithms (EAs) to PSO. This method is applied to estimate the expected runtime on the class of unimodal functions. A simple variant of the Binary PSO is considered in more detail. The -PSO only maintains one particle, hence own best and global best solutions coincide. Despite its simplicity, the -PSO is surprisingly efficient. A detailed analysis for the function OneMax shows that the -PSO is competitive to EAs.|Dirk Sudholt,Carsten Witt","58697|GECCO|2009|Estimation of particle swarm distribution algorithms bringing together the strengths of PSO and EDAs|This paper presents a framework of estimation of particle swarm distribution algorithms (EPSDAs). The aim lies in effectively combining particle swarm optimization (PSO) with estimation of distribution algorithms (EDAs) without losing on their unique features. To exhibit its practicability, an extended compact particle swarm optimization (EcPSO) is developed along the lines of the suggested framework. Empirical results have adduced grounds for its effectiveness.|Chang Wook Ahn,Hyun-Tae Kim","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule","58990|GECCO|2010|Biogeography-based optimization with blended migration for constrained optimization problems|Biogeography-based optimization (BBO) is a new evolutionary algorithm based on the science of biogeography. We propose two extensions to BBO. First, we propose blended migration. Second, we modify BBO to solve constrained optimization problems. The constrained BBO algorithm is compared with solutions based on a genetic algorithm (GA) and particle swarm optimization (PSO). Numerical results indicate that BBO generally performs better than GA and PSO in handling constrained single-objective optimization problems.|Haiping Ma,Dan Simon"],["65985|AAAI|2007|A Modal Logic for Beliefs and Pro Attitudes|Agents' pro attitudes such as goals, intentions, desires, wishes, and judgements of satisfactoriness play an important role in how agents act rationally. To provide a natural and satisfying formalization of these attitudes is a longstanding problem in the community of agent theory. Most of existing modal logic approaches are based on Kripke structures and have to face the so-called side-effect problem. This paper presents a new modal logic formalizing agents' pro attitudes, based on neighborhood models. There are three distinguishing features of this logic. Firstly, this logic naturally satisfies Bratman's requirements for agents' beliefs and pro attitudes, as well as some interesting properties that have not been discussed before. Secondly, we give a sound and complete axiom system for characterizing all the valid properties of beliefs and pro attitudes. We introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model, and this brings a new member to the family of non-normal modal logics. Finally, we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation, that is, computational grounding, which means that properties in this logic can be given an interpretation in terms of some concrete computational model. Indeed, the presented neighborhood frame can be naturally derived from probabilistic programming with utilities.|Kaile Su,Abdul Sattar,Han Lin,Mark Reynolds","65692|AAAI|2006|Unifying Logical and Statistical AI|Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system.|Pedro Domingos,Stanley Kok,Hoifung Poon,Matthew Richardson,Parag Singla","66179|AAAI|2007|Probabilistic Modal Logic|A modal logic is any logic for handling modalities concepts like possibility, necessity, and knowledge. Artificial intelligence uses modal logics most heavily to represent and reason about knowledge of agents about others' knowledge. This type of reasoning occurs in dialog, collaboration, and competition. In many applications it is also important to be able to reason about the probability of beliefs and events. In this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge. We also present exact and approximate algorithms for reasoning about the truth value of queries that are encoded as probabilistic modal logic formulas. We provide an exact algorithm which takes a probabilistic Kripke stntcture and answers probabilistic modal queries in polynomial-time in the size of the model. Then, we introduce an approximate method for applications in which we have very many or infinitely many states. Exact methods are impractical in these applications and we show that our method returns a close estimate efficiently.|Afsaneh Shirazi,Eyal Amir","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","65174|AAAI|1994|SodaBot A Software Agent Environment and Construction System|This thesis presents em SodaBot, a general-purpose software agent user- environment and construction system. Its primary component is the em basic software agent --- a computational framework for building agents which is essentially an em agent operating system. We also present a new language for programming the basic software agent whose primitives are designed around human-level descriptions of agent activity. Via this programming language, em users can easily implement a wide-range of typical software agent applications, e.g. personal on-line assistants and meeting scheduling agents. The SodaBot system has been implemented and tested, and its description comprises the bulk of this thesis.|Michael H. Coen","66389|AAAI|2008|On the Enactability of Business Protocols|Protocols specifying business interactions among autonomous parties enable reuse and promote interoperability. A protocol is specified from a global viewpoint, but enacted in a distributed manner by (agents playing) different roles. Each role describes a local representation. An ill-specified protocol may yield roles that fail to produce correct enactments of the protocol. Existing approaches lack a formal and comprehensive treatment of this problem. Building on recent work on declaratively specifying a protocol as a set of rules of causal logic, this paper formally defines the enactability of protocols. It presents necessary and sufficient conditions for the enactability of a protocol as well as a decision procedure for extracting correct roles from enactable protocols.|Nirmit Desai,Munindar P. Singh","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","66430|AAAI|2008|Argument Theory Change Applied to Defeasible Logic Programming|In this article we work on certain aspects of the belief change theory in order to make them suitable for argumentation systems. This approach is based on Defeasible Logic Programming as the argumentation formalism from which we ground the definitions. The objective of our proposal is to define an argument revision operator that inserts a new argument into a defeasible logic program in such a way that this argument ends up undefeated after the revision, thus warranting its conclusion. In order to ensure this warrant, the defeasible logic program has to be changed in concordance with a minimal change principle. Finally, we present an algorithm that implements the argument revision operation.|Martín O. Moguillansky,Nicolás D. Rotstein,Marcelo A. Falappa,Alejandro Javier García,Guillermo Ricardo Simari","57223|GECCO|2003|On Role of Implicit Interaction and Explicit Communications in Emergence of Social Behavior in Continuous Predators-Prey Pursuit Problem|We present the result of our work on use of genetic programming for evolving social behavior of agents situated in inherently cooperative environment. We use predators-prey pursuit problem to verify our hypothesis that relatively complex social behavior may emerge from simple, implicit, locally defined, and therefore - robust and highly-scalable interactions between the predator agents. We propose a proximity perception model for the predator agents where only the relative bearings and the distances to the closest predator agent and to the prey are perceived. The instance of the problem we consider is more realistic than commonly discussed in that the world, the sensory and moving abilities of agents are continuous and the sensors of agents feature limited range of \"visibility\". The results show that surrounding behavior, evolved using proposed strongly typed genetic programming with exception handling (STGPE) emerges from local, implicit and proximity-defined interactions between the predator agents in both cases when multi-agents systems comprises (i) partially inferior predator agents (with inferior moving abilities and superior sensory abilities) and with (ii) completely inferior predator agents. In the latter case the introduction of short-term memory and explicit communication contributes to the improvement of performance of STGPE.|Ivan Tanev,Katsunori Shimohara"],["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","79900|VLDB|1978|A Distributed Data Base System Using Logical Relational Machines|Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.|Michel E. Adiba,Jean-Yves Caleca,Christian Euzet","80741|VLDB|2007|Efficient Keyword Search over Virtual XML Views|Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark  open-source XML database system indicates that the proposed approach is scalable and efficient.|Feng Shao,Lin Guo,Chavdar Botev,Anand Bhaskar,Muthiah Chettiar,Fan Yang 0002,Jayavel Shanmugasundaram","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80070|VLDB|1981|IML-Inscribed Nets for Modeling Text Processing and Data Base Management Systems|Predicatetransition-nets with a suitable inscription allow building practice-oriented and expressive models of distributed information systems with concurrent processes as found in office information systems or decentralized database management systems. The paper presents the first version of an inscription language called IML (Information Management Language) which has been derived from the results of some long range database research activities. The abstract objects \"construct\" and \"form\" are introduced and basic operations are defined on them. It is shown how to apply these language elements to inscribe net symbols for high-fidelity modeling of text processing and data management.|Gernot Richter","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber"],["58868|GECCO|2009|Uncertainty handling CMA-ES for reinforcement learning|The covariance matrix adaptation evolution strategy (CMAES) has proven to be a powerful method for reinforcement learning (RL). Recently, the CMA-ES has been augmented with an adaptive uncertainty handling mechanism. Because uncertainty is a typical property of RL problems this new algorithm, termed UH-CMA-ES, is promising for RL. The UH-CMA-ES dynamically adjusts the number of episodes considered in each evaluation of a policy. It controls the signal to noise ratio such that it is just high enough for a sufficiently good ranking of candidate policies, which in turn allows the evolutionary learning to find better solutions. This significantly increases the learning speed as well as the robustness without impairing the quality of the final solutions. We evaluate the UH-CMA-ES on fully and partially observable Markov decision processes with random start states and noisy observations. A canonical natural policy gradient method and random search serve as a baseline for comparison.|Verena Heidrich-Meisner,Christian Igel","65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","57750|GECCO|2006|Does overfitting affect performance in estimation of distribution algorithms|Estimation of Distribution Algorithms (EDAs) are a class of evolutionary algorithms that use machine learning techniques to solve optimization problems. Machine learning is used to learn probabilistic models of the selected population. This model is then used to generate next population via sampling. An important phenomenon in machine learning from data is called overfitting. This occurs when the model is overly adapted to the specifics of the training data so well that even noise is encoded. The purpose of this paper is to investigate whether overfitting happens in EDAs, and to discover its consequences. What is found is overfitting does occur in EDAs overfitting correlates to EDAs performance reduction of overfitting using early stopping can improve EDAs performance.|Hao Wu,Jonathan L. Shapiro","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","59027|GECCO|2010|Adaption of XCS to multi-learner predatorprey scenarios|Learning classifier systems (LCSs) are rule-based evolutionary reinforcement learning systems. Today, especially variants of Wilson's extended classifier system (XCS) are widely applied for machine learning. Despite their widespread application, LCSs have drawbacks, e. g., in multi-learner scennarios, since the Markov property is not fulfilled. In this paper, LCSs are investigated in an instance of the generic homogeneous and non-communicating predatorprey scenario. A group of predators collaboratively observe a (randomly) moving prey as long as possible, where each predator is equipped with a single, independent XCS. Results show that improvements in learning are achieved by cleverly adapting a multi-step approach to the characteristics of the investigated scenario. Firstly, the environmental reward function is expanded to include sensory information. Secondly, the learners are equipped with a memory to store and analyze the history of local actions and given payoffs.|Clemens Lode,Urban Richter,Hartmut Schmeck","57186|GECCO|2003|Reinforcement Learning Estimation of Distribution Algorithm|This paper proposes an algorithm for combinatorial optimizations that uses reinforcement learning and estimation of joint probability distribution of promising solutions to generate a new population of solutions. We call it Reinforcement Learning Estimation of Distribution Algorithm (RELEDA). For the estimation of the joint probability distribution we consider each variable as univariate. Then we update the probability of each variable by applying reinforcement learning method. Though we consider variables independent of one another, the proposed method can solve problems of highly correlated variables. To compare the efficiency of our proposed algorithm with other Estimation of Distribution Algorithms (EDAs) we provide the experimental results of the two problems four peaks problem and bipolar function.|Topon Kumar Paul,Hitoshi Iba","58647|GECCO|2009|SGMIEC using selfish gene theory to construct mutualinformation and entropy based cluster for optimization|This paper proposes a new approach named SGMIEC in the field of Estimation of Distribution Algorithm (EDA). While the current EDAs require much time in the statistical learning process as the relationships among the variables are too complicated, the Selfish Gene Theory (SG) is deployed in this approach and a Mutual Information and Entropy based Cluster (MIEC) model with an incremental learning and resample scheme is also set to optimize the probability distribution of the virtual population. Experimental results on several benchmark problems demonstrate that, compared with BMDA and COMIT , SGMIEC often performs better in convergent reliability, convergent velocity and convergent process.|Feng Wang,Zhiyi Lin,Cheng Yang,Yuanxiang Li","65933|AAAI|2006|Hard Constrained Semi-Markov Decision Processes|In multiple criteria Markov Decision Processes (MDP) where multiple costs are incurred at every decision point, current methods solve them by minimising the expected primary cost criterion while constraining the expectations of other cost criteria to some critical values. However, systems are often faced with hard constraints where the cost criteria should never exceed some critical values at any time, rather than constraints based on the expected cost criteria. For example, a resource-limited sensor network no longer functions once its energy is depleted. Based on the semi-MDP (sMDP) model, we study the hard constrained (HC) problem in continuous time, state and action spaces with respect to both finite and infinite horizons, and various cost criteria. We show that the HCsMDP problem is NP-hard and that there exists an equivalent discrete-time MDP to every HCsMDP. Hence, classical methods such as reinforcement learning can solve HCsMDPs.|Wai-Leong Yeow,Chen-Khong Tham,Wai-Choong Wong","65892|AAAI|2006|Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains|We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.|Vishal Soni,Satinder P. Singh"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","59091|GECCO|2010|Ant colony optimization and the minimum cut problem|Ant Colony Optimization (ACO) is a powerful metaheuristic for solving combinatorial optimization problems. With this paper we contribute to the theoretical understanding of this kind of algorithm by investigating the classical minimum cut problem. An ACO algorithm similar to the one that was proved successful for the minimum spanning tree problem is studied. Using rigorous runtime analyses we show how the ACO algorithm behaves similarly to Karger and Stein's algorithm for the minimum cut problem as long as the use of pheromone values is limited. Hence optimal solutions are obtained in expected polynomial time. On the other hand, we show that high use of pheromones has a negative effect, and the ACO algorithm may get trapped in local optima resulting in an exponential runtime to obtain an optimal solution. This result indicates that ACO algorithms may be inappropriate for finding minimum cuts.|Timo Kötzing,Per Kristian Lehre,Frank Neumann,Pietro Simone Oliveto","57352|GECCO|2005|Ant colony optimization for power plant maintenance scheduling optimization|In order to maintain a reliable and economic electric power supply, the maintenance of power plants is becoming increasingly important. In this paper, a formulation that enables ant colony optimization (ACO) algorithms to be applied to the power plant maintenance scheduling optimization (PPMSO) problem is developed and tested on a -unit case study. A heuristic formulation is introduced and its effectiveness in solving the problem is investigated. The performance of two different ACO algorithms is compared, including Best Ant System (BAS) and Max-Min Ant System (MMAS), and a detailed sensitivity analysis is conducted on the parameters controlling the searching behavior of ACO algorithms. The results obtained indicate that the performance of the two ACO algorithms investigated is significantly better than that of a number of other metaheuristics, such as genetic algorithms and simulated annealing, which have been applied to the same case study previously. In addition, use of the heuristics significantly improves algorithm performance. Also, ACO is found to have similar performance for the case study considered across an identified range of parameter values.|Wai-Kuan Foong,Holger R. Maier,Angus R. Simpson","57042|GECCO|2003|Ant-Based Crossover for Permutation Problems|Crossover for evolutionary algorithms applied to permutation problems is a difficult and widely discussed topic. In this paper we use ideas from ant colony optimization to design a new permutation crossover operator. One of the advantages of the new crossover operator is the ease to introduce problem specific heuristic knowledge. Empirical tests on a travelling salesperson problem show that the new crossover operator yields excellent results and significantly outperforms evolutionary algorithms with edge recombination operator as well as pure ant colony optimization.|Jürgen Branke,Christiane Barz,Ivesa Behrens","58943|GECCO|2010|An ant colony optimization approach to the multiple-choice multidimensional knapsack problem|In this paper, we present an ant colony optimization (ACO) approach to solve the multiple-choice multidimensional knapsack problem (MMKP). This problem concerns many real life problems, and is hard to solve due to its strong constraints and NP-hard property. The ACO approach given in this paper follows the algorithmic scheme of max-min ant system, but has some new features with respect to the characteristics of the MMKP. First, a single-group-oriented solution construction method is proposed, which allows ants to generate solutions efficiently. Second, some Lagrangian dual information obtained from a Lagrangian relaxation of MMKP is integrated into ACO. In addition, we develop a novel repair operator, with which the possible infeasible solutions generated by ants can be fixed. The proposed approach has been tested on a number of MMKP instances. Computational results show that it is able to produce competitive solutions in comparison with existing algorithms.|Zhigang Ren,Zuren Feng","58312|GECCO|2008|Scaling ant colony optimization with hierarchical reinforcement learning partitioning|This paper merges hierarchical reinforcement learning (HRL) with ant colony optimization (ACO) to produce a HRL ACO algorithm capable of generating solutions for large domains. This paper describes two specific implementations of the new algorithm the first a modification to Dietterich's MAXQ-Q HRL algorithm, the second a hierarchical ant colony system algorithm. These implementations generate faster results, with little to no significant change in the quality of solutions for the tested problem domains. The application of ACO to the MAXQ-Q algorithm replaces the reinforcement learning, Q-learning, with the modified ant colony optimization method, Ant-Q. This algorithm, MAXQ-AntQ, converges to solutions not significantly different from MAXQ-Q in % of the time. This paper then transfers HRL techniques to the ACO domain and traveling salesman problem (TSP). To apply HRL to ACO, a hierarchy must be created for the TSP. A data clustering algorithm creates these subtasks, with an ACO algorithm to solve the individual and complete problems. This paper tests two clustering algorithms, k-means and G-means. The results demonstrate the algorithm with data clustering produces solutions  times faster with -% decrease in solution quality due to the effects of clustering.|Erik J. Dries,Gilbert L. Peterson","58901|GECCO|2010|A few ants are enough ACO with iteration-best update|Ant colony optimization (ACO) has found many applications in different problem domains. We carry out a first rigorous runtime analysis of ACO with iteration-best update, where the best solution in the each iteration is reinforced. This is similar to comma selection in evolutionary algorithms. We compare ACO to evolutionary algorithms for which it is well known that an offspring size of (log n), n the problem dimension, is necessary to optimize even simple functions like ONEMAX. In sharp contrast, ACO is efficient on ONEMAX even for the smallest possible number of two ants. Remarkably, this only holds if the pheromone evaporation rate is small enough the collective memory of many ants stored in the pheromones makes up for the small number of ants. We further prove an exponential lower bound for ACO with iteration-best update that depends on a trade-off between the number of ants and the evaporation rate.|Frank Neumann,Dirk Sudholt,Carsten Witt","57964|GECCO|2007|Binary ant algorithm|When facing dynamic optimization problems the goal is no longer to find the extrema, but to track their progression through the space as closely as possible. Over these kind of over changing, complex and ubiquitous real-world problems, the explorative-exploitive subtle counterbalance character of our current state-of-the-art search algorithms should be biased towards an increased explorative behavior. While counterproductive in classic problems, the main and obvious reason of using it in severe dynamic problems is simple while we engage ourselves in exploiting the extrema, the extrema moves elsewhere. In order to tackle this subtle compromise, we propose a novel algorithm for optimization in dynamic binary landscapes, stressing the role of negative feedback mechanisms. The Binary Ant Algorithm (BAA) mimics some aspects of social insects' behavior. Like Ant Colony Optimization (ACO), BAA acts by building pheromone maps over a graph of possible trails representing pseudo-solutions of increasing quality to a specific optimization problem. Main differences rely on the way this search space is represented and provided to the colony in order to exploreexploit it, while and more important, we enrol in providing strong evaporation to the problem-habitat. By a process of pheromone reinforcement and evaporation the artificial insect's trails over the graph converge to regions near the ideal solution of the optimization problem. Over each generation, positive feedbacks made available by pheromone reinforcement consolidate the best solutions found so far, while enhanced negative feedbacks given by the evaporation mechanism provided the system with population diversity and fast self-adaptive characteristics, allowing BAA to be particularly suitable for severe complex dynamic optimization problems. Experiments made with some well known test functions frequently used in the Evolutionary Algorithms' research field illustrate the efficiency of the proposed method. BAA was also compared with other algorithms, proving to be more able to track fast moving extrema on several test problems.|Carlos Fernandes,Agostinho C. Rosa,Vitorino Ramos","59096|GECCO|2010|The impact of design choices of multiobjective antcolony optimization algorithms on performance an experimental study on the biobjective TSP|Over the last few years, there have been a number of proposals of ant colony optimization (ACO) algorithms for tackling multiobjective combinatorial optimization problems. These proposals adapt ACO concepts in various ways, for example, some use multiple pheromone matrices and multiple heuristic matrices and others use multiple ant colonies. In this article, we carefully examine several of the most prominent of these proposals. In particular, we identify commonalities among the approaches by recasting the original formulation of the algorithms in different terms. For example, several proposals described in terms of multiple colonies can be cast equivalently using a single ant colony, where ants use different weights for aggregating the pheromone andor the heuristic information. We study algorithmic choices for the various proposals and we identify previously undetected trade-offs in their performance|Manuel López-Ibáñez,Thomas Stützle","57245|GECCO|2003|Revisiting Elitism in Ant Colony Optimization|Ant Colony Optimization (ACO) has been applied successfully in solving the Traveling Salesman Problem. Marco Dorigo et al. used Ant System (AS) to explore the Symmetric Traveling Salesman Problem and found that the use of a small number of elitist ants can improve algorithm performance. The elitist ants take advantage of global knowledge of the best tour found to date and reinforce this tour with pheromone in order to focus future searches more effectively. This paper discusses an alternative approach where only local information is used to reinforce good tours thereby enhancing the ability of the algorithm for multiprocessor or actual network implementation. In the model proposed, the ants are endowed with a memory of their best tour to date. The ants then reinforce this \"local best tour\" with pheromone during an iteration to mimic the search focusing of the elitist ants. The environment used to simulate this model is described and compared with Ant System.|Tony White,Simon Kaegi,Terri Oda"],["57994|GECCO|2007|Particle swarm guided evolution strategy|Evolution strategy (ES) and particle swarm optimization (PSO) are two of the most popular research topics for tackling real-parameter optimization problems in evolutionary computation. Both of them have strengths and weaknesses for their different search behaviors and methodologies. In ES, mutation, as the main operator, tries to find good solutions around each individual. While in PSO, particles are moving toward directions determined by certain global information, such as the global best particle. In order to leverage the specialties offered by both sides to our advantage, this paper combines the essential mechanism of ES and the key concept of PSO to develop a new hybrid optimization methodology, called particle swarm guided evolution strategy. We introduce swarm intelligence to the ES mutation framework to create a new mutation operator, called guided mutation, and integrate the guided mutation operator into ES. Numerical experiments are conducted on a set of benchmark functions, and the experimental results indicate that PSGES is a promising optimization methodology as well as an interesting research direction.|Chang-Tai Hsieh,Chih-Ming Chen,Ying-Ping Chen","58696|GECCO|2009|Geometric differential evolution|Geometric Particle Swarm Optimization (GPSO) is a recently introduced formal generalization of traditional Particle Swarm Optimization (PSO) that applies naturally to both continuous and combinatorial spaces. Differential Evolution (DE) is similar to PSO but it uses different equations governing the motion of the particles. This paper generalizes the DE algorithm to combinatorial search spaces extending its geometric interpretation to these spaces, analogously as what was done for the traditional PSO algorithm. Using this formal algorithm, Geometric Differential Evolution (GDE), we formally derive the specific GDE for the Hamming space associated with binary strings and present experimental results on a standard benchmark of problems.|Alberto Moraglio,Julian Togelius","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","58032|GECCO|2007|Applying particle swarm optimization to software testing|Evolutionary structural testing is an approach to automatically generating test cases that achieve high structural code coverage. It typically uses genetic algorithms (GAs) to search for relevant test cases. In recent investigations particle swarm optimization (PSO), an alternative search technique, often outperformed GAs when applied to various problems. This raises the question of how PSO competes with GAs in the context of evolutionary structural testing.In order to contribute to an answer to this question, we performed experiments with  small artificial test objects and  more complex industrial test objects taken from various development projects. The results show that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.|Andreas Windisch,Stefan Wappler,Joachim Wegener","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57263|GECCO|2003|A New Approach to Improve Particle Swarm Optimization|Particle swarm optimization (PSO) is a new evolutionary computation technique. Although PSO algorithm possesses many attractive properties, the methods of selecting inertia weight need to be further investigated. Under this consideration, the inertia weight employing random number uniformly distributed in , was introduced to improve the performance of PSO algorithm in this work. Three benchmark functions were used to test the new method. The results were presented to show that the new method is effective.|Liping Zhang,Huanjun Yu,Shangxu Hu","58967|GECCO|2010|Development of efficient particle swarm optimizers by using concepts from evolutionary algorithms|Particle swarm optimization (PSO) has been in practice for more than  years now and has gained wide popularity in various optimization tasks. In the context to single objective optimization, this paper studies two aspects of PSO (i) its ability to approach an 'optimal basin', and (ii) to find the optimum with high precision once it enters the region. of interest. We test standard PSO algorithms and discover their inability in handling both aspects efficiently. To address these issues with PSO, we propose an evolutionary algorithm (EA) which is algorithmically similar to PSO, and then borrow different EA-specific operators to enhance the PSO's performance. Our final proposed PSO contains a parent-centric recombination operator instead of usual particle update rule, but maintains PSO's individualistic trait and has a demonstrated performance comparable to a well-known GA (and outperforms the GA in some occasions). Moreover, the modified PSO algorithm is found to scale up to solve as large as -variable problems. This study emphasizes the need for similar such studies in establishing an equivalence between various geneticevolutionary and other bio-inspired algorithms, a process that may lead us to better understand the scope and usefulness of various operators associated with each algorithm.|Kalyanmoy Deb,Nikhil Padhye","58731|GECCO|2009|Using a distance metric to guide PSO algorithms for many-objective optimization|In this paper we propose to use a distance metric based on user-preferences to efficiently find solutions for many-objective problems. We use a particle swarm optimization (PSO) algorithm as a baseline to demonstrate the usefulness of this distance metric, though the metric can be used in conjunction with any evolutionary multi-objective (EMO) algorithm. Existing user-preference based EMO algorithms rely on the use of dominance comparisons to explore the search-space. Unfortunately, this is ineffective and computationally expensive for many-objective problems. In the proposed distance metric based PSO, particles update their positions and velocities according to their closeness to preferred regions in the objective-space, as specified by the decision maker. The proposed distance metric allows an EMO algorithm's search to be more effective especially for many-objective problems, and to be more focused on the preferred regions, saving substantial computational cost. We demonstrate how to use a distance metric with two user-preference based PSO algorithms, which implement the reference point and light beam search methods. These algorithms are compared to a user-preference based PSO algorithm relying on the conventional dominance comparisons. Experimental results suggest that the distance metric based algorithms are more effective and efficient especially for difficult many-objective problems.|Upali K. Wickramasinghe,Xiaodong Li","58287|GECCO|2008|Runtime analysis of binary PSO|We investigate the runtime of the Binary Particle Swarm Optimization (PSO) algorithm introduced by Kennedy and Eberhart (). The Binary PSO maintains a global best solution and a swarm of particles. Each particle consists of a current position, an own best position and a velocity vector used in a probabilistic process to update the particle's position. We present lower bounds for swarms of polynomial size. To prove upper bounds, we transfer a fitness-level argument well-established for evolutionary algorithms (EAs) to PSO. This method is applied to estimate the expected runtime on the class of unimodal functions. A simple variant of the Binary PSO is considered in more detail. The -PSO only maintains one particle, hence own best and global best solutions coincide. Despite its simplicity, the -PSO is surprisingly efficient. A detailed analysis for the function OneMax shows that the -PSO is competitive to EAs.|Dirk Sudholt,Carsten Witt","58990|GECCO|2010|Biogeography-based optimization with blended migration for constrained optimization problems|Biogeography-based optimization (BBO) is a new evolutionary algorithm based on the science of biogeography. We propose two extensions to BBO. First, we propose blended migration. Second, we modify BBO to solve constrained optimization problems. The constrained BBO algorithm is compared with solutions based on a genetic algorithm (GA) and particle swarm optimization (PSO). Numerical results indicate that BBO generally performs better than GA and PSO in handling constrained single-objective optimization problems.|Haiping Ma,Dan Simon"],["57834|GECCO|2006|A GA-based method to produce generalized hyper-heuristics for the D-regular cutting stock problem|The idea behind hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. This paper presents a GA-based method that produces general hyper-heuristics that solve two-dimensional cutting stock problems. The GA uses a variable-length representation, which evolves combinations of condition-action rules producing hyper-heuristics after going through a learning process which includes training and testing phases. Such hyper-heuristics, when tested with a large set of benchmark problems, produce outstanding results (optimal and near-optimal) for most of the cases. The testbed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Marín,Cláudia J. Farías Zárate,Peter Ross,Manuel Valenzuela-Rendón","66274|AAAI|2007|Fluxplayer A Successful General Game Player|General Game Playing (GGP) is the art of designing programs that are capable of playing previously unknown games of a wide variety by being told nothing but the rules of the game. This is in contrast to traditional computer game players like Deep Blue, which are designed for a particular game and can't adapt automatically to modifications of the rules, let alone play completely different games. General Game Playing is intended to foster the development of integrated cognitive information processing technology. In this article we present an approach to General Game Playing using a novel way of automatically constructing a position evaluation function from a formal game description. Our system is being tested with a wide range of different games. Most notably, it is the winner of the AAAI GGP Competition .|Stephan Schiffel,Michael Thielscher","65618|AAAI|2005|Simultaneous Heuristic Search for Conjunctive Subgoals|We study the problem of building effective heuristics for achieving cunjunctive goals from heuristics for individual goals. We consider a straightforward method for building conjunctive heuristics that smoothly trades off between previous common methods. In addition to first explicitly formulating the problem of designing conjunctive heuristics. our major contribution is the discovery that this straightforward method substantially outperforms previously used methods across a wide range of domains. Based on a single positive real parameter k, our heuristic measure sums the individual heuristic values for the subgoal conjuncts, each raised to the k'th power. Varying k allows loose approximation and combination of the previous min, max. and sum approaches, while mitigating some of the weaknesses in those approaches. Our empirical work shows that for many benchmark planning domains there exist fixed parameter values that perform well-- we give evidence that these values can be found automatically by training. Our method, applied to top-level conjunctive goals, shows dramatic improvements over the heuristic used in the FF planner across a wide range of planning competition benchmarks. Also, our heuristic, without computing landmarks, consistently improves upon the success ratio of a recently published landmark-based planner FF-L.|Lin Zhu,Robert Givan","58501|GECCO|2008|Hyper-heuristics for the dynamic variable ordering in constraint satisfaction problems|The idea behind hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. This paper presents a GA-based method that produces general hyper-heuristics for the dynamic variable ordering within Constraint Satisfaction Problems. The GA uses a variable-length representation, which evolves combinations of condition-action rules producing hyper-heuristics after going through a learning process which includes training and testing phases. Such hyper-heuristics, when tested with a large set of benchmark problems, produce encouraging results for most of the cases. The testebed is composed of problems randomly generated using an algorithm proposed by Prosser.|Hugo Terashima-Marín,José Carlos Ortiz-Bayliss,Peter Ross,Manuel Valenzuela-Rendón","65225|AAAI|2004|Learning Social Preferences in Games|This paper presents a machine-learning approach to modeling human behavior in one-shot games. It provides a framework for representing and reasoning about the social factors that affect people's play. The model predicts how a human player is likely to react to different actions of another player, and these predictions are used to determine the best possible strategy for that player. Data collection and evaluation of the model were performed on a negotiation game in which humans played against each other and against computer models playing various strategies. A computer player trained on human data outplayed Nash equilibrium and Nash bargaining computer players as well as humans. It also generalized to play people and game situations it had not seen before.|Ya'akov Gal,Avi Pfeffer,Francesca Marzo,Barbara J. Grosz","58001|GECCO|2007|Comparing two models to generate hyper-heuristics for the d-regular bin-packing problem|The idea behind hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. This paper presents two Evolutionary-Computation-based Models to producehyper-heuristics that solve two-dimensional bin-packing problems. The first model uses an XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The second model is based on a GA that uses a variable-length representation, which evolves combinations of condition-action rules producing hyper-heuristics after going through alearning process which includes training and testing phases.Both approaches, when tested and compared using a large set ofbenchmark problems, perform better than the combinations ofsingle heuristics. The testbed is composed of problems used inother similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Marín,Cláudia J. Farías Zárate,Peter Ross,Manuel Valenzuela-Rendón","57553|GECCO|2005|Hyper-heuristics and classifier systems for solving D-regular cutting stock problems|This paper presents a method for combining concepts of Hyper-heuristics and Learning Classifier Systems for solving D Cutting Stock Problems. The idea behind Hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. In this paper, the Hyper-heuristic is formed using a XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The XCS evolves a behavior model which determines the possible actions (selection and placement heuristics) for given states of the problem. When tested with a collection of different problems, the method finds very competitive results for most of the cases. The testebed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Marín,E. J. Flores-?lvarez,Peter Ross","57194|GECCO|2003|Learning a Procedure That Can Solve Hard Bin-Packing Problems A New GA-Based Approach to Hyper-heuristics|The idea underlying hyper-heuristics is to discover some combination of familiar, straightforward heuristics that performs very well across a whole range of problems. To be worthwhile, such a combination should outperform all of the constituent heuristics. In this paper we describe a novel messy-GA-based approach that learns such a heuristic combination for solving one-dimensional bin-packing problems. When applied to a large set of benchmark problems, the learned procedure finds an optimal solution for nearly % of them, and for the rest produces an answer very close to optimal. When compared with its own constituent heuristics, it ranks first in % of the problems.|Peter Ross,Javier G. Marín-Blázquez,Sonia Schulenburg,Emma Hart","65540|AAAI|2005|Automatically Acquiring Domain Knowledge For Adaptive Game AI Using Evolutionary Learning|Game AI is the decision-making process of computer-controlled opponents in computer games. Adaptive game AI can improve the entertainment value of computer games. It allows computer-controlled opponents to automatically fix weaknesses in the game AI and respond to changes in human-player tactics. Dynamic scripting is a recently developed approach for adaptive game AI that learns which tactics (i.e., action sequences) an opponent should select to play effectively against the human player. In previous work, these tactics were manually generated. We introduce AKADS it uses an evolutionary algorithm to automatically generate such tactics. Our experiments show that it improves dynamic scripting's performance on a real-time strategy (RTS) game. Therefore, we conclude that high-quality domain knowledge (i.e., tactics) can be automatically generated for strong adaptive AI opponents in RTS games. This reduces the time and effort required by game developers to create intelligent game AI, thus freeing them to focus on other important topics (e.g., storytelling, graphics).|Marc J. V. Ponsen,Héctor Muñoz-Avila,Pieter Spronck,David W. Aha","58069|GECCO|2007|SwarmArchitect a swarm framework for collaborative construction|Computer game development has become increasingly popular in the field of autonomous systems. One of the main topics studies the building of various architectures in computer games. A realistic human-like architecture is expected in a thematic computer game, since it strongly motivates the game players in an intuitive way. However, the task of building a human-like architecture is non-trivial since the construction is a real time process without human supervision. In this paper, we present a collective building algorithm inspired by social insects for intelligent construction based on multiple agents. A swarm of virtual agents indirectly design edifications, which resemble basic features in human-like architecture by using a stigmergic mechanism along with branching rules. The main idea of the algorithm is to map sensory information to appropriate building actions.|Yifeng Zeng,Jorge Cordero Hernandez,Dennis Plougman Buus"],["65122|AAAI|1987|From Intelligent Tutoring to Computerized Psychotherapy|Building on the successes and shortcomings of previous experiences with computerized psychotherapy, we have attempted to extend the paradigm of intelligent tutoring systems to the domain of therapeutic interaction. Based on canonical examples, I present three dimensions of .the task of tutoring systems teaching problem-solving vs. domain knowledge teaching isolated domains vs domains where students have prior misconceptions teaching with the use of functional models of the domain vs no functional models. I then show how implications of these dimensions have helped us determine the specifications of a tutoring system for sexual therapy. Our approach has consisted of engaging patients in a tutoring dialogue driven by the identification of problem areas and their associated misconceptions. A diagnostic module, implemented as a traditional expert system, uses an extensive bug library to derive an internal model of patients. A dialogue driver relies on a hierarchy of dialogue plans and demons in order to preserve a logical grouping of related topics while remaining flexible to adapt itself, at each level of the dialogue hierarchy, to the unfolding case.|David Servan-Schreiber","65590|AAAI|2005|Online Resource Allocation Using Decompositional Reinforcement Learning|This paper considers a novel application domain for reinforcement learning that of \"autonomic computing,\" i.e. selfmanaging computing systems. RL is applied to an online resource allocation task in a distributed multi-application computing environment with independent time-varying load in each application. The task is to allocate servers in real time so as to maximize the sum of performance-based expected utility in each application. This task may be treated as a composite MDP, and to exploit the problem structure, a simple localized RL approach is proposed, with better scalability than previous approaches. The RL approach is tested in a realistic prototype data center comprising real servers, real HTTP requests, and realistic time-varying demand. This domain poses a number of major challenges associated with live training in a real system, including the need for rapid training, exploration that avoids excessive penalties, and handling complex, potentially non-Markovian system effects. The early results are encouraging in overnight training, RL performs as well as or slightly better than heavily researched model-based approaches derived from queuing theory.|Gerald Tesauro","65527|AAAI|2005|New Approaches to Optimization and Utility Elicitation in Autonomic Computing|Autonomic (self-managing) computing systems face the critical problem of resource allocation to different computing elements. Adopting a recent model, we view the problem of provisioning resources as involving utility elicitation and optimization to allocate resources given imprecise utility information. In this paper, we propose a new algorithm for regret-based optimization that performs significantly faster than that proposed in earlier work. We also explore new regret-based elicitation heuristics that are able to find near-optimal allocations while requiring a very small amount of utility information from the distributed computing elements. Since regret-computation is intensive, we compare these to the more tractable Nelder-Mead optimization technique w.r.t. amount of utility information required.|Relu Patrascu,Craig Boutilier,Rajarshi Das,Jeffrey O. Kephart,Gerald Tesauro,William E. Walsh","58563|GECCO|2009|An ant based algorithm for task allocation in large-scale and dynamic multiagent scenarios|This paper addresses the problem of multiagent task allocation in extreme teams. An extreme team is composed by a large number of agents with overlapping functionality operating in dynamic environments with possible inter-task constraints. We present eXtreme-Ants, an approximate algorithm for task allocation in extreme teams. The algorithm is inspired by the division of labor in social insects and in the process of recruitment for cooperative transport observed in ant colonies. Division of labor offers fast and efficient decision-making, while the recruitment ensures the allocation of tasks that require simultaneous execution. We compare eXtreme-Ants with two other algorithms for task allocation in extreme teams and we show that it achieves balanced efficiency regarding quality of the solution, communication, and computational effort.|Fernando dos Santos,Ana L. C. Bazzan","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","65467|AAAI|2005|Heterogeneous Multirobot Coordination with Spatial and Temporal Constraints|Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.|Mary Koes,Illah R. Nourbakhsh,Katia P. Sycara","66389|AAAI|2008|On the Enactability of Business Protocols|Protocols specifying business interactions among autonomous parties enable reuse and promote interoperability. A protocol is specified from a global viewpoint, but enacted in a distributed manner by (agents playing) different roles. Each role describes a local representation. An ill-specified protocol may yield roles that fail to produce correct enactments of the protocol. Existing approaches lack a formal and comprehensive treatment of this problem. Building on recent work on declaratively specifying a protocol as a set of rules of causal logic, this paper formally defines the enactability of protocols. It presents necessary and sufficient conditions for the enactability of a protocol as well as a decision procedure for extracting correct roles from enactable protocols.|Nirmit Desai,Munindar P. Singh","66599|AAAI|2008|Agent Coordination with Regret Clearing|Sequential single-item auctions can be used for the distributed allocation of tasks to cooperating agents. We study how to improve the team performance of sequential single-item auctions while still controlling the agents in real time. Our idea is to assign that task to agents during the current round whose regret is large, where the regret of a task is defined as the difference of the second-smallest and smallest team costs resulting from assigning the task to the second-best and best agent, respectively. Our experimental results show that sequential single-item auctions with regret clearing indeed result in smaller team costs than standard sequential single-item auctions for three out of four combinations of two different team objectives and two different capacity constraints (including no capacity constraints).|Sven Koenig,Xiaoming Zheng,Craig A. Tovey,Richard B. Borie,Philip Kilby,Vangelis Markakis,Pinar Keskinocak","80238|VLDB|2002|A Logical Framework for Scheduling Workflows under Resource Allocation Constraints|A workflow consists of a collection of coordinated tasks designed to carry out a well-defined complex process, such as catalog ordering, trip planning, or a business process in an enterprise. Scheduling of workflows is a problem of finding a correct execution sequence for the workflow tasks, i.e., execution that obeys the constraints that embody the business logic of the workflow. Research on workflow scheduling has largely concentrated on temporal constraints, which specify correct ordering of tasks. Another important class of constraints -- those that arise from resource allocation -- has received relatively little attention in workflow modeling. Since typically resources are not limitless and cannot be shared, scheduling of a workflow execution involves decisions as to which resources to use and when. In this work, we present a framework for workflows whose correctness is given by a set of resource allocation constraints and develop techniques for scheduling such systems. Our framework integrates Concurrent Transaction Logic (CTR) with constraint logic programming (CLP), yielding a new logical formalism, which we call Concurrent Constraint Transaction Logic, or CCTR.|Pinar Senkul,Michael Kifer,Ismail Hakki Toroslu","80080|VLDB|1983|Database Partitioning in a Cluster of Processors|In a distributed database system the partitioning and allocation of the database over the processor nodes of the network can be a critical aspect of the database design effort. In this paper we develop and evaluate algorithms that perform this task in a computationally feasible manner. The network we consider is characterized by a relatively high communication bandwidth, considering the processing and input output capacities in its processors. Such a balance is typical if the processors are connected via busses or local networks. The common constraint that transactions have a specific root node no longer exists, so that there are more distribution choices. However, a poor distribution leads to less efficient computation, higher costs, and higher loads in the nodes or in the communication network so that the system may not be able to handle the required set of transactions. Our approach is to first split the database into fragments which constitute appropriate units for allocation. The fragments to be allocated are selected based on maximal benefit criteria using a greedy heuristic. The assignment to processor nodes uses a first-fit algorithm. The complete algorithm, called GFF, is stated in a procedural form. The complexity of the problem and of its candidate solutions are analyzed and several interesting relationships are proven. Alternate benefit metrics are considered, since the execution cost of the allocation procedure varies by orders of magnitude with the alternatives of benefit evaluation. A mixed benefit evaluation strategy is eventually proposed. A model for evaluation is presented. Two of the strategies are experimentally evaluated, and the reported results support the discussion. The approach should be suitable for other cases where resources have to be allocated subject to resource constraints.|Domenico Saccà,Gio Wiederhold"],["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","79933|VLDB|1978|The Data Dictionary Facilities of NDB|The ANSISPAN architecture for data base management systems has proposed that a data dictionary be an integral part of an overall DBMS architecture. In this paper, we develop this approach and describe its use in a prototype data base management system known as NDB. The essential feature of the NDB approach is a data dictionary which uses the same architecture as the data base itself, and is physically integrated with the data base. This constitutes a single source of meta-data organised in all respects as a general purpose data base, which both controls DBMS operations on and provides documentation to the user. The dictionary is described in terms of entities and named binary relationships, comprising information-oriented description of the meta-data. We show how it may be used in the implementation of intelligent end user facilities, and how these end user facilities may in turn be used to access the data dictionary. As a result it is possible to access and modify both the data dictionary and the database using the same interface and during the same terminal session.|G. C. H. Sharman,N. Winterbottom","79957|VLDB|1979|A Designer for DBMS-Processable Logical Database Structures|An analytical approach to the design of logical database structures is presented. Inputs to the model consist of data item types and volume, associations among items, security constraints, and item access frequencies required to satisfy known database applications. The output is a pro- totype database management system (DBMS) process- able schema for a hierarchical andor network data- base structure. The design problem is formulated as an integer programming problem and the branch and bound method is used to generate an optimal solution for the parameters specified. The objec- tive function to be minimized is logical record access, subject to constraints on record size and total database size. Experience with the model has shown it to be an excellent predictor of real performance, easy to use, and have relatively low computing cost for moderately-sized database design problems. Use of the model is illustrated through an example problem and its solution. Direct comparison with a previous methodology clearly establishes the superiority of the current approach.|Keki B. Irani,Subir Purkayastha,Toby J. Teorey","79955|VLDB|1979|Spatial Management of Data Abstract|Spatial Data Management is a technique for organizing and retrieving information by positioning it in a Graphical Data Space (GDS). This Graphical Data Space is viewed through a color raster scan display which enables users to traverse the GDS surface or zoom into the image to obtain greater detail. In contrast to conventional database management systems - in which users access data by asking questions in a formal query language, a Spatial Data Management System (SDMS) presents the information graphically in a form which seems to encourage browsing and to require less prior knowledge of the contents and organization of the database. This paper presents an overview of the SDMS concept and describes its implementation in a prototype system for retrieving information from both a symbolic database management system and from an optical videodisk.|Christopher F. Herot","80135|VLDB|1992|A Multi-Resolution Relational Data Model|The use of data at different levels of information content is essential to the performance of multimedia, scientific, and other large databases because it can significantly decrease IO and communication costs. The performance advantages of such a multi-resolution scheme can only be fully exploited by a data model that supports the convenient retrieval of data at different levels of information content. In this paper we extend the relational data model to support multi-resolution data retrieval. In particular, we introduce a new partial set construct, called the sandbag, that can support multi-resolution for the types of data used in a wide variety of next-generation database applications, as well as traditional applications. We extend the relational algebra operators to analogous operators on sandbags. The resulting extension of the relational algebra is sound and forms a foundation for future database management systems that support these types of next-generation applications.|Robert L. Read,Donald S. Fussell,Abraham Silberschatz","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","65062|AAAI|1987|KADBASE - A Prototype Expert System-Database Interface for Integrated CAE Environments|Database management systems (DBMSs) are important components of existing integrated computer-aided engineering (CAE) systems. Expert systems (ESs) are being applied to a broad range of engineering problems. However, most of the prototype expert system applications have been restricted to limited amounts of data and have no facility for sophisticated data management. KADBASE is a flexible, knowledge-based interface in which multiple expert systems and multiple databases can communicate as independent, self-descriptive components within an integrated, distributed engineering computing environment.|H. Craig Howard,Daniel R. Rehak","80290|VLDB|2003|Grid Data Management Systems  Services|The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities. Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies. The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.|Arun Jagatheesan,Reagan Moore,Norman W. Paton,Paul Watson","79937|VLDB|1978|A Micro-DBMS for a Distributed Data Base|Recent and foreseeable advances in the technologies of microprocessors and computer communications imply the viability of a new architecture for Data Base Management Systems which should develop into an attractive tool for many applications of distributed systems. The key aspect of this architecture is the data base server, a possibly portable microcomputer enabling remote access to and use of a subsidiary data base which is \"refreshed\" at intervals by communication with a central data-base system. The practicality of this arrangement depends upon achieving reasonable capability in the microcomputer. We describe a micro-DBMS called UNITY whose characteristics, as tested in one application of a working system, already approach those needed for use in certain applications. The command structure, hardware vehicle, and performance of UNITY are discussed, together with plans for current and future improvements.|P. D. Ting,Dennis Tsichritzis"],["58119|GECCO|2007|Multiobjective clustering with automatic k-determination for large-scale data|Web mining - data mining for web data - is a key factor of web technologies. Especially, web behavior mining has attracted a great deal of attention recently. Behavior mining involves analyzing the behavior of users, finding patterns of user behavior, and predicting their subsequent behaviors or interests. Web behavior mining is used in web advertising systems or content recommendation systems. To analyze huge amounts of data, such as web data, data-clustering techniques are usually used. Data clustering is a technique involving the separation of data into groups according to similarity, and is usually used in the first step of data mining. In the present study, we developed a scalable data-clustering algorithm for web mining based on existent evolutionary multiobjective clustering algorithm. To derive clusters, we applied multiobjective clustering with automatic k-determination (MOCK). It has been reported that MOCK shows better performance than k-means, agglutination methods, and other evolutionary clustering algorithms. MOCK can also find the appropriate number of clusters using the information of the trade-off curve. The k-determination scheme of MOCK is powerful and strict. However the computational costs are too high when applied to clustering huge data. In this paper, we propose a scalable automatic k-determination scheme. The proposed scheme reduces Pareto-size and the appropriate number of clusters can usually be determined.|Nobukazu Matake,Tomoyuki Hiroyasu,Mitsunori Miki,Tomoharu Senda","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","80154|VLDB|2001|Self-similarity in the Web|Algorithmic tools for searching and mining the Web are becoming increasingly sophisticated and vital. In this context, algorithms that use and exploit structural information about the Web perform better than generic methods in both efficiency and reliability.We present an extensive characterization of the graph structure of the Web, with a view to enabling high-performance applications that make use of this structure. In particular, we show that the Web emerges as the outcome of a number of essentially independent stochastic processes that evolve at various scales. A striking consequence of this scale invariance is that the structure of the Web is \"fractal\"---cohesive subregions display the same characteristics as the Web at large. An understanding of this underlying fractal nature is therefore applicable to designing data services across multiple domains and scales.We describe potential applications of this line of research to optimized algorithm design for Web-scale data analysis.|Stephen Dill,Ravi Kumar,Kevin S. McCurley,Sridhar Rajagopalan,D. Sivakumar,Andrew Tomkins","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","66313|AAAI|2008|Semantic Web Development for Traditional Chinese Medicine|Despite its centrality to Chinese culture and wide adoption in Chinese communities, Traditional Chinese Medicine (TCM) has rarely been the application domain of computational analysis in previous academic works. Here we present the first systematic adoption of the state-of-the-art Semantic Web technologies in the codification, management, and utilization of TCM information and knowledge resources. These technologies are proved effective in bridging the semantic gaps between a plurality of legacy and heterogeneous relational databases, enabling ontology-based query and search across database boundaries. A global herb-drug interaction network is constructed and represented in Semantic Web language, on which the semantic graph mining methodology is applied for discovering and interpreting interesting patterns. This deployed Semantic Web platform provides various innovative information retrieval and knowledge discovery services to the TCM domain experts with positive feedbacks. This project demonstrates Semantic Web's advantages in connecting data across domain and community boundaries to facilitate interdisciplinary and cross-cultural studies.|Zhaohui Wu,Tong Yu,Huajun Chen,Xiaohong Jiang,Chunying Zhou,Yu Zhang,Yuxin Mao,Yi Feng,Meng Cui,Aining Yin","80829|VLDB|2007|EntityRank Searching Entities Directly and Holistically|As the Web has evolved into a data-rich repository, with the standard \"page view,\" current search engines are becoming increasingly inadequate for a wide range of query tasks. While we often search for various data \"entities\" (e.g., phone number, paper PDF, date), today's engines only take us indirectly to pages. While entities appear in many pages, current engines only find each page individually. Toward searching directly and holistically for finding information of finer granularity, we study the problem of entity search, a significant departure from traditional document retrieval. We focus on the core challenge of ranking entities, by distilling its underlying conceptual model Impression Model and developing a probabilistic ranking framework, EntityRank, that is able to seamlessly integrate both local and global information in ranking. We evaluate our online prototype over a TB Web corpus, and show that EntityRank performs effectively.|Tao Cheng,Xifeng Yan,Kevin Chen-Chuan Chang","58354|GECCO|2008|EIN-WUM an AIS-based algorithm for web usage mining|With the ever expanding Web and the information published on it, effective tools for managing such data and presenting information to users based on their needs are becoming necessary. In this paper, we propose a new algorithm named \"EIN-WUM\" for Web usage mining based on artificial immune system metaphor. This algorithm introduces several novelties such as using danger theory, directed mutation and an enhanced immune network model. Experimental results show that The EIN-WUM algorithm can properly learn the frequent trends in noisy, sparse and huge Web usage data in single pass.|Adel Torkaman Rahmani,B. Hoda Helmi","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|Régis Sabbadin,Jérôme Lang,Nasolo Ravoanjanahry","66459|AAAI|2008|A Variance Analysis for POMDP Policy Evaluation|Partially Observable Markov Decision Processes have been studied widely as a model for decision making under uncertainty, and a number of methods have been developed to find the solutions for such processes. Such studies often involve calculation of the value function of a specific policy, given a model of the transition and observation probabilities, and the reward. These models can be learned using labeled samples of on-policy trajectories. However, when using empirical models, some bias and variance terms are introduced into the value function as a result of imperfect models. In this paper, we propose a method for estimating the bias and variance of the value function in terms of the statistics of the empirical transition and observation model. Such error terms can be used to meaningfully compare the value of different policies. This is an important result for sequential decision-making, since it will allow us to provide more formal guarantees about the quality of the policies we implement. To evaluate the precision of the proposed method, we provide supporting experiments on problems from the field of robotics and medical decision making.|Mahdi Milani Fard,Joelle Pineau,Peng Sun","66182|AAAI|2007|Situated Conversational Agents|A Situated Conversational Agent (SCA) is an agent that engages in dialog about the context within which it is embedded. Situated dialog is characterized by its deep connection to the embedding context, and the precise cross-timing of linguistic and non-linguistic actions. This paper describes initial research into the construction of an SCA that engages in dialog about collaborative physical tasks, in which agents engage in dialog with the joint goal of manipulating the physical context in some manner. Constructing an SCA that can interact naturally in such tasks requires an agent with the ability to interleave planning, action, and observation while operating in a partially observable environment. Consequently, I propose to model an SCA as a Partially Observable Markov Decision Process (POMDP).|William Thompson","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","66277|AAAI|2008|Exploiting Symmetries in POMDPs for Point-Based Algorithms|We extend the model minimization technique for partially observable Markov decision processes (POMDPs) to handle symmetries in the joint space of states, actions, and observations. The POMDP symmetry we define in this paper cannot be handled by the model minimization techniques previously published in the literature. We formulate the problem of finding the symmetries as a graph automorphism (GA) problem, and although not yet known to be tractable, we experimentally show that the sparseness of the graph representing the POMDP allows us to quickly find symmetries. We show how the symmetries in POMDPs can be exploited for speeding up point-based algorithms. We experimentally demonstrate the effectiveness of our approach.|Kee-Eung Kim","65172|AAAI|1994|Acting Optimally in Partially Observable Stochastic Domains|In this paper, we describe the partially observable Markov decision process (sc pomdp) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The sc pomdp approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the sc pomdp approach.|Anthony R. Cassandra,Leslie Pack Kaelbling,Michael L. Littman","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|Håkan L. S. Younes"],["57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","58133|GECCO|2007|MILCS a mutual information learning classifier system|This paper introduces a new variety of learning classifier system (LCS), called MILCS, which utilizes mutual information as fitness feedback. Unlike most LCSs, MILCS is specifically designed for supervised learning. MILCS's design draws on an analogy to the structural learning approach of cascade correlation networks. We present preliminary results, and contrast them to results from XCS. We discuss the explanatory power of the resulting rule sets, and introduce a new technique for visualizing explanatory power. Final comments include future directions for this research, including investigations in neural networks and other systems.|Robert Elliott Smith,Max Kun Jiang","58612|GECCO|2009|On the scalability of XCSF|Many successful applications have proven the potential of Learning Classifier Systems and the XCS classifier system in particular in datamining, reinforcement learning, and function approximation tasks. Recent research has shown that XCS is a highly flexible system, which can be adapted to the task at hand by adjusting its condition structures, learning operators, and prediction mechanisms. However, fundamental theory concerning the scalability of XCS dependent on these enhancements and problem difficulty is still rather sparse and mainly restricted to boolean function problems. In this article we developed a learning scalability theory for XCSF---the XCS system applied to real-valued function approximation problems. We determine crucial dependencies on functional properties and on the developed solution representation and derive a theoretical scalability model out of these constraints. The theoretical model is verified with empirical evidence. That is, we show that given a particular problem difficulty and particular representational constraints XCSF scales optimally. In consequence, we discuss the importance of appropriate prediction and condition structures regarding a given problem and show that scalability properties can be improved by polynomial orders, given an appropriate, problem-suitable representation.|Patrick O. Stalph,Martin V. Butz,David E. Goldberg,Xavier Llorà","57768|GECCO|2006|An anticipatory approach to improve XCSF|XCSF is a novel version of learning classifier systems (LCS) which extends the typical concept of LCS by introducing computable classifier prediction. In XCSF Classifier prediction is computed as a linear combination of classifier inputs and a weight vector associated to each classifier. Learning process takes place using a weight update mechanism. Initial results show that XCSF can be used to evolve accurate approximations of some functions. In this paper, we try to add an anticipatory component to XCSF improving its performance.|Amin Nikanjam,Adel Torkaman Rahmani","57228|GECCO|2003|Towards Building Block Propagation in XCS A Negative Result and Its Implications|The accuracy-based classifier system XCS is currently the most successful learning classifier system. Several recent studies showed that XCS can produce machine-learning competitive results. Nonetheless, until now the evolutionary mechanisms in XCS remained somewhat ill-understood. This study investigates the selectorecombinative capabilities of the current XCS system. We reveal the accuracy dependence of XCS's evolutionary algorithm and identify a fundamental limitation of the accuracy-based fitness approach in certain problems. Implications and future research directions conclude the paper.|Kurian K. Tharakunnel,Martin Butz,David E. Goldberg","58011|GECCO|2007|Introducing fault tolerance to XCS|In this paper, we introduce fault tolerance to XCS and propose a new XCS framework called XCS with Fault Tolerance (XCSFT). As an important branch of learning classifier systems, XCS has been proven capable of evolving maximally accurate, maximally general problem solutions. However, in practice, it oftentimes generates a lot of rules, which lower the readability of the evolved classification model, and thus, people may not be able to get the desired knowledge or useful information out of the model. Inspired by the fault tolerance mechanism proposed in field of data mining, we devise a new XCS framework by integrating the concept and mechanism of fault tolerance into XCS in order to reduce the number of classification rules and therefore to improve the readability of the generated prediction model. A series of $N$-multiplexer experiments, including -bit, -bit, -bit, and -bit multiplexers, are conducted to examine whether XCSFT can accomplish its goal of design. According to the experimental results, XCSFT can offer the same level of prediction accuracy on the test problems as XCS can, while the prediction model evolved by XCSFT consists of significantly fewer classification rules.|Hong-Wei Chen,Ying-Ping Chen","59027|GECCO|2010|Adaption of XCS to multi-learner predatorprey scenarios|Learning classifier systems (LCSs) are rule-based evolutionary reinforcement learning systems. Today, especially variants of Wilson's extended classifier system (XCS) are widely applied for machine learning. Despite their widespread application, LCSs have drawbacks, e. g., in multi-learner scennarios, since the Markov property is not fulfilled. In this paper, LCSs are investigated in an instance of the generic homogeneous and non-communicating predatorprey scenario. A group of predators collaboratively observe a (randomly) moving prey as long as possible, where each predator is equipped with a single, independent XCS. Results show that improvements in learning are achieved by cleverly adapting a multi-step approach to the characteristics of the investigated scenario. Firstly, the environmental reward function is expanded to include sensory information. Secondly, the learners are equipped with a memory to store and analyze the history of local actions and given payoffs.|Clemens Lode,Urban Richter,Hartmut Schmeck","57445|GECCO|2005|A first order logic classifier system|Motivated by the intention to increase the expressive power of learning classifier systems, we developed a new Xcs derivative, Fox-cs, where the classifier and observation languages are a subset of first order logic. We found that Fox-cs was viable at tasks in two relational task domains, poker and blocks world, which cannot be represented easily using traditional bit-string classifiers and inputs. We also found that for these tasks, the level of generality obtained by Fox-cs in the portion of population that produces optimal behaviour is consistent with Wilson's generality hypothesis.|Drew Mellor","57043|GECCO|2003|Tournament Selection Stable Fitness Pressure in XCS|Although it is known from GA literature that proportionate selection is subject to many pitfalls, the LCS community somewhat adhered to proportionate selection. Also in the accuracy-based learning classifier system XCS, introduced by Wilson in , proportionate selection is used. This paper identifies problem properties in which performance of proportionate selection is impaired. Consequently, tournament selection is introduced which makes XCS more parameter independent, noise independent, and more efficient in exploiting fitness guidance.|Martin Butz,Kumara Sastry,David E. Goldberg","57781|GECCO|2006|An open-set speaker identification system using genetic learning classifier system|This paper presents the design and implementation of an adaptive open-set speaker identification system with genetic learning classifier systems. One of the challenging problems in using learning classifier systems for numerical problems is the knowledge representation. The voice samples are a series of real numbers that must be encoded in a classifier format. We investigate several different methods for representing voice samples for classifier systems and study the efficacy of the methods. We also identify several challenges for learning classifier systems in the speaker identification problem and introduce new methods to improve the learning and classification abilities of the systems. Experimental results show that our system successfully learns  voice features at the accuracies of % to %, which is considered a strong result in the speaker identification community. This research presents the feasibility of using learning classifier systems for the speaker identification problem.|WonKyung Park,Jae C. Oh,Misty K. Blowers,Matt B. Wolf"]]},"title":{"entropy":6.286367137032243,"topics":["data base, for data, data, for systems, and data, the web, database systems, database, systems, data systems, for database, data management, data streams, the data, and for, query for, data model, natural language, and systems, reasoning about","learning for, reinforcement learning, learning, local search, for planning, and learning, search for, planning and, constraints satisfaction, search space, learning with, with, for with, with constraints, support vector, planning, with and, planning with, and search, learning classifier","genetic algorithm, algorithm for, genetic programming, genetic for, the problem, evolutionary algorithm, for the, using genetic, for problem, algorithm the, the, algorithm problem, the and, using algorithm, and genetic, evolutionary for, algorithm with, genetic with, estimation distribution, solving problem","particle swarm, for optimization, particle optimization, swarm optimization, neural networks, and for, for networks, ant colony, model for, multi-objective optimization, multiobjective optimization, differential evolution, evolution strategies, for dynamic, time series, method for, networks, evolutionary optimization, artificial immune, optimization with","for database, data streams, query for, efficient for, relational database, database systems, distributed systems, and database, database, the database, query processing, for queries, for processing, for streams, for distributed, relational systems, over data, efficient and, queries, access data","the web, the and, for web, semantic web, reasoning about, and web, and for, semantic for, and application, the semantic, web services, semantic and, for the, for application, management systems, and reasoning, for reasoning, for services, integrating and, activity recognition","with constraints, for constraints, constraints satisfaction, planning domains, support vector, for domains, and constraints, constraints, voting rules, partially observable, decision processes, for control, and domains, with continuous, decision tree, temporal constraints, for generating, markov processes, continuous action, with domains","search for, local search, and search, search space, with search, for games, search, the search, optimal for, using search, the games, for hierarchical, local for, games, search engine, heuristic search, games playing, local global, with local, hierarchical boa","genetic programming, genetic algorithm, genetic for, using genetic, using algorithm, programming for, and genetic, genetic with, using programming, and programming, the genetic, for using, with programming, the programming, using and, genetic networks, parallel algorithm, cartesian programming, for graph, for routing","evolutionary for, the evolutionary, using evolutionary, the performance, evolutionary computation, evolutionary algorithm, evolutionary and, evolutionary with, landscapes and, evolutionary, performance for, fitness for, evolutionary testing, population size, the size, the computation, the population, fitness functions, for testing, performance algorithm","analysis and, differential evolution, analysis for, evolution strategies, evolution for, evolution and, strategies for, and for, for classification, grammatical evolution, combinatorial auctions, using evolution, dimensionality reduction, evolution, the evolution, common sense, and strategies, local and, analysis, sense disambiguation","and for, artificial immune, method for, for detection, immune systems, and method, artificial systems, gene expression, for scheduling, for clustering, mechanism for, gene regulatory, social networks, job shop, matrix factorization, based and, using and, detection networks, and, for gene"],"ranking":[["80022|VLDB|1980|Status Report on ISOTCSCWG - Data Base Management Systems|This paper discussed the current status of the work of the ISOTCSCWG data base management systems working group. In existence for over five years, this working group has devoted it energies to the elaboration and extension of the notion of a \"conceptual schema\" as initially defined by the ANSIXSPARCDBMS Study Group. The focus of the ISO work has been the construction of a report on conceptual schema concepts, identification of various methodologies for their construction and the application of such methodologies to an example \"world\" in such a fashion that comparisons can be made between the different approaches. Underlying the work of this group has been the identification of fundamental philosophical differences in the various methodologies specifically that some assume the conceptual schema models the real world and others assume the conceptual schema models the data in the information base that holds information about the real world. This difference leads to considerable confusion in discussion when it is not explicitly recognized and one of the principal objectives of the ISOTCSCWG report is to expurgate that difference.|Thomas B. Steel Jr.","79809|VLDB|1975|Hierarchical Performance Analysis Models for Data Base Systems|This paper presents a comprehensive set of hierarchically organized synthetic models developed for the performance evaluation of data base management systems. The first set of algebraic models for data base management system itself contains a program behavior model, a retrieval model, a logical data base model, a physical data base model, and a data processing model. These models are intended to clarify logical and physical data base structures and essential operations in the data processing on them. Another set of models for performance analysis contains a macroscopic program behavior model, a storage model, a processor model, a user behavior model, and an interactive model. In this set this paper is particularly concerned with the first  models. The macroscopic program behavior model is based on the discussion of data locality and allows us to estimate the frequency of data page loading expected on a given application program and a data base. The storage model enables us to estimate the traverse time of data page loading. Finally the processor model allows us to evaluate the data retrieval processing time of data manipulation commands of a given data base structure, a data base management system and a set of application programs under the multiprogramming environment. These models are to be applied to the optimization of the application programs or the data base structure in order to obtain a higher data retrieval performance.|Isao Miyamoto","79850|VLDB|1977|Design and Performance Tools for Data Base Systems|Existing tools for logical and physical data base design are reviewed in this paper. Logical data base design methods are classified and their features are compared. Design models for selecting physical data base structures are classified based on their applications. Also reviewed are previous research dealing with implementation issues such as index selection, buffer management, and storage allocation. Finally, system performance evaluation techniques for database systems are surveyed.|Peter P. Chen,S. Bing Yao","80290|VLDB|2003|Grid Data Management Systems  Services|The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities. Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies. The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.|Arun Jagatheesan,Reagan Moore,Norman W. Paton,Paul Watson","79929|VLDB|1978|An Alternative Structure for Data Base Management Systems|The ANSIXSPARC proposal of standardization for architecture of Data Base Management Systems is particularly concerned with the achievement of iogical data independence. This proposal has almost universally been recognized as valid. In this paper we argue that this proposal is not a standardization of Data Base Management Systems features, but rather an implementation proposal, based on schemas mappings and transforms. A more general architecture is proposed and an implementation other than the one of the ANSIXSPARC is examined. Tradeoffs between the two implementation strategies are outlined.|Paolo Paolini","79885|VLDB|1977|Architectural Issues in Distributed Data Base Systems|The design of a distributed data base is a complex and difficult task requiring careful consideration of data organization, data distribution, user interface, updatingretrieval schemes, programdata placement, security policies and reliability issues. In this paper, we have discussed a design methodology which can be used to design a distributed data base. This design methodology is a systematic way to guide the designer in making decisions during the requirement process phase, the design process phase and the implementation phase. We conclude this paper by examining the architectural issues in the design of distributed data bases.|C. V. Ramamoorthy,Gary S. Ho,T. Kirshnarao,Benjamin W. Wah","79923|VLDB|1978|A Search Processor for Data Base Management Systems|A special processor for performing the selection and restriction operation of data base management systems by linearly searching large files with very high speed is described. It includes the reasoning, leading to the architecture and the structure of the search processor and a report on the status and the plans of a research project.|Hans-Otto Leilich,Günther Stiege,Hans Christoph Zeidler","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79814|VLDB|1975|A Multi-Level Architecture for Relational Data Base Systems|Most of the literature on implementation of relations has been directed toward user features, with little attention paid to an overall conceptual view of underlying structures. Performance oriented considerations have been treated only for isolated problems. Toward a solution to these problems we describe a multi-level architecture for relational data base systems. This architecture distinguishes clearly between user oriented features, access path structures, data structures and file organization. It also allows efficiency problems to be isolated within levels and solved independently of each other, without impacting the logical structure of the user's virtual machine. Specific problems considered here in the context of this architecture include the mapping of relations into files, the implementation of fast access paths, and some file level optimizations that are particularly useful in relational systems.|Hans Albrecht Schmid,Philip A. Bernstein","79840|VLDB|1976|An Approach to Data Communication between Different Generalized Data Base Management Systems|A number of data base management systems (DBMS) are now in operation in industry, government and the academic world. However, since most of these DBMSs have different conceptual and internal mo- dels, they cannot interact and exchange information in a network environment. This report discusses some of the problems and provides a methodology for communicating and exchanging information between hetrogeneous models of data or DBMSs that are commercially available.|E. Nahouraii,L. O. Brooks,Alfonso F. Cardenas"],["66622|AAAI|2010|Integrating Sample-Based Planning and Model-Based Reinforcement Learning|This paper introduces an abstract high dependability framework for the implementation of embedded control software with hard real-time constraints. The framework specifies time-triggered sensor readings, atomic component invocations, actuator updates, and pattern switches independent of any implementation platform. In order to leverage model continuity, XML-based description of composite component informal description is required, which supports reuse of components and model information interoperability. By separating the platform-independent from the platform-dependent concerns, Consider a quality process control system in steel industry on a distributed real-time embedded environment, we implement a simplified high dependability design framework to prove the feasibility in the validation and synthesis of embedded control component execution.|Thomas J. Walsh,Sergiu Goschin,Michael L. Littman","66409|AAAI|2008|The PELA Architecture Integrating Planning and Learning to Improve Execution|Building architectures for autonomous rational behavior requires the integration of several AI components, such as planning, learning and execution monitoring. In most cases, the techniques used for planning and learning are tailored to the specific integrated architecture, so they could not be replaced by other equivalent techniques. Also, in order to solve tasks that require lookahead reasoning under uncertainty, these architectures need an accurate domain model to feed the planning component. But the manual definition of these models is a difficult task. In this paper, we propose an architecture that uses off-the-shelf interchangeable planning and learning components to solve tasks that require flexible planning under uncertainty. We show how a relational learning component can be applied to automatically obtain accurate probabilistic action models from executions of plans. These models can be used by any classical planner that handles metric functions, or, alternatively, by any decision theoretic planner. We also show how these components can be integrated to solve tasks continuously, under an online relational learning scheme.|Sergio Jiménez,Fernando Fernández,Daniel Borrajo","66518|AAAI|2008|Learning and Inference with Constraints|Probabilistic modeling has been a dominant approach in Machine Learning research. As the field evolves, thc problems of interest become increasingly challenging and complex. Making complex decisions in real world problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. However, incorporating nonlocal depcndencies in a probabilistic model can lead to intractable training and inference. This paper presents Constraints Conditional Models (CCMs), a framework that augments probabilistic models with declarative constraints as a way to support decisions in an expressive output space while maintaining modularity and tractability of training. We further show that declarative constraints can be used to take advantage of unlabeled data when training the probabilistic model.|Ming-Wei Chang,Lev-Arie Ratinov,Nicholas Rizzolo,Dan Roth","58488|GECCO|2008|Genetic local search for rule learning|The performance of Evolutionary Algorithms for combinatorial problems can be significantly improved by adding Local Search, thus obtaining a Genetic Local Search (GLS) also called Memetic Algorithm. In this work, we adapt a previous Stochastic Local Search (SLS) algorithm and embed it into a GBML system. The adapted SLS algorithm works as a module of the system that tries to improve a random individual in the population. We perform experiments to evaluate this adapted SLS procedure and results show that this new GLS system is very effective, not losing in any of the  UCI datasets tested when compared to the system without the SLS procedure. The system either obtained significantly more accurate concepts using lower number of rules and features or it achieved the same accuracy as the system without the SLS procedure, but reduced the number of rules and features, and also the time taken to develop the solution.|Cristiano Grijó Pitangui,Gerson Zaverucha","66874|AAAI|2010|Search-Based Path Planning with Homotopy Class Constraints|An approach is investigated for the adaptive Hinfin control design for a class of nonlinear state-delayed systems. The nonlinear term is approximated by a linearly parameterized neural networks(LPNN). A linear state feedback Hinfin control law is presented. An adaptive weight adjustment mechanism for the neural networks is developed to ensure Hinfin regulation performance. It is shown that the control gain matrices and be transformed into a standard linear matrix inequality problem and solved via a developed recurrent neural network|Subhrajit Bhattacharya","65608|AAAI|2005|Learning Planning Rules in Noisy Stochastic Worlds|We present an algorithm for learning a model of the effects of actions in noisy stochastic worlds. We consider learning in a D simulated blocks world with realistic physics. To model this world, we develop a planning representation with explicit mechanisms for expressing object reference and noise. We then present a learning algorithm that can create rules while also learning derived predicates, and evaluate this algorithm in the blocks world simulator, demonstrating that we can learn rules that effectively model the world dynamics.|Luke S. Zettlemoyer,Hanna Pasula,Leslie Pack Kaelbling","65613|AAAI|2005|Learning Measures of Progress for Planning Domains|We study an approach to learning heuristics for planning domains from example solutions. There has been little work on learning heuristics for the types of domains used in deterministic and stochastic planning competitions. Perhaps one reason for this is the challenge of providing a compact heuristic language that facilitates learning. Here we introduce a new representation for heuristics based on lists of set expressions described using taxonomic syntax. Next, we review the idea of a measure of progress (parmar ), which is any heuristic that is guaranteed to be improvable at every state. We take finding a measure of progress as our learning goal, and describe a simple learning algorithm for this purpose. We evaluate our approach across a range of deterministic and stochastic planning-competition domains. The results show that often greedily following the learned heuristic is highly effective. We also show our heuristic can be combined with learned rule-based policies, producing still stronger results.|Sung Wook Yoon,Alan Fern,Robert Givan","66585|AAAI|2008|Learning to Improve Earth Observation Flight Planning|This paper describes a method and system for integrating machine learning with planning and data visualization for the management of mobile sensors for Earth science investigations. Data mining identifies discrepancies between previous observations and predictions made by Earth science models. Locations of these discrepancies become interesting targets for future observations. Such targets become goals used by a flight planner to generate the observation activities. The cycle of observation, data analysis and planning is repeated continuously throughout a multi-week Earth science investigation.|Robert A. Morris,Nikunj C. Oza,Leslie Keely,Elif Kürklü,Anthony Strawa","66169|AAAI|2007|Temporal Difference and Policy Search Methods for Reinforcement Learning An Empirical Comparison|Reinforcement learning (RL) methods have become popular in recent years because of their ability to solve complex tasks with minimal feedback. Both genetic algorithms (GAs) and temporal difference (TD) methods have proven effective at solving difficult RL problems, but few rigorous comparisons have been conducted. Thus, no general guidelines describing the methods' relative strengths and weaknesses are available. This paper summarizes a detailed empirical comparison between a GA and a TD method in Keepaway, a standard RL benchmark domain based on robot soccer. The results from this study help isolate the factors critical to the performance of each learning method and yield insights into their general strengths and weaknesses.|Matthew E. Taylor,Shimon Whiteson,Peter Stone","66777|AAAI|2010|Hierarchical Skill Learning for High-Level Planning|Query by humming (QBH) is an interactive tool for retrieving favored songs from a large database of known media via acoustic input. In this task, common method for measuring similarity between query and candidate is either by symbolic notation distance or by framed based dynamic programming. However, the former has disadvantage of error-prone to the noted symbolic feature extraction stage, while the latter is time-consuming. It has been proved that transportation distance has its remarkable merit in image query. However, we adopt a new structure for handling QBH, which is based on an improved version of this measure in combination with a string searching algorithm. More practically, we extend this method in random piece context, which means users can hum at any part of music piece. Experimental results are evaluated in MIREX . Final .% MRR has shown its significant advantages.|James MacGlashan"],["58756|GECCO|2009|Cheating for problem solving a genetic algorithm with social interactions|We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, i.e. animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.|Rafael Lahoz-Beltra,Gabriela Ochoa,Uwe Aickelin","57057|GECCO|2003|A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem|We propose a hybrid genetic algorithm for the hexagonal tortoise problem. We combined the genetic algorithm with an efficient local heuristic and aging mechanism. Another search heuristic which focuses on the space around existing solutions is also incorporated into the genetic algorithm. With the proposed algorithm, we could find the optimal solutions of up to a fairly large problem.|Heemahn Choe,Sung-Soon Choi,Byung Ro Moon","57789|GECCO|2006|Optimising cancer chemotherapy using an estimation of distribution algorithm and genetic algorithms|This paper presents a methodology for using heuristic search methods to optimise cancer chemotherapy. Specifically, two evolutionary algorithms - Population Based Incremental Learning (PBIL), which is an Estimation of Distribution Algorithm (EDA), and Genetic Algorithms (GAs) have been applied to the problem of finding effective chemotherapeutic treatments. To our knowledge, EDAs have been applied to fewer real world problems compared to GAs, and the aim of the present paper is to expand the application domain of this technique.We compare and analyse the performance of both algorithms and draw a conclusion as to which approach to cancer chemotherapy optimisation is more efficient and helpful in the decision-making activity led by the oncologists.|Andrei Petrovski,Siddhartha Shakya,John A. W. McCall","58433|GECCO|2008|Combining cartesian genetic programming with an estimation of distribution algorithm|This paper describes initial testing of a novel idea to combine a CGP with an EDA. In recent work a new improved crossover technique was successfully applied to a CGP. To implement the new method meant changing the traditional CGP representation. The new representation developed in that work lends itself very nicely to some probability distribution being implemented. The work in this paper has investigated this idea of incoporating estimated probability distributions into the new CGP method with crossover.|Janet Clegg","57116|GECCO|2003|Designing A Hybrid Genetic Algorithm for the Linear Ordering Problem|The Linear Ordering Problem(LOP), which is a well-known NP-hard problem, has numerous applications in various fields. Using this problem as an example, we illustrate a general procedure of designing a hybrid genetic algorithm, which includes the selection of crossovermutation operators, accelerating the local search module and tuning the parameters. Experimental results show that our hybrid genetic algorithm outperforms all other existing exact and heuristic algorithms for this problem.|Gaofeng Huang,Andrew Lim","57687|GECCO|2006|A genetic algorithm for the longest common subsequence problem|A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on  sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.|Brenda Hinkemeyer,Bryant A. Julstrom","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57037|GECCO|2003|A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem|Recently proved successful for variants of the vehicle routing problem (VRP) involving time windows, genetic algorithms have not yet shown to compete or challenge current best search techniques in solving the classical capacitated VRP. In this paper, a hybrid genetic algorithm to address the capacitated vehicle routing problem is proposed. The basic scheme consists in concurrently evolving two populations of solutions to minimize total traveled distance using genetic operators combining variations of key concepts inspired from routing techniques and search strategies used for a time-variant of the problem to further provide search guidance while balancing intensification and diversification. Results from a computational experiment over common benchmark problems report the proposed approach to be very competitive with the best-known methods.|Jean Berger,Mohamed Barkaoui","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","58219|GECCO|2007|A fuzzy genetic algorithm for the dynamic cell formation problem|This paper deals with a fuzzy genetic algorithm applied to a manufacturing cell formation problem. We discuss the importance of taking into account the dynamic aspect of the problem that has been poorly studied in the related literature. Using a multi-periodic planning horizon modeling, two strategies are considered passive and active. The first strategy consists of maintaining the same composition of machines during the overall planning horizon, while the second allows performing a different composition for each period. When the decision maker wants to choose the most adequate strategy for its environment, there is a need to control the proposed evolutionary solving approach, due to the complexity of the model. For that purpose, we propose an off-line fuzzy logic enhancement. The results, using this enhancement, are better than those obtained using the GA alone.|Menouar Boulif,Karim Atif"],["58008|GECCO|2007|Multi-objective particle swarm optimization on computer grids|In recent years, a number of authors have successfully extended particle swarmoptimization to problem domains with multiple objec-tives. This paper addresses theissue of parallelizing multi-objec-tive particle swarms. We propose and empirically comparetwo parallel versions which differ in the way they divide the swarminto subswarms that can be processed independently on differentprocessors. One of the variants works asynchronouslyand is thus particularly suitable for heterogeneous computer clusters asoccurring e.g. in moderngrid computing platforms.|Sanaz Mostaghim,Jürgen Branke,Hartmut Schmeck","58586|GECCO|2009|Dynamic particle swarm optimization via ring topologies|Particle Swarm Optimization (PSO) has been proven to be a fast and effective search algorithm capable of solving complex and varied problems. To date numerous swarm topologies have been proposed and investigated as a means of increasing the effectiveness of the generalized algorithm. Typical topologies employ static arrangements of particles defined at the beginning of execution and remaining constant throughout run-time. Topologies that do allow for restructuring, often do so according to predefined rules that limit the opportunity and manner in which the topology can change. Recent investigations have shown that dynamically redefining a topology by stochastically re-organizing the swarm at periodic intervals improves performance for certain types of problems. In this work the effectiveness of a novel topology \"Dynamic Ring\" and a derivative of the \"Dynamic Multi Swarm PSO\" topology dubbed \"Dynamic Multi Swarm with Ring\" are investigated. We show that these two new topologies show generally enhanced performance relative to previously proposed topologies on a suite of twelve test functions.|Frank Jones,Terence Soule","58025|GECCO|2007|A heuristic particle swarm optimization|A heuristic version of the particle swarm optimization (PSO) is introduced in this paper. In this new method called \"The heuristic particle swarm optimization(HPSO)\", we use heuristics to choose the next particle to update its velocity and position. By using heuristics , the convergence rate to local minimum is faster. To avoid premature convergence of the swarm, the particles are re-initialized with random velocity when moving too close to the global best position. The combination of heuristics and re-initialization mechanism make HPSO outperform the basic PSO and recent versions of PSO.|Hoang Thanh Lam,Popova Nina Nicolaevna,Nguyen Thoi Minh Quan","58085|GECCO|2007|MRPSO MapReduce particle swarm optimization|In optimization problems involving large amounts of data, Particle Swarm Optimization (PSO) must be parallelized because individual function evaluations may take minutes or even hours. However, large-scale parallelization is difficult because programs must communicate efficiently, balance workloads and tolerate node failures. To address these issues, we present Map Reduce Particle Swarm Optimization(MRPSO), a PSO implementation based on Google's Map Reduce parallel programming model.|Andrew W. McNabb,Christopher K. Monson,Kevin D. Seppi","59071|GECCO|2010|An parallel particle swarm optimization approach for multiobjective optimization problems|This paper proposes a parallel particle swarm optimization (PPSO) to solve the multiobjective optimization problems (MOP). PPSO makes the use of the parallel characteristic of the PSO algorithm to deal with the multiple objectives issue of the MOP. PPSO uses as many swarms as the number of the objectives in the MOP and lets each swarm optimize only one of the objectives. These swarms work in parallel and each swarm can use a standard PSO or any other improved PSO variants to solve a single objective problem. PPSO has advantages on the following two aspects. First, as each swarm focus on optimizing only one objective, PPSO can avoid the difficulty of fitness assignment because the particles can be evaluated like in the single objective optimization problem. Second, as different swarms optimize different objectives, PPSO can maintain the population diversity to make a throughout search along the whole Pareto front to obtain nondominated solutions as many as possible. The performance of PPSO is tested on a set of benchmark problems complicated Pareto sets in CEC. The experimental results compared with those obtained by the state-of-the-art algorithms demonstrate the effectiveness and efficiency of PPSO, showing the good performance of PPSO in solving the MOP with complicated Pareto sets.|Zhi-hui Zhan,Jun Zhang","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58657|GECCO|2009|Particle swarm optimization with oscillation control|Particle Swarm Optimization (PSO) is a metaheuristic that has been successfully applied to linear and non-linear optimization problems in functions with discrete and continuous domains. This paper presents a new variation of this algorithm - called oscPSO - that improves the inherent search capacity of the original (canonical) version of the PSO algorithm. This version uses a deterministic local search method whose use depends on the movement patterns of the particles in each dimension of the problem. The method proposed was assessed by means of a set of complex test functions, and the performance of this version was compared with that of the original version of the PSO algorithm. In all cases, the oscPSO variation equaled or surpassed the performance of the canonical version of the algorithm.|Javier H. López,Laura Lanzarini,Armando De Giusti","58792|GECCO|2009|Orthogonal learning particle swarm optimization|This paper proposes an orthogonal learning particle swarm optimization (OLPSO) by designing an orthogonal learning (OL) strategy through the orthogonal experimental design (OED) method. The OL strategy takes the dimensions of the problem as the orthogonal experimental factors. The levels of each dimension (factor) are the two choices of the personal best position and the neighborhood's best position. By orthogonally combining the two learning exemplars, the useful information can be discovered, preserved and utilized to construct an efficient exemplar to guide the particle to fly in a more promising direction towards the global optimum. The effectiveness and efficiency of the OL strategy is demonstrated on a set of benchmark functions by comparing the PSOs with and without OL strategy. The OL strategy improves the PSO algorithm in terms of higher quality solution and faster convergence speed.|Zhi-hui Zhan,Jun Zhang,Ou Liu","57813|GECCO|2006|Dynamic fitness inheritance proportion for multi-objective particle swarm optimization|In this paper, we propose a dynamic mechanism to vary the probability by which fitness inheritance is applied throughout the run of a multi-objective particle swarm optimizer, in order to obtain a greater reduction in computational cost (than the obtained with a fixed probability), without dramatically affecting the quality of the results. The results obtained show that it is possible to reduce the computational cost by % without affecting the quality of the obtained Pareto front.|Margarita Reyes Sierra,Carlos A. Coello Coello","58315|GECCO|2008|Magnifier particle swarm optimization for numerical optimization|A novel particle swarm optimization algorithm based on magnification transformation, called magnifier particle swarm optimization (MPSO), is proposed for the first time in this paper. In the MPSO, we enlarge the range around the best individual of each generation like using a magnifier, while the velocity of particles unchanged. In such a way, MPSO achieves much faster convergence performance and better optimization solving capability than the conventional standard particle swarm optimization and latest clonal PSO by a number of simulations. A detailed description and explanation of the MPSO algorithm are given in the paper. Experiments on fourteen benchmark test functions are conducted and shows the inspiring success that the proposed MPSO speeds up the convergence tremendously, while keeping a good search capability of global solution with much more accuracy. Experiments on fourteen benchmark test functions are conducted to demonstrate that the proposed MPSO algorithm is able to speedup the evolution process distinctly and improve the performance of global optimizer greatly.|Junqi Zhang,Kun Liu,Ying Tan,Xingui He"],["80675|VLDB|2006|Efficient XSLT Processing in Relational Database System|Efficient processing of XQuery, XPath and SQLXML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQueryXPath processing in database to achieve combined optimisations of XSLT with XQueryXPath and SQLXML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.|Zhen Hua Liu,Anguel Novoselsky","80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","79990|VLDB|1980|Distributed Concurrency Control in Database Systems|A synchronization method for distributed DBS is presented which guarantees synchronization of transactions according to the principle of serializability. No exclusive locks are used. Read requests can always be granted without being delayed or blocked by updates of other transactions delays at most arise from communication between different nodes of the distributed system. Read only transactions are never backed up. The described method is applicable to distributed DBS without copies as well as to DBS with any number of copies of arbitrary objects. The necessary modifications for dealing with the problem of copies turn out to be very simple.|Rudolf Bayer,Klaus Elhardt,Hans Heller,Angelika Reiser","80012|VLDB|1980|Some Analytic Tools for the Design of Relational Database Systems|Making use of arguments from information theory it is shown that a boolean function can represent multivalued dependencies. A method is described by which a hypergraph can be constructed to represent dependencies in a relation. A new normal form called generalized Boyce-Codd normal form is defined. An explicit formula is derived for representing dependencies that would remain in a projection of a relation. A definition of join is given which makes the derivation of many theoretical results easy. Another definition given is that of information in a relation. The information gets conserved whenever lossless decompositions are involved. It is shown that the use of null elements is important in handling data.|K. K. Nambiar","80358|VLDB|2004|Multi-objective Query Processing for Database Systems|Query processing in database systems has developed beyond mere exact matching of attribute values. Scoring database objects and retrieving only the top k matches or Pareto-optimal result sets (skyline queries) are already common for a variety of applications. Specialized algorithms using either paradigm can avoid nave linear database scans and thus improve scalability. However, these paradigms are only two extreme cases of exploring viable compromises for each user's objectives. To find the correct result set for arbitrary cases of multi-objective query processing in databases we will present a novel algorithm for computing sets of objects that are nondominated with respect to a set of monotonic objective functions. Naturally containing top k and skyline retrieval paradigms as special cases, this algorithm maintains scalability also for all cases in between. Moreover, we will show the algorithm's correctness and instance-optimality in terms of necessary object accesses and how the response behavior can be improved by progressively producing result objects as quickly as possible, while the algorithm is still running.|Wolf-Tilo Balke,Ulrich Güntzer","80421|VLDB|2004|Query Languages and Data Models for Database Sequences and Data Streams|We study the fundamental limitations of relational algebra (RA) and SQL in supporting sequence and stream queries, and present effective query language and data model enrichments to deal with them. We begin by observing the well-known limitations of SQL in application domains which are important for data streams, such as sequence queries and data mining. Then we present a formal proof that, for continuous queries on data streams, SQL suffers from additional expressive power problems. We begin by focusing on the notion of nonblocking (NB) queries that are the only continuous queries that can be supported on data streams. We characterize the notion of nonblocking queries by showing that they are equivalent to monotonic queries. Therefore the notion of NB-completeness for RA can be formalized as its ability to express all monotonic queries expressible in RA using only the monotonic operators of RA. We show that RA is not NB-complete, and SQL is not more powerful than RA for monotonic queries. To solve these problems, we propose extensions that allow SQL to support all the monotonic queries expressible by a Turing machine using only monotonic operators. We show that these extensions are (i) user-defined aggregates (UDAs) natively coded in SQL (rather than in an external language), and (ii) a generalization of the union operator to support the merging of multiple streams according to their timestamps. These query language extensions require matching extensions to basic relational data model to support sequences explicitly ordered by times-tamps. Along with the formulation of very powerful queries, the proposed extensions entail more efficient expressions for many simple queries. In particular, we show that nonblocking queries are simple to characterize according to their syntactic structure.|Yan-Nei Law,Haixun Wang,Carlo Zaniolo","79875|VLDB|1977|Design Criteria for Distributed Database Systems|This paper clarifies design criteria for adaptive heterogeneous distributed database systems. Compositional and decompositional design approaches, database sharing models, and an abstract design approach are discussed.|Tosiyasu L. Kunii,Hideko S. Kunii","79986|VLDB|1979|Query Processing in a Relational Database Management System|In this paper the various tactics for query processing in INGRESS are empirically evaluated on a test bed of sample queries.|Karel Youssefi,Eugene Wong","80002|VLDB|1980|Distributed Database Systems|The purpose of this panel is to present an overview of past achievements and new directions of research in distributed database systems. Five speakers summarize their own experiences and focus on new developments.|Georges Gardarin,Nathan Goodman,Bruce G. Lindsay,Rudolf Munz,James B. Rothnie Jr.","80439|VLDB|2004|A Multi-Purpose Implementation of Mandatory Access Control in Relational Database Management Systems|Mandatory Access Control (MAC) implementations in Relational Database Management Systems (RDBMS) have focused solely on Multilevel Security (MLS). MLS has posed a number of challenging problems to the database research community, and there has been an abundance of research work to address those problems. Unfortunately, the use of MLS RDBMS has been restricted to a few government organizations where MLS is of paramount importance such as the intelligence community and the Department of Defense. The implication of this is that the investment of building an MLS RDBMS cannot be leveraged to serve the needs of application domains where there is a desire to control access to objects based on the label associated with that object and the label associated with the subject accessing that object, but where the label access rules and the label structure do not necessarily match the MLS two security rules and the MLS label structure. This paper introduces a flexible and generic implementation of MAC in RDBMS that can be used to address the requirements from a variety of application domains, as well as to allow an RDBMS to efficiently take part in an end-to-end MAC enterprise solution. The paper also discusses the extensions made to the SQL compiler component of an RDBMS to incorporate the label access rules in the access plan it generates for an SQL query, and to prevent unauthorized leakage of data that could occur as a result of traditional optimization techniques performed by SQL compilers.|Walid Rjaibi,Paul Bird"],["66910|AAAI|2010|A General Framework for Representing and Reasoning with Annotated Semantic Web Data|Analyzed the energy consumption disciplinarian of the nodes in WSN, the node's energy attenuation forecast model (EAFM) can be established. A difference-threshold reporting mechanism (DTRM) is used to report the residual energy of nodes. The energy collection mechanism based on EAFM and DTRM can reduce energy data reporting times significantly, improve the efficient of energy data collection, save the node's energy at the same time. The experiments in the platform of telosb nodes show that the predicable rate is between % and %, and this method can extend the node's life by %  .%.|Umberto Straccia,Nuno Lopes 0002,Gergely Lukacsy,Axel Polleres","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","80861|VLDB|2007|Scalable Semantic Web Data Management Using Vertical Partitioning|Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, \"property tables.\" We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than  million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.|Daniel J. Abadi,Adam Marcus 0002,Samuel Madden,Katherine J. Hollenbach","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65290|AAAI|2004|SEM-Ether Semantic Web Based Pervasive Computing Framework - Integrating Web Devices and People|Pervasive computing aims to build an aggregated environment around a user by knitting diverse computing and communicating devices and software services into a single homogeneous unit. Our work is to develop a Pervasive computing framework which harnesses the power of Semantic Web and Web Services, facilitating the development of effective and intelligent Pervasive environments. This paper presents a high level view of the framework and how different Pervasive services can be built on this framework.|Sushil Puradkar,Sachin Singh,Chintan Patel,Kartik Vishwanath,Rahul Gupta,Yugyung Lee","80268|VLDB|2003|The Semantic Web Semantics for Data on the Web|In our tutorial on Semantic Web (SW) technology, we explain the why, the various technology thrusts and the relationship to database technology. The motivation behind presenting this tutorial is discussed and the framework of the tutorial along with the various component technologies and research areas related to the Semantic Web is presented.|Stefan Decker,Vipul Kashyap","80848|VLDB|2007|Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic|The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.|Dumitru Roman,Michael Kifer","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","66162|AAAI|2007|Reasoning about Attribute Authenticity in a Web Environment|The reliable authentication of user attributes is an important prerequisite for the security of web based applications. Digital certificates are widely used for that purpose. However, practical certification scenarios can be very complex. Each certiticate carries a validity period and can be revoked during this period. Furthermore, the verifying user has to trust the issuers of certificates and revocations. This work presents a formal model which covers these aspects and provides a theoretical foundation for the decision about attribute authenticity even in complex scenarios. The model is based on the event calculus, an AI technique from the field of temporal reasoning. It uses Clark's completion to address the frame problem. An example illustrates the application of the model.|Thomas Wölfl"],["65814|AAAI|2006|Learning Representation and Control in Continuous Markov Decision Processes|This paper presents a novel framework for simultaneously learning representation and control in continuous Markov decision processes. Our approach builds on the framework of proto-value functions, in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold. The proto-value functions correspond to the eigenfunctions of the graph Laplacian. We describe an approach to extend the eigenfunctions to novel states using the Nystrm extension. A least-squares policy iteration method is used to learn the control policy, where the underlying subspace for approximating the value function is spanned by the learned proto-value functions. A detailed set of experiments is presented using classic benchmark tasks, including the inverted pendulum and the mountain car, showing the sensitivity in performance to various parameters, and including comparisons with a parametric radial basis function method.|Sridhar Mahadevan,Mauro Maggioni,Kimberly Ferguson,Sarah Osentoski","65503|AAAI|2005|Augmenting Disjunctive Temporal Problems with Finite-Domain Constraints|We present a general framework for augmenting instances of the Disjunctive Temporal Problem (DTP) with finite-domain constraints. In this new formalism, the bounds of the temporal constraints become conditional on the finite-domain assignment. This hybridization makes it possible to reason simultaneously about temporal relationships between events as well as their nontemporal properties. We provide a special case of this hybridization that allows reasoning about a limited form of spatial constraints namely, the travel time induced by the locations of a set of activities. We develop a least-commitment algorithm for efficiently finding solutions to this combined constraint system and provide empirical results demonstrating the effectiveness of our approach.|Michael D. Moffitt,Bart Peintner,Martha E. Pollack","65186|AAAI|2004|Generating Safe Assumption-Based Plans for Partially Observable Nondeterministic Domains|Reactive planning using assumptions is a well-known approach to tackle complex planning problems for nondeterministic, partially observable domains. However, assumptions may be wrong this may cause an assumption-based plan to fail. In general, it is not possible to decide at runtime whether an assumption has failed and is putting at danger the success of the plan thus, plan execution has to be controlled taking into account every possible success-endangering assumption failure. The possibility of tracing such failures strongly depends on the actions performed by the plan. In this paper, focusing on a simple assumption language, we provide two main contributions. First, we formally characterize safe assumption-based plans, i.e. plans that not only succeed whenever the assumption holds, but also guarantee that any success-endangering assumption failure is traced by a suitable monitor. In this way, replanning may be triggered only when actually needed. Second, we extend the planner in a reactive platform in order to produce safe assumption-based plans. We experimentally show that safe assumption-based (re)planning is a good alternative to its unsafe counterpart, minimizing the need for replanning while retaining the efficiency in plan generation.|Alexandre Albore,Piergiorgio Bertoli","65467|AAAI|2005|Heterogeneous Multirobot Coordination with Spatial and Temporal Constraints|Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.|Mary Koes,Illah R. Nourbakhsh,Katia P. Sycara","65578|AAAI|2005|Conformant Planning for Domains with Constraints-A New Approach|The paper presents a pair of new conformant planners, CPApc and CPAph, based on recent developments in theory of action and change. As an input the planners take a domain description D in action language AL which allows state constraints (non-stratified axioms), together with a set of CNF formulae describing the initial state, and a set of literals representing the goal. We propose two approximations of the transition diagram T defined by D. Both approximations are deterministic transition functions and can be computed efficiently. Moreover they are sound (and sometimes complete) with respect to T. In its search for a plan, an approximation based planner analyses paths of an approximation instead of that of T. CPApc and CPAph are forward, best first search planners based on this idea. We compare them with two state-of-the-art conformant planners, KACMBP and Conformant-FF (CFF), over benchmarks in the literature, and over two new domains. One has large number of state constraints and another has a high degree of incompleteness. Our planners perform reasonably well in benchmark domains and outperform KACMBP and CFF in the first domain while still working well with the second one. Our experimental result shows that having an integral part of a conformant planner to deal with state constraints directly can significantly improve its performance extending a similar claim for classical planners in (Thiebaux. Hoffmann, & Nebel ).|Tran Cao Son,Phan Huy Tu,Michael Gelfond,A. Ricardo Morales","65312|AAAI|2004|An Effective Algorithm for Project Scheduling with Arbitrary Temporal Constraints|The resource-constrained project scheduling problem with time windows (RCPSPmax) is an important generalization of a number of well studied scheduling problems. In this paper, we present a new heuristic algorithm that combines the benefits of squeaky wheel optimization with an effective conflict resolution mechanism, called bulldozing, to address RCPSPmax problems. On a range of benchmark problems, the algorithm is competitive with state-of-the-art systematic and non-systematic methods and scales well.|Tristan B. Smith,John M. Pyle","66516|AAAI|2008|Towards Faster Planning with Continuous Resources in Stochastic Domains|Agents often have to construct plans that obey resource limits for continuous resources whose consumption can only be characterized by probability distributions. While Markov Decision Processes (MDPs) with a state space of continuous and discrete variables are popular for modeling these domains, current algorithms for such MDPs can exhibit poor performance with a scale-up in their state space. To remedy that we propose an algorithm called DPFP. DPFP's key contribution is its exploitation of the dual space cumulative distribution functions. This dual formulation is key to DPFP's novel combination of three features. First, it enables DPFP's membership in a class of algorithms that perform forward search in a large (possibly infinite) policy space. Second, it provides a new and efficient approach for varying the policy generation effort based on the likelihood of reaching different regions of the MDP state space. Third, it yields a bound on the error produced by such approximations. These three features conspire to allow DPFP's superior performance and systematic trade-off of optimality for speed. Our experimental evaluation shows that, when run stand-alone, DPFP outperforms other algorithms in terms of its any-time performance, whereas when run as a hybrid, it allows for a significant speedup of a leading continuous resource MDP solver.|Janusz Marecki,Milind Tambe","65306|AAAI|2004|Evaluating Consistency Algorithms for Temporal Metric Constraints|We study the performance of some known algorithms for solving the Simple Temporal Problem (STP) and the Temporal Constraint Satisfaction Problem (TCSP). In particular, we empirically compare the Bellman-Ford (BF) algorithm and its incremental version (incBF) by (Cesta & Oddi ) to the STP of (Xu & Choueiry a). Among the tested algorithms, we show that STP is the most efficient for determining the consistency of an STP, and that incBF combined with the heuristics of (Xu & Choueiry b) is the most efficient for solving the TCSP. We plan to improve STP by exploiting incrementality as in incBF and other new incremental algorithms.|Yang Shi,Anagh Lal,Berthe Y. Choueiry","65092|AAAI|1987|Synthesizing Algorithms with Performance Constraints|This paper describes MEDUSA, an experimental algorithm synthesizer. MEDUSA is characterized by its top-down approach, its use of cost-constraints, and its restricted number of synthesis methods. Given this model, we discuss heuristics used to keep this process from being unbounded search through the solution space. The results indicate that the performance criteria can be used effectively to help avoid combinatorial explosion. The system has synthesized a number of algorithms in its test domain (geometric intersection problems) without operator intervention.|Robert McCartney","65687|AAAI|2006|Identifying and Generating Easy Sets of Constraints for Clustering|Clustering under constraints is a recent innovation in the artificial intelligence community that has yielded significant practical benefit. However, recent work has shown that for some negative forms of constraints the associated subproblem of just finding a feasible clustering is NP-complete. These worst case results for the entire problem class say nothing of where and how prevalent easy problem instances are. In this work, we show that there are large pockets within these problem classes where clustering under constraints is easy and that using easy sets of constraints yields better empirical results. We then illustrate several sufficient conditions from graph theory to identify a priori where these easy problem instances are and present algorithms to create large and easy to satisfy constraint sets.|Ian Davidson,S. S. Ravi"],["65218|AAAI|2004|Complete Local Search for Propositional Satisfiability|Algorithms based on following local gradient information are surprisingly effective for certain classes of constraint satisfaction problems. Unfortunately, previous local search algorithms are notoriously incomplete They are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability. We present an algorithmic framework for complete local search and discuss in detail an instantiation for the propositional satisfiability problem (SAT). The fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minima. Although the algorithm has worst-case exponential space complexity, we present empirical resulls on challenging SAT competition benchmarks that suggest that our implementation can perform as well as state-of-the-art solvers based on more mature techniques. Our framework suggests a range of possible algorithms lying between tree-based search and local search.|Hai Fang,Wheeler Ruml","58415|GECCO|2008|Precision local search and unimodal functions|We investigate the effects of precision on the efficiency of various local search algorithms on -D unimodal functions. We present a (+)-EA with adaptive step size which finds the optimum in O(log n) steps, where n is the number of points used. We then consider binary and Gray representations with single bit mutations. The standard binary method does not guarantee locating the optimum, whereas using Gray code does so in O((log n)) steps. A (+)-EA with a fixed mutation probability distribution is then presented which also runs in O((log n)). Moreover, a recent result shows that this is optimal (up to some constant scaling factor), in that there exist unimodal functions for which a lower bound of ((log n)) holds regardless of the choice of mutation distribution. Finally, we show that it is not possible for a black box algorithms to efficiently optimise unimodal functions for two or more dimensions (in terms of the precision used).|Martin Dietzfelbinger,Jonathan E. Rowe,Ingo Wegener,Philipp Woelfel","66666|AAAI|2010|Local Search in Histogram Construction|In this paper, we propose an effective error concealment framework for H. decoder based on the scene change detection. The proposed framework quickly and accurately detects whether scene change occurs in the decoding frame, based on the detection result, both corrupted intra frames and damaged inter frames can be reconstructed by spatial or improved temporal EC (Error Concealment) algorithm. The experiment shows that, compared with the traditional error concealment method in the H.A VC non- normative decoder, the proposed framework has better robustness and can efficiently improve the visual quality and PSNR of the decoded video.|Felix Halim,Panagiotis Karras,Roland H. C. Yap","58488|GECCO|2008|Genetic local search for rule learning|The performance of Evolutionary Algorithms for combinatorial problems can be significantly improved by adding Local Search, thus obtaining a Genetic Local Search (GLS) also called Memetic Algorithm. In this work, we adapt a previous Stochastic Local Search (SLS) algorithm and embed it into a GBML system. The adapted SLS algorithm works as a module of the system that tries to improve a random individual in the population. We perform experiments to evaluate this adapted SLS procedure and results show that this new GLS system is very effective, not losing in any of the  UCI datasets tested when compared to the system without the SLS procedure. The system either obtained significantly more accurate concepts using lower number of rules and features or it achieved the same accuracy as the system without the SLS procedure, but reduced the number of rules and features, and also the time taken to develop the solution.|Cristiano Grijó Pitangui,Gerson Zaverucha","65199|AAAI|2004|Stochastic Local Search for POMDP Controllers|The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.|Darius Braziunas,Craig Boutilier","57869|GECCO|2006|Genetic local search for multicast routing|We describe a population-based search algorithm for cost minimization of multicast routing. The algorithm utilizes the partially mixed crossover operation (PMX) and a landscape analysis in a pre-processing step. The aim of the landscape analysis is to estimate the depth  of the deepest local minima in the landscape generated by the routing tasks and the objective function. The analysis employs simulated annealing with a logarithmic cooling schedule (LSA). The local search performs alternating sequences of descending and ascending steps for each individual of the population, where the length of a sequence with uniform direction is controlled by the estimated value of . We present results from computational experiments on a synthetic routing tasks, and we provide experimental evidence that our genetic local search procedure, that combines LSA and PMX, performs better than algorithms using either LSA or PMX only.|Mohammed S. Zahrani,Martin J. Loomes,James A. Malcolm,Andreas Alexander Albrecht","65881|AAAI|2006|Disco - Novo - GoGo Integrating Local Search and Complete Search with Restarts|A hybrid algorithm is devised to boost the performance of complete search on under-constrained problems. We suggest to use random variable selection in combination with restarts, augmented by a coarse-grained local search algorithm that learns favorable value heuristics over the course of several restarts. Numerical results show that this method can speed-up complete search by orders of magnitude.|Meinolf Sellmann,Carlos Ansótegui","65849|AAAI|2006|Overconfidence or Paranoia Search in Imperfect-Information Games|We derive a recursive formula for expected utility values in imperfect- information game trees, and an imperfect-information game tree search algorithm based on it. The formula and algorithm are general enough to incorporate a wide variety of opponent models. We analyze two opponent models. The \"paranoid\" model is an information-set analog of the minimax rule used in perfect-information games. The \"overconfident\" model assumes the opponent moves randomly. Our experimental tests in the game of kriegspiel chess (an imperfect-information variant of chess) produced surprising results () against each other, and against one of the kriegspiel algorithms presented at IJCAI-, the overconfident model usually outperformed the paranoid model () the performance of both models depended greatly on how well the model corresponded to the opponent's behavior. These results suggest that the usual assumption of perfect-information game tree search--that the opponent will choose the best possible move--isn't as useful in imperfect-information games.|Austin Parker,Dana S. Nau,V. S. Subrahmanian","66352|AAAI|2008|Local Search for Optimal Global Map Generation Using Mid-Decadal Landsat Images|NASA and the US Geological Survey (USGS) are generating image maps of the entire Earth using Landsat  Thematic Mapper (TM) and Landsat  (L) Enhanced Thematic Mapper Plus (ETM+) sensor data from the period of  through . The map is comprised of thousands of scene locations and, for each location, there are tens of different images of varying quality to chose from. Constraints and preferences on map quality make it desirable to develop an automated solution to the map generation problem. This paper formulates a Global Map Generator problem as a Constraint Optimization Problem (GMG-COP) and describes an approach to solving It usmg local search. The paper also describes the integration of a GMG solver into a user interface for visualizing and comparing solutions.|Robert A. Morris,John Gasch,Lina Khatib,Steven Covington","59066|GECCO|2010|Incremental evolution of local search heuristics|In evolutionary computation, incremental evolution refers to the process of employing an evolutionary environment that becomes increasingly complex over time. We present an implementation of this approach to develop randomised local search heuristics for constraint satisfaction problems, combining research on incremental evolution with local search heuristics evolution. A population of local search heuristics is evolved using a genetic programming framework on a simple problem for a short period and is then allowed to evolve on a more complex problem. Experiments compare the performance of this population with that of a randomly initialised population evolving directly on the more complex problem. The results obtained show that incremental evolution can represent a significant improvement in terms of optimisation speed, solution quality and solution structure.|Dara Curran,Eugene C. Freuder,Thomas Jansen"],["58970|GECCO|2010|The estimation of hlderian regularity using genetic programming|This paper presents a Genetic Programming (GP) approach to synthesize estimators for the pointwise Hlder exponent in D signals. It is known that irregularities and singularities are the most salient and informative parts of a signal. Hence, explicitly measuring these variations can be important in various domains of signal processing. The pointwise Hlder exponent provides a characterization of these types of features. However, current methods for estimation cannot be considered to be optimal in any sense. Therefore, the goal of this work is to automatically synthesize operators that provide an estimation for the Hlderian regularity in a D signal. This goal is posed as an optimization problem in which we attempt to minimize the error between a prescribed regularity and the estimated regularity given by an image operator. The search for optimal estimators is then carried out using a GP algorithm. Experiments confirm that the GP-operators produce a good estimation of the Hlder exponent in images of multifractional Brownian motions. In fact, the evolved estimators significantly outperform a traditional method by as much as one order of magnitude. These results provide further empirical evidence that GP can solve difficult problems of applied mathematics.|Leonardo Trujillo,Pierrick Legrand,Jacques Lévy Véhel","57020|GECCO|2002|Hyperspectral Image Analysis Using Genetic Programming|Genetic programming is used to evolve mineral identification functions for hyperspectral images. The input image set comprises  images from different wavelengths ranging from nm (visible blue) to nm (invisible shortwave in the infrared), taken over Cuprite, Nevada, with the AVIRIS hyperspectral sensor. A composite mineral image indicating the overall reflectance percentage of three minerals (alunite, kaolnite, buddingtonite) is used as a reference or ''solution'' image. The training set is manually selected from this composite image, and results are cross-validated with the remaining image data not used for training. The task of the GP system is to evolve mineral identifiers, where each identifier is trained to identify one of the three mineral specimens. A number of different GP experiments were undertaken, which parameterized features such as thresholded mineral reflectance intensity and target GP language. The results are promising, especially for minerals with higher reflectance thresholds, which indicate more intense concentrations.|Brian J. Ross,Anthony G. Gualtieri,Frank Fueten,Paul Budkewitsch","57703|GECCO|2006|On evolving buffer overflow attacks using genetic programming|In this work, we employed genetic programming to evolve a \"white hat\" attacker that is to say, we evolve variants of an attack with the objective of providing better detectors. Assuming a generic buffer overflow exploit, we evolve variants of the generic attack, with the objective of evading detection by signature-based methods. To do so, we pay particular attention to the formulation of an appropriate fitness function and partnering instruction set. Moreover, by making use of the intron behavior inherent in the genetic programming paradigm, we are able to explicitly obfuscate the true intent of the code. All the resulting attacks defeat the widely used 'Snort' Intrusion Detection System.|Hilmi Günes Kayacik,Malcolm I. Heywood,A. Nur Zincir-Heywood","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57053|GECCO|2003|Data Classification Using Genetic Parallel Programming|A novel Linear Genetic Programming (LGP) paradigm called Genetic Parallel Programming (GPP) has been proposed to evolve parallel programs based on a Multi-ALU Processor. It is found that GPP can evolve parallel programs for Data Classification problems. In this paper, five binary-class UCI Machine Learning Repository databases are used to test the effectiveness of the proposed GPP-classifier. The main advantages of employing GPP for data classification are ) speeding up evolutionary process by parallel hardware fitness evaluation and ) discovering parallel algorithms automatically. Experimental results show that the GPP-classifier evolves simple classification programs with good generalization performance. The accuracies of these evolved classifiers are comparable to other existing classification algorithms.|Sin Man Cheang,Kin-Hong Lee,Kwong-Sak Leung","58830|GECCO|2009|Evolution development and learning using self-modifying cartesian genetic programming|Self-Modifying Cartesian Genetic Programming (SMCGP) is a form of genetic programming that integrates developmental (self-modifying) features as a genotype-phenotype mapping. This paper asks Is it possible to evolve a learning algorithm using SMCGP|Simon Harding,Julian Francis Miller,Wolfgang Banzhaf","58433|GECCO|2008|Combining cartesian genetic programming with an estimation of distribution algorithm|This paper describes initial testing of a novel idea to combine a CGP with an EDA. In recent work a new improved crossover technique was successfully applied to a CGP. To implement the new method meant changing the traditional CGP representation. The new representation developed in that work lends itself very nicely to some probability distribution being implemented. The work in this paper has investigated this idea of incoporating estimated probability distributions into the new CGP method with crossover.|Janet Clegg","58332|GECCO|2008|Fault tolerant control using Cartesian genetic programming|The paper focuses on the evolution of algorithms for control of a machine in the presence of sensor faults, using Cartesian Genetic Programming. The key challenges in creating training sets and a fitness function that encourage a general solution are discussed. The evolved algorithms are analysed and discussed. It was found that highly novel, mathematically elegant and hitherto unknown solutions were found.|Yoshikazu Hirayama,Tim Clarke,Julian Francis Miller","57990|GECCO|2007|Solving real-valued optimisation problems using cartesian genetic programming|Classical Evolutionary Programming (CEP) and Fast Evolutionary Programming (FEP) have been applied to real-valued function optimisation. Both of these techniques directly evolve the real-values that are the arguments of the real-valued function. In this paper we have applied a form of genetic programming called Cartesian Genetic Programming (CGP) to a number of real-valued optimisation benchmark problems. The approach we have taken is to evolve a computer program that controls a writing-head, which moves along and interacts with a finite set of symbols that are interpreted as real numbers, instead of manipulating the real numbers directly. In other studies, CGP has already been shown to benefit from a high degree of neutrality. We hope to exploit this for real-valued function optimisation problems to avoid being trapped on local optima. We have also used an extended form of CGP called Embedded CGP (ECGP) which allows the acquisition, evolution and re-use of modules. The effectiveness of CGP and ECGP are compared and contrasted with CEP and FEP on the benchmark problems. Results show that the new techniques are very effective.|James Alfred Walker,Julian Francis Miller"],["57706|GECCO|2006|On the local performance of simulated annealing and the  evolutionary algorithm|Simulated annealing and the (+) EA, a simple evolutionary algorithm, are both general randomized search heuristics that optimize any objective function with probability converging to . But they use very different techniques to achieve this global convergence. The (+) EA applies global mutations than can reach any point in the search space in one step together with an elitist selection mechanism. Simulated annealing restricts its search to a neighborhood but employs a randomized selection scheme where the probability for accepting a move to a new point in the search space depends on the difference in function values as well as on the current time step. Otherwise, the two algorithms are equal. It is known that the different philosophies of search implemented in the two heuristics can lead to exponential performance gaps between the two algorithms with respect to the expected optimization time. Even for very restricted classes of objective functions where the differences in function values between neighboring points are strictly limited the performance differences can be huge. Here, a more local point of view is taken. Considering obstacles in the fitness landscapes it is proven that the local performance of the two algorithms is remarkably similar in spite of their different search behaviors.|Thomas Jansen,Ingo Wegener","57508|GECCO|2005|Predicting population dynamics and evolutionary trajectories based on performance evaluations in alife simulations|Evolutionary investigations are often very expensive in terms of the required computational resources and many general questions regarding the utility of a feature F of an agent (e.g., in competitive environments) or the likelihood of F evolving (or not evolving) are therefore typically difficult, if not practically impossible to answer. We propose and demonstrate in extensive simulations a methodology that allows us to answer such questions in setups where good predictors of performance in a task T are available. These predictors evaluate the performance of an agent kind A in a task T*, which can then transformed by including costs and additional factors to make predictions about the performance of A in T.|Matthias Scheutz,Paul W. Schermerhorn","57636|GECCO|2006|A specification-based fitness function for evolutionary testing of object-oriented programs|Encapsulation of states in object-oriented programs hinders the search for test data using evolutionary testing. As client code is oblivious to the internal state of a server object, no guidance is available to test the client code using evolutionary testing i.e., it is difficult to determine the fitness or goodness of test data, as it may depend on the hidden internal state. Nevertheless, evolutionary testing is a promising new approach of which effectiveness has been shown by several researchers. We propose a specification-based fitness function for evolutionary testing of object-oriented programs. Our approach is modular in that fitness value calculation doesn't depend on source code of server classes, thus it works even if the server implementation is changed or no code is available----which is frequently the case for reusable object-oriented class libraries and frameworks.|Yoonsik Cheon,Myoung Kim","58646|GECCO|2009|An adaptive strategy for improving the performance of genetic programming-based approaches to evolutionary testing|This paper proposes an adaptive strategy for enhancing Genetic Programming-based approaches to automatic test case generation. The main contribution of this study is that of proposing an adaptive Evolutionary Testing methodology for promoting the introduction of relevant instructions into the generated test cases by means of mutation the instructions from which the algorithm can choose are ranked, with their rankings being updated every generation in accordance to the feedback obtained from the individuals evaluated in the preceding generation. The experimental studies developed show that the adaptive strategy proposed improves the algorithm's efficiency considerably, while introducing a negligible computational overhead.|José Carlos Bregieiro Ribeiro,Mário Zenha Rela,Francisco Fernández de Vega","57842|GECCO|2006|Pairwise sequence comparison for fitness evaluation in evolutionary structural software testing|Evolutionary algorithms are among the metaheuristic search methods that have been applied to the structural test data generation problem. Fitness evaluation methods play an important role in the performance of evolutionary algorithms and various methods have been devised for this problem. In this paper, we propose a new fitness evaluation method based on pairwise sequence comparison also used in bioinformatics. Our preliminary study shows that this method is easy to implement and produces promising results.|H. Turgut Uyar,A. Sima Etaner-Uyar,A. Emre Harmanci","58160|GECCO|2007|Fitness calculation approach for nested if-else construct in evolutionary testing|This poster paper addresses the fitness calculation problem fornested if-else constructs. A new term \"optimism level\" is incorporated into the fitness function to assess the branch distance of nested branches for a given test data.|Xiyang Liu,Lei Wang,Xiubin Zhu,Zhiwen Bai,Miao Zhang,Hehui Liu","57030|GECCO|2003|Evolutionary Testing of Flag Conditions|Evolutionary Testing (ET) has been shown to be very successful in testing real world applications . However, it has been pointed out , that further research is necessary if flag variables appear in program expressions. The problems increase when ET is used to test state-based applications where the encoding of states hinders successful evolutionary tests. This is because the ET performance is reduced to a random test in case of the use of flag variables or variables that encode an enumeration type. The authors have developed an ET System to provide easy access to automatic testing. An extensive set of programs has been tested using this system , . This system is extended for new areas of software testing and research has been carried out to improve its performance. This paper introduces a new approach for solving ET problems with flag conditions. The problematic constructs are explained with the help of code examples originally found in large real world applications.|André Baresel,Harmen Sthamer","57736|GECCO|2006|Revisiting evolutionary algorithms with on-the-fly population size adjustment|In an evolutionary algorithm, the population has a very important role as its size has direct implications regarding solution quality, speed, and reliability. Theoretical studies have been done in the past to investigate the role of population sizing in evolutionary algorithms. In addition to those studies, several self-adjusting population sizing mechanisms have been proposed in the literature. This paper revisits the latter topic and pays special attention to the genetic algorithm with adaptive population size (APGA), for which several researchers have claimed to be very effective at autonomously (re)sizing the population.As opposed to those previous claims, this paper suggests a complete opposite view. Specifically, it shows that APGA is not capable of adapting the population size at all. This claim is supported on theoretical grounds and confirmed by computer simulations.|Fernando G. Lobo,Cláudio F. Lima","65859|AAAI|2006|A Sequential Covering Evolutionary Algorithm for Expressive Music Performance|In this paper, we describe an evolutionary approach to one of the most challenging problems in computer music modeling the knowledge applied by a musician when performing a score of a piece in order to produce an expressive performance of the piece. We extract a set of acoustic features from Jazz recordings thereby providing a symbolic representation of the musician's expressive performance. By applying a sequential covering evolutionary algorithm to the symbolic representation, we obtain an expressive performance computational model capable of endowing a computer generated music performance with the timing and energy expressiveness that characterizes human generated music.|Rafael Ramirez,Amaury Hazan,Jordi Marine,Esteban Maestre","57164|GECCO|2003|The State Problem for Evolutionary Testing|This paper shows how the presence of states in test objects can hinder or render impossible the search for test data using evolutionary testing. Additional guidance is required to find sequences of inputs that put the test object into some necessary state for certain test goals to become feasible. It is shown that data dependency analysis can be used to identify program statements responsible for state transitions, and then argued that an additional search is needed to find required transition sequences. In order to be able to deal with complex examples, the use of ant colony optimization is proposed. The results of a simple initial experiment are reported.|Phil McMinn,Mike Holcombe"],["57034|GECCO|2003|Optimal Elevator Group Control by Evolution Strategies|Efficient elevator group control is important for the operation of large buildings. Recent developments in this field include the use of fuzzy logic and neural networks. This paper summarizes the development of an evolution strategy (ES) that is capable of optimizing the neuro-controller of an elevator group controller. It extends the results that were based on a simplified elevator group controller simulator. A threshold selection technique is presented as a method to cope with noisy fitness function values during the optimization run. Experimental design techniques are used to analyze first experimental results.|Thomas Beielstein,Claus-Peter Ewald,Sandor Markon","57036|GECCO|2003|Theoretical Analysis of Simple Evolution Strategies in Quickly Changing Environments|Evolutionary algorithms applied to dynamic optimization problems has become a promising research area. So far, all papers in the area have assumed that the environment changes only between generations. In this paper, we take a first look at possibilities to handle a change during a generation. For that purpose, we derive an analytical model for a (, ) evolution strategy and show that sometimes it is better to ignore the environmental change until the end of the generation, than to evaluate each individual with the most up-to-date fitness function.|Jürgen Branke,Wei Wang","58770|GECCO|2009|Efficient natural evolution strategies|Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.|Yi Sun,Daan Wierstra,Tom Schaul,Jürgen Schmidhuber","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,Jörn Mehnen,Thomas Michelitsch","58277|GECCO|2008|Performance analysis of derandomized evolution strategies in quantum control experiments|Genetic Algorithms (GAs) are historically the most commonly used optimization method in Quantum Control (QC) experiments. We transfer specific Derandomized Evolution Strategies (DES) that have performed well on noise-free theoretical Quantum Control calculations, including the Covariance Matrix Adaptation (CMA-ES) algorithm, into the noisy environment of Quantum Control experiments. We study the performance of these DES variants in laboratory experiments, and reveal the underlying strategy dynamics of first- versus second-order landscape information. It is experimentally observed that global maxima of the given QC landscapes are located when only first-order information is used during the search. We report on the disruptive effects to which DES are exposed in these experiments, and study covariance matrix learning in noisy versus noise-free environments. Finally, we examine the characteristic behavior of the algorithms on the given landscapes, and draw some conclusions regarding the use of DES in QC laboratory experiments.|Ofer M. Shir,Jonathan Roslund,Thomas Bäck,Herschel Rabitz","57728|GECCO|2006|Mixed-integer optimization of coronary vessel image analysis using evolution strategies|In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.|Rui Li,Michael Emmerich,Jeroen Eggermont,Ernst G. P. Bovenkamp","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","57525|GECCO|2005|Niching in evolution strategies|EAs have the tendency to converge quickly into a single solution. Niching methods, the extension of EAs to address this issue, have been investigated up to date mainly within the field of Genetic Algorithms (GAs). In our study we investigate the basis for niching methods within Evolution Strategies (ES), and propose the first ES niching method. Results show that this method can reliably find and maintain multiple niches even for high-dimensional problems.|Ofer M. Shir,Thomas Bäck","59059|GECCO|2010|On the analysis of self-adaptive evolution strategies on elliptic model first results|In this paper, first results on the analysis of self-adaptive evolution strategies (ES) with intermediate multirecombination on the elliptic model are presented. Equations describing the ES behavior on the ellipsoid will be derived using a deterministic approach and experimentally verified. A relationship between newly obtained formulae for the elliptic model and previous theoretical results will be discussed.|Alexander Melkozerov,Hans-Georg Beyer"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","58865|GECCO|2009|On the evolution of neural networks for pairwise classification using gene expression programming|Neural networks are a common choice for solving classification problems, but require experimental adjustments of the topology, weights and thresholds to be effective. Success has been seen in the development of neural networks with evolutionary algorithms, making the extension of this work to classification problems a logical step. This paper presents the first known use of the Gene Expression Programming-based GEP-NN algorithm to design neural networks for classification purposes. The system uses pairwise decomposition to produce a series of binary classifiers for a given multi-class problem, with the results of the classifier set being combined by majority vote.|Stephen Johns,Marcus V. dos Santos","57536|GECCO|2005|On the contribution of gene libraries to artificial immune systems|Gene libraries have been added to Artificial Immune Systems in analogy to biological immune systems, but to date no careful study of their effect has been made. This work investigates the contribution of gene libraries to Artificial Immune Systems by reproducing and extending an earlier system that used gene libraries. Performance on a job-shop scheduling problem is evaluated empirically with and without gene libraries, and with many different library configurations. We propose that gene libraries encourage diversity in a population of solutions and that the number of components in the gene library parameterises this effect. The number of gene libraries used is found to affect solution fitness and indeed using larger numbers of libraries (and therefore libraries of smaller components) enables higher fitness to be attained. We conclude that gene libraries are likely to be of use in applications where there is a need to maintain the diversity of solutions.|Peter Spellward,Tim Kovacs","57462|GECCO|2005|Inference of gene regulatory networks using s-system and differential evolution|In this work we present an improved evolutionary method for inferring S-system model of genetic networks from the time series data of gene expression. We employed Differential Evolution (DE) for optimizing the network parameters to capture the dynamics in gene expression data. In a preliminary investigation we ascertain the suitability of DE for a multimodal and strongly non-linear problem like gene network estimation. An extension of the fitness function for attaining the sparse structure of biological networks has been proposed. For estimating the parameter values more accurately an enhancement of the optimization procedure has been also suggested. The effectiveness of the proposed method was justified performing experiments on a genetic network using different numbers of artificially created time series data.|Nasimul Noman,Hitoshi Iba","58916|GECCO|2010|Image compression of natural images using artificial gene regulatory networks|A novel approach to image compression using a gene regulatory network (GRN) based artificial developmental system (ADS) is introduced. The proposed algorithm exploits the fact that a series of complex organisms ( developmental states) can be represented via a GRN description and the indices of the developmental steps in which they occur. Organisms are interpreted as tiles of an image at each developmental step which results in the (re-)construction of an image during the developmental process. It is shown that GRNs are suitable for image compression and achieve higher compression rates than JPEG when optimised for a particular image. It is also shown that the same GRN has the potential to encode multiple images, each represented by a different series of numbers of developmental steps.|Martin Trefzer,Tüze Kuyucu,Julian F. Miller,Andy M. Tyrrell","58223|GECCO|2007|Modelling danger and anergy in artificial immune systems|Artificial Immune Systems are engineering systems which have been inspired from the functioning of the biological immune system. We present an immune system model which incorporates two biologically motivated mechanisms to protect against autoimmune reactions, or false positives. The first, anergy, has been subject to the intense focus of immunologists as a possible key to autoimmune disease. The second is danger theory, which has attracted much interest as a possible alternative to traditional self-nonself selection models.We adopt a published immunological model, validate and extend it. Using the same calculations and assumptions as the original model, we integrate danger theory into the software.Without anergy, both models - the original and the danger model - produce similar results. When anergy is added, both models' performance improves. However, there seems to be some synergy between the mechanisms anergy has a greater effect on the danger model than the original model. These findings should be of interest both to AIS practitioners and to the immunological community.|Steve Cayzer,Julie Sullivan","57032|GECCO|2003|Artificial Immune System for Classification of Gene Expression Data|DNA microarray experiments generate thousands of gene expression measurement simultaneously. Analyzing the difference of gene expression in cell and tissue samples is useful in diagnosis of disease. This paper presents an Artificial Immune System for classifying microarray-monitored data. The system evolutionarily selects important features and optimizes their weights to derive classification rules. This system was applied to two datasets of cancerous cells and tissues. The primary result found few classification rules which correctly classified all the test samples and gave some interesting implications for feature selection.|Shin Ando,Hitoshi Iba","66310|AAAI|2008|Using Knowledge Driven Matrix Factorization to Reconstruct Modular Gene Regulatory Network|Reconstructing gene networks from micro-array data can provide information on the mechanisms that govern cellular processes. Numerous studies have been devoted to addressing this problem. A popular method is to view the gene network as a Bayesian inference network, and to apply structure learning methods to determine the topology of the gene network. There are, however, several shortcomings with the Bayesian structure learning approach for reconstructing gene networks. They include high computational cost associated with analyzing a large number of genes and inefficiency in exploiting prior knowledge of co-regulation that could be derived from Gene Ontology (GO) information. In this paper, we present a knowledge driven matrix factorization (KMF) framework for reconstructing modular gene networks that addresses these shortcomings. In KMF, gene expression data is initially used to estimate the correlation matrix. The gene modules and the interactions among the modules are derived by factorizing the correlation matrix. The prior knowledge in GO is integrated into matrix factorization to help identify the gene modules. An alternating optimization algorithm is presented to efficiently find the solution. Experiments show that our algorithm performs significantly better in identifying gene modules than several state-of-the-art algorithms, and the interactions among the modules uncovered by our algorithm are proved to be biologically meaningful.|Yang Zhou,Zheng Li,Xuerui Yang,Linxia Zhang,Shireesh Srivastava,Rong Jin,Christina Chan","57804|GECCO|2006|A dynamic approach to artificial immune systems utilizing neural networks|The purpose of this work is to propose an immune-inspired setup to use a self-organizing map as a computational model for the interaction of antigens and antibodies. The proposed approach may be used as a part in other immune algorithms, or can possibly be used to detect anomalies in time series data.|Stefan Schadwinkel,Werner Dilger","58267|GECCO|2008|The gene regulatory network an application to optimal coverage in sensor networks|This paper proposes a new approach for biologically inspired computing on the basis of Gene Regulatory Networks. These networks are models of genes and dynamic interactions that take place between them. The differential equation representations of such networks resemble neural networks as well as idiotypic networks in immune system. Although several potential applications have been outlined, an example, the problem of placing sensors optimally in a distributed environment is considered in detail. A comparison with NSGA-II suggest that the new method is able to accomplish near-optimal coverage of sensors in a network.|Sanjoy Das,Praveen Koduru,Xinye Cai,Stephen Welch,Venkatesh Sarangan"]]}}