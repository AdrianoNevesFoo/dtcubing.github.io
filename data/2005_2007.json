{"abstract":{"entropy":7.003345372509793,"topics":["recent years, plays role, description logic, artificial intelligence, multi-agent systems, support vector, agents, real world, arc consistency, data management, recent research, sensor networks, autonomous agents, resource allocation, vector svm, important applications, petri nets, applications, artificial immune, applications data","machine learning, natural language, reinforcement learning, web page, present approach, web search, learning classifier, learning, speech recognition, novel approach, data mining, systems, search engine, learning systems, word sense, describes systems, sense disambiguation, classifier systems, present systems, data integration","genetic algorithm, evolutionary algorithm, algorithm, problem, genetic programming, optimization problem, particle swarm, present algorithm, optimization algorithm, constraint satisfaction, neural networks, optimization, markov decision, solving problem, consider problem, search algorithm, algorithm problem, constraint problem, evolutionary optimization, markov processes","division multiplexing, present novel, orthogonal division, frequency division, orthogonal multiplexing, proposes, blind separation, orthogonal frequency, frequency multiplexing, present based, ultra uwb, space-time block, motion estimation, scheme, ultra wideband, power consumption, wideband uwb, proposes novel, paper based, estimation distribution","multi-agent systems, support vector, arc consistency, sensor networks, data management, vector svm, petri nets, vector machine, important applications, belief revision, task, resource allocation, support machine, agents, support svm, systems agents, mobile need, agents allocation, problem agents, management systems","artificial immune, reasoning important, spatial reasoning, immune systems, case-based reasoning, temporal reasoning, situation calculus, expressive power, qualitative reasoning, inspired immune, reasoning, ubiquitous computing, reasoning actions, preferences aggregation, testing approaches, world possible, argumentation arguments, important issue, computing, distributed computing","query learning, query data, systems components, question answering, query, type data, statistical relational, entailment given, data streaming, present structure, query language, given data, relational database, query processing, components analysis, statistical model, structure protein, database query, textual given, given","systems user, materialized views, information systems, user, formalization trust, systems use, recommender systems, intrusion detection, systems trust, detection systems, frequently data, information, intrusion systems, use, model information, information user, current systems, data parameters, developing systems, user interface","optimization problem, neural networks, consider problem, solving problem, problem, algorithm problem, solve problem, traveling salesman, differential evolution, paper problem, feature selection, study problem, problem networks, address problem, combinatorial problem, algorithm solving, multi-objective optimization, networks, present problem, global routing","particle swarm, markov decision, particle optimization, markov processes, markov model, particle pso, partially observable, decision processes, swarm optimization, swarm pso, decision problem, optimization pso, graph edge, decision making, observable pomdps, observable markov, observable decision, partially pomdps, partially markov, algorithm markov","image based, discrete transform, reliability growth, motion video, wavelet transform, introduce zero-correlation, construction zero-correlation, sequence zero-correlation, present adaptive, scheme image, exponential growth, class zero-correlation, present based, adaptive algorithm, image, zero-correlation zone, jpeg image, motion coding, fast algorithm, paper transform","division multiplexing, orthogonal division, frequency division, motion estimation, orthogonal multiplexing, orthogonal frequency, frequency multiplexing, present based, multiplexing ofdm, orthogonal ofdm, division ofdm, frequency ofdm, proposes, based, proposes novel, hmm-based speech, present filter, novel based, proposes based, paper based"],"ranking":[["16548|IJCAI|2007|A Distributed Architecture for Symbolic Data Fusion|This paper presents a distributed knowledge representation and data fusion system designed for highly integrated Ambient Intelligence applications. The architecture, based on the idea of an ecosystem of interacting artificial entities, is a framework for collaborating agents to perform an intelligent multi-sensor data fusion. In particular, we focus on the cognitive layers leading the overall data acquisition process. The approach has been thoroughly tested in simulation, and part of it has been already exploited in successful applications.|Fulvio Mastrogiovanni,Antonio Sgorbissa,Renato Zaccaria","16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","80730|VLDB|2007|Probabilistic Graphical Models and their Role in Databases|Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.|Amol Deshpande,Sunita Sarawagi","66092|AAAI|2007|A Connectionist Cognitive Model for Temporal Synchronisation and Learning|The importance of the efforts towards integrating the symbolic and connectionist paradigms of artificial intelligence has been widely recognised. Integration may lead to more effective and richer cognitive computational models, and to a better understanding of the processes of artificial intelligence across the field. This paper presents a new model for the representation, computation, and learning of temporal logic in connectionist systems. The model allows for the encoding of past and future temporal logic operators in neural networks, through a neural-symbolic translation algorithms introduced in the paper. The networks are relatively simple and can be used for reasoning about time and for learning by examples with the use of standard neural learning algorithms. We validate the model in a well-known application dealing WIth temporal synchronisation in distributed knowledge systems. This opens several interesting research paths in cognitive modelling, with potential applications in agent technology, learning and reasoning.|Lu√≠s C. Lamb,Rafael V. Borges,Artur S. d'Avila Garcez","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","16505|IJCAI|2007|Market Based Resource Allocation with Incomplete Information|Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).|Bo An,Chunyan Miao,Zhiqi Shen","43906|IEICE Transations|2007|On Reachability Analysis of Multi Agent Nets|Petri nets are known as a modeling language for concurrent and distributed systems. In recent years, various object-oriented Petri nets were proposed, and we are proposing a kind of object-oriented Petri nets, called multi agent nets (MANs). In this letter, we consider the reachability analysis of MANs. We propose an algorithm for generating an abstract state space of a multi agent net, and report results of computational experiments.|Toshiyuki Miyamoto,Masaki Sakamoto,Sadatoshi Kumagai","44240|IEICE Transations|2007|Improved Classification for Problem Involving Overlapping Patterns|The support vector machine has received wide acceptance for its high generalization ability in real world classification applications. But a drawback is that it uniquely classifies each pattern to one class or none. This is not appropriate to be applied in classification problem involves overlapping patterns. In this paper, a novel multi-model classifier (DR-SVM) which combines SVM classifier with kNN algorithm under rough set technique is proposed. Instead of classifying the patterns directly, patterns lying in the overlapped region are extracted firstly. Then, upper and lower approximations of each class are defined on the basis of rough set technique. The classification operation is carried out on these new sets. Simulation results on synthetic data set and benchmark data sets indicate that, compared with conventional classifiers, more reasonable and accurate information about the pattern's category could be obtained by use of DR-SVM.|Yaohua Tang,Jinghuai Gao","44074|IEICE Transations|2007|Multi-Fractality Analysis of Time Series in Artificial Stock Market Generated by Multi-Agent Systems Based on the Genetic Programming and Its Applications|There are several methods for generating multi-fractal time series, but the origin of the multi-fractality is not discussed so far. This paper deals with the multi-fractality analysis of time series in an artificial stock market generated by multi-agent systems based on the Genetic Programming (GP) and its applications to feature extractions. Cognitive behaviors of agents are modeled by using the GP to introduce the co-evolutionary (social) learning as well as the individual learning. We assume five types of agents, in which a part of the agents prefer forecast equations or forecast rules to support their decision making, and another type of the agents select decisions at random like a speculator. The agents using forecast equations and rules usually use their own knowledge base, but some of them utilize their public (common) knowledge base to improve trading decisions. For checking the multi-fractality we use an extended method based on the continuous time wavelet transform. Then, it is shown that the time series of the artificial stock price reveals as a multi-fractal signal. We mainly focus on the proportion of the agents of each type. To examine the role of agents of each type, we classify six cases by changing the composition of agents of types. As a result, in several cases we find strict multi-fractality in artificial stock prices, and we see the relationship between the realizability (reproducibility) of multi-fractality and the system parameters. By applying a prediction method for mono-fractal time series as counterparts, features of the multi-fractal time series are extracted. As a result, we examine and find the origin of multi-fractal processes in artificial stock prices.|Yoshikazu Ikeda,Shozo Tokinaga"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","16122|IJCAI|2005|Feature Generation for Text Categorization Using World Knowledge|We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing--synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.|Evgeniy Gabrilovich,Shaul Markovitch","44120|IEICE Transations|2007|A Machine Learning Approach for an Indonesian-English Cross Language Question Answering System|We have built a CLQA (Cross Language Question Answering) system for a source language with limited data resources (e.g. Indonesian) using a machine learning approach. The CLQA system consists of four modules question analyzer, keyword translator, passage retriever and answer finder. We used machine learning in two modules, the question classifier (part of the question analyzer) and the answer finder. In the question classifier, we classify the EAT (Expected Answer Type) of a question by using SVM (Support Vector Machine) method. Features for the classification module are basically the output of our shallow question parsing module. To improve the classification score, we use statistical information extracted from our Indonesian corpus. In the answer finder module, using an approach different from the common approach in which answer is located by matching the named entity of the word corpus with the EAT of question, we locate the answer by text chunking the word corpus. The features for the SVM based text chunking process consist of question features, word corpus features and similarity scores between the word corpus and the question keyword. In this way, we eliminate the named entity tagging process for the target document. As for the keyword translator module, we use an Indonesian-English dictionary to translate Indonesian keywords into English. We also use some simple patterns to transform some borrowed English words. The keywords are then combined in boolean queries in order to retrieve relevant passages using IDF scores. We first conducted an experiment using , questions (about % are used as the test data) obtained from  Indonesian college students. We next conducted a similar experiment using the NTCIR (NII Test Collection for IR Systems)  CLQA task by translating the English questions into Indonesian. Compared to the Japanese-English and Chinese-English CLQA results in the NTCIR , we found that our system is superior to others except for one system that uses a high data resource employing  dictionaries. Further, a rough comparison with two other Indonesian-English CLQA systems revealed that our system achieved higher accuracy score.|Ayu Purwarianti,Masatoshi Tsuchiya,Seiichi Nakagawa","16136|IJCAI|2005|Learning Strategies for Open-Domain Natural Language Question Answering|We present an approach to automatically learning strategies for natural language question answering from examples composed of textual sources, questions, and answers. Our approach formulates QA as a problem of first order inference over a suitably expressive, learned representation. This framework draws on prior work in learning action and problem-solving strategies, as well as relational learning methods. We describe the design of a system implementing this model in the framework of natural language question answering for story comprehension. Finally, we compare our approach to three prior systems, and present experimental results demonstrating the efficacy of our model.|Eugene Grois,David C. Wilkins","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","44230|IEICE Transations|2007|Zero-Anaphora Resolution in Chinese Using Maximum Entropy|In this paper, we propose a learning classifier based on maximum entropy (ME) for resolving zero-anaphora in Chinese text. Besides regular grammatical, lexical, positional and semantic features motivated by previous research on anaphora resolution, we develop two innovative Web-based features for extracting additional semantic information from the Web. The values of the two features can be obtained easily by querying the Web using some patterns. Our study shows that our machine learning approach is able to achieve an accuracy comparable to that of state-of-the-art systems. The Web as a knowledge source can be incorporated effectively into the ME learning framework and significantly improves the performance of our approach.|Jing Peng,Kenji Araki","57971|GECCO|2007|Towards clustering with XCS|This paper presents a novel approach to clustering using an accuracy-based Learning Classifier System. Our approach achieves this by exploiting the generalization mechanisms inherent to such systems. The purpose of the work is to develop an approach to learning rules which accurately describe clusters without prior assumptions as to their number within a given dataset. Favourable comparisons to the commonly used k-means algorithm are demonstrated on a number of synthetic datasets.|Kreangsak Tamee,Larry Bull,Ouen Pinngern","66224|AAAI|2007|PLOW A Collaborative Task Learning Agent|To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies deep natural language understanding, knowledge representation and reasoning, dialogue systems, planningagent-based systems and machine learning. A formal evaluation shows the approach has great promise.|James F. Allen,Nathanael Chambers,George Ferguson,Lucian Galescu,Hyuckchul Jung,Mary D. Swift,William Taysom","57413|GECCO|2005|XCS with computed prediction in multistep environments|XCSF extends the typical concept of learning classifier systems through the introduction of computed classifier prediction. Initial results show that XCSF's computed prediction can be used to evolve accurate piecewise linear approximations of simple functions. In this paper, we take XCSF one step further and apply it to typical reinforcement learning problems involving delayed rewards. In essence, we use XCSF as a method of generalized (linear) reinforcement learning to evolve piecewise linear approximations of the payoff surfaces of typical multistep problems. Our results show that XCSF can easily evolve optimal and near optimal solutions for problems introduced in the literature to test linear reinforcement learning methods.|Pier Luca Lanzi,Daniele Loiacono,Stewart W. Wilson,David E. Goldberg","16605|IJCAI|2007|An Adaptive Context-Based Algorithm for Term Weighting Application to Single-Word Question Answering|Term weighting systems are of crucial importance in Information Extraction and Information Retrieval applications. Common approaches to term weighting are based either on statistical or on natural language analysis. In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies by adopting a machine learning approach. In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms of the same context. The Context Function is learned from examples, allowing the use of statistical and linguistic information at the same time. The novel algorithm was successfully tested on crossword clues, which represent a case of Single-Word Question Answering.|Marco Ernandes,Giovanni Angelini,Marco Gori,Leonardo Rigutini,Franco Scarselli"],["16250|IJCAI|2005|A Scalable Method for Multiagent Constraint Optimization|We present in this paper a new, complete method for distributed constraint optimization, based on dynamic programming. It is a utility propagation method, inspired by the sum-product algorithm, which is correct only for tree-shaped constraint networks. In this paper, we show how to extend that algorithm to arbitrary topologies using a pseudotree arrangement of the problem graph. Our algorithm requires a linear number of messages, whose maximal size depends on the induced width along the particular pseudotree chosen. We compare our algorithm with backtracking algorithms, and present experimental results. For some problem types we report orders of magnitude fewer messages, and the ability to deal with arbitrarily large problems. Our algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well.|Adrian Petcu,Boi Faltings","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Su√°rez,Manuel Valenzuela-Rend√≥n,Hugo Terashima-Mar√≠n,Eduardo Uresti-Charre","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","57369|GECCO|2005|Statistical analysis of heuristics for evolving sorting networks|Designing efficient sorting networks has been a challenging combinatorial optimization problem since the early 's. The application of evolutionary computing to this problem has yielded human-competitive results in recent years. We build on previous work by presenting a genetic algorithm whose parameters and heuristics are tuned on a small instance of the problem, and then scaled up to larger instances. Also presented are positive and negative results regarding the efficacy of several domain-specific heuristics.|Lee K. Graham,Hassan Masum,Franz Oppacher","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","58203|GECCO|2007|Hybrid evolutionary algorithms on minimum vertex cover for random graphs|This paper analyzes the hierarchical Bayesian optimization algorithm (hBOA) on minimum vertex cover for standard classes of random graphs and transformed SAT instances. The performance of hBOA is compared with that of the branch-and-bound problem solver (BB), the simple genetic algorithm (GA) and the parallel simulated annealing (PSA). The results indicate that BB is significantly outperformed by all the other tested methods, which is expected as BB is a complete search algorithm and minimum vertex cover is an NP-complete problem. The best performance is achieved with hBOA nonetheless, the performance differences between hBOA and other evolutionary algorithms are relatively small, indicating that mutation-based search and recombination-based search lead to similar performance on the tested problem instances.|Martin Pelikan,Rajiv Kalapala,Alexander K. Hartmann","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57910|GECCO|2007|A genetic algorithm for resident physician scheduling problem|This paper formally presents the resident physician scheduling problem, which is one of the most important scheduling problems in hospital. The resident physician scheduling problem is characterized as satisfying the fair schedule constraint, the physician specification constraint and the safe schedule constraint simultaneously. To minimize the penalties from violating the constraints, this study adopts the evolutionary approach to propose a genetic algorithm for solving the problems. In addition the well-known genetic operators, this study proposed a new mutation operator called dynamic mutation for solving the resident physician scheduling problem. The experimental results show that the proposed algorithm performs well in searching optimal schedules.|Chi-Way Wang,Lei-Ming Sun,Ming-Hui Jin,Chung-Jung Fu,Li Liu,Chen-hsiung Chan,Cheng-Yan Kao","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule"],["43816|IEICE Transations|2007|MLSE Detection with Blind Linear Prediction and Subcarriers Interpolation for DSTBC-OFDM Systems|This paper proposes low-complexity blind detection for orthogonal frequency division multiplexing (OFDM) systems with the differential space-time block code (DSTBC) under time-varying frequency-selective Rayleigh fading. The detector employs the maximum likelihood sequence estimation (MLSE) in cooperation with the blind linear prediction (BLP), of which prediction coefficients are determined by the method of Lagrange multipliers. Interpolation of channel frequency responses is also applied to the detector in order to reduce the complexity. A complexity analysis and computer simulations demonstrate that the proposed detector can reduce the complexity to about a half, and that the complexity reduction causes only a loss of  dB in average EbN at BER of - when the prediction order and the degree of polynomial approximation are  and , respectively.|Seree Wanichpakdeedecha,Kazuhiko Fukawa,Hiroshi Suzuki,Satoshi Suyama","43877|IEICE Transations|2007|Finite Parameter Model for Doubly-Selective Channel Estimation in OFDM|To describe joint time-and frequency-selective (doubly-selective) channels in mobile broadband wireless communications, we propose to use the finite parameter model based on the same Bessel functions for each tap (Bessel model). An expression of channel estimation mean squared error (MSE) based on the finite parameter models in Orthogonal Frequency Division Multiplexing (OFDM) systems is derived. Then, our Bessel model is compared with commonly used finite parameter models in terms of the channel estimation MSE. Even if the channel taps have different channel correlations and some of the taps do not coincide with the Bessel function, the channel estimation MSE of the Bessel model is shown to be comparable or outperform existing models as validated by Monte-Carlo simulations over an ensemble of channels in typical urban and suburban environments.|Kok Ann Donny Teo,Shuichi Ohno","44073|IEICE Transations|2007|A Tree-Structured Blind Algorithm for Joint Parametric Channel Estimation and Non-coherent Data Detection in an OFDM-CDMA Wireless System with Antenna Arrays|A blind joint parametric channel estimation and non-coherent data detection algorithm is proposed for the downlink of an orthogonal-frequency-division-multiplexing code-division-multiple-access (OFDM-CDMA) system with multiple-input-multiple-output (MIMO) antenna arrays. To reduce the computational complexity, we first develop a tree-structured algorithm to estimate high dimensional parameters predominantly describing the involved multipath channels by employing several stages of low dimensional parameter estimation algorithms. In the tree structure, to exploit the space-time distribution of the receive multipath signals, spatial beamformers and spectral filters are adopted for clustered-multipath grouping and path isolation. In conjunction with the multiple access interference (MAI) suppression techniques, the proposed tree architecture algorithm jointly estimates the direction of arrivals, propagation delays, carrier frequency offsets and fading amplitudes of the downlink wireless channels in a MIMO OFDM-CDMA system. With the outputs of the tree architecture, the signals of interest can then be naturally detected with a path-wise maximum ratio combining scheme.|Yung-Yi Wang,Shih-Jen Yang,Jiunn-Tsair Chen","43860|IEICE Transations|2007|A Combined Coding and Modulation to Support Both Coherent and Non-coherent Ultra-Wideband Receivers|This letter proposes a simple combined coding and modulation based on super-orthogonal convolutional codes (SOCs) in order to support both coherent and non-coherent ultra-wideband (UWB) receivers. In the proposed scheme, the coherent receivers obtain a coding gain as large as the SOC while simultaneously supporting non-coherent receivers. In addition, their performance can be freely adapted by changing the encoder constraint length and the number of PPM slots according to its application. Thus, the proposal enables a more flexible system design for low data-rate UWB systems.|Tomoko Matsumoto,Ryuji Kohno","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","42522|IEICE Transations|2005|A Simple Bit Allocation Scheme Based on Adaptive Coding for MIMO-OFDM Systems with V-BLAST Detector|We present a simple bit allocation scheme based on adaptive coding for MIMO-OFDM (Multiple Input Multiple Output - Orthogonal Frequency Division Multiplexing) systems with V-BLAST (Vertical-Bell laboratories LAyered Space-Time) detector. The proposed scheme controls the code rate of the channel coding and assigns the same modulation and coding to the set of selected sub-channels, which greatly reduces the feedback burden while achieving good performance. Simulation results show that the proposed scheme with minimal feedback provides significant performance improvement over other systems.|Jongwon Kim,Sanhae Kim,Min-Cheol Hong,Yoan Shin","44048|IEICE Transations|2007|A Short Delay Relay Scheme Using Shared Frequency Repeater for UWB Impulse Radio|The transmit power of Ultra Wideband (UWB) is limited in short range communications to avoid the interference with existing narrow-band communication systems. Since this limits UWB communication range, this paper proposes a novel relay scheme that uses shared frequency repeaters for impulse UWB signal relay to improve system range. After considering possible problems with the repeater, in particular the coupling interference between the input and output and relay-delay, a switching control method is proposed that offers short relay-delay and suppresses the coupling interference at the repeaters. With respect to the proposed relay scheme, Pulse-Position-Modulation (PPM) UWB-based signal relay is evaluated by analyzing its BER performance using the point-to-point transmission link model.|Chihong Cho,Honggang Zhang,Masao Nakagawa","42442|IEICE Transations|2005|Pilot Tone Design with Low Peak-to-Average Power Ratio in OFDM|In Orthogonal Frequency Division Multiplexing (OFDM), the composite time signal exhibits a high peak-to-average power ratio (PAPR). Due to non-linearities of the transmit power amplifiers, this high PAPR generates in-band distortion, out of band noise (OBN) or spectral spreading, which degrades the bit-error rate (BER) performance. In this paper, we propose a simple way to combat this problem without sacrificing channel estimation and frequency-offset tracking accuracy, by designing a sub-optimal configuration of the pilot tones. The effectiveness of the newly designed pilot tones in reducing PAPR is validated by Monte-Carlo simulations. The corresponding improvement in BER is also verified by simulations under IEEE .a standard settings, by using the channel with perfect CSI and the designed pilot-aided estimated channel for coherent detection.|Shinji Hosokawa,Kok Ann Donny Teo,Shuichi Ohno,Takao Hinamoto","44046|IEICE Transations|2007|Reduced-Complexity Detection for DPC-OFTDMA System Enhanced by Multi-Layer MIMO-OFDM in Wireless Multimedia Communications|During these years, we have been focusing on developing ultra high-data-rate wireless access systems for future wireless multimedia communications. One of such kind of systems is called DPC-OFTDMA (dynamic parameter controlled orthogonal frequency and time division multiple access) which targets at beyond  Mbps data rate. In order to support higher data rates, e.g., several hundreds of Mbps or even Gbps for future wireless multimedia applications (e.g., streaming video and file transfer), it is necessary to enhance DPC-OFTDMA system based on MIMO-OFDM (multiple-input multiple-output orthogonal frequency division multiplexing) platform. In this paper, we propose an enhanced DPC-OFTDMA system based on Multi-Layer MIMO-OFDM scheme which combines both diversity and multiplexing in order to exploit potentials of both techniques. The performance investigation shows the proposed scheme has better performance than its counterpart based on full-multiplexing MIMO-OFDM scheme. In addition to the Exhaustive Detection (EXD) scheme which applies the same detection algorithm on each subcarrier independently, we propose the Reduced-Complexity Detection (RCD) scheme. The complexity reduction is achieved by exploiting the suboptimal Layer Detection Order and subcarrier correlation. The simulation results show that huge complexity can be reduced with very small performance loss, by using the proposed detection scheme. For example, .% complexity can be cut off with only . dB performance loss for the    enhanced DPC-OFTDMA system.|Ming Lei,Hiroshi Harada","42624|IEICE Transations|2005|A Complexity-Efficient Wireless OFDM with Frequency Diversity and Low PAPR|This letter proposes a modified orthogonal frequency division multiplexing (OFDM) system with low peak-to-average power ratio (PAPR) and reduced complexity. To do this, OFDM system exploits a frequency diversity equipped with a simple symbol repetition. From the presented results, we can see that the investigated OFDM system with one transmit antenna gives the same diversity gain to two-branch transmit diversity and can be implemented with reduced transmitter complexity and low peak power at the cost of decoding delay.|Young-Hwan You,Sang-Tae Kim,Sung-Kwon Hong,Intae Hwang,Hyoung-Kyu Song"],["65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","44120|IEICE Transations|2007|A Machine Learning Approach for an Indonesian-English Cross Language Question Answering System|We have built a CLQA (Cross Language Question Answering) system for a source language with limited data resources (e.g. Indonesian) using a machine learning approach. The CLQA system consists of four modules question analyzer, keyword translator, passage retriever and answer finder. We used machine learning in two modules, the question classifier (part of the question analyzer) and the answer finder. In the question classifier, we classify the EAT (Expected Answer Type) of a question by using SVM (Support Vector Machine) method. Features for the classification module are basically the output of our shallow question parsing module. To improve the classification score, we use statistical information extracted from our Indonesian corpus. In the answer finder module, using an approach different from the common approach in which answer is located by matching the named entity of the word corpus with the EAT of question, we locate the answer by text chunking the word corpus. The features for the SVM based text chunking process consist of question features, word corpus features and similarity scores between the word corpus and the question keyword. In this way, we eliminate the named entity tagging process for the target document. As for the keyword translator module, we use an Indonesian-English dictionary to translate Indonesian keywords into English. We also use some simple patterns to transform some borrowed English words. The keywords are then combined in boolean queries in order to retrieve relevant passages using IDF scores. We first conducted an experiment using , questions (about % are used as the test data) obtained from  Indonesian college students. We next conducted a similar experiment using the NTCIR (NII Test Collection for IR Systems)  CLQA task by translating the English questions into Indonesian. Compared to the Japanese-English and Chinese-English CLQA results in the NTCIR , we found that our system is superior to others except for one system that uses a high data resource employing  dictionaries. Further, a rough comparison with two other Indonesian-English CLQA systems revealed that our system achieved higher accuracy score.|Ayu Purwarianti,Masatoshi Tsuchiya,Seiichi Nakagawa","57978|GECCO|2007|Controlling overfitting with multi-objective support vector machines|Recently, evolutionary computation has been successfully integrated into statistical learning methods. A Support Vector Machine (SVM) using evolution strategies for its optimization problem frequently deliver better results with respect to the optimization criterion and the prediction accuracy. Moreover, evolutionary computation allows for the efficient large margin optimization of a huge family of new kernel functions, namely non-positive semi definite kernels as the Epanechnikov kernel. For these kernel functions, evolutionary SVM even outperform other learning methods like the Relevance Vector Machine. In this paper, we will discuss another major advantage of evolutionary SVM compared to traditional SVM solutions we can explicitly optimize the inherent trade-off between training error and model complexity by embedding multi-objective optimization into the evolutionary SVM. This leads to three advantages first, it is no longer necessary to tune the SVM parameter C which weighs both conflicting criteria. This is a very time-consuming task for traditional SVM. Second, the shape and size of the Pareto front give interesting insights about the complexity of the learning task at hand. Finally, the user can actually see the point where overfitting occurs and can easily select a solution from the Pareto front best suiting his or her needs.|Ingo Mierswa","42777|IEICE Transations|2005|Corporate Knowledge in Cyberworlds|The aim of this paper is to propose a modeling of corporate knowledge in cyberworlds. An enterprise is considered in the framework of multiagent methodology as a distributed computational system. The Agent-Oriented Abstraction paradigm was proposed earlier to describe in a fully generic way agents and societies of agents. In this paper, we are investigating the application of this paradigm to the abstract modeling of corporate knowledge, extending the scope of traditional knowledge management approaches. We show that such an abstraction mechanism leads to very practical applications for cyberworlds whether on the web or on any other medium. Our approach covers the broader possible scope of corporate knowledge, emphasizing the distributivity and autonomy of agents within cyber systems. This approach can be further used to better simulate and support knowledge management processes.|Pierre Maret,Jacques Calmet","16140|IJCAI|2005|Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale|A martingale framework is proposed to enable support vector machine (SVM) to adapt to timevarying data streams. The adaptive SVM is a onepass incremental algorithm that (i) does not require a sliding window on the data stream, (ii) does not require monitoring the performance of the classifier as data points are streaming, and (iii) works well for high dimensional, multi-class data streams. Our experiments show that the novel adaptive SVM is effective at handling time-varying data streams simulated using both a synthetic dataset and a multiclass real dataset.|Shen-Shyang Ho,Harry Wechsler","16505|IJCAI|2007|Market Based Resource Allocation with Incomplete Information|Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).|Bo An,Chunyan Miao,Zhiqi Shen","57932|GECCO|2007|Genetically designed multiple-kernels for improving the SVM performance|Classical kernel-based classifiers only use a single kernel, butthe real world applications have emphasized the need to con-sider a combination of kernels also known as a multiple kernel in order to boost the performance. Our purpose isto automatically find the mathematical expression of a multiple kernel by evolutionary means. In order to achieve this purpose we propose a hybrid model that combines a Genetic Programming (GP) algorithm and a kernel-based Support Vector Machine (SVM) classifier. Each GP chromosome isa tree encoding the mathematical expression of a multiple kernel. Numerical experiments show that the SVM embedding the evolved multiple kernel performs better than the standard kernels for the considered classification problems.|Laura Diosan,Mihai Oltean,Alexandrina Rogozan,Jean-Pierre P√©cuchet","58006|GECCO|2007|Support vector regression for classifier prediction|In this paper we introduce XCSF with support vector predictionthe problem of learning the prediction function is solved as a support vector regression problem and each classifier exploits a Support Vector Machine to compute the prediction. In XCSF with support vector prediction, XCSFsvm, the genetic algorithm adapts classifier conditions, classifier actions, and the SVM kernel parameters.We compare XCSF with support vector prediction to XCSF with linear prediction on the approximation of four test functions.Our results suggest that XCSF with support vector prediction compared to XCSF with linear prediction (i) is able to evolve accurate approximations of more difficult functions, (ii) has better generalization capabilities and (iii) learns faster.|Daniele Loiacono,Andrea Marelli,Pier Luca Lanzi","44240|IEICE Transations|2007|Improved Classification for Problem Involving Overlapping Patterns|The support vector machine has received wide acceptance for its high generalization ability in real world classification applications. But a drawback is that it uniquely classifies each pattern to one class or none. This is not appropriate to be applied in classification problem involves overlapping patterns. In this paper, a novel multi-model classifier (DR-SVM) which combines SVM classifier with kNN algorithm under rough set technique is proposed. Instead of classifying the patterns directly, patterns lying in the overlapped region are extracted firstly. Then, upper and lower approximations of each class are defined on the basis of rough set technique. The classification operation is carried out on these new sets. Simulation results on synthetic data set and benchmark data sets indicate that, compared with conventional classifiers, more reasonable and accurate information about the pattern's category could be obtained by use of DR-SVM.|Yaohua Tang,Jinghuai Gao","44255|IEICE Transations|2007|Self-Adaptive Mobile Agent Population Control in Dynamic Networks Based on the Single Species Population Model|Mobile-agent-based distributed computing is one of the most promising paradigms to support autonomic computing in a large-scale of distributed system with dynamics and diversity mobile agents traverse the distributed system and carry out a sophisticated task at each node adaptively. In mobile-agent-based systems, a larger number of agents generally require shorter time to complete the whole task but consume more resources (e.g., processing power and network bandwidth). Therefore, it is indispensable to keep an appropriate number of agents for the application on the mobile-agent-based system. This paper considers the mobile agent population control problem in dynamic networks it requires adjusting the number of agents to a constant fraction of the current network size. This paper proposes algorithms inspired by the single species population model, which is a well-known population ecology model. These two algorithms are different in knowledge of networks each node requires. The first algorithm requires global information at each node, while the second algorithm requires only the local information. This paper shows by simulations that the both algorithms realize self-adaptation of mobile agent population in dynamic networks, but the second algorithm attains slightly lower accuracy than the first one.|Tomoko Suzuki,Taisuke Izumi,Fukuhito Ooshita,Toshimitsu Masuzawa"],["66206|AAAI|2007|Predicate Projection in a Bimodal Spatial Reasoning System|Spatial reasoning is a fundamental aspect of intelligent behavior, which cognitive architectures must address in a problem-independent way. Bimodal systems, employing both qualitative and quantitative representations of spatial information, are efficient and psychologically plausible means for spatial reasoning. Any such system must employ a translation from the qualitative level to the quantitative, where new objects (images) are created through the process of predicate projection. This translation has received little scrutiny. We examine this issue in the context of a bimodal spatial reasoning system integrated with a cognitive architecture (Soar). As part of this system, we define an expressive language for predicate projection that supports general and flexible image creation. We demonstrate this system on multiple spatial reasoning problems in the ORTS real-time strategy game environment.|Samuel Wintermute,John E. Laird","65962|AAAI|2007|ESP A Logic of Only-Knowing Noisy Sensing and Acting|When reasoning about actions and sensors in realistic domains, the ability to cope with uncertainty often plays an essential role. Among the approaches dealing with uncertainty, the one by Bacchus, Halpern and Levesque, which uses the situation calculus, is perhaps the most expressive. However, there are still some open issues. For example, it remains unclear what an agent's knowledge base would actually look like. The formalism also requires second-order logic to represent uncertain beliefs, yet a first-order representation clearly seems preferable. In this paper we show how these issues can be addressed by incorporating noisy sensors and actions into an existing logic of only-knowing.|Alfredo Gabaldon,Gerhard Lakemeyer","16055|IJCAI|2005|Propositional Argumentation and Causal Reasoning|The paper introduces a number of propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics. An assumption-based argumentation framework Bondarenko et al.,  will constitute a special case of this construction. In addition, a stronger argumentation system in a full classical language will be shown to be equivalent to a system of causal reasoning Giunchiglia et al., . The implications of this correspondence for the respective nonmonotonic theories of argumentation and causal reasoning are discussed.|Alexander Bochman","16482|IJCAI|2007|Generalizing Temporal Controllability|In this paper, we focus on extending the expressive power of constraint-based temporal reasoning formalisms. We begin with the well-known Simple Temporal Problem with Uncertainty, and incorporate three extensions prior observability, in which the values of uncontrollable events become known prior to their actual occurrence partial shrinkage, in which an observation event triggers the reduction of a contingent temporal interval and a generalization of partial shrinkage to requirement links, making it possible to express certain types of uncertainty that may arise even when the time points in a problem are themselves fully controllable. We describe levels of controllability in the resulting formalism, the Generalized STPU, and relate this formalism to related developments in disjunctive temporal reasoning. Throughout, we motivate our approach with simple, real-world examples that illustrate the limitations of existing formalisms and the flexibility of our proposed extensions.|Michael D. Moffitt,Martha E. Pollack","16783|IJCAI|2007|Modelling Well-Structured Argumentation Lines|Abstract argumentation systems are formalisms for defeasible reasoning where some components remain unspecified, the structure of arguments being the main abstraction. In the dialectical process carried out to identify accepted arguments in the system some controversial situations may appear. These relate to the reintroduction of arguments into the process which cause the onset of circularity. This must be avoided in order to prevent an infinite analysis. Some systems apply the sole restriction of not allowing the introduction of previously considered arguments in an argumentation line. However, repeating an argument is not the only possible cause for the risk mentioned, as subarguments must be taken into account. In this work, we introduce an extended argumentation framework and a definition for progressive defeat path. A credulous extension is also presented.|Diego C. Mart√≠nez,Alejandro Javier Garc√≠a,Guillermo Ricardo Simari","16623|IJCAI|2007|Property Persistence in the Situation Calculus|We develop an algorithm for reducing universally quantified situation calculus queries to a form more amenable to automated reasoning. Universal quantification in the situation calculus requires a second-order induction axiom, making automated reasoning difficult for such queries. We show how to reduce queries about property persistence, a common family of universally-quantified query, to an equivalent form that does not quantify over situations. The algorithm for doing so utilizes only first-order reasoning. We give several examples of important reasoning tasks that are facilitated by our approach, including checking for goal impossibility and reasoning about knowledge with partial observability of actions.|Ryan F. Kelly,Adrian R. Pearce","44319|IEICE Transations|2007|Ontology-Based Context Modeling and Reasoning for U-HealthCare|In order to prepare the health care industry for an increasingly aging society, a ubiquitous health care infrastructure is certainly needed. In a ubiquitous computing environment, it is important that all applications and middleware should be executed on an embedded system. To provide personalized health care services to users anywhere and anytime, a context-aware framework should convert low-level context to high-level context. Therefore, ontology and rules were used in this research to convert low-level context to high-level context. In this paper, we propose context modeling and context reasoning in a context-aware framework which is executed on an embedded wearable system in a ubiquitous computing environment for U-HealthCare. The objective of this research is the development of the standard ontology foundation for health care services and context modeling. A system for knowledge inference technology and intelligent service deduction is also developed in order to recognize a situation and provide customized health care service. Additionally, the context-aware framework was tested experimentally.|Eun Jung Ko,Hyung Jik Lee,Jeun Woo Lee","16114|IJCAI|2005|Representing Flexible Temporal Behaviors in the Situation Calculus|In this paper we present an approach to representing and managing temporally-flexible behaviors in the Situation Calculus based on a model of time and concurrent situations. We define a new hybrid framework combining temporal constraint reasoning and reasoning about actions. We show that the Constraint Based Interval Planning approach can be imported into the Situation Calculus by defining a temporal and concurrent extension of the basic action theory. Finally, we provide a version of the Golog interpreter suitable for managing flexible plans on multiple timelines.|Alberto Finzi,Fiora Pirri","16099|IJCAI|2005|Explaining preferences with argument positions|When deciding what to do agents must choose among alternative actions and different agents may make different choices according to what they wish to achieve in the light of their preferences and values. It cannot be assumed, however, that agents have a conscious understanding of their value preferences independent of the reasoning situations in which they engage. In this paper we consider an extension to a generic framework for reasoning about arguments justifying actions in terms of values in which the preferences amongst values emerge from the reasoning process.|Sylvie Doutre,Trevor J. M. Bench-Capon,Paul E. Dunne","16523|IJCAI|2007|Qualitative Temporal Reasoning about Vague Events|The temporal boundaries of many real-world events are inherently vague. In this paper, we discuss the problem of qualitative temporal reasoning about such vague events. We show that several interesting reasoning tasks, such as checking satisfiability, checking entailment, and calculating the best truth value bound, can be reduced to reasoning tasks in a well-known point algebra with disjunctions. Furthermore, we identify a maximal tractable subset of qualitative relations to support efficient reasoning.|Steven Schockaert,Martine De Cock,Etienne E. Kerre"],["43003|IEICE Transations|2005|Processing Aggregate Queries with Materialized Views in Data Warehouse Environment|Materialized views, which are derived from base relations and stored in the database, offer opportunities for significant performance gain in query evaluation by providing quick access to the pre-computed data. A materialized view can be utilized in evaluating a query if it has pre-computed result of some part of the query plan. Although many approaches to utilizing materialized views in evaluating a query have been proposed, there exist several restrictions in selecting such views. This paper proposes new ways of utilizing materialized views in answering an aggregate query. Views including relations that are not referred to in the given query are utilized. Attributes missing from a view can be recovered under certain conditions. We identify the conditions where a view may be used in evaluating a query and present the algorithm to search for the most efficient query among the equivalent ones. We also report on a simulation based on the TPC-H and GRID databases. Simulation results show that our approach provides impressive performance improvements to the data warehousing environment where aggregate views are often pre-computed and materialized.|Jae-young Chang,Han-joon Kim","80757|VLDB|2007|On the Production of Anorexic Plan Diagrams|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduction has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, identifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches. We investigate here the plan reduction issue from theoretical, statistical and empirical perspectives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPCH-based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to \"anorexic\" (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs.|Harish D.,Pooja N. Darera,Jayant R. Haritsa","80775|VLDB|2007|MIST Distributed Indexing and Querying in Sensor Networks using Statistical Models|The modeling of high level semantic events from low level sensor signals is important in order to understand distributed phenomena. For such content-modeling purposes, transformation of numeric data into symbols and the modeling of resulting symbolic sequences can be achieved using statistical models---Markov Chains (MCs) and Hidden Markov Models (HMMs). We consider the problem of distributed indexing and semantic querying over such sensor models. Specifically, we are interested in efficiently answering (i) range queries return all sensors that have observed an unusual sequence of symbols with a high likelihood, (ii) top- queries return the sensor that has the maximum probability of observing a given sequence, and (iii) -NN queries return the sensor (model) which is most similar to a query model. All the above queries can be answered at the centralized base station, if each sensor transmits its model to the base station. However, this is communication-intensive. We present a much more efficient alternative---a distributed index structure, MIST (Model-based Index STructure), and accompanying algorithms for answering the above queries. MIST aggregates two or more constituent models into a single composite model, and constructs an in-network hierarchy over such composite models. We develop two kinds of composite models the first kind captures the average behavior of the underlying models and the second kind captures the extreme behaviors of the underlying models. Using the index parameters maintained at the root of a subtree, we bound the probability of observation of a query sequence from a sensor in the subtree. We also bound the distance of a query model to a sensor model using these parameters. Extensive experimental evaluation on both real-world and synthetic data sets show that the MIST schemes scale well in terms of network size and number of model states. We also show its superior performance over the centralized schemes in terms of update, query, and total communication costs.|Arnab Bhattacharya,Anand Meka,Ambuj K. Singh","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80854|VLDB|2007|Sum-Max Monotonic Ranked Joins for Evaluating Top-K Twig Queries on Weighted Data Graphs|In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirabilitypenalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.|Yan Qi 0002,K. Sel√ßuk Candan,Maria Luisa Sapino","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,V√©ronique Benzaken,Ioana Manolescu,Ravi Vijay","80487|VLDB|2005|Pathfinder XQuery - The Relational Way|Relational query processors are probably the best understood (as well as the best engineered) query engines available today. Although carefully tuned to process instances of the relational model (tables of tuples), these processors can also provide a foundation for the evaluation of \"alien\" (non-relational) query languages if a relational encoding of the alien data model and its associated query language is given, the RDBMS may act like a special-purpose processor for the new language.|Peter A. Boncz,Torsten Grust,Maurice van Keulen,Stefan Manegold,Jan Rittinger,Jens Teubner"],["65621|AAAI|2005|Goal-Directed Site-Independent Recommendations from Passive Observations|This paper introduces a novel method to find Web pages that satisfy the user's current information need. The method infers the user's need from the content of the pages the user has visited and the actions the user has applied to these pages. Unlike content-based systems that attempt to learn a user's long-tenn interests, our system learns user-independent patterns of behavior that identify the user's current information need, based on hisher current browsing session, then uses this information to suggest specific pages intended to address this need. Our system learns these behavior patterns from labeled data collected during a five-week user study, involving over one hundred participants working on their day-today tasks. We tested this learned model in a second phase of this same study, and found that this model can effectively identify the information needs of new users as they browse previously unseen pages, and that we can use this information to help them find relevant pages.|Tingshao Zhu,Russell Greiner,Gerald H√§ubl,Kevin Jewell,Robert Price","57958|GECCO|2007|Dendritic cells for SYN scan detection|Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to the activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the fused data signals with a secondary data stream. Aggregate output of a population of cells is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.|Julie Greensmith,Uwe Aickelin","57883|GECCO|2007|XCS for adaptive user-interfaces|We outline our context learning framework that harnesses information from a user's environment to learn user preferences for application actions. Within this framework, we employ XCS in a real world application for personalizing user-interface actions to individual users. Sycophant, our context aware calendaring application and research test-bed, uses XCS to adaptively generate user-preferred alarms for ten users in our study. Our results show that XCS' alarm prediction performance equals or surpasses the performance of One-R and a decision tree algorithm for all the users. XCS' average performance is close to $$ percent on the alarm prediction task for all ten users. These encouraging results further highlight the feasibility of using XCS for predictive data mining tasks and the promise of a classifier systems based approach to personalize user interfaces.|Anil Shankar,Sushil J. Louis,Sergiu Dascalu,Ramona Houmanfar,Linda J. Hayes","66126|AAAI|2007|Unsupervised Shilling Detection for Collaborative Filtering|Collaborative Filtering systems are essentially social systems which base their recommendation on the judgment of a large number of people. However, like other social systems, they are also vulnerable to manipulation. Lies and Propaganda may be spread by malicious users who may have an interest in promoting an item, or downplaying the popularity of another one. By doing this systematically, with either multiple identities, or by involving more people, malicious shilling user profiles can be injected into a collaborative recommender system which can significantly affect the robustness of a recommender system. While current detection algorithms are able to use certain characteristics of shilling profiles to detect them, they suffer from low precision, and require a large amount of training data. The aim of this work is to explore simpler unsupervised alternatives which exploit the nature of shilling profiles, and can be easily plugged into collaborative filtering framework to add robustness. Two statistical methods are developed and experimentally shown to provide high accuracy in shilling attack detection.|Bhaskar Mehta","65389|AAAI|2005|Dissertation in Progress An Empirical Analysis of the Costs and Benefits of Naturalness in Spoken Dialog Systems|In this paper I describe work for my Ph.D. dissertation whieh is currently in progress. The overarching goal of the work is to develop a methodology for empirically evaluating the effects of different interface design decisions in spoken dialogue systems. The methodology I will use is the dual-task method, borrowed from cognitive psychology, which is advantageous because it provides fine-grained information about the cognitive load of the user while heshe is engaged in interacting with the system. For my dissertation I will focus specifically on the use of definite referring expressions and the question of whether \"natural\" or \"fully-specified\" definite referring expressions are easier for users to generate andor understand. The answers are important because both strategies are used in systems on the market today. More importantly, I hope my work will provide a tool for software developers, and encourage them to carefully weigh the empirically observed costs and benefits of various design decisions.|Ellen Campana","66259|AAAI|2007|SUNNY A New Algorithm for Trust Inference in Social Networks Using Probabilistic Confidence Models|In many computing systems, information is produced and processed by many people. Knowing how much a user trusts a source can be very useful for aggregating, filtering, and ordering of information. Furthermore, if trust is used to support decision making, it is important to have an accurate estimate of trust when it is not directly available, as well as a measure of confidence in that estimate. This paper describes a new approach that gives an explicit probabilistic interpretation for confidence in social networks. We describe SUNNY, a new trust inference algorithm that uses a probabilistic sampling technique to estimate our confidence in the trust information from some designated sources. SUNNY computes an estimate of trust based on only those information sources with high confidence estimates. In our experiments, SUNNY produced more accurate trust estimates than the well known trust inference algorithm TIDALTRUST (Golbeck ), demonstrating its effectiveness.|Ugur Kuter,Jennifer Golbeck","80507|VLDB|2005|iMeMex Escapes from the Personal Information Jungle|Modern computer work stations provide thousands of applications that store data in  . files on the file system of the underlying OS. To handle these files data processing logic is reinvented inside each application. This results in a jungle of data processing solutions and a jungle of data and file formats. For a user, it is extremely hard to manage information in this jungle. Most of all it is impossible to use data distributed among different files and formats for combined queries, e.g., join and union operations. To solve the problems arising from file based data management, we present a software system called iMeMex as a unified solution to personal information management and integration. iMeMex is designed to integrate seamlessly into existing operating systems like Windows, Linux and Mac OS X. Our system enables existing applications to gradually dispose file based storage. By using iMeMex modern operating systems are enabled to make use of sophisticated DBMS, IR and data integration technologies. The seamless integration of iMeMex into existing operating systems enables new applications that provide concepts of data storage and analysis unseen before.|Jens-Peter Dittrich,Marcos Antonio Vaz Salles,Donald Kossmann,Lukas Blunschi","42918|IEICE Transations|2005|A License Management Protocol for Protecting User Privacy and Digital Contents in Digital Rights Management Systems|Although the Digital Rights Management (DRM) systems have been rapidly developed to protect copyrights, they have not considered user privacy because they regard this as an unnecessary element in achieving their goals. However, the protection of user privacy becomes one of the most important issues in DRM systems as the number of people who suffer from accidents caused by the infringement of individual information dramatically increases. This paper suggests a license management protocol which is a more powerful protocol to protect individual information in DRM. To protect the exposure of information of user identification, the proposed protocol uses alias like a TID and a token instead of the identity of content users. Due to using alias, this protocol can guarantee the anonymity of content users. Also, it can prevent the leakage of individual information through encryption of usage information. In this way, it can protect the privacy of content users.|Bok-Nyong Park,Wonjun Lee,Jae-Won Kim","80532|VLDB|2005|Personalized Systems Models and Methods from an IR and DB Perspective|In today's knowledge-driven society, information abundance and personal electronic device ubiquity have made it difficult for users to find the right information at the right time and at the right level of detail. To solve this problem, researchers have developed systems that adapt their behavior to the goals, tasks, interests, and other characteristics of their users. Based on models that capture important user characteristics, these personalized systems maintain their users' profiles and take them into account to customize the content generated or its presentation to the different individuals.|Yannis E. Ioannidis,Georgia Koutrika","42954|IEICE Transations|2005|Interactive Object Recognition System for a Helper Robot Using Photometric Invariance|We are developing a helper robot that carries out tasks ordered by the user through speech. The robot needs a vision system to recognize the objects appearing in the orders. It is, however, difficult to realize vision systems that can work in various conditions. Thus, we have proposed to use the human user's assistance through speech. When the vision system cannot achieve a task, the robot makes a speech to the user so that the natural response by the user can give helpful information for its vision system. Our previous system assumes that it can segment images without failure. However, if there are occluded objects andor objects composed of multicolor parts, segmentation failures cannot be avoided. This paper presents an extended system that tries to recover from segmentation failures using photometric invariance. If the system is not sure about segmentation results, the system asks the user by appropriate expressions depending on the invariant values. Experimental results show the usefulness of the system.|Md. Altab Hossain,Rahmadi Kurnia,Akio Nakamura,Yoshinori Kuno"],["57890|GECCO|2007|Optimal antenna placement using a new multi-objective chc algorithm|Radio network design (RND) is a fundamental problem in cellular networks for telecommunications. In these networks, the terrain must be covered by a set of base stations (or antennae), each of which defines a covered area called cell. The problem may be reduced to figure out the optimal placement of antennae out of a list of candidate sites trying to satisfy two objectives to maximize the area covered by the radio signal and to reduce the number of used antennae. Consequently, RND is a bi-objective optimization problem. Previous works have solved the problem by using single-objective techniques which combine the values of both objectives. The used techniques have allowed to find optimal solutions according to the defined objective, thus yielding a unique solution instead of the set of Pareto optimal solutions. In this paper, we solve the RND problem using a multi-objective version of the algorithm CHC, which is the metaheuristic having reported the best results when solving the single-objective formulation of RND. This new algorithm, called MOCHC, is compared against a binary-coded NSGA-II algorithm and also against the provided results in the literature. Our experiments indicate that MOCHC outperfoms NSGA-II and, more importantly, it is more efficient finding the optimal solutions than single-objectives techniques.|Antonio J. Nebro,Enrique Alba,Guillermo Molina,J. Francisco Chicano,Francisco Luna,Juan Jos√© Durillo","42773|IEICE Transations|2005|Dynamic RWA Based on the Combination of Mobile Agents Technique and Genetic Algorithms in WDM Networks with Sparse Wavelength Conversion|Genetic Algorithms (GA) provide an attractive approach to solving the challenging problem of dynamic routing and wavelength assignment (RWA) in optical Wavelength Division Multiplexing (WDM) networks, because they usually achieve a significantly low blocking probability. Available GA-based dynamic RWA algorithms were designed mainly for WDM networks with a wavelength continuity constraint, and they cannot be applied directly to WDM networks with wavelength conversion capability. Furthermore, the available GA-based dynamic RWA algorithms suffer from the problem of requiring a very time consuming process to generate the first population of routes for a request, which may results in a significantly large delay in path setup. In this paper, we study the dynamic RWA problem in WDM networks with sparse wavelength conversion and propose a novel hybrid algorithm for it based on the combination of mobile agents technique and GA. By keeping a suitable number of mobile agents in the network to cooperatively explore the network states and continuously update the routing tables, the new hybrid algorithm can promptly determine the first population of routes for a new request based on the routing table of its source node, without requiring the time consuming process associated with current GA-based dynamic RWA algorithms. To achieve a good load balance in WDM networks with sparse wavelength conversion, we adopt in our hybrid algorithm a new reproduction scheme and a new fitness function that simultaneously takes into account the path length, number of free wavelengths, and wavelength conversion capability in route selection. Our new hybrid algorithm achieves a better load balance and results in a significantly lower blocking probability than does the Fixed-Alternate routing algorithm, both for optical networks with sparse and full-range wavelength converters and for optical networks with sparse and limited-range wavelength converters. This was verified by an extensive simulation study on the ns- network simulator and two typical network topologies. The ability to guarantee both a low blocking probability and a small setup delay makes the new hybrid dynamic RWA algorithm very attractive for current optical circuit switching networks and also for the next generation optical burst switching networks.|Vinh Trong Le,Xiaohong Jiang,Son-Hong Ngo,Susumu Horiguchi","66255|AAAI|2007|Extracting Influential Nodes for Information Diffusion on a Social Network|We consider the combinatorial optimization problem of finding the most influential nodes on a large-scale social network for two widely-used fundamental stochastic diffusion models. It was shown that a natural greedy strategy can give a good approximate solution to this optimization problem. However, a conventional method under the greedy algorithm needs a large amount of computation, since it estimates the marginal gains for the expected number of nodes influenced by a set of nodes by simulating the random process of each model many times. In this paper, we propose a method of efficiently estimating all those quantities on the basis of bond percolation and graph theory, and apply it to approximately solving the optimization problem under the greedy algorithm. Using real-world large-scale networks including blog networks, we experimentally demonstrate that the proposed method can outperform the conventional method, and achieve a large reduction in computational cost.|Masahiro Kimura,Kazumi Saito,Ryohei Nakano","43989|IEICE Transations|2007|A Genetic Algorithm with Conditional Crossover and Mutation Operators and Its Application to Combinatorial Optimization Problems|In this paper, we present a modified genetic algorithm for solving combinatorial optimization problems. The modified genetic algorithm in which crossover and mutation are performed conditionally instead of probabilistically has higher global and local search ability and is more easily applied to a problem than the conventional genetic algorithms. Three optimization problems are used to test the performances of the modified genetic algorithm. Experimental studies show that the modified genetic algorithm produces better results over the conventional one and other methods.|Rong Long Wang,Shinichi Fukuta,Jiahai Wang,Kozo Okazaki","57369|GECCO|2005|Statistical analysis of heuristics for evolving sorting networks|Designing efficient sorting networks has been a challenging combinatorial optimization problem since the early 's. The application of evolutionary computing to this problem has yielded human-competitive results in recent years. We build on previous work by presenting a genetic algorithm whose parameters and heuristics are tuned on a small instance of the problem, and then scaled up to larger instances. Also presented are positive and negative results regarding the efficacy of several domain-specific heuristics.|Lee K. Graham,Hassan Masum,Franz Oppacher","57537|GECCO|2005|Identifying valid solutions for the inference of regulatory networks|In this paper, we address the problem of finding gene regulatory networks from experimental DNA microarray data. The problem often is multi-modal and therefore appropriate optimization strategies become necessary. We propose to use a clustering based niching evolutionary algorithm to maintain diversity in the optimization population to prevent premature convergence and to raise the probability of finding the global optimum by identifying multiple alternative networks than standard algorithms. With this set of alternatives, the identification of the true solution has then to be addressed in a second post-processing step.|Christian Spieth,Felix Streichert,Nora Speer,Andreas Zell","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57297|GECCO|2005|Solving geometric TSP with ants|This paper presents an ant-based approach for solving the Traveling Salesman Problem (TSP). Novel concepts of this algorithm that distinguish it from the other heuristics are the inclusion of a preprocessing stage and the use of a modified version of an ant-based approach with local optimization in multi stages. Experimental results show that this algorithm outperforms ACS  and is comparable to MMAS  for Euclidean TSP instances. Of the  instances of Euclidean TSP from TSPLIB  that were tested, this algorithm found the optimal solution for  instances. For the remaining instances, this algorithm returned solutions that were within .% of the optimum.|Thang Nguyen Bui,Mufit Colpan","57519|GECCO|2005|Computing the epistasis variance of large-scale traveling salesman problems|The interaction among variables of an optimization problem is known as epistasis, and its degree is an important measure for the nonlinearity of the problem. We address the problem of enormous time complexity for computing Davidor's epistasis variance of the traveling salesman problem (TSP). To reduce the complexity, we introduce the concept of schema-linear problem (SLP), show that TSP is a SLP, and present a relevant lemma, called Summation Rule. Using the Summation Rule, we provide a closed formula for epistasis that reduces the time complexity from O(nn) to O(n). Additionally, we propose a new more scalable measure of epistasis by a careful derivation from the original.|Dong-il Seo,Byung Ro Moon","44272|IEICE Transations|2007|Particle Swarms for Feature Extraction of Hyperspectral Data|This paper presents a novel feature extraction algorithm based on particle swarms for processing hyperspectral imagery data. Particle swarm optimization, originally developed for global optimization over continuous spaces, is extended to deal with the problem of feature extraction. A formulation utilizing two swarms of particles was developed to optimize simultaneously a desired performance criterion and the number of selected features. Candidate feature sets were evaluated on a regression problem. Artificial neural networks were trained to construct linear and nonlinear models of chemical concentration of glucose in soybean crops. Experimental results utilizing real-world hyperspectral datasets demonstrate the viability of the method. The particle swarms-based approach presented superior performance in comparison with conventional feature extraction methods, on both linear and nonlinear models.|Sildomar Takahashi Monteiro,Yukio Kosugi"],["16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","16147|IJCAI|2005|Solving POMDPs with Continuous or Large Discrete Observation Spaces|We describe methods to solve partially observable Markov decision processes (POMDPs) with continuous or large discrete observation spaces. Realistic problems often have rich observation spaces, posing significant problems for standard POMDP algorithms that require explicit enumeration of the observations. This problem is usually approached by imposing an a priori discretisation on the observation space, which can be sub-optimal for the decision making task. However, since only those observations that would change the policy need to be distinguished, the decision problem itself induces a lossless partitioning of the observation space. This paper demonstrates how to find this partition while computing a policy, and how the resulting discretisation of the observation space reveals the relevant features of the application domain. The algorithms are demonstrated on a toy example and on a realistic assisted living task.|Jesse Hoey,Pascal Poupart","66222|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","16267|IJCAI|2005|Conditional Planning in the Discrete Belief Space|Probabilistic planning with observability restrictions, as formalized for example as partially observable Markov decision processes (POMDP), has a wide range of applications, but it is computationally extremely difficult. For POMDPs, the most general decision problems about existence of policies satisfying certain properties are undecidable. We consider a computationally easier form of planning that ignores exact probabilities, and give an algorithm for a class of planning problems with partial observability. We show that the basic backup step in the algorithm is NP-complete. Then we proceed to give an algorithm for the backup step, and demonstrate how it can be used as a basis of an efficient algorithm for constructing plans.|Jussi Rintanen","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","65451|AAAI|2005|Planning in Models that Combine Memory with Predictive Representations of State|Models of dynamical systems based on predictive state representations (PSRs) use predictions of future observations as their representation of state. A main departure from traditional models such as partially observable Markov decision processes (POMDPs) is that the PSR-model state is composed entirely of observable quantities. PSRs have recently been extended to a class of models called memory-PSRs (mPSRs) that use both memory of past observations and predictions of future observations in their state representation. Thus, mPSRs preserve the PSR-property of the state being composed of observable quantities while potentially revealing structure in the dynamical system that is not exploited in PSRs. In this paper, we demonstrate that the structure captured by mPSRs can be exploited quite naturally for stochastic planning based on value-iteration algorithms. In particular, we adapt the incremental-pruning (IP) algorithm defined for planning in POMDPs to mPSRs. Our empirical results show that our modified IP on mPSRs outperforms, in most cases, IP on both PSRs and POMDPs.|Michael R. James,Satinder P. Singh","66009|AAAI|2007|Scaling Up Solving POMDPs through Value Based Clustering|Partially Observable Markov Decision Processes (POMDPs) provide an appropriately rich model for agents operating under partial knowledge of the environment. Since finding an optimal POMDP policy is intractable, approximation techniques have been a main focus of research, among them point-based algorithms, which scale up relatively well - up to thousands of states. An important decision in a point-based algorithm is the order of backup operations over belief states. Prioritization techniques for ordering the sequence of backup operations reduce the number of needed backups considerably, but involve significant overhead. This paper suggests a new way to order backups, based on a soft clustering of the belief space. Our novel soft clustering method relies on the solution of the underlying MDP. Empirical evaluation verifies that our method rapidly computes a good order of backups, showing orders of magnitude improvement in runtime over a number of benchmarks.|Yan Virin,Guy Shani,Solomon Eyal Shimony,Ronen I. Brafman","16621|IJCAI|2007|Solving POMDPs Using Quadratically Constrained Linear Programs|Developing scalable algorithms for solving partially observable Markov decision processes (POMDPs) is an important challenge. One approach that effectively addresses the intractable memory requirements of POMDP algorithms is based on representing POMDP policies as finite-state controllers. In this paper, we illustrate some fundamental disadvantages of existing techniques that use controllers. We then propose a new approach that formulates the problem as a quadratically constrained linear program (QCLP), which defines an optimal controller of a desired size. This representation allows a wide range of powerful nonlinear programming algorithms to be used to solve POMDPs. Although QCLP optimization techniques guarantee only local optimality, the results we obtain using an existing optimization method show significant solution improvement over the state-of-the-art techniques. The results open up promising research directions for solving large POMDPs using nonlinear programming methods.|Christopher Amato,Daniel S. Bernstein,Shlomo Zilberstein","65617|AAAI|2005|Planning and Execution with Phase Transitions|We consider a special type of continuous-time Markov decision processes (MDPs) that arise when phase-type distributions are used to model the timing of non-Markovian events and actions. We focus, primarily, on the execution of phase-dependent policies. Phases are introduced into a model to represent relevant execution history, but there is no physical manifestation of phases in the real world. We treat phases as partially observable state features and show how a belief distribution over phase configurations can be derived from observable state features through the use of transient analysis for Markov chains. This results in an efficient method for phase tracking during execution that can be combined with the QMDP value method for POMDPs to make action choices. We also discuss, briefly, how the structure of MDPs with phase transitions can be exploited in structured value iteration with symbolic representation of vectors and matrices.|H√•kan L. S. Younes"],["43002|IEICE Transations|2005|Content-Based Motion Estimation with Extended Temporal-Spatial Analysis|In adaptive motion estimation, spatial-temporal correlation based motion type inference has been recognized as an effective way to guide the motion estimation strategy adjustment according to video contents. However, the complexity and the reliability of those methods remain two crucial problems. In this paper, a motion vector field model is introduced as the basis for a new spatial-temporal correlation based motion type inference method. For each block, Full Search with Adaptive Search Window (ASW) and Three Step Search (TSS), as two search strategy candidates, can be employed alternatively. Simulation results show that the proposed method can constantly reduce the dynamic computational cost to as low as % to % of that of Full Search (FS), while remaining a closer approximation to FS in terms of visual quality than other fast algorithms for various video sequences. Due to its efficiency and reliability, this method is expected to be a favorable contribution to the mobile video communication where low power real-time video coding is necessary.|Shen Li,Yong Jiang,Takeshi Ikenaga,Satoshi Goto","43814|IEICE Transations|2007|Zero-Correlation Zone Sequence Set Constructed from a Perfect Sequence|The present paper introduces the construction of a class of sequence sets with zero-correlation zones called zero-correlation zone sequence sets. The proposed zero-correlation zone sequence set can be generated from an arbitrary perfect sequence, the length of which is longer than . The proposed sets of ternary sequences, which can be constructed from an arbitrary perfect sequence, can successfully provide CDMA communication without co-channel interference. In an ultrasonic synthetic aperture imaging system, the proposed sequence set can improve the signal-to-noise ratio of the acquired image.|Takafumi Hayashi","43974|IEICE Transations|2007|Zero-Correlation Zone Sequence Set Construction Using an Even-Perfect Sequence and an Odd-Perfect Sequence|The present paper introduces a new construction of a class of binary periodic sequence set having a zero-correlation zone sequence set. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary zcz sequence set by using an arbitrary pair of an even-perfect sequence and an odd-perfect sequence. The proposed zcz sequence set reaches the theoretical upper bound of the member size of the sequence set.|Takafumi Hayashi","43874|IEICE Transations|2007|An Integrated Sequence Construction of Binary Zero-Correlation Zone Sequences|The present paper introduces an integrated construction of binary sequences having a zero-correlation zone. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The proposed method enables more flexible design of the binary zero-correlation zone sequence set with respect to its member size, length, and width of zero-correlation zone. Several previously reported sequence construction methods of binary zero-correlation zone sequence sets can be explained as special cases of the proposed method.|Takafumi Hayashi","42467|IEICE Transations|2005|A Method of Guaranteeing Image-Quality for Quantization-Based Watermarking Using a Nonorthogonal Transformation|This paper proposes a quantization-based image-quality guaranteed watermarking (IQGW) method using a nonorthogonal discrete wavelet transformation. An IQGW method generates watermarked images of a desired image quality for any image, neither with trial and error nor with image-dependent parameters. To guarantee the image-quality, the proposed method adjusts the energy of the watermark sequence to be embedded based on the relationship between a nonorthogonally transformed domain and the spatial domain for the signal energy. This proposed method extracts the embedded watermark by quantization of watermarked coefficients, no reference image, thus, is required. In addition, it is capable of controlling the objective and subjective image-quality of a watermarked image independently. With features mentioned above, the proposed method is suitable for real-time embedding of Motion JPEG  videos. Moreover, it is able to fuse quantization- and correlation-based watermarking.|Masaaki Fujiyoshi,Osamu Watanabe,Hitoshi Kiya","42607|IEICE Transations|2005|Switching Wavelet Transform for ROI Image Coding|In region-of-interest (ROI) image coding based on wavelet transforms, the tap length of the wavelet filter as well as energy compaction characteristics affect the quality of the restored image. This paper presents a wavelet transform comprised of two wavelet filter sets with different tap lengths. The wavelet filter is switched to the shorter-length set to code a ROI of an image and to the longer-length one for the remaining region, the region of non-interest (RONI). ROI coding examples demonstrate that this switching wavelet transform provides better quality levels than fixed transforms under the same total bits the quality of the recovered ROI is improved in the lossy coding of both regions while that of the full image is improved in the lossless coding of the ROI.|Shinji Fukuma,Toshihiko Tanaka,Masahiko Nawate","42586|IEICE Transations|2005|Upper Bounds of the Correlation Functions of a Class of Binary Zero-Correlation-Zone Sequences|The present letter describes the estimation of the upper bounds of the correlation functions of a class of zero-correlation-zone sequences constructed from an arbitrary Hadamard matrix.|Takafumi Hayashi,Takao Maeda,Satoshi Okawa","42581|IEICE Transations|2005|Binary Zero-Correlation Zone Sequence Set Construction Using a Primitive Linear Recursion|The present paper introduces a new construction of a class of binary sequence set having a zero-correlation zone (hereafter binary ZCZ sequence set). The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The present paper shows that such a construction generates a binary ZCZ sequence set by using a primitive linear recursion over GF(), the finite field of integers modulo .|Takafumi Hayashi","44281|IEICE Transations|2007|A New Coding Technique for Digital Holographic Video Using Multi-View Prediction|In this paper, we proposed an efficient coding method for digital hologram (fringe pattern) acquired with a CCD camera or by computer generation using multi-view prediction and MPEG video compression standard techniques. It processes each R, G, or B color component separately. The basic processing unit is a partial image segmented as the size of M  N. Each partial image retains the information of the whole object. This method generates an assembled image for a column of the segmented and frequency-transformed partial images, which is the basis of the coding process. That is, a motion estimation and compensation technique of MPEG is applied between the reconstructed images from the assembled images with the disparities found during generation of assembled image and the original partial images. Therefore the compressed results are the disparity of each partial image to form the assembled image for the corresponding column, assembled image, and the motion vectors and the compensated image for each partial image. The experimental results with the implemented algorithm showed that the proposed method has NC (Normalized Correlation) values about % higher than the previous method at the same compression ratios, which convinced us that ours has better compression efficiency. Consequently, the proposed method is expected to be used effectively in the application areas to transmit or store in digital format the digital hologram data.|Young-Ho Seo,Hyun-Jun Choi,Jin-Woo Bae,Hoon-Jong Kang,Seung-Hyun Lee,Ji-Sang Yoo,Dong-Wook Kim","42455|IEICE Transations|2005|Construction of Sequences with Large Zero Correlation Zone|In order to judge the goodness of zero correlation zone sequence sets, a new concept, called ZCZ characteristic, is proposed. Then by defining a sequence operation, i.e. correlation product, and establishing its basic properties, a new approach to construct sets of sequences with a large zero correlation zone is presented.|Daiyuan Peng,Pingzhi Fan,Naoki Suehiro"],["43816|IEICE Transations|2007|MLSE Detection with Blind Linear Prediction and Subcarriers Interpolation for DSTBC-OFDM Systems|This paper proposes low-complexity blind detection for orthogonal frequency division multiplexing (OFDM) systems with the differential space-time block code (DSTBC) under time-varying frequency-selective Rayleigh fading. The detector employs the maximum likelihood sequence estimation (MLSE) in cooperation with the blind linear prediction (BLP), of which prediction coefficients are determined by the method of Lagrange multipliers. Interpolation of channel frequency responses is also applied to the detector in order to reduce the complexity. A complexity analysis and computer simulations demonstrate that the proposed detector can reduce the complexity to about a half, and that the complexity reduction causes only a loss of  dB in average EbN at BER of - when the prediction order and the degree of polynomial approximation are  and , respectively.|Seree Wanichpakdeedecha,Kazuhiko Fukawa,Hiroshi Suzuki,Satoshi Suyama","42596|IEICE Transations|2005|A Timing Synchronization Method with Low-Volume DSP for OFDM Packet Transmission Systems|This paper proposes a simple timing synchronization method in order to design a timing synchronization circuit with low-complex and low-volume digital signal processing (DSP) for orthogonal frequency division multiplexing (OFDM) packet transmission systems. The proposed method utilizes the subtraction process for acquirement of a timing metric of fast Fourier transform (FFT) window, whereas the conventional methods utilize the multiplication process. This paper adopts the proposed method to a standardized OFDM format, IEEE .a, and elucidates that the proposed one shows good transmission performance as well as the conventional one in fast time-variant multi-path Rayleigh fading channels by computer simulation.|Ryota Kimura,Ryuhei Funada,Hiroshi Harada,Manabu Sawada,Shoji Shinoda","43877|IEICE Transations|2007|Finite Parameter Model for Doubly-Selective Channel Estimation in OFDM|To describe joint time-and frequency-selective (doubly-selective) channels in mobile broadband wireless communications, we propose to use the finite parameter model based on the same Bessel functions for each tap (Bessel model). An expression of channel estimation mean squared error (MSE) based on the finite parameter models in Orthogonal Frequency Division Multiplexing (OFDM) systems is derived. Then, our Bessel model is compared with commonly used finite parameter models in terms of the channel estimation MSE. Even if the channel taps have different channel correlations and some of the taps do not coincide with the Bessel function, the channel estimation MSE of the Bessel model is shown to be comparable or outperform existing models as validated by Monte-Carlo simulations over an ensemble of channels in typical urban and suburban environments.|Kok Ann Donny Teo,Shuichi Ohno","43953|IEICE Transations|2007|Peak Reduction Improvement in Iterative Clipping and Filtering with a Graded Band-Limiting Filter for OFDM Transmission|The large PAPR of orthogonal frequency division multiplexing (OFDM) transmission is one of the serious problems for mobile communications that require severe power saving. Iterative clipping and filtering is an effective method for the PAPR reduction of OFDM signals. This paper evaluates PAPR reduction effect with a graded band-limiting filter in the iterative clipping and filtering method. The evaluation result by computer simulation shows that the excellent peak reduction effect can be obtained in the fewer iteration numbers by using a roll-off filter instead of the conventional rectangular filter, and the iteration number with the roll-off filter achieving the same PAPR is fewer by twice. The result confirms that the clipping and filtering method by using a graded band-limiting filter can achieve low peak OFDM transmission with less computational complexity.|Toshiyuki Matsuda,Shigeru Tomisato,Masaharu Hata,Hiromasa Fujii,Junichiro Hagiwara","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","43986|IEICE Transations|2007|Analysis of Symmetric Cancellation Coding for OFDM over a Multi-Path Rayleigh Fading Channel|Orthogonal frequency division multiplexing (OFDM) systems for mobile applications suffer from inter-carrier-interference (ICI) due to frequency offset and to time-variation of the channels and from high peak-to-average-power ratio (PAPR). In this paper, we revisit symmetric cancellation coding (SCC) proposed by Sathananthan et al. and compare the effectiveness of SCC with a fixed subtraction combining and the well-known polynomial cancellation coding (PCC) over Rayleigh fading channels with Doppler spread in terms of the signal-to-interference plus noise power ratio (SINR) and bit-error-rate (BER). We also compare SCC with subtraction combining and SCC of Sathananthan et al. with maximum ratio combining (MRC). Our results show that SCC-OFDM with subtraction combining gives higher SINR than PCC-OFDM over the flat Rayleigh fading channel and that this superiority is not maintained under multi-path induced frequency-selective fading unless diversity combining is used. A simulation result shows, however, that SCC-OFDM with subtraction combining may perform better than PCC-OFDM for a certain range of Doppler spread when differential modulation is employed. Finally, we also demonstrate that the SCC-OFDM signal has less PAPR compared to the normal OFDM and PCC-OFDM and hence may be more practical.|Abdullah S. Alaraimi,Takeshi Hashimoto","42522|IEICE Transations|2005|A Simple Bit Allocation Scheme Based on Adaptive Coding for MIMO-OFDM Systems with V-BLAST Detector|We present a simple bit allocation scheme based on adaptive coding for MIMO-OFDM (Multiple Input Multiple Output - Orthogonal Frequency Division Multiplexing) systems with V-BLAST (Vertical-Bell laboratories LAyered Space-Time) detector. The proposed scheme controls the code rate of the channel coding and assigns the same modulation and coding to the set of selected sub-channels, which greatly reduces the feedback burden while achieving good performance. Simulation results show that the proposed scheme with minimal feedback provides significant performance improvement over other systems.|Jongwon Kim,Sanhae Kim,Min-Cheol Hong,Yoan Shin","43752|IEICE Transations|2007|Collision Recovery for OFDM System over Wireless Channel|We present an effective method of collision recovery for orthogonal frequency division multiplexing (OFDM)-based communications. For the OFDM system, the modulated message data can be demodulated using the partial time-domain OFDM signal. Therefore, the partial time-domain signal can be adopted to reconstruct the whole OFDM time-domain signal with estimated channel information. This property can be utilized to recover packets from the collisions. Since most collisions are cases in which a long packet collides with a short packet, the collided part is assumed to be short. The simulated results show that the method can recover the two collided packets with a certain probability and can be developed to solve the problem of hidden terminals. This method will dramatically benefit the protocol design of wireless networks, including ad hoc and sensor networks.|Yafei Hou,Masanori Hamamura","44046|IEICE Transations|2007|Reduced-Complexity Detection for DPC-OFTDMA System Enhanced by Multi-Layer MIMO-OFDM in Wireless Multimedia Communications|During these years, we have been focusing on developing ultra high-data-rate wireless access systems for future wireless multimedia communications. One of such kind of systems is called DPC-OFTDMA (dynamic parameter controlled orthogonal frequency and time division multiple access) which targets at beyond  Mbps data rate. In order to support higher data rates, e.g., several hundreds of Mbps or even Gbps for future wireless multimedia applications (e.g., streaming video and file transfer), it is necessary to enhance DPC-OFTDMA system based on MIMO-OFDM (multiple-input multiple-output orthogonal frequency division multiplexing) platform. In this paper, we propose an enhanced DPC-OFTDMA system based on Multi-Layer MIMO-OFDM scheme which combines both diversity and multiplexing in order to exploit potentials of both techniques. The performance investigation shows the proposed scheme has better performance than its counterpart based on full-multiplexing MIMO-OFDM scheme. In addition to the Exhaustive Detection (EXD) scheme which applies the same detection algorithm on each subcarrier independently, we propose the Reduced-Complexity Detection (RCD) scheme. The complexity reduction is achieved by exploiting the suboptimal Layer Detection Order and subcarrier correlation. The simulation results show that huge complexity can be reduced with very small performance loss, by using the proposed detection scheme. For example, .% complexity can be cut off with only . dB performance loss for the    enhanced DPC-OFTDMA system.|Ming Lei,Hiroshi Harada","42624|IEICE Transations|2005|A Complexity-Efficient Wireless OFDM with Frequency Diversity and Low PAPR|This letter proposes a modified orthogonal frequency division multiplexing (OFDM) system with low peak-to-average power ratio (PAPR) and reduced complexity. To do this, OFDM system exploits a frequency diversity equipped with a simple symbol repetition. From the presented results, we can see that the investigated OFDM system with one transmit antenna gives the same diversity gain to two-branch transmit diversity and can be implemented with reduced transmitter complexity and low peak power at the cost of decoding delay.|Young-Hwan You,Sang-Tae Kim,Sung-Kwon Hong,Intae Hwang,Hyoung-Kyu Song"]]},"title":{"entropy":6.295911437762648,"topics":["method for, speech recognition, and for, for speech, for recognition, for using, and speech, for image, for based, speech synthesis, for channel, estimation for, for systems, analysis for, for with, method using, adaptive for, for noise, maximum likelihood, using and","for systems, framework for, and for, for data, efficient for, control systems, for control, and systems, the web, model for, petri nets, natural language, data, learning for, support vector, and data, data mining, multi-agent systems, web page, from","algorithm for, genetic algorithm, genetic programming, genetic for, particle swarm, for problem, using genetic, evolutionary algorithm, for optimization, algorithm the, neural networks, and genetic, evolutionary for, and algorithm, particle optimization, swarm optimization, bayesian networks, optimization algorithm, algorithm with, using algorithm","for the, the and, its application, and its, the, and application, and for, with and, description logic, the problem, for planning, for logic, between and, constraint satisfaction, and constraint, search space, and search, and, logic programs, planning with","adaptive for, for systems, for with, estimation for, design for, for ofdm, for signal, scheme for, adaptive with, estimation with, algorithm for, estimation and, ultra wideband, techniques for, reduction for, for power, frequency for, adaptive using, and for, systems with","method for, for recognition, speech recognition, for speech, for based, and speech, and recognition, speech synthesis, motion estimation, speech using, based and, recognition using, method based, feature for, method using, speech based, new for, and method, extraction using, feature extraction","for systems, and systems, and for, petri nets, multi-agent systems, support vector, for object, support machine, data management, for retrieval, for management, support for, the systems, access control, for service, protocol for, and retrieval, for communication, management systems, classifier systems","for distributed, for task, for knowledge, for reasoning, from the, survival the, distributed pomdps, distributed systems, distributed and, tool for, for from, from, social networks, and reasoning, extracting from, for case-based, for pattern, knowledge and, networked pomdps, for spatial","genetic programming, genetic algorithm, genetic for, using genetic, and genetic, and programming, using programming, programming for, the genetic, model for, genetic with, genetic, evolution for, model and, using model, evolution strategies, strategies for, named entity, differential evolution, for dynamic","learning for, learning and, reinforcement learning, evolutionary for, learning, sense disambiguation, and evolutionary, word disambiguation, word sense, building block, evolutionary computation, learning with, using learning, for structure, inference for, the learning, structure and, using evolutionary, evolutionary the, vector machine","for planning, for with, and planning, with and, planning with, with, the with, history branch, for scheduling, reasoning about, new for, computation the, temporal and, computation and, planning domain, planning, situation calculus, heuristic for, the heuristic, the scheduling","for the, the and, the, the problem, test for, analysis the, the function, generation for, between and, the complexity, algorithm the, design for, method the, the application, using the, for core, the performance, the design, test generation, the with"],"ranking":[["42342|IEICE Transations|2005|A Data-Driven Model Parameter Compensation Method for Noise-Robust Speech Recognition|A data-driven approach that compensates the HMM parameters for the noisy speech recognition is proposed. Instead of assuming some statistical approximations as in the conventional methods such as the PMC, the various statistical information necessary for the HMM parameter adaptation is directly estimated by using the Baum-Welch algorithm. The proposed method has shown improved results compared with the PMC for the noisy speech recognition.|Yongjoo Chung","42403|IEICE Transations|2005|Noise-Robust Speech Analysis Using Running Spectrum Filtering|This paper proposes a new robust adaptive processing algorithm that is based on the extended least squares (ELS) method with running spectrum filtering (RSF). By utilizing the different characteristics of running spectra between speech signals and noise signals, RSF can retain speech characteristics while noise is effectively reduced. Then, by using ELS, autoregressive moving average (ARMA) parameters can be estimated accurately. In experiments on real speech contaminated by white Gaussian noise and factory noise, we found that the method we propose offered spectrum estimates that were robust against additive noise.|Qi Zhu,Noriyuki Ohtsuki,Yoshikazu Miyanaga,Norinobu Yoshida","42830|IEICE Transations|2005|Robust Speech Recognition Using Discrete-Mixture HMMs|This paper introduces new methods of robust speech recognition using discrete-mixture HMMs (DMHMMs). The aim of this work is to develop robust speech recognition for adverse conditions that contain both stationary and non-stationary noise. In particular, we focus on the issue of impulsive noise, which is a major problem in practical speech recognition system. In this paper, two strategies were utilized to solve the problem. In the first strategy, adverse conditions are represented by an acoustic model. In this case, a large amount of training data and accurate acoustic models are required to present a variety of acoustic environments. This strategy is suitable for recognition in stationary or slow-varying noise conditions. The second is based on the idea that the corrupted frames are treated to reduce the adverse effect by compensation method. Since impulsive noise has a wide variety of features and its modeling is difficult, the second strategy is employed. In order to achieve those strategies, we propose two methods. Those methods are based on DMHMM framework which is one type of discrete HMM (DHMM). First, an estimation method of DMHMM parameters based on MAP is proposed aiming to improve trainability. The second is a method of compensating the observation probabilities of DMHMMs by threshold to reduce adverse effect of outlier values. Observation probabilities of impulsive noise tend to be much smaller than those of normal speech. The motivation in this approach is that flooring the observation probability reduces the adverse effect caused by impulsive noise. Experimental evaluations on Japanese LVCSR for read newspaper speech showed that the proposed method achieved the average error rate reduction of .% in impulsive noise conditions. Also the experimental results in adverse conditions that contain both stationary and impulsive noises showed that the proposed method achieved the average error rate reduction of .%.|Tetsuo Kosaka,Masaharu Katoh,Masaki Kohda","44257|IEICE Transations|2007|Average-Voice-Based Speech Synthesis Using HSMM-Based Speaker Adaptation and Adaptive Training|In speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as F and phone duration. For simultaneous adaptation of spectrum, F and phone duration within the HMM framework, we need to transform not only the state output distributions corresponding to spectrum and F but also the duration distributions corresponding to phone duration. However, it is not straightforward to adapt the state duration because the original HMM does not have explicit duration distributions. Therefore, we utilize the framework of the hidden semi-Markov model (HSMM), which is an HMM having explicit state duration distributions, and we apply an HSMM-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. Furthermore, we propose an HSMM-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. We incorporate these techniques into our HSMM-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.|Junichi Yamagishi,Takao Kobayashi","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","42822|IEICE Transations|2005|Alaryngeal Speech Enhancement Using Pattern Recognition Techniques|An alaryngeal speech enhancement system is proposed to improve the intelligibility and quality of speech signals generated by an artificial larynx transducer (ALT). Proposed system identifies the voiced segments of alaryngeal speech signal, by using pattern recognition methods, and replaces these by their equivalent voiced segments of normal speech. Evaluation results show that proposed system provides a fairly good improvement of the quality and intelligibility of ALT generated speech.|Gualberto Aguilar-Torres,Mariko Nakano-Miyatake,H√©ctor M. P√©rez Meana","42328|IEICE Transations|2005|Speech Recognition Using Finger Tapping Timings|Behavioral synchronization between speech and finger tapping provides a novel approach to improving speech recognition accuracy. We combine a sequence of finger tapping timings recorded alongside an utterance using two distinct methods in the first method, HMM state transition probabilities at the word boundaries are controlled by the timing of the finger tapping in the second, the probability (relative frequency) of the finger tapping is used as a 'feature' and combined with MFCC in a HMM recognition system. We evaluate these methods through connected digit recognition under different noise conditions (AURORA-J). Leveraging the synchrony between speech and finger tapping provides a % relative improvement in connected digit recognition experiments.|Hiromitsu Ban,Chiyomi Miyajima,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","42363|IEICE Transations|2005|An Unsupervised Speaker Adaptation Method for Lecture-Style Spontaneous Speech Recognition Using Multiple Recognition Systems|This paper describes an accurate unsupervised speaker adaptation method for lecture style spontaneous speech recognition using multiple LVCSR systems. In an unsupervised speaker adaptation framework, the improvement of recognition performance by adapting acoustic models remarkably depends on the accuracy of labels such as phonemes and syllables. Therefore, extraction of the adaptation data guided by confidence measure is effective for unsupervised adaptation. In this paper, we looked for the high confidence portions based on the agreement between two LVCSR systems, adapted acoustic models using the portions attached with high accurate labels, and then improved the recognition accuracy. We applied our method to the Corpus of Spontaneous Japanese (CSJ) and the method improved the recognition rate by about .% in comparison with a traditional method.|Seiichi Nakagawa,Tomohiro Watanabe,Hiromitsu Nishizaki,Takehito Utsuro","42715|IEICE Transations|2005|Adaptive Nonlinear Regression Using Multiple Distributed Microphones for In-Car Speech Recognition|In this paper, we address issues in improving hands-free speech recognition performance in different car environments using multiple spatially distributed microphones. In the previous work, we proposed the multiple linear regression of the log spectra (MRLS) for estimating the log spectra of speech at a close-talking microphone. In this paper, the concept is extended to nonlinear regressions. Regressions in the cepstrum domain are also investigated. An effective algorithm is developed to adapt the regression weights automatically to different noise environments. Compared to the nearest distant microphone and adaptive beamformer (Generalized Sidelobe Canceller), the proposed adaptive nonlinear regression approach shows an advantage in the average relative word error rate (WER) reductions of .% and .%, respectively, for isolated word recognition under  real car environments.|Weifeng Li,Chiyomi Miyajima,Takanori Nishino,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura"],["16030|IJCAI|2005|Language Learning in Multi-Agent Systems|We present the problem of learning to communicate in decentralized and stochastic environments, analyzing it formally in a decision-theoretic context and illustrating the concept experimentally. Our approach allows agents to converge upon coordinated communication and action over time.|Martin Allen,Claudia V. Goldman,Shlomo Zilberstein","42904|IEICE Transations|2005|Dynamic Replica Control Based on Fairly Assigned Variation of Data for Loosely Coupled Distributed Database Systems|This paper proposes a decentralized and asynchronous replica control method based on a fair assignment of the variation in numerical data that has weak consistency for loosely coupled database systems managed or used by different organizations of human activity. Our method eliminates the asynchronous abort of already committed transactions even if replicas in all network partitions continue to process transactions when network partitioning occurs. A decentralized and asynchronous approach is needed because it is difficult to keep a number of loosely coupled systems in working order, and replica operations performed in a centralized and synchronous way can degrade the performance of transaction processing. We eliminate the transaction abort by fairly distributing the variation in numerical data to replicas according to their demands and updating the distributed variation using only asynchronously propagated update transactions without calculating the precise global state among reachable replicas. In addition, fairly assigning the variation of data to replicas equalizes the disadvantages of processing update transactions among replicas. Fairness control for assigning the data variation is performed by averaging the variation requested by the replicas. A simulation showed that our system can achieve extremely high performance for processing update transactions and fairness among replicas.|Takao Yamashita","44339|IEICE Transations|2007|Rate-Sensitive Load Shedding in Data Stream Systems|Traditional load shedding algorithms for data stream systems calculate current operator selectivity over several run periods and use them to determine where to shed load during the next run period. In this paper, we show that the current selectivity may change due to the implementation of load shedding. Our algorithm, called RLS, determines the optimum drop location by these changed selectivity rather than those pre-calculated values. Simulation results demonstrate that RLS achieves higher accuracy than traditional algorithms.|Zhiwu Yin,Shangteng Huang,Xun Fan","80557|VLDB|2005|Mapping Maintenance for Data Integration Systems|To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over  real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.|Robert McCann,Bedoor K. AlShebli,Quoc Le,Hoa Nguyen,Long Vu,AnHai Doan","65519|AAAI|2005|Data-Driven MCMC for Learning and Inference in Switching Linear Dynamic Systems|Switching Linear Dynamic System (SLDS) models are a popular technique for modeling complex nonlinear dynamic systems. An SLDS has significantly more descriptive power than an HMM, but inference in SLDS models is computationally intractable. This paper describes a novel inference algorithm for SLDS models based on the Data-Driven MCMC paradigm. We describe a new proposal distribution which substantially increases the convergence speed. Comparisons to standard deterministic approximation methods demonstrate the improved accuracy of our new approach. We apply our approach to the problem of learning an SLDS model of the bee dance. Honeybees communicate the location and distance to food sources through a dance that takes place within the hive. We learn SLDS model parameters from tracking data which is automatically extracted from video. We then demonstrate the ability to successfully segment novel bee dances into their constituent parts, effectively decoding the dance of the bees.|Sang Min Oh,James M. Rehg,Tucker R. Balch,Frank Dellaert","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","43943|IEICE Transations|2007|Control-Invariance of Sampled-Data Hybrid Systems with Clocked Events and Jitters|Silva and Krogh formulate a sampled-data hybrid automaton to deal with time-driven events and discuss its verification. In this paper, we consider a state feedback control problem of the automaton. First, we introduce two transition systems as semantics of the automaton. Next, using these transition systems, we derive necessary and sufficient conditions for a predicate to be control-invariant. Finally, we show that there always exists the supremal control-invariant subpredicate for any predicate.|Yoshiyuki Tsuchie,Toshimitsu Ushio","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","58220|GECCO|2007|Initial results from the use of learning classifier systems to control neuronal networks|In this paper we describe the use of a learning classifier system to control the electrical stimulation of cultured neuronal networks. The aim is to manipulate the environment of the cells such that they display elementary learning, i.e., so that they respond to a given input signal in a pre-specified way. Results indicate that this is possible and that the learned stimulation protocols identify seemingly fundamental properties of in vitro neuronal networks.allUse of another learning scheme and simpler stimulation confirms these properties.|Larry Bull,Ivan S. Uroukov"],["58126|GECCO|2007|A genetic algorithm for privacy preserving combinatorial optimization|We propose a protocol for a local search and a genetic algorithm for the distributed traveling salesman problem (TSP). In the distributed TSP, information regarding the cost function such as traveling costs between cities and cities to be visited are separately possessed by distributed parties and both are kept private each other. We propose a protocol that securely solves the distributed TSP by means of a combination of genetic algorithms and a cryptographic technique, called the secure multiparty computation. The computation time required for the privacy preserving optimization is practical at some level even when the city-size is more than a thousand.|Jun Sakuma,Shigenobu Kobayashi","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Mu√±oz Zavala,Arturo Hern√°ndez Aguirre,Enrique Ra√∫l Villa Diharce","57324|GECCO|2005|A case study of process facility optimization using discrete event simulation and genetic algorithm|Optimization problems such as resource allocation, job-shop scheduling, equipment utilization and process scheduling occur in a broad range of processing industries. This paper presents modeling, simulation and optimization of a port facility such that effective operational management is obtained. A GA base approach has been integrated with the port system model to optimize its operation. A case study of bulk material port handling systems is considered.|Keshav P. Dahal,Stuart Galloway,Graeme M. Burt,Jim R. McDonald,Ian Hopkins","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58021|GECCO|2007|GARS an improved genetic algorithm with reserve selection for global optimization|This paper investigates how genetic algorithms (GAs) can be improved to solve large-scale and complex problems more efficiently. First of all, we review premature convergence, one of the challenges confronted with when applying GAs to real-world problems. Next, some of the methods now available to prevent premature convergence and their intrinsic defects are discussed. A qualitative analysis is then done on the cause of premature convergence that is the loss of building blocks hosted in less-fit individuals during the course of evolution. Thus, we propose a new improver - GAs with Reserve Selection (GARS), where a reserved area is set up to save potential building blocks and a selection mechanism based on individual uniqueness is employed to activate the potentials. Finally, case studies are done in a few standard problems well known in the literature, where the experimental results demonstrate the effectiveness and robustness of GARS in suppressing premature convergence, and also an enhancement is found in global optimization capacity.|Yang Chen,Jinglu Hu,Kotaro Hirasawa,Songnian Yu","57309|GECCO|2005|Optimization of passenger car design for the mitigation of pedestrian head injury using a genetic algorithm|The problem of pedestrian injury is a significant one throughout the world. In , there were  pedestrian fatalities in Europe and  in the US. Significant advances have been made by automotive safety researchers and vehicle manufacturers to address this issue with respect to the design of vehicles, but the complex nature of pedestrian accident scenarios has resulted in great difficulty when using traditional statistical methods. Specifically, problems have been encountered when attempting to study the effects of individual parameters of vehicle front-end geometry on pedestrian head injury. This paper attempts to demonstrate the feasibility of applying the field of evolutionary computation to the problem of pedestrian safety by using a simple genetic algorithm to optimize the centre-line geometry of a car's front-end for the reduction of pedestrian head and thoracic injury. The fitness of each design is assessed by creating a multi-body mathematical model of the vehicle front and simulating impacts with models of different sized pedestrians, and ranking according to the combined injury scores.|Emma Carter,Steve Ebdon,Clive Neal-Sturgess","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["65408|AAAI|2005|State Agnostic Planning Graphs and the Application to Belief-Space Planning|Planning graphs have been shown to be a rich source of heuristic information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states. The naive technique enumerates the graphs individually. This is equivalent to solving an all-pairs shortest path problem by iterating a single-source algorithm over each source. We introduce a structure, the state agnostic planning graph, that directly, solves the all-pairs problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in classical planning. The more prominent application of tnis technique is in belief-space planning, where an optimization results in drastically improved theoretical complexity. Our experimental evaluation quantifies this performance boost. and demonstrates that heuristic belief-space progression planning using our technique is competitive with the state of t the art.|William Cushing,Daniel Bryce","16786|IJCAI|2007|An Extension to Conformant Planning Using Logic Programming|In this paper we extend the logic programming based conformant planner described in Son et al., a to allow it to work on planning problems with more complex descriptions of the initial states. We also compare the extended planner with other concurrent conformant planners.|A. Ricardo Morales,Phan Huy Tu,Tran Cao Son","16596|IJCAI|2007|Lambda Depth-First Proof Number Search and Its Application to Go|Thomsen's  search and Nagai's depth-first proof-number (DFPN) search are two powerful but very different ANDOR tree search algorithms. Lambda Depth-First Proof Number search (LDFPN) is a novel algorithm that combines ideas from both algorithms.  search can dramatically reduce a search space by finding different levels of threat sequences. DFPN employs the notion of proof and disproof numbers to expand nodes expected to be easiest to prove or disprove. The method was shown to be effective for many games. Integrating  order with proof and disproof numbers enables LDFPN to select moves more effectively, while preserving the efficiency of DFPN. LDFPN has been implemented for capturing problems in Go and is shown to be more efficient than DFPN and more robust than an algorithm based on classical  search.|Kazuki Yoshizoe,Akihiro Kishimoto,Martin M√ºller 0003","66142|AAAI|2007|A Logic of Agent Programs|We present a sound and complete logic for reasoning about Simple APL programs. Simple APL is a fragment of the agent programming language APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of Simple APL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.|Natasha Alechina,Mehdi Dastani,Brian Logan,John-Jules Ch. Meyer","43898|IEICE Transations|2007|A New Equivalence Relation of Logic Functions and Its Application in the Design of AND-OR-EXOR Networks|This paper presents a design method for AND-OR-EXOR three-level networks, where a single two-input exclusive-OR (EXOR) gate is used. The network realizes an EXOR of two sum-of-products expressions (EX-SOPs). The problem is to minimize the total number of products in the two sum-of-products expressions (SOPs). We introduce the notion of -equivalence of logic functions to develop exact minimization algorithms for EX-SOPs with up to five variables. We minimized all the NP-representative functions for up to five variables and showed that five-variable functions require  or fewer products in minimum EX-SOPs. For n-variable functions, minimum EX-SOPs require at most . n- (n  ) products. This upper bound is smaller than n-, which is the upper bound for SOPs. We also found that, for five-variable functions, on the average, minimum EX-SOPs require about % fewer literals than minimum SOPs.|Debatosh Debnath,Tsutomu Sasao","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","16512|IJCAI|2007|Collaborative Inductive Logic Programming for Path Planning|In distributed systems, learning does not necessarily involve the participation of agents directly in the inductive process itself. Instead, many systems frequently employ multiple instances of induction separately. In this paper, we develop and evaluate a new approach for learning in distributed systems that tightly integrates processes of induction between agents, based on inductive logic programming techniques. The paper's main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction. The new approach facilitates a systematic approach to the sharing of knowledge and invention of predicates only when required. We illustrate the approach using the well-known path planning problem and compare results empirically to (multiple instances of) single agent-based induction over varying distributions of data. Given a chosen path planning algorithm, our algorithm enables agents to combine their local knowledge in an effective way to avoid central control while significantly reducing communication costs.|Jian Huang,Adrian R. Pearce","16539|IJCAI|2007|Epistemic Reasoning in Logic Programs|Although epistemic logic programming has an enhanced capacity to handle complex incomplete information reasoning and represent agents' epistemic behaviours, it embeds a significantly higher computational complexity than non-disjunctive and disjunctive answer set programming. In this paper, we investigate some important properties of epistemic logic programs. In particular, we show that Lee and Lifschitz's result on loop formulas for disjunctive logic programs can be extended to a special class of epistemic logic programs. We also study the polysize model property for epistemic logic programs. Based on these discoveries, we identify two non-trivial classes of epistemic logic programs whose consistency checking complexity is reduced from PSPACE-complete to NP-complete and P -complete respectively. We observe that many important applications on epistemic representation fall into these two classes of epistemic logic programs.|Yan Zhang","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cede√±o","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman"],["42444|IEICE Transations|2005|Explicitly-Adaptive Time Delay Estimation for Wide-Band Signals|A new method of explicitly adaptive time delay estimation (EATDE) algorithm is proposed for estimating a varying time delay parameter. The proposed method is based on the Haar wavelet transform of cross-correlations. The proposed algorithm can be viewed as a gradient-based optimization of lowpass filtered cross-correlations, but requires less computational power. The algorithm shows a global convergence property for wide-band signals with uncorrelated noises. A convergence analysis including mean behavior, mean-square-error behavior, and steady-state error of delay estimate is given. Simulation results are also provided to demonstrate the performance of the proposed algorithm.|Doh-Hyoung Kim,Youngjin Park","43945|IEICE Transations|2007|Ultra-Wideband Time-of-Arrival and Angle-of-Arrival Estimation Using a Signal Model Based on Measurements|This paper presents an ultra wideband (UWB) channel sounding scheme with a technique for estimating time of arrival (TOA) and angle of arrival (AOA) using measurement signals. Since the power spectrum over the UWB bandwidth can be measured in advance, we propose a signal model using the measurement power spectrum to design the proper UWB signals model. This signal model is more similar to measurement signals than the flat spectrum model which is an ideal model. If more than three waves impinge on a receiver, we must determine the proper grouping of the elements of TOA vector and AOA vector. It is difficult to determine the grouping using only measurement signals because of many degradation factors. We also propose pairing the elements of TOA vector and that of AOA vector using correlation method based on measurement signals and the proposed signal model. This technique is available for more than the case of three paths if pairing the estimated TOAs and AOAs of measurement signals is not accurately determined. We evaluated the proposed techniques for a vector network analyzer (VNA) with a three-dimensional virtual antenna array.|Naohiko Iwakiri,Takehiko Kobayashi","42546|IEICE Transations|2005|Joint Estimation of Doppler Spread and Carrier Frequency Offset for OFDM Systems|In this letter, a joint estimation algorithm of Doppler spread and frequency offset for OFDM systems in Rayleigh fading channels is proposed based on the autocorrelation function between the last part of the received OFDM signal and its copy in guard interval. It is shown by computer simulations that the proposed algorithm performs well for different Doppler spread values and carrier frequency offsets.|Bin Sheng,Xiaohu You","42416|IEICE Transations|2005|An Adaptive Receiver for Power-Line Communications with the Estimation of Instantaneous Noise Power|The noise on power-lines is non-stationary, while the instantaneous noise power in different frequency bands are dependent. Under such noise environments, the instantaneous noise power in a frequency band can be estimated by observing the noise in other frequency bands. In this paper, we propose a receiver structure which uses the estimated instantaneous noise power in the decoding process and show its superiority in BER performance to conventional systems.|Yuichi Hirayama,Hiraku Okada,Takaya Yamazato,Masaaki Katayama","42598|IEICE Transations|2005|Peak Power Reduction Method Using Adaptive Peak Reduction Signal Level Control for OFDM Transmission Systems|Future broadband mobile communication systems are necessary to achieve the bit rates of  Mbits. Orthogonal Frequency Division Multiplexing (OFDM) transmission is an attractive technology because it can remove the influence of frequency selective fading in broadband transmission by adding a suitable guard interval to each OFDM symbol. However, peak-to-average power ratio (PAPR) is very large in OFDM transmission. In this paper, we propose a new PAPR reduction method which can be applied even when unusable bands are inside the system band. In the proposed method, peak reduction signals are generated by iterative signal processing only in the usable frequency band, and filtering to remove out-of-band components of the peak reduction signals is incorporated into the iterative signal processing. The results of computer simulation show that the proposed method can effectively reduce peak power without expanding the spectrum both outside the system band and into unusable bands inside the system band. By using the proposed method, the broadband mobile communication system with low peak power and high flexibility of frequency band use can be realized.|Shigeru Tomisato,Masaharu Hata","43851|IEICE Transations|2007|Low-Complexity Maximum Likelihood Frequency Offset Estimation for OFDM|This letter proposes a low-complexity estimation method of integer frequency offset in orthogonal frequency division multiplexing (OFDM) systems. The performance and complexity of the proposed method are compared with that of Morelli and Mengali's method based on maximum likelihood (ML) technique. The results show that the performance of the proposed method is comparable to that of M&M method with reduced complexity.|Hyun Yang,Hyoung-Kyu Song,Young-Hwan You","42810|IEICE Transations|2005|Self-Adaptive AlgorithmicArchitectural Design for Real-Time Low-Power Video Systems|With reference to video motion estimation in the framework of the new H.AVC video coding standard, this paper presents algorithmic and architectural solutions for the implementation of context-aware coprocessors in real-time, low-power embedded systems. A low-complexity context-aware controller is added to a conventional Full Search (FS) motion estimation engine. While the FS coprocessor is working, the context-aware controller extracts from the intermediate processing results information related to the input signal statistics in order to automatically configure the coprocessor itself in terms of search area size and number of reference frames thus unnecessary computations and memory accesses can be avoided. The achieved complexity saving factor ranges from . to  depending on the input signal while keeping unaltered performance in terms of motion estimation accuracy. The increased efficiency is exploited both for (i) processing time reduction in case of software implementation on a programmable platform (ii) power consumption reduction in case of dedicated hardware implementation in CMOS technology.|Luca Fanucci,Sergio Saponara,Massimiliano Melani,Pierangelo Terreni","42522|IEICE Transations|2005|A Simple Bit Allocation Scheme Based on Adaptive Coding for MIMO-OFDM Systems with V-BLAST Detector|We present a simple bit allocation scheme based on adaptive coding for MIMO-OFDM (Multiple Input Multiple Output - Orthogonal Frequency Division Multiplexing) systems with V-BLAST (Vertical-Bell laboratories LAyered Space-Time) detector. The proposed scheme controls the code rate of the channel coding and assigns the same modulation and coding to the set of selected sub-channels, which greatly reduces the feedback burden while achieving good performance. Simulation results show that the proposed scheme with minimal feedback provides significant performance improvement over other systems.|Jongwon Kim,Sanhae Kim,Min-Cheol Hong,Yoan Shin","42615|IEICE Transations|2005|Accelerated Adaptive Algorithms with Application to Direction-of-Arrival Estimation by Subspace Tracking|Direction-of-arrival (DOA) estimation based on subspace methods has collected much interest over a few decades, and adaptive DOA estimation with rapidly changing parameters will be necessary for wireless communications. This paper is concerned with a new subspace tracking scheme by using an accelerated LMS and RLS algorithms for time-varying parameters. The proposed accelerated adaptive algorithms are based on the internal model principle by approximately expressing the changing parameters by an expansion of polynomial time functions. Thus its application to DOA estimation based on the MUSIC and MODE schemes is presented and the effectiveness is validated in numerical simulations.|Shohei Kikuchi,Akira Sano","58218|GECCO|2007|Adaptive variance scaling in continuous multi-objective estimation-of-distribution algorithms|Recent research into single-objective continuous Estimation-of-Distribution Algorithms (EDAs)has shown that when maximum-likelihood estimationsare used for parametric distributions such as thenormal distribution, the EDA can easily suffer frompremature convergence. In this paper we argue thatthe same holds for multi-objective optimization.Our aim in this paper is to transfer a solutioncalled Adaptive Variance Scaling (AVS) from thesingle-objective case to the multi-objectivecase. To this end, we zoom in on an existing EDAfor continuous multi-objective optimization, theMIDEA, which employs mixturedistributions. We propose a means to combine AVSwith the normal mixture distribution, as opposedto the single normal distribution for which AVS wasintroduced. In addition, we improve the AVS schemeusing the Standard-Deviation Ratio(SDR) trigger. Intuitively put, variance scalingis triggered by the SDR trigger only ifimprovements are found to be far awayfrom the mean. For the multi-objective case,this addition is important to keep the variancefrom being scaled to excessively large values.From experiments performed on five well-knownbenchmark problems, the addition of SDR andAVS is found to enlarge the class of problems thatcontinuous multi-objective EDAs can solve reliably.|Peter A. N. Bosman,Dirk Thierens"],["42830|IEICE Transations|2005|Robust Speech Recognition Using Discrete-Mixture HMMs|This paper introduces new methods of robust speech recognition using discrete-mixture HMMs (DMHMMs). The aim of this work is to develop robust speech recognition for adverse conditions that contain both stationary and non-stationary noise. In particular, we focus on the issue of impulsive noise, which is a major problem in practical speech recognition system. In this paper, two strategies were utilized to solve the problem. In the first strategy, adverse conditions are represented by an acoustic model. In this case, a large amount of training data and accurate acoustic models are required to present a variety of acoustic environments. This strategy is suitable for recognition in stationary or slow-varying noise conditions. The second is based on the idea that the corrupted frames are treated to reduce the adverse effect by compensation method. Since impulsive noise has a wide variety of features and its modeling is difficult, the second strategy is employed. In order to achieve those strategies, we propose two methods. Those methods are based on DMHMM framework which is one type of discrete HMM (DHMM). First, an estimation method of DMHMM parameters based on MAP is proposed aiming to improve trainability. The second is a method of compensating the observation probabilities of DMHMMs by threshold to reduce adverse effect of outlier values. Observation probabilities of impulsive noise tend to be much smaller than those of normal speech. The motivation in this approach is that flooring the observation probability reduces the adverse effect caused by impulsive noise. Experimental evaluations on Japanese LVCSR for read newspaper speech showed that the proposed method achieved the average error rate reduction of .% in impulsive noise conditions. Also the experimental results in adverse conditions that contain both stationary and impulsive noises showed that the proposed method achieved the average error rate reduction of .%.|Tetsuo Kosaka,Masaharu Katoh,Masaki Kohda","44196|IEICE Transations|2007|Fast Concatenative Speech Synthesis Using Pre-Fused Speech Units Based on the Plural Unit Selection and Fusion Method|We have previously developed a concatenative speech synthesizer based on the plural speech unit selection and fusion method that can synthesize stable and human-like speech. In this method, plural speech units for each speech segment are selected using a cost function and fused by averaging pitch-cycle waveforms. This method has a large computational cost, but some platforms require a speech synthesis system that can work within limited hardware resources. In this paper, we propose an offline unit fusion method that reduces the computational cost. In the proposed method, speech units are fused in advance to make a pre-fused speech unit database. At synthesis time, a speech unit for each segment is selected from the pre-fused speech unit database and the speech waveform is synthesized by applying prosodic modification and concatenation without the computationally expensive unit fusion process. We compared several algorithms for constructing the pre-fused speech unit database. From the subjective and objective evaluations, the effectiveness of the proposed method is confirmed by the results that the quality of synthetic speech of the offline unit fusion method with  MB database is close to that of the online unit fusion method with  MB JP database and is slightly lower to that of the  MB US database, while the computational time is reduced by %. We also show that the frequency-weighted VQ-based method is effective for construction of the pre-fused speech unit database.|Masatsune Tamura,Tatsuya Mizutani,Takehiko Kagoshima","42345|IEICE Transations|2005|Recent Progress in Corpus-Based Spontaneous Speech Recognition|This paper overviews recent progress in the development of corpus-based spontaneous speech recognition technology. Although speech is in almost any situation spontaneous, recognition of spontaneous speech is an area which has only recently emerged in the field of automatic speech recognition. Broadening the application of speech recognition depends crucially on raising recognition performance for spontaneous speech. For this purpose, it is necessary to build large spontaneous speech corpora for constructing acoustic and language models. This paper focuses on various achievements of a Japanese -year national project \"Spontaneous Speech Corpus and Processing Technology\" that has recently been completed. Because of various spontaneous-speech specific phenomena, such as filled pauses, repairs, hesitations, repetitions and disfluencies, recognition of spontaneous speech requires various new techniques. These new techniques include flexible acoustic modeling, sentence boundary detection, pronunciation modeling, acoustic as well as language model adaptation, and automatic summarization. Particularly automatic summarization including indexing, a process which extracts important and reliable parts of the automatic transcription, is expected to play an important role in building various speech archives, speech-based information retrieval systems, and human-computer dialogue systems.|Sadaoki Furui","42348|IEICE Transations|2005|Applying Sparse KPCA for Feature Extraction in Speech Recognition|This paper presents an analysis of the applicability of Sparse Kernel Principal Component Analysis (SKPCA) for feature extraction in speech recognition, as well as, a proposed approach to make the SKPCA technique realizable for a large amount of training data, which is an usual context in speech recognition systems. Although the KPCA (Kernel Principal Component Analysis) has proved to be an efficient technique for being applied to speech recognition, it has the disadvantage of requiring training data reduction, when its amount is excessively large. This data reduction is important to avoid computational unfeasibility andor an extremely high computational burden related to the feature representation step of the training and the test data evaluations. The standard approach to perform this data reduction is to randomly choose frames from the original data set, which does not necessarily provide a good statistical representation of the original data set. In order to solve this problem a likelihood related re-estimation procedure was applied to the KPCA framework, thus creating the SKPCA, which nevertheless is not realizable for large training databases. The proposed approach consists in clustering the training data and applying to these clusters a SKPCA like data reduction technique generating the reduced data clusters. These reduced data clusters are merged and reduced in a recursive procedure until just one cluster is obtained, making the SKPCA approach realizable for a large amount of training data. The experimental results show the efficiency of SKPCA technique with the proposed approach over the KPCA with the standard sparse solution using randomly chosen frames and the standard feature extraction techniques.|Amaro Lima,Heiga Zen,Yoshihiko Nankaku,Keiichi Tokuda,Tadashi Kitamura,Fernando Gil Resende","44038|IEICE Transations|2007|Speech Enhancement Based on MAP Estimation Using a Variable Speech Distribution|In this paper, a novel speech enhancement algorithm based on the MAP estimation is proposed. The proposed speech enhancer adaptively changes the speech spectral density used in the MAP estimation according to the sum of the observed power spectra. In a speech segment, the speech spectral density approaches to Rayleigh distribution to keep the quality of the enhanced speech. While in a non-speech segment, it approaches to an exponential distribution to reduce noise effectively. Furthermore, when the noise is super-Gaussian, we modify the width of Gaussian so that the Gaussian model with the modified width approximates the distribution of the super-Gaussian noise. This technique is effective in suppressing residual noise well. From computer experiments, we confirm the effectiveness of the proposed method.|Yuta Tsukamoto,Arata Kawamura,Youji Iiguni","42822|IEICE Transations|2005|Alaryngeal Speech Enhancement Using Pattern Recognition Techniques|An alaryngeal speech enhancement system is proposed to improve the intelligibility and quality of speech signals generated by an artificial larynx transducer (ALT). Proposed system identifies the voiced segments of alaryngeal speech signal, by using pattern recognition methods, and replaces these by their equivalent voiced segments of normal speech. Evaluation results show that proposed system provides a fairly good improvement of the quality and intelligibility of ALT generated speech.|Gualberto Aguilar-Torres,Mariko Nakano-Miyatake,H√©ctor M. P√©rez Meana","42322|IEICE Transations|2005|Feature Extraction with Combination of HMT-Based Denoising and Weighted Filter Bank Analysis for Robust Speech Recognition|In this paper, we propose a new feature extraction method that combines both HMT-based denoising and weighted filter bank analysis for robust speech recognition. The proposed method is made up of two stages in cascade. The first stage is denoising process based on the wavelet domain Hidden Markov Tree model, and the second one is the filter bank analysis with weighting coefficients obtained from the residual noise in the first stage. To evaluate performance of the proposed method, recognition experiments were carried out for additive white Gaussian and pink noise with signal-to-noise ratio from  dB to  dB. Experiment results demonstrate the superiority of the proposed method to the conventional ones.|Sungyun Jung,Jong-Mok Son,Keun-Sung Bae","42328|IEICE Transations|2005|Speech Recognition Using Finger Tapping Timings|Behavioral synchronization between speech and finger tapping provides a novel approach to improving speech recognition accuracy. We combine a sequence of finger tapping timings recorded alongside an utterance using two distinct methods in the first method, HMM state transition probabilities at the word boundaries are controlled by the timing of the finger tapping in the second, the probability (relative frequency) of the finger tapping is used as a 'feature' and combined with MFCC in a HMM recognition system. We evaluate these methods through connected digit recognition under different noise conditions (AURORA-J). Leveraging the synchrony between speech and finger tapping provides a % relative improvement in connected digit recognition experiments.|Hiromitsu Ban,Chiyomi Miyajima,Katsunobu Itou,Kazuya Takeda,Fumitada Itakura","42451|IEICE Transations|2005|Harmonicity Based Dereverberation for Improving Automatic Speech Recognition Performance and Speech Intelligibility|A speech signal captured by a distant microphone is generally smeared by reverberation, which severely degrades both the speech intelligibility and Automatic Speech Recognition (ASR) performance. Previously, we proposed a single-microphone dereverberation method, named \"Harmonicity based dEReverBeration (HERB).\" HERB estimates the inverse filter for an unknown room transfer function by utilizing an essential feature of speech, namely harmonic structure. In previous studies, improvements in speech intelligibility was shown solely with spectrograms, and improvements in ASR performance were simply confirmed by matched condition acoustic model. In this paper, we undertook a further investigation of HERB's potential as regards to the above two factors. First, we examined speech intelligibility by means of objective indices. As a result, we found that HERB is capable of improving the speech intelligibility to approximately that of clean speech. Second, since HERB alone could not improve the ASR performance sufficiently, we further analyzed the HERB mechanism with a view to achieving further improvements. Taking the analysis results into account, we proposed an appropriate ASR configuration and conducted experiments. Experimental results confirmed that, if HERB is used with an ASR adaptation scheme such as MLLR and a multicondition acoustic model, it is very effective for improving ASR performance even in unknown severely reverberant environments.|Keisuke Kinoshita,Tomohiro Nakatani,Masato Miyoshi","42363|IEICE Transations|2005|An Unsupervised Speaker Adaptation Method for Lecture-Style Spontaneous Speech Recognition Using Multiple Recognition Systems|This paper describes an accurate unsupervised speaker adaptation method for lecture style spontaneous speech recognition using multiple LVCSR systems. In an unsupervised speaker adaptation framework, the improvement of recognition performance by adapting acoustic models remarkably depends on the accuracy of labels such as phonemes and syllables. Therefore, extraction of the adaptation data guided by confidence measure is effective for unsupervised adaptation. In this paper, we looked for the high confidence portions based on the agreement between two LVCSR systems, adapted acoustic models using the portions attached with high accurate labels, and then improved the recognition accuracy. We applied our method to the Corpus of Spontaneous Japanese (CSJ) and the method improved the recognition rate by about .% in comparison with a traditional method.|Seiichi Nakagawa,Tomohiro Watanabe,Hiromitsu Nishizaki,Takehito Utsuro"],["80549|VLDB|2005|RankSQL Supporting Ranking Queries in Relational Database Management Systems|Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.|Chengkai Li,Mohamed A. Soliman,Kevin Chen-Chuan Chang,Ihab F. Ilyas","42966|IEICE Transations|2005|An RBAC-Based Access Control Model for Object-Oriented Systems Offering Dynamic Aspect Features|This paper proposes a model for access control within object-oriented systems. The model is based on RBAC (role-based access control) and is called DRBAC (dynamic RBAC). Although RBAC is powerful in access control, the original design of RBAC required that user-role assignments and role-permission assignments should be handled statically (i.e., the assignments should be handled by human beings). Nevertheless, the following dynamic features are necessary in access control within a software system (a) managing dynamic role switching, (b) avoiding Trojan horses, (c) managing role associations, and (d) handling dynamic role creation and deletion. DRBAC offers the dynamic features. This paper proposes DRBAC.|Shih-Chien Chou","44163|IEICE Transations|2007|Managing Contradictions in Multi-Agent Systems|The specification of a Multi-Agent System (MAS) involves the identification of a large number of entities and their relationships. This is a non-trivial task that requires managing different views of the system. Many problems concerning this issue originate in the presence of contradictory goals and tasks, inconsistencies, and unexpected behaviours. Such troublesome configurations should be detected and prevented during the development process in order to study alternative ways to cope with them. In this paper, we present methods and tools that support the management of contradictions during the analysis and design of MAS. Contradiction management in MAS has to consider both individual (i.e. agent) and social (i.e. organization) aspects, and their dynamics. Such issues have already been considered in social sciences, and more concretely in the Activity Theory, a social framework for the study of interactions in activity systems. Our approach applies knowledge from Activity Theory in MAS, especially its base of contradiction patterns. That requires a formalization of this social theory in order to be applicable in a software engineering context and its adaptation to agent-oriented methodologies. Then, it will be possible to check the occurrence of contradiction patterns in a MAS specification and provide solutions to those situations. This technique has been validated by implementing an assistant for the INGENIAS Development Kit and has been tested with several case studies. This paper shows part of one of these experiments for a web application.|Rub√©n Fuentes-Fern√°ndez,Jorge J. G√≥mez-Sanz,Juan Pav√≥n","16030|IJCAI|2005|Language Learning in Multi-Agent Systems|We present the problem of learning to communicate in decentralized and stochastic environments, analyzing it formally in a decision-theoretic context and illustrating the concept experimentally. Our approach allows agents to converge upon coordinated communication and action over time.|Martin Allen,Claudia V. Goldman,Shlomo Zilberstein","42918|IEICE Transations|2005|A License Management Protocol for Protecting User Privacy and Digital Contents in Digital Rights Management Systems|Although the Digital Rights Management (DRM) systems have been rapidly developed to protect copyrights, they have not considered user privacy because they regard this as an unnecessary element in achieving their goals. However, the protection of user privacy becomes one of the most important issues in DRM systems as the number of people who suffer from accidents caused by the infringement of individual information dramatically increases. This paper suggests a license management protocol which is a more powerful protocol to protect individual information in DRM. To protect the exposure of information of user identification, the proposed protocol uses alias like a TID and a token instead of the identity of content users. Due to using alias, this protocol can guarantee the anonymity of content users. Also, it can prevent the leakage of individual information through encryption of usage information. In this way, it can protect the privacy of content users.|Bok-Nyong Park,Wonjun Lee,Jae-Won Kim","80474|VLDB|2005|NILE-PDT A Phenomenon Detection and Tracking Framework for Data Stream Management Systems|In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.|Mohamed H. Ali,Walid G. Aref,Raja Bose,Ahmed K. Elmagarmid,Abdelsalam Helal,Ibrahim Kamel,Mohamed F. Mokbel","42948|IEICE Transations|2005|Document Image Retrieval for QA Systems Based on the Density Distributions of Successive Terms|Question answering (QA) is the task of retrieving an answer in response to a question by analyzing documents. Although most of the efforts in developing QA systems are devoted to dealing with electronic text, we consider it is also necessary to develop systems for document images. In this paper, we propose a method of document image retrieval for such QA systems. Since the task is not to retrieve all relevant documents but to find the answer somewhere in documents, retrieval should be precision oriented. The main contribution of this paper is to propose a method of improving precision of document image retrieval by taking into account the co-occurrence of successive terms in a question. The indexing scheme is based on two-dimensional distributions of terms and the weight of co-occurrence is measured by calculating the density distributions of terms. The proposed method was tested by using  pages of documents about the major league baseball with  questions and found that it is superior to the baseline method proposed by the authors.|Koichi Kise,Shota Fukushima,Keinosuke Matsumoto","57441|GECCO|2005|Emergence of communication in competitive multi-agent systems a pareto multi-objective approach|In this paper we investigate the emergence of communication in competitive multi-agent systems. A competitive environment is created with two teams of agents competing in an exploration task the quickest team to explore the largest area wins. One team uses indirect communication and is controlled by an artificial neural network evolved using a Pareto multi-objective approach. The second team uses direct communication and a fixed strategy for exploration. A comparison is made between agents with and without communication. Results show that as the fitness function vary differing exploration strategies emerge. Experiments with communication produced cooperative strategies while the experiments without communication produced effective strategies but with individuals acting independently.|Michelle McPartland,Stefano Nolfi,Hussein A. Abbass","58137|GECCO|2007|Classifier systems that compute action mappings|The learning in a niche based learning classifier system depends both on the complexity of the problem space and on the number of available actions. In this paper, we introduce a version of XCS with computed actions, briefly XCSCA, that can be applied to problems involving a large number of actions. We report experimental results showing that XCSCA can evolve accurate and compact representations of binary functions which would be challenging for typical learning classifier system models.|Pier Luca Lanzi,Daniele Loiacono","42783|IEICE Transations|2005|A Coordinator for Workflow Management Systems with Information Access Control|This paper proposes a coordinator for workflow management systems (WFMSs). It is a basic module for developing WFMSs. It is also a coordinator to coordinate multiple WFMSs. The coordinator provides functions to facilitate executing workflows and to ensure secure access of workflow information. Facilitating workflow execution is well-known, but ensuring secure access of workflow information is identified as important only recently. Although many models ensure secure workflow information access, they fail to offer the features we need. We thus developed a new model for the control. This paper presents the coordinator its access control model.|Shih-Chien Chou,Chien-Jung Wu"],["16481|IJCAI|2007|DiPRA Distributed Practical Reasoning Architecture|DiPRA (Distributed Practical Reasoning Architecture) implements the main principles of practical reasoning via the distributed action selection paradigm. We introduce and motivate the underlying theoretical and computational peculiarities of DiPRA and we describe its components, also providing as a case study a guards-and-thieves task.|Giovanni Pezzulo,Gianguglielmo Calvi,Cristiano Castelfranchi","16230|IJCAI|2005|Networked Distributed POMDPs A Synergy of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present a distributed policy generation algorithm that performs local search.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","16603|IJCAI|2007|Extracting Chatbot Knowledge from Online Discussion Forums|This paper presents a novel approach for extracting high-quality thread-title, reply pairs as chat knowledge from online discussion forums so as to efficiently support the construction of a chatbot for a certain domain. Given a forum, the high-quality thread-title, reply pairs are extracted using a cascaded framework. First, the replies logically relevant to the thread title of the root message are extracted with an SVM classifier from all the replies, based on correlations such as structure and content. Then, the extracted thread-title, reply pairs are ranked with a ranking SVM based on their content qualities. Finally, the Top-N thread-title, reply pairs are selected as chatbot knowledge. Results from experiments conducted within a movie forum show the proposed approach is effective.|Jizhou Huang,Ming Zhou,Dan Yang","66218|AAAI|2007|Automatic Synthesis of a Global Behavior from Multiple Distributed Behaviors|We consider the problem of synthesizing a team of local behavior controllers to realize a fully controllable target behavior from a set of available partially controllable behaviors that execute distributively within a shared partially predictable, but fully observable, environment. Available behaviors stand for existing distributed components and are represented with (finite) nondeterministic transition systems. The target behavior is assumed to be fully deterministic and stands for the collective behavior that the system as a whole needs to guarantee. We formally define the problem within a general framework, characterize its computational complexity, and propose techniques to actually generate a solution. Also, we investigate the relationship between the distributed solutions and the centralized ones, in which a single global controller is conceivable.|Sebastian Sardi√±a,Fabio Patrizi,Giuseppe De Giacomo","16635|IJCAI|2007|From Generic Knowledge to Specific Reasoning for Medical Image Interpretation Using Graph based Representations|In several domains of spatial reasoning, such as medical image interpretation, spatial relations between structures play a crucial role since they are less prone to variability than intrinsic properties of structures. Moreover, they constitute an important part of available knowledge. We show in this paper how this knowledge can be appropriately represented by graphs and fuzzy models of spatial relations, which are integrated in a reasoning process to guide the recognition of individual structures in images. However pathological cases may deviate substantially from generic knowledge. We propose a method to adapt the knowledge representation to take into account the influence of the pathologies on the spatial organization of a set of structures, based on learning procedures. We also propose to adapt the reasoning process, using graph based propagation and updating.|Jamal Atif,C√©line Hudelot,Geoffroy Fouquier,Isabelle Bloch,Elsa D. Angelini","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","16712|IJCAI|2007|Extracting Keyphrases to Represent Relations in Social Networks from Web|Social networks have recently garnered considerable interest. With the intention of utilizing social networks for the Semantic Web, several studies have examined automatic extraction of social networks. However, most methods have addressed extraction of the strength of relations. Our goal is extracting the underlying relations between entities that are embedded in social networks. To this end, we propose a method that automatically extracts labels that describe relations among entities. Fundamentally, the method clusters similar entity pairs according to their collective contexts in Web documents. The descriptive labels for relations are obtained from results of clustering. The proposed method is entirely unsupervised and is easily incorporated with existing social network extraction methods. Our experiments conducted on entities in researcher social networks and political social networks achieved clustering with high precision and recall. The results showed that our method is able to extract appropriate relation labels to represent relations among entities in the social networks.|Junichiro Mori,Mitsuru Ishizuka,Yutaka Matsuo","16197|IJCAI|2005|From knowledge-based programs to graded belief-based programs part II off-line reasoning|Belief-based programs generalize knowledgebased programs Fagin et al.,  by allowing for incorrect beliefs, unreliable observations, and branching conditions that refer to implicit graded beliefs, such as in \"while my belief about the direction to the railway station is not strong enough do ask someone\". We show how to reason off-line about the possible executions of a belief-based program, which calls for introducing second-order uncertainty in the model.|No√´l Laverny,J√©r√¥me Lang","16287|IJCAI|2005|Aspects of Distributed and Modular Ontology Reasoning|We investigate a formalism for reasoning with multiple local ontologies, connected by directional semantic mappings. We propose () a relatively small change of semantics which localizes inconsistency (thereby making unnecessary global satisfiability checks), and preserves directionality of \"knowledge import\" () a characterization of inferences using a fixed-point operator, which can form the basis of a cache-based implementation for local reasoners () a truly distributed tableaux algorithm for cases when the local reasoners use subsets of SHIQ. Throughout, we indicate the applicability of the results to several recent proposals for knowledge representation and reasoning that support modularity, scalability and distributed reasoning.|Luciano Serafini,Alexander Borgida,Andrei Tamilin"],["57311|GECCO|2005|Multi-chromosomal genetic programming|This paper introduces an evolutionary algorithm which uses multiple chromosomes to evolve solutions to a symbolic regression problem. Inspiration for this algorithm is provided by the existence of multiple chromosomes in natural evolution, particularly in plants. A multi-chromosomal system usually requires a dominance system and subsequently dominance in nature and in previous artificial evolutionary systems has also been considered. An implementation of a multi-chromosomal system is presented with initial results which support the use of multi-chromosomal techniques in evolutionary algorithms.|Rachel Cavill,Stephen L. Smith,Andrew M. Tyrrell","58113|GECCO|2007|Best SubTree genetic programming|The result of the program encoded into a Genetic Programming(GP) tree is usually returned by the root of that tree. However, this is not a general strategy. In this paper we present and investigate a new variant where the best subtree is chosen to provide the solution of the problem. The other nodes (not belonging to the best subtree) are deleted. This will reduce the size of the chromosome in those cases where its best subtree is different from the entire tree. We have tested this strategy on a wide range of regression and classification problems. Numerical experiments have shown that the proposed approach can improve both the search speed and the quality of results.|Oana Muntean,Laura Diosan,Mihai Oltean","58114|GECCO|2007|Coevolution of intelligent agents using cartesian genetic programming|A coevolutionary competitive learning environment for two antagonistic agents is presented. The agents are controlled by a new kind of computational network based on a compartmentalised model of neurons. We have taken the view that the genetic basis of neurons is an important  and neglected aspect of previous approaches. Accordingly, we have defined a collection of chromosomes representing various aspects of the neuron soma, dendrites and axon branches, and synaptic connections. Chromosomes are represented and evolved using a form of genetic programming known as Cartesian Genetic Programming. The network formed by running the chromosomal programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change in response to environmental interactions. The idea of this paper is to demonstrate the importance of the genetic transfer of learned experience and life time learning. The learning is a consequence of the complex dynamics produced as a result of interaction (coevolution) between two intelligent agents. Our results show that both agents exhibit interesting learning capabilities.|Gul Muhammad Khan,Julian Francis Miller,David M. Halliday","58000|GECCO|2007|Dynamic populations and length evolution key factors for analyzing fault tolerance on parallel genetic programming|This paper presents an experimental research on the size of individuals when fixed and dynamic size populationsare employed with Genetic Programming (GP). We propose an improvement to the Plague operator (PO), that we have called Random Plague (RPO). Then by further studies based on the RPO results we analyzed the Fault Tolerance onParallel Genetic Programming.|Daniel Lombra√±a Gonzalez,Francisco Fern√°ndez de Vega","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57550|GECCO|2005|Learned mutation strategies in genetic programming for evolution and adaptation of simulated snakebot|In this work we propose an approach of incorporating learned mutation strategies (LMS) in genetic programming (GP) employed for evolution and adaptation of locomotion gaits of simulated snake-like robot (Snakebot). In our approach the LMS are implemented via learned probabilistic context-sensitive grammar (LPCSG). The LPCSG is derived from the originally defined context-free grammar, which usually expresses the syntax of genetic programs in canonical GP. Applying LMS implies that the probabilities of applying each of particular production rules in LPCGS during the mutation depend on the context. These probabilities are learned from the aggregated reward values obtained from the parsed syntax of the evolved best-of-generation Snakebots. Empirically obtained results verify that LMS contributes to the improvement of computational effort of both (i) the evolution of the fastest possible locomotion gaits for various fitness conditions and (ii) the adaptation of these locomotion gaits to challenging environment and degraded mechanical abilities of Snakebot. In all of the cases considered in this study, the locomotion gaits, evolved and adapted employing GP with LMS feature higher velocity and are obtained faster than with canonical GP.|Ivan Tanev","58106|GECCO|2007|Linear genetic programming of metaheuristics|We suggest a flavour of linear Genetic Programming indomain-specific languages that acts as a hyperheuristic (HH).|Robert E. Keller,Riccardo Poli","57881|GECCO|2007|Evolution of non-uniform cellular automata using a genetic algorithm diversity and computation|We used a genetic algorithm to evaluate the cost  benefit of diversity in evolving sets of rules for non-uniform cellular automata solving the density classification problem.|Forrest Sondahl,William Rand","57279|GECCO|2005|Evolutionary tree genetic programming|We introduce a clustering-based method of subpopulation management in genetic programming (GP) called Evolutionary Tree Genetic Programming (ETGP). The biological motivation behind this work is the observation that the natural evolution follows a tree-like phylogenetic pattern. Our goal is to simulate similar behavior in artificial evolutionary systems such as GP. To test our model we use three common GP benchmarks the Ant Algorithm, -Multiplexer, and Parity problems.The performance of the ETGP system is empirically compared to those of the GP system. Code size and variance are consistently reduced by a small but statistically significant percentage, resulting in a slight speedup in the Ant and -Multiplexer problems, while the same comparisons on the Parity problem are inconclusive.|J√°n Antol√≠k,William H. Hsu","57484|GECCO|2005|Backward-chaining genetic programming|This paper presents a backward-chaining version of GP.|Riccardo Poli,William B. Langdon"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","16726|IJCAI|2007|Combining Learning and Word Sense Disambiguation for Intelligent User Profiling|Understanding user interests from text documents can provide support to personalized information recommendation services. Typically, these services automatically infer the user profile, a structured model of the user interests, from documents that were already deemed relevant by the user. Traditional keyword-based approaches are unable to capture the semantics of the user interests. This work proposes the integration of linguistic knowledge in the process of learning semantic user profiles that capture concepts concerning user interests. The proposed strategy consists of two steps. The first one is based on a word sense disambiguation technique that exploits the lexical database WordNet to select, among all the possible meanings (senses) of a polysemous word, the correct one. In the second step, a nave Bayes approach learns semantic sense-based user profiles as binary text classifiers (user-likes and user-dislikes) from disambiguated documents. Experiments have been conducted to compare the performance obtained by keyword-based profiles to that obtained by sense-based profiles. Both the classification accuracy and the effectiveness of the ranking imposed by the two different kinds of profile on the documents to be recommended have been considered. The main outcome is that the classification accuracy is increased with no improvement on the ranking. The conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those documents whose classification score is close to the likesdislikes threshold (the items for which the classification is highly uncertain).|Giovanni Semeraro,Marco Degemmis,Pasquale Lops,Pierpaolo Basile","57362|GECCO|2005|Feature influence for evolutionary learning|This paper presents an approach that deals with the feature selection problem, and includes two main aspects first, the selection is done during the evolutionary learning process, i.e., it is a dynamic approach and second, the selection is local, i.e., the algorithm selects the best features from the best space region to learn at a given time of the exploration process. While the traditional feature selection is based on the attribute relevance, our approach is based on a new concept, called feature influence, which is aware of the dynamics and locality of the concept. The feature influence provides a measure of the attribute relevance at a certain instant of the evolutionary learning process, since it depends on each generation. Experimental results have been obtained by comparing an EA--based supervised learning algorithm to its modified version to include the concept approached. The results show an excellent performance, as the new adapted algorithm achieves the same classification results while using less rules, less conditions in rules and much less generations. The experiments include the statistical significance of the improvement over a set of sixteen datasets from the UCI repository.|Ra√∫l Gir√°ldez,Jes√∫s S. Aguilar-Ruiz","58217|GECCO|2007|Learning and anticipation in online dynamic optimization with evolutionary algorithms the stochastic case|The focus of this paper is on how to design evolutionaryalgorithms (EAs) for solving stochastic dynamicoptimization problems online, i.e.as time goes by.For a proper design, the EA must not only be capableof tracking shifting optima, it must also take intoaccount the future consequences of the evolveddecisions or actions. A previousframework describes how to build such EAs in thecase of non-stochastic problems. Most real-worldproblems however are stochastic. In this paper weshow how this framework can be extended to properlytackle stochasticity. We point out how thisnaturally leads to evolving strategiesrather than explicit decisions. We formalizeour approach in a new framework. The newframework and the various sourcesof problem-difficulty at hand are illustratedwith a running example. We also apply ourframework to inventory management problems, an importantreal-world application area in logistics. Our results show,as a proof of principle, the feasibility and benefitsof our novel approach.|Peter A. N. Bosman,Han La Poutr√©","66186|AAAI|2007|Learning Graphical Model Structure Using L-Regularization Paths|Sparsity-promoting L-regularization has recently been succesfully used to learn the structure of undirected graphical models. In this paper, we apply this technique to learn the structure of directed graphical models. Specifically, we make three contributions. First, we show how the decomposability of the MDL score, plus the ability to quickly compute entire regularization paths, allows us to efficiently pick the optimal regularization parameter on a per-node basis. Second, we show how to use L variable selection to select the Markov blanket, before a DAG search stage. Finally, we show how L variable selection can be used inside of an order search algorithm. The effectiveness of these L-based approaches are compared to current state of the art methods on  datasets.|Mark W. Schmidt,Alexandru Niculescu-Mizil,Kevin P. Murphy","58139|GECCO|2007|Ensemble learning for free with evolutionary algorithms|Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.|Christian Gagn√©,Mich√®le Sebag,Marc Schoenauer,Marco Tomassini","58175|GECCO|2007|Learning building block structure from crossover failure|In the classical binary genetic algorithm, although crossover within a building block (BB) does not always cause a decrease in fitness, any decrease in fitness results from the destruction of some building blocks, in problems where such structures are well defined, such as those considered here. Those crossovers that cause both offspring to be worse, or one to be worse and one unchanged, are here designated as failed crossovers. Counting the failure frequency of single-point crossovers performed at each locus reveals something of the BB structure. Guided by the failure record, GA operators could choose appropriate points for crossover, in order to work moreefficiently and effectively. Experiments on test functions RoyalRoad R and R, Holland's Royal Road Challenge function and H-IFF functions show that such a guided operator improves performance. While many methods exist to discover building blocks, this \"quick-and-dirty\" method can sketch the linkage nearly \"for free\", requiring very little extra computation.|Zhenhua Li,Erik D. Goodman","42327|IEICE Transations|2005|Unsupervised Word-Sense Disambiguation Using Bilingual Comparable Corpora|An unsupervised method for word-sense disambiguation using bilingual comparable corpora was developed. First, it extracts word associations, i.e., statistically significant pairs of associated words, from the corpus of each language. Then, it aligns word associations by consulting a bilingual dictionary and calculates correlation between senses of a target polysemous word and its associated words, which can be regarded as clues for identifying the sense of the target word. To overcome the problem of disparity of topical coverage between corpora of the two languages as well as the problem of ambiguity in word-association alignment, an algorithm for iteratively calculating a sense-vs.-clue correlation matrix for each target word was devised. Word-sense disambiguation for each instance of the target word is done by selecting the sense that maximizes the score, i.e., a weighted sum of the correlations between each sense and clues appearing in the context of the instance. An experiment using Wall Street Journal and Nihon Keizai Shimbun corpora together with the EDR bilingual dictionary showed that the new method has promising performance namely, the Fmeasure of its sense selection was .% compared to a baseline of .%. The developed method will possibly be extended into a fully unsupervised method that features automatic division and definition of word senses.|Hiroyuki Kaji,Yasutsugu Morimoto","65540|AAAI|2005|Automatically Acquiring Domain Knowledge For Adaptive Game AI Using Evolutionary Learning|Game AI is the decision-making process of computer-controlled opponents in computer games. Adaptive game AI can improve the entertainment value of computer games. It allows computer-controlled opponents to automatically fix weaknesses in the game AI and respond to changes in human-player tactics. Dynamic scripting is a recently developed approach for adaptive game AI that learns which tactics (i.e., action sequences) an opponent should select to play effectively against the human player. In previous work, these tactics were manually generated. We introduce AKADS it uses an evolutionary algorithm to automatically generate such tactics. Our experiments show that it improves dynamic scripting's performance on a real-time strategy (RTS) game. Therefore, we conclude that high-quality domain knowledge (i.e., tactics) can be automatically generated for strong adaptive AI opponents in RTS games. This reduces the time and effort required by game developers to create intelligent game AI, thus freeing them to focus on other important topics (e.g., storytelling, graphics).|Marc J. V. Ponsen,H√©ctor Mu√±oz-Avila,Pieter Spronck,David W. Aha","16072|IJCAI|2005|Word Sense Disambiguation with Distribution Estimation|A word sense disambiguation (WSD) system trained on one domain and applied to a different domain will show a decrease in performance. One major reason is the different sense distributions between different domains. This paper presents novel application of two distribution estimation algorithms to provide estimates of the sense distribution of the new domain data set. Even though our training examples are automatically gathered from parallel corpora, the sense distributions estimated are good enough to achieve a relative improvement of % when incorporated into our WSD system.|Yee Seng Chan,Hwee Tou Ng"],["16322|IJCAI|2005|Disjunctive Temporal Planning with Uncertainty|Driven by planning problems with both disjunctive constraints and contingency, we define the Disjunctive Temporal Problem with Uncertainty (DTPU), an extension of the DTP that includes contingent events. Generalizing existing work on Simple Temporal Problems with Uncertainty, we divide the time-points into controllable and uncontrollable classes, and propose varying notions of controllability to replace the notion of consistency.|Kristen Brent Venable,Neil Yorke-Smith","65440|AAAI|2005|New Admissible Heuristics for Domain-Independent Planning|Admissible heuristics are critical for effective domain-independent planning when optimal solutions must be guaranteed. Two useful heuristics are the hm heuristics, which generalize the reachability heuristic underlying the planning graph, and pattern database heuristics. These heuristics, however, have serious limitations reachability heuristics capture only the cost of critical paths in a relaxed problem, ignoring the cost of other relevant paths, while PDB heuristics, additive or not, cannot accommodate too many variables in patterns, and methods for automatically selecting patterns that produce good estimates are not known. We introduce two refinements of these heuristics First, the additive hm heuristic which yields an admissible sum of hm heuristics using a partitioning of the set of actions. Second, the constrained PDB heuristic which uses constraints from the original problem to strengthen the lower bounds obtained from abstractions. The new heuristics depend on the way the actions or problem variables are partitioned. We advance methods for automatically deriving additive hm and PDB heuristics from STRIPS encodings. Evaluation shows improvement over existing heuristics in several domains, although, not surprisingly, no heuristic dominates all the others over all domains.|Patrik Haslum,Blai Bonet,Hector Geffner","16132|IJCAI|2005|Integrating Planning and Temporal Reasoning for Domains with Durations and Time Windows|The treatment of exogenous events in planning is practically important in many domains. In this paper we focus on planning with exogenous events that happen at known times, and affect the plan actions by imposing that the execution of certain plan actions must be during some time windows. When actions have durations, handling such constraints adds an extra difficulty to planning, which we address by integrating temporal reasoning into planning. We propose a new approach to planning in domains with durations and time windows, combining graph-based planning and disjunctive constraint-based temporal reasoning. Our techniques are implemented in a planner that took part in the th International Planning Competition showing very good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina","16753|IJCAI|2007|A Heuristic Search Approach to Planning with Temporally Extended Preferences|Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.|Jorge A. Baier,Fahiem Bacchus,Sheila A. McIlraith","16803|IJCAI|2007|Using Learned Policies in Heuristic-Search Planning|Many current state-of-the-art planners rely on forward heuristic search. The success of such search typically depends on heuristic distance-to-the-goal estimates derived from the plangraph. Such estimates are effective in guiding search for many domains, but there remain many other domains where current heuristics are inadequate to guide forward search effectively. In some of these domains, it is possible to learn reactive policies from example plans that solve many problems. However, due to the inductive nature of these learning techniques, the policies are often faulty, and fail to achieve high success rates. In this work, we consider how to effectively integrate imperfect learned policies with imperfect heuristics in order to improve over each alone. We propose a simple approach that uses the policy to augment the states expanded during each search step. In particular, during each search node expansion, we add not only its neighbors, but all the nodes along the trajectory followed by the policy from the node until some horizon. Empirical results show that our proposed approach benefits both of the leveraged automated techniques, learning and heuristic search, outperforming the state-of-the-art in most benchmark planning domains.|Sung Wook Yoon,Alan Fern,Robert Givan","65578|AAAI|2005|Conformant Planning for Domains with Constraints-A New Approach|The paper presents a pair of new conformant planners, CPApc and CPAph, based on recent developments in theory of action and change. As an input the planners take a domain description D in action language AL which allows state constraints (non-stratified axioms), together with a set of CNF formulae describing the initial state, and a set of literals representing the goal. We propose two approximations of the transition diagram T defined by D. Both approximations are deterministic transition functions and can be computed efficiently. Moreover they are sound (and sometimes complete) with respect to T. In its search for a plan, an approximation based planner analyses paths of an approximation instead of that of T. CPApc and CPAph are forward, best first search planners based on this idea. We compare them with two state-of-the-art conformant planners, KACMBP and Conformant-FF (CFF), over benchmarks in the literature, and over two new domains. One has large number of state constraints and another has a high degree of incompleteness. Our planners perform reasonably well in benchmark domains and outperform KACMBP and CFF in the first domain while still working well with the second one. Our experimental result shows that having an integral part of a conformant planner to deal with state constraints directly can significantly improve its performance extending a similar claim for classical planners in (Thiebaux. Hoffmann, & Nebel ).|Tran Cao Son,Phan Huy Tu,Michael Gelfond,A. Ricardo Morales","16202|IJCAI|2005|Planning with Loops|Unlike the case for sequential and conditional planning, much of the work on iterative planning (planning where loops may be needed) leans heavily on theorem-proving. This paper does the following it proposes a different approach where generating plans is decoupled from verifying them describes the implementation of an iterative planner based on the situation calculus presents a few examples illustrating the sorts of plans that can be generated shows some of the strengths and weaknesses of the approach and finally sketches the beginnings of a theory, where validation of plans is done offline.|Hector J. Levesque","65979|AAAI|2007|Nonmyopic Informative Path Planning in Spatio-Temporal Models|In many sensing applications we must continuously gather information to provide a good estimate of the state of the environment at every point in time. A robot may tour an environment, gathering information every hour. In a wireless sensor network, these tours correspond to packets being transmitted. In these settings, we are often faced with resource restrictions, like energy constraints. The users issue queries with certain expectations on the answer quality. Thus, we must optimize the tours to ensure the satisfaction of the user constraints, while at the same time minimize the cost of the query plan. For a single timestep, this optimization problem is NP-hard, but recent approximation algorithms with theoretical guarantees provide good solutions. In this paper, we present a new efficient algorithm, exploiting dynamic programming and submodularity of the information collected, that efficiently plans data collection tours for an entire (finite) horizon. Our algorithm can use any single step procedure as a black box, and, based on its properties, provides strong theoretical guarantees for the solution. We also provide an extensive empirical analysis demonstrating the benefits of nonmyopic planning in two real world sensing applications.|Alexandra Meliou,Andreas Krause,Carlos Guestrin,Joseph M. Hellerstein","16650|IJCAI|2007|When is Temporal Planning Really Temporal|While even STRIPS planners must search for plans of unbounded length, temporal planners must also cope with the fact that actions may start at any point in time. Most temporal planners cope with this challenge by restricting action start times to a small set of decision epochs, because this enables search to be carried out in state-space and leverages powerful state-based reachability heuristics, originally developed for classical planning. Indeed, decision-epoch planners won the International Planning Competition's Temporal Planning Track in ,  and . However, decision-epoch planners have a largely unrecognized weakness they are incomplete. In order to characterize the cause of incompleteness, we identify the notion of required concurrency, which separates expressive temporal action languages from simple ones. We show that decisionepoch planners are only complete for languages in the simpler class, and we prove that the simple class is 'equivalent' to STRIPS Surprisingly, no problems with required concurrency have been included in the planning competitions. We conclude by designing a complete state-space temporal planning algorithm, which we hope will be able to achieve high performance by leveraging the heuristics that power decision epoch planners.|William Cushing,Subbarao Kambhampati,Mausam,Daniel S. Weld","65473|AAAI|2005|Using Domain-Configurable Search Control for Probabilistic Planning|We describe how to improve the performance of MDP planning algorithms by modifying them to use the search-control mechanisms of planners such as TLPlan, SHOP, and TALplanner. In our experiments, modified versions of RTDP, LRTDP, and Value Iteration were exponentially faster than the original algorithms. On the largest problems the original algorithms could solve, the modified ones were about , times faster. On another set. of problems whose state spaces were more than , times larger than the original algorithms could solve, the modified algorithms took only about  second.|Ugur Kuter,Dana S. Nau"],["44312|IEICE Transations|2007|Acceleration of Test Generation for Sequential Circuits Using Knowledge Obtained from Synthesis for Testability|In this paper, we propose a method of accelerating test generation for sequential circuits by using the knowledge about the availability of state justification sequences, the bound on the length of state distinguishing sequences, differentiation between valid and invalid states, and the existence of a reset state. We also propose a method of synthesis for testability (SfT) which takes the features of our test generation method into consideration to synthesize sequential circuits from given FSM descriptions. The SfT method guarantees that the test generator will be able to find a state distinguishing sequence. The proposed method extracts the state justification sequence from the FSM produced by the synthesizer to improve the performance of its test generation process. Experimental results show that the proposed method can achieve % fault efficiency in relatively short test generation time.|Masato Nakasato,Satoshi Ohtake,Kewal K. Saluja,Hideo Fujiwara","44124|IEICE Transations|2007|Low-Cost IP Core Test Using Tri-Template-Based Codes|A tri-template-based codes (TTBC) method is proposed to reduce test cost of intellectual property (IP) cores. In order to reduce test data volume (TDV), the approach utilizes three templates, i.e., all , all , and the previously applied test data, for generating the subsequent test data by flipping the inconsistent bits. The approach employs a small number of test channels I to supply a large number of internal scan chains I- such that it can achieve significant reduction in test application time (TAT). Furthermore, as a non-intrusive and automatic test pattern generation (ATPG) independent solution, the approach is suitable for IP core testing because it requires neither redesign of the core under test (CUT) nor running any additional ATPG for the encoding procedure. In addition, the decoder has low hardware overhead, and its design is independent of the CUT and the given test set. Theoretical analysis and experimental results for ISCAS  benchmark circuits have proven the efficiency of the proposed approach.|Gang Zeng,Hideo Ito","42917|IEICE Transations|2005|Scan Design for Two-Pattern Test without Extra Latches|There are three well-known approaches to using scan design to apply two-pattern testing broadside testing (functional justification), skewed-load testing and enhanced scan testing. The broadside and skewed-load testing use the standard scan design, and thus the area overheads are not high. However fault coverage is low. The enhanced scan testing uses the enhanced scan design. The design uses extra latches, and allows scan-in any two-pattern testing. While this method achieves high fault coverage, it causes high area overhead because of extra latches. This paper presents a new scan design where two-pattern testing with high fault coverage can be performed with area overhead as low as the standard scan design. The proposed scan-FFs are based on master-slave FFs. The input of each scan-FF is connected to the output of the master latch and not the slave latch of the previous FF. Every scan-FF maintains the output value during scan-shift operations.|Kazuteru Namba,Hideo Ito","16701|IJCAI|2007|Fault-Model-Based Test Generation for Embedded Software|Testing embedded software systems on the control units of vehicles is a safety-relevant task, and developing the test suites for performing the tests on test benches is time-consuming. We present the foundations and results of a case study to automate the generation of tests for control software of vehicle control units based on a specification of requirements in terms of finite state machines. This case study builds upon our previous work on generation of tests for physical systems based on relational behavior models. In order to apply the respective algorithms, the finite state machine representation is transformed into a relational model. We present the transformation, the application of the test generation algorithm to a real example, and discuss the results and some specific challenges regarding software testing.|Michael Esser,Peter Struss","58081|GECCO|2007|A multi-objective approach to search-based test data generation|There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage.The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade-offs between the two simultaneous objectives.|Kiran Lakhotia,Mark Harman,Phil McMinn","42985|IEICE Transations|2005|Swiss Cheese Test Case Generation for Web Services Testing|Current Web services testing techniques are unable to assure the desired level of trustworthiness, which presents a barrier to WS applications in mission and business critical environments. This paper presents a framework that assures the trustworthiness of Web services. New assurance techniques are developed within the framework, including specification verification via completeness and consistency checking, test case generation, and automated Web services testing. Traditional test case generation methods only generate positive test cases that verify the functionality of software. The proposed Swiss Cheese test case generation method is designed to generate both positive and negative test cases that also reveal the vulnerability of Web services. This integrated development process is implemented in a case study. The experimental evaluation demonstrates the effectiveness of this approach. It also reveals that the Swiss Cheese negative testing detects even more faults than positive testing and thus significantly reduces the vulnerability of Web services.|Wei-Tek Tsai,Xiao Wei,Yinong Chen,Raymond A. Paul,Bingnan Xiao","57505|GECCO|2005|New evolutionary techniques for test-program generation for complex microprocessor cores|Checking if microprocessor cores are fully functional at the end of the productive process has become a major issue. Traditional functional approaches are not sufficient when considering modern designs. This paper describes new improvements for an existing evolutionary algorithm, called GP, able to generate Turing-complete programs these are exploited, along with hardware acceleration techniques, to add content to a qualifying test campaign by automatically generating assembly programs. The approach is suitable for medium-sized processor cores. The experimental evaluation performed on a SPARCv clearly shows the potentiality of the approach, and the effectiveness of the enhancements to the evolutionary core.|Ernesto S√°nchez,Massimiliano Schillaci,Matteo Sonza Reorda,Giovanni Squillero,Luca Sterpone,Massimo Violante","42787|IEICE Transations|2005|Deterministic Delay Fault BIST Using Adjacency Test Pattern Generation|In delay fault BIST (Built-In-Self-Test), an adjacency test pattern generation scheme effectively generates robust test patterns. The traditional adjacency test pattern generation schemes use LFSR to generate first patterns, and thus they cannot generate test patterns for circuits with more than  inputs with high fault coverage in a practical amount of time. This paper proposes a deterministic delay fault BIST method using adjacency test pattern generation. The proposed scheme uses first patterns generated by a deterministic algorithm based on the analysis of independent partial circuits on the circuit under test. Experiments show that test patterns generated by the proposed method have both high fault coverage and short test length, resulting in a short test time.|Kazuteru Namba,Hideo Ito","43932|IEICE Transations|2007|Design Method for Numerical Function Generators Using Recursive Segmentation and EVBDDs|Numerical function generators (NFGs) realize arithmetic functions, such as ex, sin(x), and x, in hardware. They are used in applications where high-speed is essential, such as in digital signal or graphics applications. We introduce the edge-valued binary decision diagram (EVBDD) as a means of reducing the delay and memory requirements in NFGs. We also introduce a recursive segmentation algorithm, which divides the domain of the function to be realized into segments, where the given function is realized as a polynomial. This design reduces the size of the multiplier needed and thus reduces delay. It is also shown that an adder can be replaced by a set of -input AND gates, further reducing delay. We compare our results to NFGs designed with multi-terminal BDDs (MTBDDs). We show that EVBDDs yield a design that has, on the average, only % of the memory and % of the delay of NFGs designed using MTBDDs.|Shinobu Nagayama,Tsutomu Sasao,Jon T. Butler","58059|GECCO|2007|Automatic mutation test input data generation via ant colony|Fault-based testing is often advocated to overcome limitations ofother testing approaches however it is also recognized as beingexpensive. On the other hand, evolutionary algorithms have beenproved suitable for reducing the cost of data generation in the contextof coverage based testing. In this paper, we propose a newevolutionary approach based on ant colony optimization for automatictest input data generation in the context of mutation testingto reduce the cost of such a test strategy. In our approach the antcolony optimization algorithm is enhanced by a probability densityestimation technique. We compare our proposal with otherevolutionary algorithms, e.g., Genetic Algorithm. Our preliminaryresults on JAVA testbeds show that our approach performed significantlybetter than other alternatives.|Kamel Ayari,Salah Bouktif,Giuliano Antoniol"]]}}