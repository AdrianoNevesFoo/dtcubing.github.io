{"abstract":{"entropy":6.880697203778712,"topics":["video coding, semantic web, web information, web services, data, support vector, web pages, information, information retrieval, web, addresses problem, paper novel, ldpc codes, machine data, data mining, scheme video, transfer improve, mining clustering, causal effects, problem data","machine learning, knowledge base, recent years, reinforcement learning, learning, natural language, description logic, logic, belief revision, reasoning, programming asp, horn transaction, planning domains, logic programming, nonmonotonic logic, case-based reasoning, preferences voting, combinatorial auctions, recent work, situations calculus","artificial intelligence, system, play role, widely used, mobile robots, computer vision, social network, natural language, agents environment, model-based diagnosis, sensor network, robots, multi-agent system, intelligent system, describe system, autonomous robots, activity recognition, agents goal, computer science, environment","markov decision, partially observable, markov processes, decision processes, present algorithm, bayesian network, consider problem, search algorithm, solving problem, algorithm, dynamic programming, present novel, constraint problem, decision making, constraint satisfaction, present based, observable markov, discriminant analysis, model, partially markov","problem data, data, addresses problem, support vector, learning data, machine data, data mining, data clustering, mining clustering, uncertainty source, well known, system data, data structure, teaching students, classification problem, involves uncertainty, labeled data, vector data, algorithm data, present data","automatically data, problem given, external memory, given data, problem area, given, area, automatically, find, memory, computation, particular, means, part, offer, core, input, emerging, determining, automation","preferences voting, preferences aggregation, self-interested agents, games player, aggregation voting, voting rule, games strategy, rank voting, games, multiple voting, negotiation agents, agents, formation coalitions, agents voting, general voting, problem agents, key problem, rule preferences, present approach, preferences","reasoning, reasoning approach, case-based reasoning, reasoning based, planning actions, uncertainty actions, situations calculus, actions effects, reasoning problem, past decade, reasoning system, present reasoning, case reasoning, temporal reasoning, actions domains, problem temporal, physical qualitative, measure similarity, situations actions, actions","intelligent system, applications requires, mobile robots, agents goal, agents, agents environment, distributed system, control system, autonomous robots, system need, agents system, system mobile, autonomous agents, agents coalitions, human behavior, resource allocation, robots, multiagent system, agents need, task agents","widely used, computer vision, social network, sensor network, design system, modelling arguments, expert system, system automatically, modelling representation, modelling formal, wireless network, modelling argumentation, technologies provide, expert knowledge, modelling individual, security based, individual arguments, mechanisms design, system generate, tool system","bayesian network, solving problem, constraint problem, constraint satisfaction, discriminant analysis, solutions problem, algorithm solving, study problem, problem, time series, satisfaction problem, constraint programming, linear analysis, linear discriminant, constraint, dynamic programming, kernel discriminant, paper study, algorithm constraint, variables constraint","consider problem, present novel, present based, present algorithm, present approach, present model, present, network based, algorithm optimization, genetic algorithm, based constraint, present framework, paper based, paper present, based, present general, develop model, introduce algorithm, algorithm based, existing methods"],"ranking":[["65827|AAAI|2006|Corpus-based and Knowledge-based Measures of Text Semantic Similarity|This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to % error rate reduction with respect to the traditional vector-based similarity metric.|Rada Mihalcea,Courtney Corley,Carlo Strapparava","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","65893|AAAI|2006|The Synthy Approach for End to End Web Services Composition Planning with Decoupled Causal and Resource Reasoning|Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them - not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in Al planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two-staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI.|Biplav Srivastava","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","66313|AAAI|2008|Semantic Web Development for Traditional Chinese Medicine|Despite its centrality to Chinese culture and wide adoption in Chinese communities, Traditional Chinese Medicine (TCM) has rarely been the application domain of computational analysis in previous academic works. Here we present the first systematic adoption of the state-of-the-art Semantic Web technologies in the codification, management, and utilization of TCM information and knowledge resources. These technologies are proved effective in bridging the semantic gaps between a plurality of legacy and heterogeneous relational databases, enabling ontology-based query and search across database boundaries. A global herb-drug interaction network is constructed and represented in Semantic Web language, on which the semantic graph mining methodology is applied for discovering and interpreting interesting patterns. This deployed Semantic Web platform provides various innovative information retrieval and knowledge discovery services to the TCM domain experts with positive feedbacks. This project demonstrates Semantic Web's advantages in connecting data across domain and community boundaries to facilitate interdisciplinary and cross-cultural studies.|Zhaohui Wu,Tong Yu,Huajun Chen,Xiaohong Jiang,Chunying Zhou,Yu Zhang,Yuxin Mao,Yi Feng,Meng Cui,Aining Yin","65934|AAAI|2006|Automatic Wrapper Generation Using Tree Matching and Partial Tree Alignment|This paper is concerned with the problem of structured data extraction from Web pages. The objective of the research is to automatically segment data records in a page, extract data itemsfields from these records and store the extracted data in a database. In this paper, we first introduce the extraction problem, and then discuss the main existing approaches and their limitations. After that, we introduce a novel technique (called DEPTA) to automatically perform Web data extraction. The method consists of three steps () identifying data records with similar patterns in a page, () aligning and extracting data items from the identified data records and () generating tree-based regular expressions to facilitate later extraction from other similar pages. The key innovation is the proposal of a new multiple tree alignment algorithm called partial tree alignment, which was found to be particularly suitable for Web data extraction. This paper is based on our work published in KDD- and WWW-.|Yanhong Zhai,Bing Liu","65682|AAAI|2006|Comparative Experiments on Sentiment Classification for Online Product Reviews|Evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc. This paper looks at a simplified version of the problem classifying online product reviews into positive and negative classes. We discuss a series of experiments with different machine learning algorithms in order to experimentally evaluate various trade-offs, using approximately K product reviews from the web.|Hang Cui,Vibhu O. Mittal,Mayur Datar","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65717|AAAI|2006|Overview of AutoFeed An Unsupervised Learning System for Generating Webfeeds|The AutoFeed system automatically extracts data from semistructured web sites. Previously, researchers have developed two types of supervised learning approaches for extracting web data methods that create precise, site-specific extraction rules and methods that learn less-precise site-independent extraction rules. In either case, significant training is required. AutoFeed follows a third, more ambitious approach, in which unsupervised learning is used to analyze sites and discover their structure. Our method relies on a set of heterogeneous \"experts\", each of which is capable of identifying certain types of generic structure. Each expert represents its discoveries as \"hints\". Based on these hints, our system clusters the pages and identifies semi-structured data that can be extracted. To identify a good clustering, we use a probabilistic model of the hint-generation process. This paper summarizes our formulation of the fully-automatic web-extraction problem, our clustering approach, and our results on a set of experiments.|Bora Gazen,Steven Minton","65850|AAAI|2006|Using the Semantic Web to Integrate Ecoinformatics Resources|We demonstrate an end-to-end use case of the semantic web's utility for synthesizing ecological and environmental data. ELVIS (the Ecosystem Location Visualization and Information System) is a suite of tools for constructing food webs for a given location. ELVIS functionality is exposed as a collection of web services, and all input and output data is expressed in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents.|Cynthia Sims Parr,Andriy Parafiynyk,Joel Sachs,Rong Pan,Lushan Han,Li Ding,Tim Finin,David Wang"],["65692|AAAI|2006|Unifying Logical and Statistical AI|Intelligent agents must be able to handle the complexity and uncertainty of the real world. Logical AI has focused mainly on the former, and statistical AI on the latter. Markov logic combines the two by attaching weights to first-order formulas and viewing them as templates for features of Markov networks. Inference algorithms for Markov logic draw on ideas from satisfiability, Markov chain Monte Carlo and knowledge-based model construction. Learning algorithms are based on the voted perceptron, pseudo-likelihood and inductive logic programming. Markov logic has been successfully applied to problems in entity resolution, link prediction, information extraction and others, and is the basis of the open-source Alchemy system.|Pedro Domingos,Stanley Kok,Hoifung Poon,Matthew Richardson,Parag Singla","66161|AAAI|2007|Forgetting Actions in Domain Descriptions|Forgetting irrelevantproblematic actions in a domain description can be useful in solving reasoning problems, such as query answering, planning, conftict resolution, prediction, postdiction, etc.. Motivated by such applications, we study what forgetting is, how forgetting can be done, and for which applications forgetting can be useful and how, in the context of reasoning about actions. We study these questions in the action language C (a formalism based on causal explanations), and relate it to forgetting in classical logic and logic programming.|Esra Erdem,Paolo Ferraris","66316|AAAI|2008|Optimizations and Extensions for the Horn Transaction Logic Programs|My thesis describes optimization techniques and extensions for the Horn Transaction Logic. The Horn Transaction Logic is an extension of the classical logic programming with state updates and it has a SLD-style evaluation algorithm. This SLD-style algorithm enters into infinite loops when computing answers to many recursive programs when they change the underlying state of the knowledge base. We solve this problem by tabling the calls, states and answers in a searchable structure, so that the same call is not re-executed ad infinitum. With these techniques, we can efficiently compute queries to transaction logic programs, and when the underlying programs have the bounded term-depth property, these techniques are guaranteed to terminate. I also present extensions to Transaction Logic, for instance a definite semantics for the existentially quantified values that occur in facts, queries and updates of facts. The applications of these techniques promise great improvements in the uses of transaction logic state-changing systems, artificial intelligence planning, dynamic constraints on transaction execution, workflow modeling and verification, and systems involving financial transactions.|Paul Fodor","65839|AAAI|2006|Learning of Agents with Limited Resources|In our research we investigate rational agent which consciously balances deliberation and acting, and uses learning to augment its reasoning. It creates several partial plans, uses past experience to choose the best one and, by executing it, gams new Knowledge about the world. We analyse a possible application of Inductive Logic Programming to learn how to evaluate partial plans in a resource-constrained way. We also discuss how ILP framework can generalise partial plans.|Slawomir Nowaczyk","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","66497|AAAI|2008|Using Answer Set Programming and Lambda Calculus to Characterize Natural Language Sentences with Normatives and Exceptions|One way to solve the knowledge acquisition bottleneck is to have ways to translate natural language sentences and discourses to a formal knowledge representation language, especially ones that are appropriate to express domain knowledge in sciences, such as Biology. While there have been several proposals, including by Montague (), to give model theoretic semantics for natural language and to translate natural language sentences and discourses to classical logic, none of these approaches use knowledge representation languages that can express domain knowledge involving normative statements and exceptions. In this paper we take a first step to illustrate how one can automatically translate natural language sentences about normative statements and exceptions to representations in the knowledge representation language Answer Set Programming (ASP). To do this, we use -calculus representation of words and their composition as dictated by a CCG grammar.|Chitta Baral,Juraj Dzifcak,Tran Cao Son","66365|AAAI|2008|Querying Sequential and Concurrent Horn Transaction Logic Programs Using Tabling Techniques|In this poster we describe the tabling techniques for Sequential and Concurrent Horn Transaction Logic. Horn Transaction Logic is an extension of classical logic programming with state updates and it has a SLD-style evaluation algorithm. This SLD-style algorithm enters into infinite loops when computing answers to many recursive programs when they change the underlying state of the knowledge base. We solve this problem by tabling (caching) the calls, call states and answers (unifications and return states) in a searchable structure for the Sequential Transaction Logic, or building a graph for the query and memoize the \"hot\" vertices (vertices, currently, possible to execute) for the Propositional Concurrent Transaction Logic, so that the same call is not re-executed ad infinum. With these techniques, we can efficiently compute queries to transaction logic programs, and when the underlying programs have the bounded term-depth property (Transaction Datalog) the techniques are guaranteed to terminate. The applications of these techniques promise termination and great improvements in the uses of transaction logic state-changing systems, artificial intelligence planning, dynamic constraints on transaction execution, workflow modeling and verification, and systems involving financial transactions.|Paul Fodor","66430|AAAI|2008|Argument Theory Change Applied to Defeasible Logic Programming|In this article we work on certain aspects of the belief change theory in order to make them suitable for argumentation systems. This approach is based on Defeasible Logic Programming as the argumentation formalism from which we ground the definitions. The objective of our proposal is to define an argument revision operator that inserts a new argument into a defeasible logic program in such a way that this argument ends up undefeated after the revision, thus warranting its conclusion. In order to ensure this warrant, the defeasible logic program has to be changed in concordance with a minimal change principle. Finally, we present an algorithm that implements the argument revision operation.|Mart√≠n O. Moguillansky,Nicol√°s D. Rotstein,Marcelo A. Falappa,Alejandro Javier Garc√≠a,Guillermo Ricardo Simari","66450|AAAI|2008|What Is Answer Set Programming|Answer set programming (ASP) is a form of declarative programming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.|Vladimir Lifschitz","66428|AAAI|2008|Horn Complements Towards Horn-to-Horn Belief Revision|Horn-to-Horn belief revision asks for the revision of a Horn knowledge base such that the revised knowledge base is also Horn. Horn knowledge bases are important whenever one is concerned with efficiency--of computing inferences, of knowledge acquisition, etc. Horn-to-Horn belief revision could be of interest, in particular, as a component of any efficient system requiring large commonsense knowledge bases that may need revisions because, for example, new contradictory information is acquired. Recent results on belief revision for general logics show that the existence of a belief contraction operator satisfying the generalized AGM postulates is equivalent to the existence of a complement. Here we provide a first step towards efficient Horn-to-Horn belief revision, by characterizing the existence of a complement of a Horn consequence of a Horn knowledge base. A complement exists if and only if the Horn consequence is not the consequence of a modified knowledge base obtained from the original by an operation called body building. This characterization leads to the efficient construction of a complement whenever it exists.|Marina Langlois,Robert H. Sloan,Bal√°zs Sz√∂r√©nyi,Gy√∂rgy Tur√°n"],["66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","65828|AAAI|2006|Intuitive linguistic Joint Object Reference in Human-Robot Interaction Human Spatial Reference Systems and Function-Based Categorization for Symbol Grounding|The visionary goal of an easy to use service robot implies intuitive styles of interaction between humans and robots. Such natural interaction can only be achieved if means are found to bridge the gap between the forms of object perception and spatial knowledge maintained by such robots, and the forms of language, used by humans, to communicate such knowledge. Part of bridging this gap consists of allowing user and robot to establish joint reference on objects in the environment - without forcing the user to use unnatural means for object reference. We present an approach to establishing joint object reference which makes use of natural object classification and a computational model of basic intrinsic and relative reference systems. Our object recognition approach assigns natural categories (e.g. \"desk\", \"chair\", \"table\") to new objects based on their functional design. With basic objects within the environment classified, we can then make use of a computational reference model, to process natural projective relations (e.g. \"the briefcase to the left of the chair\"), allowing users to refer to objects which cannot be classified reliably by the recognition system alone.|Reinhard Moratz","66590|AAAI|2008|Agent Organized Networks Redux|Individual robots or agents will often need to form coalitions to accomplish shared tasks, e.g., in sensor networks or markets. Furthermore, in most real systems it is infeasible for entities to interact with all peers. The presence of a social network can alleviate this problem by providing a neighborhood system within which entities interact with a reduced number of peers. Previous research has shown that the topology of the underlying social network has a dramatic effect on the quality of coalitions formed and consequently on system performance (Gaston & deslardins a). It has also been shown that it is feasible to develop agents which dynamically alter connections to improve an organization's ability to form coalitions on the network. However those studies have not analysed the network topologies that result from connectivity adaptation strategies. In this paper the resulting network topologies were analysed and it was found that high performance and rapid convergence were attained because scale free networks were being formed. However it was observed that organizational performance is not impacted by limiting the number of links per agent to the total number of skills available within the population. implying that bandwidth was wasted by previous approaches. We used these observations to inform the design of a token based algorithm that attains higher performance using an order of magnitude less messages for both uniform and non-uniform distributions of skills.|Robin Glinton,Katia P. Sycara,Paul Scerri","66570|AAAI|2008|An Extended Interpreted System Model for Epistemic Logics|The interpreted system model offers a computationally grounded model, in terms of the states of computer processes, to S epistemic logics. This paper extends the interpreted system model, and provides a computationally grounded one, called the interpreted perception system model, to those episternic logics other than S. It is usually assumed, in the interpreted system model, that those parts of the environment that are visible to an agent are correctly perceived by the agent as a whole. The essential idea of the interpreted perception system model is that an agent may have incorrect perception or observations to the visible parts of the environment and the agent may not be aware of this. The notion of knowledge can be defined so that an agent knows a statement iff the statement holds in those states that the agent can not distinguish (from the current state) by using only her correct observations. We establish a logic of knowledge and certainty, called KC logic, with a sound and complete proof system. The knowledge modality in this logic is S valid. It becomes S if we assume an agent always has correct observations and more interestingly, it can be S. or S. under other natural constraints on agents and their sensors to the environment.|Kaile Su,Abdul Sattar","65695|AAAI|2006|Traffic Intersections of the Future|Few concepts embody the goals of artificial intelligence as well as fully autonomous robots. Countless films and stories have been made that focus on a future filled with autonomous agents that complete menial tasks or run errands that humans do not want or are too busy to carry out. One such task is driving automobiles. In this paper, we summarize the work we have dune towards a future of fully-autonomous vehicles, specifically coordinating such vehicles safely and efficiently at intersections. We then discuss the implications this work has for other areas of AI, including planning, multiagent learning, and computer vision.|Kurt M. Dresner,Peter Stone","65890|AAAI|2006|Multiagent Coalition Formation for Computer-Supported Cooperative Learning|In this paper, we describe a computer-supported cooperative learning system in education and the results of its deployment. The system, called I-MINDS, consists of a set of teacher agents, group agents, and student agents. While the agents possess individual intelligent capabilities, the novel invention of I-MINDS lies in multiagent intelligence and coalition formation. I-MINDS supports student participation and collaboration and helps the instructor manage large, distance classrooms. Specifically, it uses a Vickrey auction-based and learning-enabled algorithm called VALCAM to form student groups in a structured cooperative learning setting. We have deployed I-MINDS in an introductory computer science course (CS) and conducted experiments in the Spring and Fall semesters of  to study how I-MINOS-supported collaboration fares against traditional, face-to-face collaboration. Results showed that students using I-MINDS performed (and outperformed in some aspects) as well as students in traditional settings.|Leen-Kiat Soh,Nobel Khandaker,Hong Jiang","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir","66175|AAAI|2007|An Integrated Robotic System for Spatial Understanding and Situated Interaction in Indoor Environments|A major challenge in robotics and artificial intelligence lies in creating robots that are to cooperate with people in human-populated environments, e.g. for domestic assistance or elderly care. Such robots need skills that allow them to interact with the world and the humans living and working therein. In this paper we investigate the question of spatial understanding of human-made environments. The functionalities of our system comprise perception of the world, natural language, learning, and reasoning. For this purpose we integrate state-of-the-art components from different disciplines in AI, robotics and cognitive systems into a mobile robot system. The work focuses on the description of the principles we used for the integration, including cross-modal integration, ontology-based mediation, and multiple levels of abstraction of perception. Finally, we present experiments with the integrated \"CoSy Explorer\" system and list some of the major lessons that were learned from its design, implementation, and evaluation.|Hendrik Zender,Patric Jensfelt,√\u201Cscar Mart√≠nez Mozos,Geert-Jan M. Kruijff,Wolfram Burgard","65187|AAAI|2004|Intelligent Systems Demonstration The Secure Wireless Agent Testbed SWAT|We will demonstrate the Secure Wireless Agent Testbed (SWAT), a unique facility developed at Drexel University to study integration, networking and information assurance for next-generation wireless mobile agent systems. SWAT is an implemented system that fully integrates ) mobile agents, ) wireless ad hoc multi-hop networks, and ) security. The demonstration will show the functionality of a number of decentralized agent-based applications, including applications for authentication, collaboration, messaging, and remote sensor monitoring. The demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen nodes (PDAs, tablet PCs, and laptops) and hundreds of mobile software agents.|Gustave Anderson,Andrew Burnheimer,Vincent A. Cicirello,David Dorsey,Saturnino Garcia,Moshe Kam,Joseph Kopena,Kris Malfettone,Andrew Mroczkowski,Gaurav Naik,Maxim Peysakhov,William C. Regli,Joshua Shaffer,Evan Sultanik,Kenneth Tsang,Leonardo Urbano,Kyle Usbeck,Jacob Warren","65269|AAAI|2004|Task Allocation via Self-Organizing Swarm Coalitions in Distributed Mobile Sensor Network|This paper presents a task allocation scheme via self-organizing swarm coalitions for distributed mobile sensor network coverage. Our approach uses the concepts of ant behavior to self-regulate the regional distributions of sensors in proportion to that of the moving targets to be tracked in a non-stationary environment. As a result, the adverse effects of task interference between robots are minimized and sensor network coverage is improved. Quantitative comparisons with other tracking strategies such as static sensor placement, potential fields, and auction-based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental changes.|Kian Hsiang Low,Wee Kheng Leow,Marcelo H. Ang Jr."],["65917|AAAI|2006|Compact Convex Upper Bound Iteration for Approximate POMDP Planning|Partially observable Markov decision processes (POMDPs) are an intuitive and general way to model sequential decision making problems under uncertainty. Unfortunately, even approximate planning in POMDPs is known to be hard, and developing heuristic planners that can deliver reasonable results in practice has proved to be a significant challenge. In this paper, we present a new approach to approximate value-iteration for POMDP planning that is based on quadratic rather than piecewise linear function approximators. Specifically, we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratics, and optimize it at each stage by semidefinite programming. We demonstrate that our approach can achieve competitive approximation quality to current techniques while still maintaining a bounded size representation of the function approximator. Moreover, an upper bound on the optimal value function can be preserved if required. Overall, the technique requires computation time and space that is only linear in the number of iterations (horizon time).|Tao Wang,Pascal Poupart,Michael H. Bowling,Dale Schuurmans","66436|AAAI|2008|Symbolic Heuristic Search Value Iteration for Factored POMDPs|We propose Symbolic heuristic search value iteration (Symbolic HSVI) algorithm, which extends the heuristic search value iteration (HSVI) algorithm in order to handle factored partially observable Markov decision processes (factored POMDPs). The idea is to use algebraic decision diagrams (ADDs) for compactly representing the problem itself and all the relevant intermediate computation results in the algorithm. We leverage Symbolic Perseus for computing the lower bound of the optimal value function using ADD operators, and provide a novel ADD-based procedure for computing the upper bound. Experiments on a number of standard factored POMDP problems show that we can achieve an order of magnitude improvement in performance over previously proposed algorithms.|Hyeong Seop Sim,Kee-Eung Kim,Jin Hyung Kim,Du-Seong Chang,Myoung-Wan Koo","65199|AAAI|2004|Stochastic Local Search for POMDP Controllers|The search for finite-state controllers for partially observable Markov decision processes (POMDPs) is often based on approaches like gradient ascent, attractive because of their relatively low computational cost. In this paper, we illustrate a basic problem with gradient-based methods applied to POMDPs, where the sequential nature of the decision problem is at issue, and propose a new stochastic local search method as an alternative. The heuristics used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming (DP) approaches. We show that our algorithm consistently finds higher quality controllers than gradient ascent, and is competitive with (and, for some problems, superior to) other state-of-the-art controller and DP-based algorithms on large-scale POMDPs.|Darius Braziunas,Craig Boutilier","65906|AAAI|2006|Point-based Dynamic Programming for DEC-POMDPs|We introduce point-based dynamic programming (DP) for decentralized partially observable Markov decision processes (DEC-POMDPs), a new discrete DP algorithm for planning strategies for cooperative multi-agent systems. Our approach makes a connection between optimal DP algorithms for partially observable stochastic games, and point-based approximations for single-agent POMDPs. We show for the first time how relevant multi-agent belief states can be computed. Building on this insight, we then show how the linear programming part in current multi-agent DP algorithms can be avoided, and how multi-agent DP can thus be applied to solve larger problems. We derive both an optimal and an approximated version of our algorithm, and we show its efficiency on test examples from the literature.|Daniel Szer,Fran√ßois Charpillet","65786|AAAI|2006|Incremental Least Squares Policy Iteration for POMDPs|We present a new algorithm, called incremental least squares policy iteration (ILSPI), for finding the infinite-horizon stationary policy for partially observable Markov decision processes (POMDPs). The ILSPI algorithm computes a basis representation of the infinite-horizon value function by minirnizing the square of Bellman residual and performs policy improvenent in reachable belief states. A number of optimal basis functions are determined by the algorithm to minimize the Bellman residual incrementally, via efficient computations. We show that, by using optimally determined basis functions, the policy can be improved successively on a set of most probable belief points sampled from the reachable belief set. As the ILSPI is based on belief sample points, it represents a point-based policy iteration method. The results on four benchmark problems show that the ILSPI compares competitively to its value-iteration counterparts in terms of both performance and computational efficiency.|Hui Li,Xuejun Liao,Lawrence Carin","66064|AAAI|2007|Optimizing Anthrax Outbreak Detection Using Reinforcement Learning|The potentially catastrophic impact of a bioterrorist attack makes developing effective detection methods essential for public health. In the case of anthrax attack, a delay of hours in making a right decision can lead to hundreds of lives lost. Current detection methods trade off reliability of alarms for early detection of outbreaks. The performance of these methods can be improved by modem disease-specific modeling techniques which take into account the potential costs and effects of an attack to provide optimal warnings. We study this optimization problem in the reinforcement learning framework. The key contribution of this paper is to apply Partially Observable Markov Decision Processes (POMDPs) on outbreak detection mechanism for improving alarm function in anthrax outbreak detection. Our approach relies on estimating the future benefit of true alarms and the costs of false alarms and using these quantities to identify an optimal decision. We present empirical evidence illustrating that the performance of detection methods with respect to sensitivity and timeliness is improved significantly by utilizing POMDPs.|Masoumeh T. Izadi,David L. Buckeridge","66277|AAAI|2008|Exploiting Symmetries in POMDPs for Point-Based Algorithms|We extend the model minimization technique for partially observable Markov decision processes (POMDPs) to handle symmetries in the joint space of states, actions, and observations. The POMDP symmetry we define in this paper cannot be handled by the model minimization techniques previously published in the literature. We formulate the problem of finding the symmetries as a graph automorphism (GA) problem, and although not yet known to be tractable, we experimentally show that the sparseness of the graph representing the POMDP allows us to quickly find symmetries. We show how the symmetries in POMDPs can be exploited for speeding up point-based algorithms. We experimentally demonstrate the effectiveness of our approach.|Kee-Eung Kim","65422|AAAI|2005|Efficient Maximization in Solving POMDPs|We present a simple, yet effective improvement to the dynamic programming algorithm for solving partially observable Markov decision processes. The technique targets the vector pruning operation during the maximization step, a key source of complexity in POMDP algorithms. We identify two types of structures in the belief space and exploit them to reduce significantly the number of constraints in the linear programs used for pruning. The benefits of the new technique are evaluated both analytically and experimentally, showing that it can lead to significant performance improvement. The results open up new research opportunities to enhance the performance and scalability of several POMDP algorithms.|Zhengzhu Feng,Shlomo Zilberstein","65172|AAAI|1994|Acting Optimally in Partially Observable Stochastic Domains|In this paper, we describe the partially observable Markov decision process (sc pomdp) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The sc pomdp approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the sc pomdp approach.|Anthony R. Cassandra,Leslie Pack Kaelbling,Michael L. Littman","65240|AAAI|2004|Dynamic Programming for Partially Observable Stochastic Games|We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.|Eric A. Hansen,Daniel S. Bernstein,Shlomo Zilberstein"],["66814|AAAI|2010|Exploiting Monotonicity in Interval Constraint Propagation|Model-based clustering is one of the most important ways for time series data mining. However, the process of clustering may encounter several problems. In this paper, a novel clustering algorithm of time-series which incorporates recursive hidden Markov model(HMM) training is proposed. Our contributions are as follows ) We recursively train models and use these model information in the process agglomerative hierarchical clustering. ) We built HMM of time-series clusters to describe clusters. To evaluate the effectiveness of the algorithm, several experiments are conducted on both synthetic data and real world data. The result shows that the proposed approach can achieve better performance in correctness rate than the traditional HMM-based clustering algorithm|Ignacio Araya,Gilles Trombettoni,Bertrand Neveu","66369|AAAI|2008|Importance of Semantic Representation Dataless Classification|Traditionally, text categorization has been studied as the problem of training of a classifier using labeled data. However, people can categorize documents into named categories without any explicit training because we know the meaning of category names. In this paper, we introduce Dataless Classification, a learning protocol that uses world knowledge to induce classifiers without the need for any labeled data. Like humans, a dataless classifier interprets a string of words as a set of semantic concepts. We propose a model for dataless classification and show that the label name alone is often sufficient to induce classifiers. Using Wikipedia as our source of world knowledge, we get .% accuracy on tasks from the  Newsgroup dataset and .% accuracy on tasks from a Yahoo Answers dataset without any labeled or unlabeled data from the data sets. With unlabeled data, we can further improve the results and show quite competitive performance to a supervised learning algorithm that uses  labeled examples.|Ming-Wei Chang,Lev-Arie Ratinov,Dan Roth,Vivek Srikumar","66833|AAAI|2010|Transductive Learning on Adaptive Graphs|Spatial data mining can mine automatically or semi-automatically unknown, creditable, effective, integrative or schematic knowledge which can be understood from the increasingly complex spatial database and enhance the ability of interpreting data to generate useful knowledge. There are large amounts of data existing in the database of government GIS. However, a great deal of the data is idle, which has caused a huge waste of data due to rarely effectively utilization in practice. It is very necessary to deal with the task of data mining based on Government GIS. In this paper, an example on land utilization and land cover in a certain region of Guizhou Province was presented to describe the course of spatial data mining. There into, a derived star-type model was used to organize the raster data to form multi dimension data set. Under these conditions, clustering method was utilized to carry out data mining aiming at the raster data. Based on corresponding analyses, users can find out which types of vegetation were suitable for being cultivated in this region by using related knowledge in a macroscopic view. Therefore, feasible service information could be provided to promote economic development in the region.|Yan-Ming Zhang,Yu Zhang,Dit-Yan Yeung,Cheng-Lin Liu,Xinwen Hou","66139|AAAI|2007|Clustering with Local and Global Regularization|Clustering is an old research topic in data mining and machine learning communities. Most of the traditional clustering methods can be categorized local or global ones. In this paper, a novel clustering method that can explore both the local and global information in the dataset is proposed. The method, Clustering with Local and Global Consistency (CLGR), aims to minimize a cost function that properly trades off the local and global costs. We will show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix, which can be done efficiently by some iterative methods. Finally the experimental results on several datasets are presented to show the effectiveness of our method.|Fei Wang,Changshui Zhang,Tao Li","65532|AAAI|2005|Spectral Clustering of Biological Sequence Data|In this paper, we apply spectral techniques to clustering biological sequence data that has proved more difficult to cluster effectively. For this purpose, we have to () extend spectral clustering algorithms to deal with asymmetric affinities. like the alignment scores used in the comparison of biological sequences. and () devise a hierarchical algorithm that can handle many clusters with imbalanced sizes robustly. We present an algorithm for clustering asymmetric affinity data, and demonstrate the performance of this algorithm at recovering the higher levels of the Structural Classification of Proteins (SCOP) on a data base of highly conserved subsequences.|William Pentney,Marina Meila","65525|AAAI|2005|Redescription Mining Structure Theory and Algorithms|We introduce a new data mining problem--redescription mining--that unifies considerations of conceptual clustering, constructive induction, and logical formula discovery. Redescription mining begins with a collection of sets, views it as a propositional vocabulary, and identifies clusters of data that can be defined in at least two ways using this vocabulary. The primary contributions of this paper are conceptual and theoretical (i) we formally study the space of redescriptions underlying a dataset and characterize their intrinsic structure, (ii) we identify impossibility as well as strong possibility results about when mining redescriptions is feasible, (iii) we present several scenarios of how we can custom-build redescription mining solutions for various biases, and (iv) we outline how many problems studied in the larger machine learning community are really special cases of redescription mining. By highlighting its broad scope and relevance. we aim to establish the importance of redescription mining and make the case for a thrust in this new line of research.|Laxmi Parida,Naren Ramakrishnan","66197|AAAI|2007|Refining Rules Incorporated into Knowledge-Based Support Vector Learners Via Successive Linear Programming|Knowledge-based classification and regression methods are especially powerful forms of learning. They allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm, combining that knowledge with data to produce accurate models. A limitation of the use of prior knowledge occurs when the provided knowledge is incorrect. Such knowledge likely still contains useful information, but knowledge-based learners might not be able to fully exploit such information. In fact, incorrect knowledge can lead to poorer models than result from knowledge-free learners. We present a support-vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge, but also suggests changes to the provided knowledge. Our approach is built on the knowledge-based classification and regression methods presented by Fung, Mangasarian, & Shavlik ( ) and by Mangasarian, Shavlik, & Wild (). Experiments on artificial data sets with known properties, as well as on a real-world data set, demonstrate that our method learns more accurate models while also adjusting the provided rules in intuitive ways. Our new algorithm provides an appealing extension to knowledge-based, support-vector learning that is not only able to combine knowledge from rules with data, but is also able to use the data to modify and change those rules to better fit the data.|Richard Maclin,Edward W. Wild,Jude W. Shavlik,Lisa Torrey,Trevor Walker","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","65506|AAAI|2005|A Constraint Satisfaction Approach to Geospatial Reasoning|The large number of data sources on the Internet can be used to augment and verify the accuracy of geospatial sources, such as gazetteers and annotated satellite imagery. Data sources such as satellite imagery, maps, gazetteers and vector data have been traditionally used in geographic infonnation systems (GIS), but nontraditional geospatial data, such as online phone books and property records are more difficult to relate to imagery. In this paper, we present a novel approach to combining extracted information from imagery, road vector data, and online data sources. We represent the problem of identifying buildings in satellite images as a constraint satisfing problem (CSP) and use constraint programming to solve it. We apply this technique to real-world data sources in EI Segundo, CA and our experimental evaluation shows how this approach can accurately identify buildings when provided with both traditional and nontraditional data sources.|Martin Michalowski,Craig A. Knoblock","66498|AAAI|2008|Structure Learning on Large Scale Common Sense Statistical Models of Human State|Research has shown promise in the design of large scale common sense probabilistic models to infer human state from environmental sensor data. These models have made use of mined and preexisting common sense data and traditional probabilistic machine learning techniques to improve recognition of the state of everyday human life. In this paper, we demonstrate effective techniques for structure learning on graphical models designed for this domain, improving the SRCS system of (Pentney et al. ) by learning additional dependencies between variables. Because the models used for common sense reasoning typically involve a large number of variables, issues of scale arise in searching for additional dependencies we discuss how we use data mining techniques to address this problem. We show experimentally that these techniques improve the accuracy of state prediction, and that, with a good prior model, the use of a common sense model with structure learning provides better prediction of unlabeled variables as well as labeled variables. The results also demonstrate that it is possible to collect new common sense information about daily life using such a statistical model and labeled data.|William Pentney,Matthai Philipose,Jeff A. Bilmes"],["65620|AAAI|2005|External-Memory Pattern Databases Using Structured Duplicate Detection|A pattern database is a lookup table that stores an exact evaluation function for a relaxed search problem, which provides an admissible heuristic for the original search problem. In general, the larger the pattern database, the more accurate the heuristic function. We consider how to build large pattern databases that are stored in external memory, such as disk, and how to use an external-memory pattern database efficiently in heuristic search. To limit the number of slow disk IO operations needed to construct and query an external-memory pattern data-base, we adapt an approach to external-memory graph search called structured duplicate detection that localizes memory references by leveraging an abstraction of the state space. We present results that show this approach increases the scalability of heuristic search by allowing larger and more accurate pattern database heuristics.|Rong Zhou,Eric A. Hansen","65603|AAAI|2005|Unsupervised Activity Recognition Using Automatically Mined Common Sense|A fundamental difficulty in recognizing human activities is obtaining the labeled data needed to learn models of those activities. Given emerging sensor technology, however, it is possible to view activity data as a stream of natural language terms. Activity models are then mappings from such terms to activity names, and may be extracted from text corpora such as the web. We show that models so extracted are sufficient to automatically produce labeled segmentations of activity data with an accuracy of % over  activities, well above the .% baseline. The segmentation so obtained is sufficient to bootstrap learning, with accuracy of learned models increasing to %. To our knowledge, this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned, even for labeling.|Danny Wyatt,Matthai Philipose,Tanzeem Choudhury","65782|AAAI|2006|Automatically Labeling the Inputs and Outputs of Web Services|Information integration systems combine data from multiple heterogeneous Web services to answer complex user queries, provided a user has semantically modeled the service first. To model a service, the user has to specify semantic types of the input and output data it uses and its functionality. As large number of new services come online, it is impractical to require the user to come up with a semantic model of the service or rely on the service providers to conform to a standard. Instead, we would like to automatically learn the semantic model of a new service. This paper addresses one part of the problem namely, automatically recognizing semantic types of the data used by Web services. We describe a metadata-based classification method for recognizing input data types using only the terms extracted from a Web Service Definition file. We then verify the classifier's predictions by invoking the service with some sample data of that type. Once we discover correct classification, we invoke the service to produce output data samples. We then use content-based classifiers to recognize semantic types of the output data. We provide performance results of both classification methods and validate our approach on several live Web services.|Kristina Lerman,Anon Plangprasopchok,Craig A. Knoblock","66534|AAAI|2008|Optimal Scheduling of Contract Algorithms with Soft Deadlines|A contract algorithm is an algorithm which is given, as part of its input, a specified amount of allowable computation time. In contrast, interruptible algorithms may be interrupted throughout their execution, at which point they must report their current solution. Simulating interruptible algorithms by means of schedules of executions of contract algorithms in parallel processors is a well-studied problem with significant applications in AI. In the classical case, the interruptions are hard deadlines in which a solution must be reported immediately at the time the interruption occurs. In this paper we study the more general setting of scheduling contract algorithms at the presence of soft deadlines. This is motivated by the observation of practitioners that soft deadlines are as common an occurrence as hard deadlines, if not more common. In our setting, at the time t of interruption the algorithm is given an additional window of time w(t)  c  t to continue the contract or, indeed, start a new contract (for some fixed constant c). We explore this variation using the acceleration ratio, which is the canonical measure of performance for these schedules, and derive schedules of optimal acceleration ratio for all functions w.|Spyros Angelopoulos,Alejandro L√≥pez-Ortiz,Ang√®le M. Hamel","65635|AAAI|2006|Model-Checking Memory Requirements of Resource-Bounded Reasoners|Memory bounds may limit the ability of a reasoner to make inferences and therefore affect the reasoner's usefulness. In this paper, we propose a framework to automatically verify the reasoning capabilities of propositional memory-bounded reasoners which have a sequential architecture. Our framework explicitly accounts for the use of memory both to store facts and to support backtracking in the course of deductions. We describe an implementation of our framework in which proof existence is recast as a strong planning problem, and present results of experiments using the MBP planner which indicate that memory bounds may not be trivial to infer even for simple problems, and that memory bounds and length of derivations are closely inter-related.|Alexandre Albore,Natasha Alechina,Piergiorgio Bertoli,Chiara Ghidini,Brian Logan,Luciano Serafini","66481|AAAI|2008|Efficient Haplotype Inference with Answer Set Programming|Identifying maternal and paternal inheritance is essential to be able to find the set of genes responsible for a particular disease. Although we have access to genotype data (genetic makeup of an individual), determining haplotypes (genetic makeup of the parents) experimentally is a costly and time consuming procedure due to technological limitations. With these biological motivations, we study a computational problem, called Haplotype Inference with Pure Parsimony (HIPP), that asks for the minimal number of haplotypes that form a given set of genotypes. We introduce a novel approach to solving HIPP, using Answer Set Programming (ASP). According to our experiments with a large number of problem instances (some automatically generated and some real), our ASP-based approach solves the most number of problems compared to other approaches based on, e.g., integer linear programming, branch and bound algorithms, SAT-based algorithms, or pseudo-boolean optimization methods.|Ferhan T√ºre,Esra Erdem","65188|AAAI|2004|Spatial Aggregation for Qualitative Assessment of Scientific Computations|Qualitative assessment of scientific computations is an emerging application area that applies a data-driven approach to characterize, at a high level, phenomena including conditioning of matrices, sensitivity to various types of error propagation, and algorithmic convergence behavior. This paper develops a spatial aggregation approach that formalizes such analysis in terms of model selection utilizing spatial structures extracted from matrix perturbation datasets. We focus in particular on the characterization of matrix eigenstructure, both analyzing sensitivity of computations with spectral portraits and determining eigenvalue multiplicity with Jordan portraits. Our approach employs spatial reasoning to overcome noise and sparsity by detecting mutually reinforcing interpretations, and to guide subsequent data sampling. It enables quantitative evaluation of properties of a scientific computation in terms of confidence in a model, explainable in terms of the sampled data and domain knowledge about the underlying mathematical structure. Not only is our methodology more rigorous than the common approach of visual inspection, but it also is often substantially more efficient, due to well-defined stopping criteria. Results show that the mechanism efficiently samples perturbation space and successfully uncovers high level properties of matrices.|Chris Bailey-Kellogg,Naren Ramakrishnan","65126|AAAI|1987|Memory-based Reasoning Applied to English Pronunciation|Memory-based Reasoning is a paradigm for AI in which best-match recall from memory is the primary inference mechanism. In its simplest form, it is a method of solving the inductive inference (learning) problem. The primary topics of this paper are a simple memory-based reasoning algorithm, the problem of pronouncing english words, and MBRtalk, a program which uses memory-based reasoning to solve the pronunciation problem. Experimental results demonstrate the properties of the algorithm as training-set size is varied, as distracting information is added, and as noise is added to the data.|Craig Stanfill","66295|AAAI|2008|Efficient Haplotype Inference with Answer Set Programming|Identifying maternal and paternal inheritance is essential to be able to find the set of genes responsible for a particular disease. However, due to technological limitations, we have access to genotype data (genetic makeup of an individual), and determining haplotypes (genetic makeup of the parents) experimentally is a costly and time consuming procedure. With these biological motivations, we study a computational problem, called Haplotype Inference by Pure Parsimony (HIPP), that asks for the minimal number of haplotypes that form a given set of genotypes. HIPP has been studied using integer linear programming, branch and bound algorithms, SAT-based algorithms, or pseudo-boolean optimization methods. We introduce a new approach to solving HIPP, using Answer Set Programming (ASP). According to our experiments with a large number of problem instances (some automatically generated and some real), our ASP-based approach solves the most number of problems compared with other approaches. Due to the expressivity of the knowledge representation language of ASP, our approach allows us to solve variations of HIPP, e.g., with additional domain specific information, such as patternsparts of haplotypes observed for some gene family, or with some missing genotype information. In this sense, the ASP-based approach is more general than the existing approaches to haplotype inference.|Esra Erdem,Ferhan T√ºre","66343|AAAI|2008|Supporting Manual Mapping Revision using Logical Reasoning|Finding correct semantic correspondences between ontologies is one of the most challenging problems in the area of semantic web technologies. Experiences with benchmarking matching systems revealed that even the manual revision of automatically generated mappings is a very difficult problem because it has to take the semantics of the ontologies as well as interactions between correspondences into account. In this paper, we propose methods for supporting human experts in the task of reviSing automatically created mappings. In particular, we present non-standard reasoning methods for detecting and propagating implications of expert decisions on the correctness of a mapping. We show that the use of these reasoning methods significantly reduces the effort of mapping revision in terms of the number of decisions that have to be made by the expert.|Christian Meilicke,Heiner Stuckenschmidt,Andrei Tamilin"],["65677|AAAI|2006|Computing Slater Rankings Using Similarities among Candidates|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One important voting rule is the Slater rule. It selects a ranking of the alternatives (or candidates) to minimize the number of pairs of candidates such that the ranking disagrees with the pairwise majority vote on these two candidates. The use of the Slater rule has been hindered by a lack of techniques to compute Slater rankings. In this paper, we show how we can decompose the Slater problem into smaller subproblems if there is a set of similar candidates. We show that this technique suffices to compute a Slater ranking in linear time if the pairwise majority graph is hierarchically structured. For the general case, we also give an efficient algorithm for finding a set of similar candidates. We provide experimental results that show that this technique significantly (sometimes drastically) speeds up search algorithms, Finally, we also use the technique of similar sets to show that computing an optimal Slater ranking is NP-hard. even in the absence of pairwise ties.|Vincent Conitzer","65210|AAAI|2004|A Computational Study of the Kemeny Rule for Preference Aggregation|We consider from a computational perspective the problem of how to aggregate the ranking preferences of a number of alternatives by a number of different voters into a single consensus ranking, following the majority voting rule. Social welfare functions for aggregating preferences in this way have been widely studied since the time of Condorcet (). One drawback of majority voting procedures when three or more alternatives are being ranked is the presence of cycles in the majority preference relation. The Kemeny order is a social welfare function whicll has been designed to tackle the presence of such cycles. However computing a Kemeny order is known to be NP-hard. We develop a greedy heuristic and an exact branch and bound procedure for computing Kemeny orders. We present results of a computational study on these procedures.|Andrew J. Davenport,Jayant Kalagnanam","66163|AAAI|2007|Computational Complexity of Weighted Threshold Games|Weighted threshold games are coalitional games in which each player has a weight (intuitively corresponding to its voting power), and a coalition is successful if the sum of its weights exceeds a given threshold. Key questions in coalitional games include finding coalitions that are stable (in the sense that no member of the coalition has any rational incentive to leave it), and finding a division of payoffs to coalition members (an imputation) that is fair. We investigate the computational complexity of such questions for weighted threshold games. We study the core, the least core, and the nucleolus, distinguishing those problems that are polynomial-time computable from those that are NP-hard, and providing pseudopolynomial and approximation algorithms for the NP-hard problems.|Edith Elkind,Leslie Ann Goldberg,Paul W. Goldberg,Michael Wooldridge","66401|AAAI|2008|Determining Possible and Necessary Winners under Common Voting Rules Given Partial Orders|Usually a voting rule or correspondence requires agents to give their preferences as linear orders. However, in some cases it is impractical for an agent to give a linear order over all the alternatives. It has been suggested to let agents submit partial orders instead. Then, given a profile of partial orders and a candidate c, two important questions arise first, is c guaranteed to win, and second, is it still possible for c to win These are the necessary winner and possible winner problems, respectively. We consider the setting where the number of alternatives is unbounded and the votes are unweighted. We prove that for Copeland, maximin, Bucklin, and ranked pairs, the possible winner problem is NP-complete also, we give a sufficient condition on scoring rules for the possible winner problem to be NP-complete (Borda satisfies this condition). We also prove that for Copeland and ranked pairs, the necessary winner problem is coNP-complete. All the hardness results hold even when the number of undetermined pairs in each vote is no more than a constant. We also present polynomial-time algorithms for the necessary winner problem for scoring rules, maximin, and Bucklin.|Lirong Xia,Vincent Conitzer","66425|AAAI|2008|Voting on Multiattribute Domains with Cyclic Preferential Dependencies|In group decision making, often the agents need to decide on multiple attributes at the same time, so that there are exponentially many alternatives. In this case, it is unrealistic to ask agents to communicate a full ranking of all the alternatives. To address this, earlier work has proposed decomposing such voting processes by using local voting rules on the individual attributes. Unfortunately, the existing methods work only with rather severe domain restrictions, as they require the voters' preferences to extend acyclic CP-nets compatible with a common order on the attributes. We first show that this requirement is very restrictive, by proving that the number of linear orders extending an acyclic CP-net is exponentially smaller than the number of all linear orders. Then, we introduce a very general methodology that allows us to aggregate preferences when voters express CP-nets that can be cyclic. There does not need to be any common structure among the submitted CP-nets. Our methodology generalizes the earlier, more restrictive methodology. We study whether properties of the local rules transfer to the global rule, and vice versa. We also address how to compute the winning alternatives.|Lirong Xia,Vincent Conitzer,J√©r√¥me Lang","65299|AAAI|2004|mCP Nets Representing and Reasoning with Preferences of Multiple Agents|We introduce mCP nets, an extension of the CP net formalism to model and handle the qualitative and conditional preferences of multiple agents. We give a number of different semantics for reasoning with mCP nets. The semantics are all based on the idea of individual agents voting. We describe how to test optimality and preference ordering within a mCP net, and we give complexity results for such tasks. We also discuss whether the voting schemes fairly combine together the preference s of the individual agents.|Francesca Rossi,Kristen Brent Venable,Toby Walsh","65678|AAAI|2006|Improved Bounds for Computing Kemeny Rankings|Voting (or rank aggregation) is a general method for aggregating the preferences of multiple agents. One voting rule of particular interest is the Kemeny rule, which minimizes the number of cases where the final ranking disagrees with a vote on the order of two alternatives. Unfortunately, Kemeny rankings are NP-hard to compute. Recent work on computing Kemeny rankings has focused on producing good bounds to use in search-based methods. In this paper, we extend on this work by providing various improved bounding techniques. Some of these are based on cycles in the pairwise majority graph, others are based on linear programs. We completely characterize the relative strength of all of these bounds and provide some experimental results.|Vincent Conitzer,Andrew J. Davenport,Jayant Kalagnanam","66494|AAAI|2008|Optimal False-Name-Proof Voting Rules with Costly Voting|One way for agents to reach a joint decision is to vote over the alternatives. In open, anonymous settings such as the Internet, an agent can vote more than once without being detected. A voting rule is false-name-proof if no agent ever benefits from casting additional votes. Previous work has shown that all false-name-proof voting rules are unresponsive to agents' preferences. However, that work implicitly assumes that casting additional votes is costless. In this paper, we consider what happens if there is a cost to casting additional votes. We characterize the optimal (most responsive) false-name-proofwith-costs voting rule for  alternatives. In sharp contrast to the costless setting, we prove that as the voting population grows larger, the probability that this rule selects the majority winner converges to . We also characterize the optimal group false-name-proof rule for  alternatives, which is robust to coalitions of agents sharing the costs of additional votes. Unfortunately, the probability that this rule chooses the majority winner as the voting population grows larger is relatively low. We derive an analogous rule in a setting with  alternatives, and provide bounding results and computational approaches for settings with  or more alternatives.|Liad Wagman,Vincent Conitzer","66350|AAAI|2008|On the Dimensionality of Voting Games|In a yesno voting game, a set of voters must determine whether to accept or reject a given alternative. Weighted voting games are a well-studied subclass of yesno voting games, in which each voter has a weight, and an alternative is accepted if the total weight of its supporters exceeds a certain threshold. Weighted voting games are naturally extended to k-vector weighted voting games, which are intersections of k different weighted voting games a coalition wins if it wins in every component game. The dimensionality, k, of a k- vector weighted voting game can be understood as a measure of the complexity of the game. In this paper, we analyse the dimensionality of such games from the point of view of complexity theory. We consider the problems of equivalence, (checking whether two given voting games have the same set of winning coalitions), and minimality, (checking whether a given k-vector voting game can be simplified by deleting one of the component games, or, more generally, is equivalent to a k-weighted voting game with k  k). We show that these problems are computationally hard, even if k   or all weights are  or . However, we provide efficient algorithms for cases where both k is small and the weights are polynomially bounded. We also study the notion of monotonicity in voting games, and show that monotone yesno voting games are essentially as hard to represent and work with as general games.|Edith Elkind,Leslie Ann Goldberg,Paul W. Goldberg,Michael Wooldridge","65679|AAAI|2006|Nonexistence of Voting Rules That Are Usually Hard to Manipulate|Aggregating the preferences of self-interested agents is a key problem for multiagent systems, and one general method for doing so is to vote over the alternatives (candidates). Unfortunately, the Gibbard-Satterthwaite theorem shows that when there are three or more candidates, all reasonable voting rules are manipulable (in the sense that there exist situations in which a voter would benefit from reporting its preferences insincerely). To circumvent this impossibility result, recent research has investigated whether it is possible to make finding a beneficial manipulation computationally hard. This approach has had some limited success, exhibiting rules under which the problem of finding a beneficial manipulation is NP-hard, P-hard, or even PSPACE-hard. Thus, under these rules, it is unlikely that a computationally efficient algorithm can be constructed that always finds a beneficial manipulation (when it exists). However, this still does not preclude the existence of an efficient algorithm that often finds a successful manipulation (when it exists). There have been attempts to design a rule under which finding a beneficial manipulation is usually hard, but they have failed. To explain this failure, in this paper, we show that it is in fact impossible to design such a rule, if the rule is also required to satisfy another property a large fraction of the manipulable instances are both weakly monotone, and allow the manipulators to make either of exactly two candidates win. We argue why one should expect voting rules to have this property, and show experimentally that common voting rules clearly satisfy it. We also discuss approaches for potentially circumventing this impossibility result.|Vincent Conitzer,Tuomas Sandholm"],["66520|AAAI|2008|Planning with Problems Requiring Temporal Coordination|We present the first planner capable of reasoning with both the full semantics of PDDL. (level ) temporal planning and with numeric resources. Our planner, CRIKEY, employs heuristic forward search, using the start-and-end semantics of PDDL. to manage temporal actions. The planning phase is interleaved with a scheduling phase, using a Simple Temporal Network, in order to ensure that temporal constraints are met. To guide search, we introduce a new temporal variant of the Relaxed Planning Graph heuristic that is capable of reasoning with the features of this class of domains, along with the Timed Initial Literals of PDDL.. CRIKEY extends the state-of-the-art in handling the full temporal expressive power of PDDL., including numeric temporal domains.|Andrew Coles,Maria Fox,Derek Long,Amanda Smith","65962|AAAI|2007|ESP A Logic of Only-Knowing Noisy Sensing and Acting|When reasoning about actions and sensors in realistic domains, the ability to cope with uncertainty often plays an essential role. Among the approaches dealing with uncertainty, the one by Bacchus, Halpern and Levesque, which uses the situation calculus, is perhaps the most expressive. However, there are still some open issues. For example, it remains unclear what an agent's knowledge base would actually look like. The formalism also requires second-order logic to represent uncertain beliefs, yet a first-order representation clearly seems preferable. In this paper we show how these issues can be addressed by incorporating noisy sensors and actions into an existing logic of only-knowing.|Alfredo Gabaldon,Gerhard Lakemeyer","66161|AAAI|2007|Forgetting Actions in Domain Descriptions|Forgetting irrelevantproblematic actions in a domain description can be useful in solving reasoning problems, such as query answering, planning, conftict resolution, prediction, postdiction, etc.. Motivated by such applications, we study what forgetting is, how forgetting can be done, and for which applications forgetting can be useful and how, in the context of reasoning about actions. We study these questions in the action language C (a formalism based on causal explanations), and relate it to forgetting in classical logic and logic programming.|Esra Erdem,Paolo Ferraris","65836|AAAI|2006|Reasoning about Partially Observed Actions|Partially observed actions are observations of action executions in which we are uncertain about the identity of objects, agents, or locations involved in the actions (e.g., we know that action move(o, x, y) occurred, but do not know o, y). Observed-Action Reasoning is the problem of reasoning about the world state after a sequence of partial observations of actions and states. In this paper we formalize Observed-Action Reasoning, prove intractability results for current techniques, and find tractable algorithms for STRIPS and other actions. Our new algorithms update a representation of all possible world states (the belief state) in logic using new logical constants for unknown objects. A straightforward application of this idea is incorrect, and we identify and add two key amendments. We also present successful experimental results for our algorithm in Blocks-world domains of varying sizes and in Kriegspiel (partially observable chess). These results are promising for relating sensors with symbols, partial-knowledge games, multi-agent decision making, and AI planning.|Megan Nance,Adam Vogel,Eyal Amir","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66005|AAAI|2007|Beyond Individualism Modeling Team Playing Behavior in Robot Soccer through Case-Based Reasoning|We propose a Case-Based Reasoning approach for action selection in the robot soccer domain presented in the th European Conference on Case-Based Reasoning (). Based on the current state of a game, the robots retrieve the most similar past situation and then the team reproduces the sequence of actions performed in that occasion. In this domain we have to deal with all the difficulties that a real environment involves.|Raquel Ros,Manuela M. Veloso,Ramon L√≥pez de M√°ntaras,Carles Sierra,Josep Llu√≠s Arcos","66111|AAAI|2007|Action-Based Alternating Transition Systems for Arguments about Action|This paper presents a formalism to describe practical reasoning in terms of an Action-based Alternating Transition System (AATS). The starting point is a previously specified account of practical reasoning that treats reasoning about what action should be chosen as presumptive argumentation using argument schemes and associated critical questions. This paper describes how this account can be extended to situations where the effect of an action is partially dependent upon the choices of another agent. In this context we see practical reasoning as proceeding in three stages. The first involves determining the representation of the particular problem scenario as an AATS. Next the agent must resolve its uncertainties as to its position in the scenario. Finally, the agent moves to choosing a particular action to achieve its ends, proposing presumptive reasons for particular actions and subjecting them to a critique to establish their suitability, taking into account the choices that can be made by the other agents involved. This account thus provides a well-specified basis for addressing the problems of practical reasoning as presumptive argumentation in a multi-agent context.|Katie Atkinson,Trevor J. M. Bench-Capon","65024|AAAI|1987|MU A Development Environment for Prospective Reasoning Systems|We describe a style of problem solving, prospective reasoning, and a development environment, MU, for building prospective reasoning systems. Prospective reasoning is a form of planning in which knowledge of the state of the world and the effects of actions is incomplete. We illustrate one implementation of prospective reasoning in MU with examples from medical diagnosis.|Paul R. Cohen,Michael Greenberg,Jefferson DeLisio","65018|AAAI|1987|Reasoning about Exceptions during Plan Execution Monitoring|In a cooperative problem-solving environment, such as an office, a hierarchical planner can be incorporated into an intelligent interface to accomplish tasks. During plan execution monitoring, user actions may be inconsistent with system expectations. In this paper, we present an approach towards reasoning about these exceptions in an attempt to accommodate them into an evolving plan. We propose a representation for plans and domain objects that facilitates reasoning about exceptions.|Carol A. Broverman,W. Bruce Croft","65095|AAAI|1987|Curing Anomalous Extensions|In a recent paper, Hanks and McDermott presented a simple problem in temporal reasoning which showed that a seemingly natural representation of a frame axiom in non monotonic logic can give rise to an anomalous extension, i.e., one which is counter-intuitive in that it does not appear to be supported by the known facts. An alternative, less formal approach to nonmonotonic reasoning uses the mechanism of a truth maintenance system (TMS). Surprisingly, when reformulated in terms of a TMS, the anomalous extension noted by Hanks and McDermott disappears. We analyze the reasons for this. First it is seen that anomalous extensions are not limited to temporal reasoning, but can occur in simple non-temporal default reasoning as well. In these cases also, the natural TMS representation avoids the problem. Exploring further, it is observed that the form of the TMS justifications resembles that of nonnormal default rules. Nonnormal rules have already been proposed as a means of avoiding anomalous extensions in some non-temporal reasoning situations. It appears that, suitably formulated, they can exclude the anomalous extension in the Hanks-McDermott case also, although the representation does not adjust smoothly to fresh information, as does the TMS. Some variant of nonnormal default appears to be required to provide a correct semantic basis for truth maintenance systems.|Paul Morris"],["65394|AAAI|2005|Robust and Self-Repairing Formation Control for Swarms of Mobile Agents|We describe a decentralized algorithm for coordinating a swarm of identically-programmed mobile agents to spatially self-aggregate into arbitrary shapes using only local interactions. Our approach, called SHAPEBUGS, generates a consensus coordinate system by agents continually performing local trilaterations, and achieves shape formation by simultaneously allowing agents to disperse within the defined D shape using a Contained Gas Model. This approach has several novel features () agents can easily aggregate into arbitrary user-specified shapes, using a formation process that is independent of the number of agents () the system automatically adapts to influx and death of agents, as well as accidental displacement. We show that the consensus coordinate system is robust and provides reasonable accuracy in the face of significant sensor and movement error.|Jimming Cheng,Winston Cheng,Radhika Nagpal","65921|AAAI|2006|Trust Representation and Aggregation in a Distributed Agent System|This paper considers a distributed system of software agents who cooperate in helping their users to find services, provided by different agents. The agents need to ensure that the service providers they select are trustworthy. Because the agents are autonomous and there is no central trusted authority, the agents help each other determine the trustworthiness of the service providers they are interested in. This help is rendered via a series of referrals to other agents, culminating in zero or more trustworthy service providers being identified. A trust network is a multiagent system where each agent potentially rates the trustworthiness of another agent. This paper develops a formal treatment of trust networks. At the base is a recently proposed representation of trust via a probability certainty distribution. The main contribution of this paper is the definition of two operators, concatenation and aggregation, using which trust ratings can be combined in a trust network. This paper motivates and establishes some important properties regarding these operators, thereby ensuring that trust can be combined correctly. Further, it shows that effects of malicious agents, who give incorrect information, are limited.|Yonghong Wang,Munindar P. Singh","65629|AAAI|2006|Biconnected Structure for Multi-Robot Systems|Many applications of distributed autonomous robotic systems can benefit from, or even may require, the team of robots staying within communication connectivity. For example, consider the problem of multirobot surveillance (Ahmadi & Stone ), in which a team of robots must collaboratively patrol a given area. If any two robots can directly communicate at all times, the robots can coordinate for efficient behavior. This condition holds trivially in environments that are smaller than the robots' communication range. However in larger environments, the robots must actively maintain physical locations such that any two robots can communicate -- possibly through a series of other robots. Otherwise, the robots may lose track of each others' activities and become miscoordinated. Furthermore, since robots are relatively unreliable andor may need to change tasks (for example if a robot is suddenly called by a human user to perform some other task), in a stable multirobot surveillance system, if one of the robots leaves or crashes, the rest should still be able to communicate. Some examples of other tasks that could benefit from any pair of robots being able to communicate with each other, are multi-robot exploration, search and rescue, and cleaning robots. We say that robot R is connected to robot R if there is a series of robots, each within communication range of the previous, which can pass a message from R to R. It is not possible to maintain connectivity in the face of arbitrary numbers of robot departures if there are any two robots that are not within communication of one another and all other robots simultaneously depart, the system becomes disconnected. Thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust its positioning in response to a departure more quickly than a second departure will occur. In order for the team to stay connected, even in the face of any single departure, it must be the case that every robot is connected to each other robot either directly or via two distinct paths that do not share any robots in common. We call this property biconnectivity the removal of any one robot from the system does not disconnect the remaining robots from each other.|Mazda Ahmadi,Peter Stone","66072|AAAI|2007|Acquiring Visibly Intelligent Behavior with Example-Guided Neuroevolution|Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central rule in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated example, of designers, players, or subject-matter experts.|Bobby D. Bryant,Risto Miikkulainen","66590|AAAI|2008|Agent Organized Networks Redux|Individual robots or agents will often need to form coalitions to accomplish shared tasks, e.g., in sensor networks or markets. Furthermore, in most real systems it is infeasible for entities to interact with all peers. The presence of a social network can alleviate this problem by providing a neighborhood system within which entities interact with a reduced number of peers. Previous research has shown that the topology of the underlying social network has a dramatic effect on the quality of coalitions formed and consequently on system performance (Gaston & deslardins a). It has also been shown that it is feasible to develop agents which dynamically alter connections to improve an organization's ability to form coalitions on the network. However those studies have not analysed the network topologies that result from connectivity adaptation strategies. In this paper the resulting network topologies were analysed and it was found that high performance and rapid convergence were attained because scale free networks were being formed. However it was observed that organizational performance is not impacted by limiting the number of links per agent to the total number of skills available within the population. implying that bandwidth was wasted by previous approaches. We used these observations to inform the design of a token based algorithm that attains higher performance using an order of magnitude less messages for both uniform and non-uniform distributions of skills.|Robin Glinton,Katia P. Sycara,Paul Scerri","65566|AAAI|2005|OAR A Formal Framework for Multi-Agent Negotiation|In Multi-Agent systems, agents often need to make decisions about how to interact with each other when negotiating over task allocation. In this paper, we present OAR, a formal framework to address the question of how the agents should interact in an evolving environment in order to achieve their different goals. The traditional categorization of self-interested and cooperative agents is unified by adopting a utility view. We illustrate mathematically that the degree of cooperativeness of an agent and the degree of its selfdirectness are not directly related. We also show how OAR can be used to evaluate different negotiation strategies and to develop distributed mechanisms that optimize the performance dynamically. This research demonstrates that sophisticated probabilistic modeling can be used to understand the behaviors of a system with complex agent interactions.|Jiaying Shen,Ingo Weber,Victor R. Lesser","65883|AAAI|2006|Behaviosites Manipulation of Multiagent System Behavior through Parasitic Infection|In this paper we present the Behaviosite Paradigm, a new approach to coordination and control of distributed agents in a multiagent system, inspired by biological parasites with behavior manipulation properties. Behaviosites are code modules that \"infect\" a system, attaching themselves to agents and altering the sensory activity and actions of those agents. These behavioral changes can be used to achieve altered, potentially improved, performance of the overall system thus, Behaviosites provide a mechanism for distributed control over a distributed system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it. To demonstrate our approach, we use behaviosites to control the behavior of a swarm of simple agents. With a relatively low infection rate, a few behaviosites can engender desired behavior over the swarm as a whole keeping it in one place, leading it through checkpoints, or moving the swarm from one stable equilibrium to another. We contrast behaviosites as a distributed swarm control mechanism with alternatives, such as the use of group leaders, herders, or social norms.|Amit Shabtay,Zinovi Rabinovich,Jeffrey S. Rosenschein","65574|AAAI|2005|MGLAIR Agents in Virtual and Other Graphical Environments|We are demonstrating several intelligent agents built according to the MGLAIR (Modal Grounded Layered Architecture with Integrated Reasoning) agent architecture. The top layer of MGLAIR is implemented in SNePS and its acting subsystem, SNeRE (the SNePS Rational Engine). The major demonstration will be act  of The Trial The Trail, an interactive drama running on an immersive Virtual Reality system, in which a human participant interacts with several MGLAIR actor-agents. We will also demonstrate several olher MGLAIR agents that operate in non-VR graphical environments. All these agents illustrate our approach to building agents with integrated first-person, on-line reasoning and acting.|Stuart C. Shapiro,Josephine Anstey,David E. Pape,Trupti Devdas Nayak,Michael Kandefer,Orkan Telhan","66245|AAAI|2007|Stochastic Filtering in a Probabilistic Action Model|Stochastic filtering is the problem of estimating the state of a dynamic system after time passes and given partial observations. It is fundamental to automatic tracking, planning, and control of real-world stochastic systems such as robots, programs, and autonomous agents. This paper presents a novel sampling-based filtering algorithm. Its expected error is smaller than sequential Monte Carlo sampling techniques given a fixed number of samples, as we prove and show empirically. It does so by sampling deterministic action sequences and then performing exact filtering on those sequences. These results are promising for applications in stochastic planning, natural language processing, and robot control.|Hannaneh Hajishirzi,Eyal Amir","65187|AAAI|2004|Intelligent Systems Demonstration The Secure Wireless Agent Testbed SWAT|We will demonstrate the Secure Wireless Agent Testbed (SWAT), a unique facility developed at Drexel University to study integration, networking and information assurance for next-generation wireless mobile agent systems. SWAT is an implemented system that fully integrates ) mobile agents, ) wireless ad hoc multi-hop networks, and ) security. The demonstration will show the functionality of a number of decentralized agent-based applications, including applications for authentication, collaboration, messaging, and remote sensor monitoring. The demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen nodes (PDAs, tablet PCs, and laptops) and hundreds of mobile software agents.|Gustave Anderson,Andrew Burnheimer,Vincent A. Cicirello,David Dorsey,Saturnino Garcia,Moshe Kam,Joseph Kopena,Kris Malfettone,Andrew Mroczkowski,Gaurav Naik,Maxim Peysakhov,William C. Regli,Joshua Shaffer,Evan Sultanik,Kenneth Tsang,Leonardo Urbano,Kyle Usbeck,Jacob Warren"],["66147|AAAI|2007|Real Arguments Are Approximate Arguments|There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A common assumption for logic-based argumentation is that an argument is a pair (, ) where  is minimal subset of the knowledgebase such that  is consistent and  entails the claim . However, real arguments (i.e. arguments presented by humans) usually do not have enough explicitly presented premises for the entailment of the claim. This is because there is some common knowledge that can be assumed by a proponent of an argument and the recipient of it. This allows the proponent of an argument to encode an argument into a real argument by ignoring the common knowledge, and it allows a recipient of a real argument to decode it into an argument by drawing on the common knowledge. If both the proponent and recipient use the same common knowledge, then this process is straightforward. Unfortunately, this is not always the case, and raises the need for an approximation of the notion of an argument for the recipient to cope with the disparities between the different views on what constitutes common knowledge.|Anthony Hunter","66746|AAAI|2010|Learning Simulation Control in General Game-Playing Agents|Technology trends in today's cooperative design environments are making it more and more important to monitor the network performance and ensure the network security. This paper describes the design and implementation of a distributed network traffic monitoring system based on embedded NetFlow hardware and software engines. The system architecture and design principles were introduced in the paper, some discussions were also presented about the NetFlow-based network monitoring technologies. The system had been successfully used to monitor highspeed campus networks at full wire speed without packet sampling in scenarios where commercial NetFlow collectors could not be used due to their limitations. Results show that this is an effective mechanism to identify, diagnose, and determine controls for network activities in CSCW environments and other network-based applications.|Hilmar Finnsson,Yngvi Bj√∂rnsson","66590|AAAI|2008|Agent Organized Networks Redux|Individual robots or agents will often need to form coalitions to accomplish shared tasks, e.g., in sensor networks or markets. Furthermore, in most real systems it is infeasible for entities to interact with all peers. The presence of a social network can alleviate this problem by providing a neighborhood system within which entities interact with a reduced number of peers. Previous research has shown that the topology of the underlying social network has a dramatic effect on the quality of coalitions formed and consequently on system performance (Gaston & deslardins a). It has also been shown that it is feasible to develop agents which dynamically alter connections to improve an organization's ability to form coalitions on the network. However those studies have not analysed the network topologies that result from connectivity adaptation strategies. In this paper the resulting network topologies were analysed and it was found that high performance and rapid convergence were attained because scale free networks were being formed. However it was observed that organizational performance is not impacted by limiting the number of links per agent to the total number of skills available within the population. implying that bandwidth was wasted by previous approaches. We used these observations to inform the design of a token based algorithm that attains higher performance using an order of magnitude less messages for both uniform and non-uniform distributions of skills.|Robin Glinton,Katia P. Sycara,Paul Scerri","65242|AAAI|2004|Making Argumentation More Believable|There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A problem with these proposals is that they do not consider the believability of the arguments from the perspective of the intended audience. In this paper, we start by reviewing a logic-based framework for argumentation based on argument trees which provide a way of exhaustively collating arguments and counter-arguments. We then extend this framework to it model-theoretic evaluation of the believability of arguments. This extension assumes that the beliefs of a typical member of the audience for argumentation can be represented by a set of classical formulae (a beliefbase). We compare a beliefbase with each argument to evaluate the empathy (or similarly the antipathy) that an agent has for the argument. We show how we can use empathy and antipathy to define a pre-ordering relation over argument trees that captures how nne argument tree is \"more believable\" than another. We also use these to define criteria for deciding whether an argument at the root of an argument tree is defeated or undeleated given the other arguments in the tree.|Anthony Hunter","65138|AAAI|1987|Plan Inference and Student Modeling in ICAI|This paper adresses the problem of building user models within the framework of Computer Assisted Instruction (ICAI), and more particularly for systems teaching elementary arithmetic or algebra. By 'model building\" we mean the understanding of the student's perfonnances, as well as a global description and evaluation of hisher ability (competence), including a representation of some errors. As an application domain we have here retained the learning of 'calculus' in the field of rational numbers, as an intennediate area between arithmetic and algebra. The aim of our system is to contml the way in which the pupil solves exercises. In the light of the particular nature of the chosen application, the main points to be stressed are the following  - calculations are described as plan generation and execution  consequently the student's modelling consists primarily in plan inferencing - the system takes into account the non detenninistic nature of the task, and recognizes valid variants of expert calculation plans - numerous errors are detected and categorized - the system accepts that the student write the calculations in a more or less elliptic manner whenever ambiguities occur, the student is precisely asked about implicit stcps of his calculations, and the system uses the answers given to reduce the uncertainties - a global model of the student is generated, which incorporates observations and appreciations  this model, in tum, determines the subsequent interpretations. All these questions are discussed both at the fundamental and the methodological levels.|Y. M. Visetti,Philippe Dague","65071|AAAI|1987|Rules for the Implicit Acquisition of Knowledge about the User|A major problem with incorporating a user model into an application has been the difficulty of acquiring the information for the user model. To make the user model effective, past approaches have relied heavily upon the explicit encoding of a large amount of information about potential system users. This paper discusses techniques for acquiring knowledge about the user implicitly (as the interaction with the user proceeds) in interactions between users and cooperative advisory systems. These techniques were obtained by analyzing transcripts of a large number of interactions between advice-seekers and a human expert, and have been encoded as a set of user model acquisition rules. Furthermore, the rules are domain independent, supporting the feasibility of building a general user modelling module.|Robert Kass,Tim Finn","65243|AAAI|2004|Towards Higher Impact Argumentation|There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments, An example is the framework by Besnard and Hunter that is based on classical logic and in which an argument (obtained from a knowledgebase) is a pair where the first item is a minimal consistent set of formulae that proves the second item (which is a formula). In the framework, the only counter-arguments (defeaters) that need to be taken into account are canonical arguments (a form of minimal undercut). Argumem trees then provide a way of exhaustively collating arguments and counter-arguments. A problem with this set up is that some argument trees may be \"too big\" to have sufficient impact. In this paper, we address the need to increase the impact of argumentation by using pruned argument trees. We formalize this in terms of how arguments resonate with the intended audience of the arguments. For example, if a politician Wants to make a case for raising taxes, the arguments used would depend on what is important to the audience Arguments based on increased taxes are needed to pay for improved healthcare would resonate better with an audience of pensioners, whereas arguments based on increased taxes are needtd to pay for improved transport infrastructure would resonate better with an audience of business executives. By analysing the resonance of arguments, we can prune argument trees to raise their impact.|Anthony Hunter","66485|AAAI|2008|Reasoning about the Appropriateness of Proponents for Arguments|Formal approaches to modelling argumentation provide ways to present arguments and counterarguments, and to evaluate which arguments are, in a formal sense, warranted. While these proposals allow for evaluating object-level arguments and counterarguments, they do not give sufficient consideration to evaluating the proponents of the arguments. Yet in everyday life we consider both the contents of an argument and its proponent. So if we do not trust a proponent, we may choose to not trust their arguments. Or if we are faced with an argument that we do not have the expertise to assess (for example when deciding whether to agree to having a particular surgical operation), we tend to agree to an argument by someone who is an expert. In general, we see that for each argument, we need to determine the appropriateness of the proponent for it. So for an argument about our health, our doctor is normally an appropriate proponent, but for an argument about our investments, our doctor is normally not an appropriate proponent. In this way, a celebrity is rarely an appropriate proponent for an argument, and a liar is not necessarily an inappropriate proponent for an argument. In this paper, we provide a logic-based framework for evaluating arguments in terms of the appropriateness of the proponents.|Anthony Hunter","65058|AAAI|1987|An Experimental Comparison of Knowledge Engineering for Expert Systems and for Decision Analysis|Decision analysis provides a set of techniques for structuring and encoding expert knowledge, comparable with knowledge engineering techniques for rule-based expert systems. In order to compare the expert systems and decision analysis approach, each was applied to the same task, namely the diagnosis and treatment of root disorders in apple trees. This experiment illustrates a variety of theoretical and practical differences between them, including the semantics of the network representations (inference net vs. influence diagram or Bayes' belief net), approaches to modelling uncertainty and preferences, the relative effort required, and their attitudes to human reasoning under uncertainty, as the ideal to be emulated or as unreliable and to be improved upon.|Max Henrion,Daniel R. Cooley","65367|AAAI|2005|Practical First-Order Argumentation|There are many frameworks for modelling argumentation in logic. They include a formal representation of individual arguments and techniques for comparing conflicting arguments. A problem with these proposals is that they do not consider arguments for and against first-order formulae. We present a framework for first-order logic argumentation based on argument trees that provide a way of exhaustively collating arguments and counter-arguments. A difficulty with first-order argumentation is that there may be many arguments and counterarguments even with a relatively small knowledgebase. We propose rationalizing the arguments under consideration with the aim of reducing redundancy and highlighting key points.|Philippe Besnard,Anthony Hunter"],["65197|AAAI|2004|Detecting and Eliminating the Cascade Vulnerability Problem from Multilevel Security Networks Using Soft Constraints|The security of a network configuration is based, not just on the security of its individual components and their direct interconnections, but it is also based on the potential for systems to interoperate indirectly across network routes. Such interoperation has been shown to provide the potential for cascading paths that violate security, in a circuitous manner, across a network. In this paper we show how constraint programming provides a natural approach to expressing the necessary constraints to ensure multilevel security across a network configuration. In particular, soft constraints are used to detect and eliminate the cascading network paths that violate security. Taking this approach results in practical advancements over existing solutions to this problem. In particular, constraint satisfaction highlights the set of all cascading paths, upon which we can compute in polynomial time an optimal reconfiguration of the network and ensure security.|Stefano Bistarelli,Simon N. Foley,Barry O'Sullivan","65650|AAAI|2006|Acquiring Constraint Networks Using a SAT-based Version Space Algorithm|Constraint programming is a commonly used technology for solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. We propose a basis for addressing this problem a new SAT-based version space algorithm for acquiring constraint networks from examples of solutions and non-solutions of a target problem. An important advantage of the algorithm is the ease with which domain-specific knowledge can be exploited.|Christian Bessi√®re,Remi Coletta,Fr√©d√©ric Koriche,Barry O'Sullivan","65475|AAAI|2005|Neighborhood Interchangeability and Dynamic Bundling for Non-Binary Finite CSPs|Neighborhood Interchangeability (NI) identifies the equivalent values in the domain of a variable of a Constraint Satisfaction Problem (CSP) by considering only the constraints that directly apply to the variable. Freuder described an algorithm for efficiently computing NI values in binary CSPs. In this paper, we show that the generalization of this algorithm to non-binary CSPs is not straightforward, and introduce an efficient algorithm for computing. NI values in the presence of non-binary constraints. Further, we show how to interleave this mechanism with search for solving CSPs, thus yielding a dynamic bundling strategy. While the goal of dynamic bundling is to produce multiple robust solutions, we empirically show that it does not increase (but significantly decreases) the cost of search.|Anagh Lal,Berthe Y. Choueiry,Eugene C. Freuder","66283|AAAI|2008|Distance Metric Learning Versus Fisher Discriminant Analysis|There has been much recent attention to the problem of learning an appropriate distance metric, using class labels or other side information. Some proposed algorithms are iterative and computationally expensive. In this paper, we show how to solve one of these methods with a closed-form solution, rather than using semidefinite programming. We provide a new problem setup in which the algorithm performs better or as well as some standard methods, but without the computational complexity. Furthermore, we show a strong relationship between these methods and the Fisher Discriminant Analysis.|Babak Alipanahi,Michael Biggs,Ali Ghodsi","66306|AAAI|2008|A Hybrid Approach to Convoy Movement Planning in an Urban City|In this paper, we consider a high-fidelity Convoy Movement Problem motivated by the coordination and routing of convoys within a road transportation network in an urban city. It encompasses two classical combinatorial optimization problems - vehicle routing and resource constrained scheduling. We present an effective hybrid algorithm to dynamically manage the movement of convoys, where we combine the standard Dijkstra's shortest-path algorithm with constraint programming techniques. The effectiveness of the algorithm is illustrated with testing on varying problem sizes and complexity.|Ramesh Thangarajoo,Lucas Agussurja,Hoong Chuin Lau","65976|AAAI|2007|Optimal Multi-Agent Scheduling with Constraint Programming|We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.|Willem Jan van Hoeve,Carla P. Gomes,Bart Selman,Michele Lombardi","66773|AAAI|2010|A Two-Dimensional Topic-Aspect Model for Discovering Multi-Faceted Topics|Linear and nonlinear (i.e., kernel) discriminant analysis have been proposed to address the difficulties of the small sample problem, the curse of dimensionality, and the multi-modality of image data distribution in content-based image retrieval (CBIR). The existing discriminant analysis is implemented either in a regular way, such as MDA (multiple discriminant analysis), or in a biased way, such as biased discriminant analysis (BDA). A rich set of parameterized discriminant analysis is proposed as an alternative to the regular MDA and BDA when taking regularization into account to avoid the singularity of the scatter matrices. Extensive experiments are carried out for performance evaluation and the results show the superior performance of the parameterized discriminant analysis over regular MDA and BDA for both linear and nonlinear settings.|Michael Paul,Roxana Girju","65502|AAAI|2005|A Framework for Representing and Solving NP Search Problems|NP search and decision problems occur widely in AI, and a number of general-purpose methods for solving them have been developed. The dominant approaches include propositional satisfiability (SAT), constraint satisfaction problems (CSP), and answer set programming (ASP). Here, we propose a declarative constraint programming framework which we believe combines many strengths of these approaches, while addressing weaknesses in each of them. We formalize our approach as a model extension problem, which is based on the classical notion of extension of a structure by new relations. A parameterized version of this problem captures NP. We discuss properties of the formal framework intended to support effective modelling, and prospects for effective solver design.|David G. Mitchell,Eugenia Ternovska","65306|AAAI|2004|Evaluating Consistency Algorithms for Temporal Metric Constraints|We study the performance of some known algorithms for solving the Simple Temporal Problem (STP) and the Temporal Constraint Satisfaction Problem (TCSP). In particular, we empirically compare the Bellman-Ford (BF) algorithm and its incremental version (incBF) by (Cesta & Oddi ) to the STP of (Xu & Choueiry a). Among the tested algorithms, we show that STP is the most efficient for determining the consistency of an STP, and that incBF combined with the heuristics of (Xu & Choueiry b) is the most efficient for solving the TCSP. We plan to improve STP by exploiting incrementality as in incBF and other new incremental algorithms.|Yang Shi,Anagh Lal,Berthe Y. Choueiry","65241|AAAI|2004|Robust Solutions for Constraint Satisfaction and Optimization|Super solutions are solutions in which, if a small number of variables lose their values, we are guaranteed to be able to repair the solution with only a few changes. In this paper, we stress the need to extend the super solution framework along several dimensions to make it more useful practically. We demonstrate the usefulness of those extensions on an example from jobshop scheduling, an optimization problem solved through constraint satisfaction. In such a case there is indeed a trade-off between optimality and robustness, however robustness may be increased without sacrificing optimality.|Emmanuel Hebrard"],["65218|AAAI|2004|Complete Local Search for Propositional Satisfiability|Algorithms based on following local gradient information are surprisingly effective for certain classes of constraint satisfaction problems. Unfortunately, previous local search algorithms are notoriously incomplete They are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability. We present an algorithmic framework for complete local search and discuss in detail an instantiation for the propositional satisfiability problem (SAT). The fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minima. Although the algorithm has worst-case exponential space complexity, we present empirical resulls on challenging SAT competition benchmarks that suggest that our implementation can perform as well as state-of-the-art solvers based on more mature techniques. Our framework suggests a range of possible algorithms lying between tree-based search and local search.|Hai Fang,Wheeler Ruml","65516|AAAI|2005|Networked Distributed POMDPs A Synthesis of Distributed Constraint Optimization and POMDPs|In many real-world multiagent applications such as distributed sensor nets, a network of agents is formed based on each agent's limited interactions with a small number of neighbors. While distributed POMDPs capture the real-world uncertainty in multiagent domains, they fail to exploit such locality of interaction. Distributed constraint optimization (DCOP) captures the locality of interaction but fails to capture planning under uncertainty. This paper present a new model synthesized from distributed POMDPs and DCOPs, called Networked Distributed POMDPs (ND-POMDPs). Exploiting network structure enables us to present two novel algorithms for ND-POMDPs a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal.|Ranjit Nair,Pradeep Varakantham,Milind Tambe,Makoto Yokoo","65657|AAAI|2006|Fast Hierarchical Goal Schema Recognition|We present our work on using statistical, corpus-based machine learning techniques to simultaneously recognize an agent's current goal schemas at various levels of a hierarchical plan. Our recognizer is based on a novel type of graphical model, a Cascading Hidden Markov Model, which allows the algorithm to do exact inference and make predictions at each level of the hierarchy in time quadratic to the number of possible goal schemas. We also report results of our recognizer's performance on a plan corpus.|Nate Blaylock,James F. Allen","65467|AAAI|2005|Heterogeneous Multirobot Coordination with Spatial and Temporal Constraints|Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.|Mary Koes,Illah R. Nourbakhsh,Katia P. Sycara","66306|AAAI|2008|A Hybrid Approach to Convoy Movement Planning in an Urban City|In this paper, we consider a high-fidelity Convoy Movement Problem motivated by the coordination and routing of convoys within a road transportation network in an urban city. It encompasses two classical combinatorial optimization problems - vehicle routing and resource constrained scheduling. We present an effective hybrid algorithm to dynamically manage the movement of convoys, where we combine the standard Dijkstra's shortest-path algorithm with constraint programming techniques. The effectiveness of the algorithm is illustrated with testing on varying problem sizes and complexity.|Ramesh Thangarajoo,Lucas Agussurja,Hoong Chuin Lau","65608|AAAI|2005|Learning Planning Rules in Noisy Stochastic Worlds|We present an algorithm for learning a model of the effects of actions in noisy stochastic worlds. We consider learning in a D simulated blocks world with realistic physics. To model this world, we develop a planning representation with explicit mechanisms for expressing object reference and noise. We then present a learning algorithm that can create rules while also learning derived predicates, and evaluate this algorithm in the blocks world simulator, demonstrating that we can learn rules that effectively model the world dynamics.|Luke S. Zettlemoyer,Hanna Pasula,Leslie Pack Kaelbling","65819|AAAI|2006|Temporal Preference Optimization as Weighted Constraint Satisfaction|We present a new efficient algorithm for obtaining utilitarian optimal solutions to Disjunctive Temporal Problems with Preferences (DTPPs). The previous state-of-the-art system achieves temporal preference optimization using a SAT formulation, with its creators attributing its performance to advances in SAT solving techniques. We depart from the SAT encoding and instead introduce the Valued DTP (VDTP). In contrast to the traditional semiring-based formalism that annotates legal tuples of a constraint with preferences, our framework instead assigns elementary costs to the constraints themselves. After proving that the VDTP can express the same set of utilitarian optimal solutions as the DTPP with piecewise-constant preference functions, we develop a method for achieving weighted constraint satisfaction within a meta-CSP search space that has traditionally been used to solve DTPs without preferences. This allows us to directly incorporate several powerful techniques developed in previous decision-based DTP literature. Finally, we present empirical results demonstrating that an implementation of our approach consistently outperforms the SAT-based solver by orders of magnitude.|Michael D. Moffitt,Martha E. Pollack","66346|AAAI|2008|Semi-supervised Classification Using Local and Global Regularization|In this paper, we propose a semi-supervised learning (SSL) algorithm based on local and global regularization. In the local regularization part, our algorithm constructs a regularized classifier for each data point using its neighborhood, while the global regularization part adopts a Laplacian regularizer to smooth the data labels predicted by those local classifiers. We show that some existing SSL algorithms can be derived from our framework. Finally we present some experimental results to show the effectiveness of our method.|Fei Wang,Tao Li,Gang Wang,Changshui Zhang","65510|AAAI|2005|Temporal Dynamic Controllability Revisited|An important issue for temporal planners is the ability to handle temporal uncertainty. We revisit the question of how to determine whether a given set of temporal requirements are feasible in the light of uncertain durations of some processes. In particular, we consider how best to determine whether a network is Dynamically Controllable, i.e., whether a dynamic strategy exisls for executing the network that is guaranteed to satisfy the requirements. Previous work has shown the existence of a pseudo-polynomial algorithm for testing Dynamic Controllability. Here, we simplify the previous framework, and present a strongly polynomial algorithm with a termination criterion based on the structure of the network.|Paul H. Morris,Nicola Muscettola","66519|AAAI|2008|Terminological Reasoning in SHIQ with Ordered Binary Decision Diagrams|We present a new algorithm for reasoning in the description logic SHIQ, which is the most prominent fragment of the Web Ontology Language OWL. The algorithm is based on ordered binary decision diagrams (OBDDs) as a datastructure for storing and operating on large model representations. We thus draw on the success and the proven scalability of OBDD-based systems. To the best of our knowledge, we present the very first agorithm for using OBDDs for reasoning with general Tboxes.|Sebastian Rudolph,Markus Kr√∂tzsch,Pascal Hitzler"]]},"title":{"entropy":6.628223921473263,"topics":["reinforcement learning, semantic web, reasoning about, the web, bayesian networks, social networks, for networks, models for, support vector, for data, activity recognition, learning and, from data, learning for, partially observable, information extraction, using, learning, sense disambiguation, models","system for, system, for agents, system and, artificial intelligence, common sense, agents, and agents, mobile robot, human-robot interaction, for intelligent, and for, learning for, for robot, for environments, robot, games playing, architecture for, and control, large scale","planning with, mechanism design, for planning, local search, algorithm for, search for, and search, planning domains, decision processes, combinatorial auctions, belief revision, search, duplicate detection, planning and, new for, markov decision, planning, markov processes, voting rules, value iteration","with constraint, the and, answer set, natural language, for constraint, the, constraint, for the, and for, logic programs, answer programming, algorithm for, and constraint, for logic, constraint satisfaction, solving problem, set programming, the problem, case study, description logic","for data, from data, information extraction, from web, matrix factorization, and analysis, data and, for retrieval, mining web, from, and for, from the, and document, and from, features for, information retrieval, and, for text, and text, for extraction","the web, semantic web, for web, models for, for semantic, web service, and service, causal models, and web, for service, probabilistic models, service composition, the semantic, semantic and, trust social, models, web composition, for probabilistic, web planning, models and","system for, system and, for reasoning, artificial intelligence, and reasoning, system, representation and, reasoning, representation for, spatial reasoning, for task, the system, case-based reasoning, intelligence and, the reasoning, the intelligence, coalition generation, experimental comparison, and processing, case-based for","for intelligent, learning for, common sense, architecture for, intelligent system, system for, large scale, efficient learning, cognitive architecture, cognitive models, intelligent tutoring, for adaptive, intelligent search, dynamic for, cognitive for, the architecture, adaptive learning, and architecture, engineering for, for vehicle","search for, and search, mechanism design, local search, mechanism for, belief revision, search, for belief, dimensionality reduction, search space, for domains, for generating, graphical models, the search, fast algorithm, general for, fast for, belief and, search algorithm, best-first search","algorithm for, bound for, combinatorial auctions, the impact, voting rules, methods for, nash equilibria, lower bound, for max-sat, and its, and voting, lower for, for auctions, and for, for games, and auctions, evolutionary algorithm, for mdps, for finding, and combinatorial","for the, the and, the, the with, the web, the logic, case study, situation calculus, the semantic, the application, the reasoning, and challenge, the case, for propagation, the challenge, exploration the, for study, structure for, the probability, structure and","natural language, and for, description logic, and, the and, for language, language and, knowledge compilation, description for, and logic, knowledge and, the complexity, and complexity, for natural, stable for, complexity for, the description, the instance, for quantified, and results"],"ranking":[["66509|AAAI|2008|Learning Hidden Curved Exponential Family Models to Infer Face-to-Face Interaction Networks from Situated Speech Data|In this paper, we present a novel probabilistic framework for recovering global, latent social network structure from local, noisy observations. We extend curved exponential random graph models to include two types of variables hidden variables that capture the structure of the network and observational variables that capture the behavior between actors in the network. We develop a novel combination of informative and intuitive conversational (local) and structural (global) features to specify our model. The model learns, in an unsupervised manner, the relationship between observable behavior and hidden social structure while simultaneously learning properties of the latent structure itself. We present empirical results on both synthetic data and a real world dataset of face-to-face conversations collected from  individuals using wearable sensors over the course of  months.|Danny Wyatt,Tanzeem Choudhury,Jeff A. Bilmes","66656|AAAI|2010|Learning to Predict Opinion Share in Social Networks|De-noising by the traditional wavelet transform, the result is affected by the choosing of wavelet base. Because the wavelet base is fixed in the traditional wavelet transform, either the smoothness or singularity of the signal canpsilat be fitted quite well. To overcome the limitation, a new self-adaptive lifting scheme based on modulus maximum analysis is presented. Modulus maximum sequence of the large scale wavelet coefficients can locate the point of the signal with big singularity precisely. According to the position of the point with big singularity, proper neighborhood is fixed, and prediction operator can be chosen self-adaptively. In this way, the prediction operator is fitted to the local feature of the signal. The simulation and engineering application showed that the proposed method could overcome the de-noising disadvantage of traditional wavelet transform. It not only can filter noise from original signal effectively but also can hold local characteristics of original signal in the de-noised signals.|Masahiro Kimura,Kazumi Saito,Kouzou Ohara,Hiroshi Motoda","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","66893|AAAI|2010|Nonparametric Bayesian Approaches for Reinforcement Learning in Partially Observable Domains|The Domain system's objective is to integrate mainframe capability with local area networking and raster graphics capabilities at a cost appropriate to engineering and graphics applications.|Finale Doshi-Velez","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","65884|AAAI|2006|Learning Partially Observable Action Schemas|We present an algorithm that derives actions' effects and preconditions in partially observable, relational domains. Our algorithm has two unique features an expressive relational language, and an exact tractable computation. An action-schema language that we present permits learning of preconditions and effects that include implicit objects and unstated relationships between objects. For example, we can learn that replacing a blown fuse turns on all the lights whose switch is set to on. The algorithm maintains and outputs a relational-logical representation of all possible action-schema models after a sequence of executed actions and partial observations. Importantly, our algorithm takes polynomial time in the number of time steps and predicates. Time dependence on other domain parameters varies with the action-schema language. Our experiments show that the relational structure speeds up both learning and generalization, and outperforms propositional learning methods. It also allows establishing apriori-unknown connections between objects (e.g. light bulbs and their switches), and permits learning conditional effects in realistic and complex situations. Our algorithm takes advantage of a DAG structure that can be updated efficiently and preserves compactness of representation.|Dafna Shahaf,Eyal Amir","65885|AAAI|2006|Learning Partially Observable Action Models Efficient Algorithms|We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 's of features (more than  states).|Dafna Shahaf,Allen Chang,Eyal Amir","66549|AAAI|2008|Reinforcement Learning for Vulnerability Assessment in Peer-to-Peer Networks|Proactive assessment of computer-network vulnerability to unknown future attacks is an important but unsolved computer security problem where AI techniques have significant impact potential. In this paper, we investigate the use of reinforcement learning (RL) for proactive security in the context of denial-of-service (DoS) attacks in peer-to-peer (PP) networks. Such a tool would be useful for network administrators and designers to assess and compare the vulnerability of various network configurations and security measures in order to optimize those choices for maximum security. We first discuss the various dimensions of the problem and how to formulate it as RL. Next we introduce compact parametric policy representations for both single attacker and botnets and derive a policy-gradient RL algorithm. We evaluate these algorithms under a variety of network configurations that employ recent fair-use DoS security mechanisms. The results show that nur RL-based approach is able to significantly outperform a number of heuristic strategies in terms of the severity of the attacks discovered. The results also suggest some possible network design lessons for reducing the attack potential of an intelligent attacker.|Scott Dejmal,Alan Fern,Thinh Nguyen","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka","65014|AAAI|1987|Modular Learning in Neural Networks|In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.|Dana H. Ballard"],["65583|AAAI|2005|Mobile Robot Mapping and Localization in Non-Static Environments|Whenever mobile robots act in the real world, they need to be able to deal with non-static objects. In the context of mapping, a common technique to deal with dynamic objects is to filter out the spurious measurements corresponding to such objects. In this paper, we present a novel approach to estimate typical configurations of dynamic areas in the environment of a mobile robot. Our approach clusters local grid maps to identify the possible configurations. We furthermore describe how these clusters can be utilized within a Rao-Blackwellized particle filter to localize a mobile robot in a non-static environment. In practical experiments carried out with a mobile robot in a typical office environment, we demonstrate the advantages of our approach compared to alternative techniques for mapping and localization in dynamic environments.|Cyrill Stachniss,Wolfram Burgard","65969|AAAI|2007|The Marchitecture A Cognitive Architecture for a Robot Baby|The Marchitecture is a cognitive architecture for autonomous development of representations. The goals of The Marchitecture are domain independence, operating in the absence of knowledge engineering, learning an ontology of parameterized relational concepts, and elegance of design. To this end, The Marchitecture integrates classification, parsing, reasoning, and explanation. The Marchitecture assumes an ample amount of raw data to develop its representations, and it is therefore appropriate for long lived agents.|Marc Pickett,Tim Oates","65649|AAAI|2006|Perspective Taking An Organizing Principle for Learning in Human-Robot Interaction|The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach.|Matt Berlin,Jesse Gray,Andrea Lockerd Thomaz,Cynthia Breazeal","66746|AAAI|2010|Learning Simulation Control in General Game-Playing Agents|Technology trends in today's cooperative design environments are making it more and more important to monitor the network performance and ensure the network security. This paper describes the design and implementation of a distributed network traffic monitoring system based on embedded NetFlow hardware and software engines. The system architecture and design principles were introduced in the paper, some discussions were also presented about the NetFlow-based network monitoring technologies. The system had been successfully used to monitor highspeed campus networks at full wire speed without packet sampling in scenarios where commercial NetFlow collectors could not be used due to their limitations. Results show that this is an effective mechanism to identify, diagnose, and determine controls for network activities in CSCW environments and other network-based applications.|Hilmar Finnsson,Yngvi Bj√∂rnsson","65664|AAAI|2006|The Robot Intelligence Kernel|The Robot Intelligence Kernel (RIK) is a portable, reconfigurable suite of perceptual, behavioral, and cognitive capabilities that can be used across many different platforms, environments, and tasks. The RIK coupled with a virtual D interface have been shown to dramatically improve human-robot interactions across a variety of navigation and exploration tasks.|David J. Bruemmer,Douglas A. Few,Miles C. Walton,Curtis W. Nielsen","65580|AAAI|2005|Autonomous Color Learning on a Mobile Robot|Color segmentation is a challenging subtask in computer vision. Most popular approaches are computationally expensive, involve an extensive off-line training phase andor rely on a stationary camera. This paper presents an approach for color learning on-board a legged robot with limited computational and memory resources. A key defining feature of the approach is that it works without any labeled training data. Rather, it trains autonomously from a color-coded model of its environment. The process is fully implemented, completely autonomous, and provides high degree of segmentation accuracy.|Mohan Sridharan,Peter Stone","65563|AAAI|2005|Toward Affective Cognitive Robots for Human-Robot Interaction|We present a brief overview of an architecture for a complex affective robot for human-robot interaction.|Matthias Scheutz,James F. Kramer,Christopher Middendorff,Paul W. Schermerhorn,Michael Heilman,David Anderson,P. Bui","65824|AAAI|2006|Towards a Higher Level of Human-Robot Interaction and Integration|Spartacus, our  AAAI Mobile Robot Challenge entry, integrated planning and scheduling, sound source localization, tracking and separation, message reading, speech recognition and generation, and autonomous navigation capabilities onboard a custom-made interactive robot. Integration of such a high number of capabilities revealed interesting new issues such as coordinating audiovisualgraphical capabilities, monitoring the impacts of the capabilities in usage by the robot, and inferring the robot's intentions and goals. Our  entry will be used to address these issues, to add new capabilities to the robot and to improve our software and computational architectures, with the objective of increasing, evaluating and improving our understanding of human-robot interaction and integration with an autonomous mobile platform.|Fran√ßois Michaud,Dominic L√©tourneau,M. Fr√©chette,Eric Beaudry,Carle C√¥t√©,Froduald Kabanza","65139|AAAI|1987|Visual Estimation of -D Line Segments from Motion - A Mobile Robot Vision System|An efficient technique is presented for detecting, tracking and locating three-dimensional (-D) line segments. The utility of this technique has been demonstrated by the SRI mobile robot, which uses it to locate features in an office environment in real time (one Hz frame rate). A formulation of Structure-from-Motion using line segments is described. The formulation uses longitudinal as well as transverse information about the endpoints of image line segments. Although two images suffice to form an estimate of a world line segment, more images are used here to obtain a better estimate. The system operates in a sequential fashion, using prediction-based feature detection to eliminate the need for global image processing.|William M. Wells III","66498|AAAI|2008|Structure Learning on Large Scale Common Sense Statistical Models of Human State|Research has shown promise in the design of large scale common sense probabilistic models to infer human state from environmental sensor data. These models have made use of mined and preexisting common sense data and traditional probabilistic machine learning techniques to improve recognition of the state of everyday human life. In this paper, we demonstrate effective techniques for structure learning on graphical models designed for this domain, improving the SRCS system of (Pentney et al. ) by learning additional dependencies between variables. Because the models used for common sense reasoning typically involve a large number of variables, issues of scale arise in searching for additional dependencies we discuss how we use data mining techniques to address this problem. We show experimentally that these techniques improve the accuracy of state prediction, and that, with a good prior model, the use of a common sense model with structure learning provides better prediction of unlabeled variables as well as labeled variables. The results also demonstrate that it is possible to collect new common sense information about daily life using such a statistical model and labeled data.|William Pentney,Matthai Philipose,Jeff A. Bilmes"],["65429|AAAI|2005|Fast Planning in Domains with Derived Predicates An Approach Based on Rule-Action Graphs and Local Search|The ability to express \"derived predicates\" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of \"Rule-Action Graphs\", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina,Paolo Toninelli","66874|AAAI|2010|Search-Based Path Planning with Homotopy Class Constraints|An approach is investigated for the adaptive Hinfin control design for a class of nonlinear state-delayed systems. The nonlinear term is approximated by a linearly parameterized neural networks(LPNN). A linear state feedback Hinfin control law is presented. An adaptive weight adjustment mechanism for the neural networks is developed to ensure Hinfin regulation performance. It is shown that the control gain matrices and be transformed into a standard linear matrix inequality problem and solved via a developed recurrent neural network|Subhrajit Bhattacharya","66217|AAAI|2007|Purely Epistemic Markov Decision Processes|Planning under uncertainty involves two distinct sources of uncertainty uncertainty about the effects of actions and uncertainty about the current state of the world. The most widely developed model that deals with both sources of uncertainty is that of Partially Observable Markov Decision Processes (POMDPs). Simplifying POMDPs by getting rid of the second source of uncertainty leads to the well-known framework of fully observable MDPs. Getting rid of the first source of uncertainty leads to a less widely studied framework, namely, decision processes where actions cannot change the state of the world and are only intended to bring some information about the (static) state of the world. Such \"purely epistemic\" processes are very relevant, since many practical problems (such as diagnosis, database querying, or preference elicitation) fall into this class. However, it is not known whether this specific restriction of POMDP is computationally simpler than POMDPs. In this paper we establish several complexity results for purely epistemic MDPs (EMDPs). We first show that short-horizon policy existence in EMDPs is PSPACE-complete. Then we focus on the specific case of EMDPs with reliable observations and show that in this case, policy existence is \"only\" NP-complete however, we show that this problem cannot be approximated with a bounded performance ratio by a polynomial-time algorithm.|R√©gis Sabbadin,J√©r√¥me Lang,Nasolo Ravoanjanahry","66287|AAAI|2008|Fast Planning by Search in Domain Transition Graph|Recent advances in classical planning have used the SAS+ formalism, and several effective heuristics have been developed based on the SAS+ formalism. Comparing to the traditional STRIPSADL formalism, SAS+ is capable of capturing vital information such as domain transition structures and causal dependencies. In this paper, we propose a new SAS+ based incomplete planning approach. Instead of using SAS+ to derive heuristics within a heuristic search planner, we directly search in domain transition graphs (DTGs) and causal graphs (CGs) derived from the SAS+ formalism. The new method is efficient because the SAS+ representation is often much more compact than STRIPS. The CGs and DTGs provide rich information of domain structures that can effectively guide the search towards solutions. Experimental results show strong performance of the proposed planner on recent international planning competition domains.|Yixin Chen,Ruoyun Huang,Weixiong Zhang","65772|AAAI|2006|Controlled Search over Compact State Representations in Nondeterministic Planning Domains and Beyond|Two of the most efficient planners for planning in nondeterministic domains are MBP and ND-SHOP. MBP achieves its efficiency by using Binary Decision Diagrams (BDDs) to represent sets of states that share some common properties, so it can plan for all of these states simultaneously. ND-SHOP achieves its efficiency by using HTN task decomposition to focus the search. In some environments, ND-SHOP runs exponentially faster than MBP, and in others the reverse is true. In this paper, we discuss the following  We describe how to combine ND-SHOP's HTNs with MBP's BDDs. Our new planning algorithm, YoYo, performs task decompositions over classes of states that are represented as BDDs. In our experiments, YoYo easily outperformed both MBP and ND-SHOP, often by several orders of magnitude.  HTNs are just one of several techniques that are originally developed for classical planning domains and that can be adapted to work in nondeterministic domains. By combining those techniques with a BDD representation, it should be possible to get great speedups just as we did here. We discuss how these same ideas can be generalized for use in several other research areas, such as planning with Markov Decision Processes, synthesizing controllers for hybrid systems, and composing Semantic Web Services.|Ugur Kuter,Dana S. Nau","65220|AAAI|2004|Metrics for Finite Markov Decision Processes|We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.|Norm Ferns,Prakash Panangaden,Doina Precup","65643|AAAI|2006|Planning with First-Order Temporally Extended Goals using Heuristic Search|Temporally extended goals (TEGs) refer to properties that must hold over intermediate andor final states of a plan. The problem of planning with TEGs is of renewed interest because it is at the core of planning with temporal preferences. Currently, the fastest domain-independent classical planners employ some kind of heuristic search. However, existing planners for TEGs are not heuristic and are only able to prune the search space by progressing the TEG. In this paper we propose a method for planning with TEGs using heuristic search. We represent TEGs using a rich and compelling subset of a first-order linear temporal logic. We translate a planning problem with TEGs to a classical planning problem. With this translation in hand, we exploit heuristic search to determine a plan. Our translation relies on the construction of a parameterized nondeterministic finite automaton for the TEG. We have proven the correctness of our algorithm and analyzed the complexity of the resulting representation. The translator is fully implemented and available. Our approach consistently outperforms TLPLAN on standard benchmark domains, often by orders of magnitude.|Jorge A. Baier,Sheila A. McIlraith","65795|AAAI|2006|Functional Value Iteration for Decision-Theoretic Planning with General Utility Functions|We study how to find plans that maximize the expected total utility for a given MDP, a planning objective that is important for decision making in high-stakes domains. The optimal actions can now depend on the total reward that has been accumulated so far in addition to the current state. We extend our previous work on functional value iteration from one-switch utility functions to all utility functions that can be approximated with piecewise linear utility functions (with and without exponential tails) by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function. Functional value iteration does not maintain a value for every state but a value function that maps the total reward that has been accumulated so far into a value. We describe how functional value iteration represents these value functions in finite form, how it performs dynamic programming by manipulating these representations and what kinds of approximation guarantees it is able to make. We also apply it to a probabilistic blocksworld problem, a standard test domain for decision-theoretic planners.|Yaxin Liu,Sven Koenig","65473|AAAI|2005|Using Domain-Configurable Search Control for Probabilistic Planning|We describe how to improve the performance of MDP planning algorithms by modifying them to use the search-control mechanisms of planners such as TLPlan, SHOP, and TALplanner. In our experiments, modified versions of RTDP, LRTDP, and Value Iteration were exponentially faster than the original algorithms. On the largest problems the original algorithms could solve, the modified ones were about , times faster. On another set. of problems whose state spaces were more than , times larger than the original algorithms could solve, the modified algorithms took only about  second.|Ugur Kuter,Dana S. Nau","65652|AAAI|2006|An Iterative Algorithm for Solving Constrained Decentralized Markov Decision Processes|Despite the significant progress to extend Markov Decision Processes (MDP) to cooperative multi-agent systems, developing approaches that can deal with realistic problems remains a serious challenge. Existing approaches that solve Decentralized Markov Decision Processes (DEC-MDPs) suffer from the fact that they can only solve relatively small problems without complex constraints on task execution. OC-DEC-MDP has been introduced to deal with large DEC-MDPs under resource and temporal constraints. However, the proposed algorithm to solve this class of DEC-MDPs has some limits it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep (or iteration). In this paper, we propose to overcome these limits by first introducing the notion of Expected Opportunity Cost to better assess the influence of a local decision of an agent on the others. We then describe an iterative version of the algorithm to incrementally improve the policies of agents leading to higher quality solutions in some settings. Experimental results are shown to support our claims.|Aur√©lie Beynier,Abdel-Illah Mouaddib"],["66844|AAAI|2010|Automated Modelling and Solving in Constraint Programming|Text segmentation, or named text binarization, is usually an essential step for text information extraction from images and videos. However, most existing text segmentation methods have difficulties in extracting multi-polarity texts, where multi-polarity texts mean those texts with multiple colors or intensities in the same line. In this paper, we propose a novel algorithm for multi- polarity text segmentation based on graph theory. By representing a text image with an undirected weighted graph and partitioning it iteratively, multi-polarity text image can be effectively split into several single-polarity text images. As a result, these text images are then segmented by single-polarity text segmentation algorithms. Experiments on thousands of multi-polarity text images show that our algorithm can effectively segment multi-polarity texts.|Barry O'Sullivan","66116|AAAI|2007|Solving a Stochastic Queueing Design and Control Problem with Constraint Programming|A facility with front room and back room operations has the option of hiring specialized or, more expensive, cross-trained workers. Assuming stochastic customer arrival and service times, we seek a smallest-cost combination of cross-trained and specialized workers satisfying constraints on the expected customer waiting time and expected number of workers in the back room. A constraint programming approach using logic-based Benders' decomposition is presented. Experimental results demonstrate the strong performance of this approach across a wide variety of problem parameters. This paper provides one of the first links between queueing optimization problems and constraint programming.|Daria Terekhov,J. Christopher Beck,Kenneth N. Brown","65781|AAAI|2006|Weighted Constraint Satisfaction with Set Variables|Set variables are ubiquitous in modeling (soft) constraint problems, but efforts on practical consistency algorithms for Weighted Constraint Satisfaction Problems (WCSPs) have only been on integer variables. We adapt the classical notion of set bounds consistency for WCSPs, and propose efficient representation schemes for set variables and common unary, binary, and ternary set constraints, as well as cardinality constraints. Instead of reasoning consistency on an entire set variable directly, we propose local consistency check at the set element level, and demonstrate that this apparent \"micro\"-management of consistency does imply set bounds consistency at the variable level. In addition, we prove that our framework captures classical CSPs with set variables, and degenerates to the classical case when the weights in the problem contain only  and T. Last but not least, we verify the feasibility and efficiency of our proposal with a prototype implementation, the efficiency of which is competitive against ILOG Solver on classical problems and orders of magnitude better than WCSP models using - variables to simulate set variables on soft problems.|J. H. M. Lee,C. F. K. Siu","65268|AAAI|2004|Logic Programs with Abstract Constraint Atoms|We propose and study extensions of logic programming with constraints represented as generalized atoms of the form C(X), where X is a finite set of atoms and C is an abstract constraint (formally, a collection of sets of atoms). Atoms C(X) are satisfied by an interpretation (set of atoms) M, if M  X  C. We focus here on monotone constraints, that is, those collections C that are closed under the superset. They include, in particular, weight (or pseudo-boolean) constraints studied both by the logic programming and SAT communities. We show that key concepts of the theory of normal logic programs such as the one-step provability operator, the semantics of supported and stable models, as well as several of their properties including complexity results, can be lifted to such case.|Victor W. Marek,Miroslaw Truszczynski","66237|AAAI|2007|An Experimental Comparison of Constraint Logic Programming and Answer Set Programming|Answer Set Programming (ASP) and Constraint Logic Programming over finite domains (CLP(FD)) are two declarative progmmming paradigms that have been extensively used to encode applications involving search, optimization, and reasoning (e.g., commonsense reasoning and planning). This paper presents experimental comparisons between the declarative encodings of various computationally hard problems in both frameworks. The objective is to investigate how the solvers in the two domains respond to different problems, highlighting strengths and weaknesses of their implementations, and suggesting criteria for choosing one approach over the other. Ultimately, the work in this paper is expected to lay the foundations for transfer of technology between the two domains, e.g., suggesting ways to use CLP(FD) in the execution of ASP.|Agostino Dovier,Andrea Formisano,Enrico Pontelli","66271|AAAI|2007|Generating and Solving Logic Puzzles through Constraint Satisfaction|Solving logic puzzles has become a very popular past-time, particularly since the Sudoku puzzle started appearing in newspapers all over the world. We have developed a puzzle generator for a modification of Sudoku, called Jidoku, in which clues are binary disequalities between cells on a    grid. Our generator guarantees that puzzles have unique solutions, have graded difficulty, and can be solved using inference alone. This demonstration provides a fun application of many standard constraint satisfaction techniques, such as problem formulation, global constraints, search and inference. It is ideal as both an education and outreach tool. Our demonstration will allow people to generate and interactively solve puzzles of user-selected difficulty, with the aid of hints if required, through a specifically built Java applet.|Barry O'Sullivan,John Horan","66450|AAAI|2008|What Is Answer Set Programming|Answer set programming (ASP) is a form of declarative programming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.|Vladimir Lifschitz","66508|AAAI|2008|A Meta-Programming Technique for Debugging Answer-Set Programs|Answer-set programming (ASP) is widely recognised as a viable tool for declarative problem solving. However, there is currently a lack of tools for developing answer-set programs. In particular, providing tools for debugging answer-set programs has recently been identified as a crucial prerequisite for a wider acceptance of ASP. In this paper, we introduce a meta-programming technique for debugging in ASP. The basic question we address is why interpretations expected to be answer sets are not answer sets of the program to debug. We thus deal with finding semantical errors of programs. The explanations provided by our method are based on an intuitive scheme of errors that relies on a recent characterisation of the answer-set semantics. Furthermore, as we are using a meta-programming technique, debugging queries are expressed in terms of answer-set programs themselves, which has several benefits For one, we can directly use ASP solvers for processing debugging queries. Indeed, our technique can easily be implemented, and we devised a corresponding prototype debugging system. Also, our approach respects the declarative nature of ASP, and the capabilities of the system can easily be extended to incorporate differing debugging features.|Martin Gebser,J√∂rg P√ºhrer,Torsten Schaub,Hans Tompits","65891|AAAI|2006|Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms|In this paper, we present two alternative approaches to defining answer sets for logic programs with arbitrary types of abstract constraint atoms (c-atoms). These approaches generalize the fixpoint-based and the level mapping based answer set semantics of normal logic programs to the case of logic programs with arbitrary types of c-atoms. The results are four different answer set definitions which are equivalent when applied to normal logic programs. The standard fixpoint-based semantics of logic programs is generalized in two directions, called answer set by reduct and answer set by complement. These definitions, which differ from each other in the treatment of negation-as-failure (naf) atoms, make use of an immediate consequence operator to perform answer set checking, whose definition relies on the notion of conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other two definitions, called strongly and weakly well-supported models, are generalizations of the notion of well-supported models of normal logic programs to the case of programs with c-atoms. As for the case of fixpoint-based semantics, the difference between these two definitions is rooted in the treatment of naf atoms. We prove that answer sets by reduct (resp. by complement) are equivalent to weakly (resp. strongly) well-supported models of a program, thus generalizing the theorem on the correspondence between stable models and well-supported models of a normal logic program to the class of programs with c-atoms. We show that the newly defined semantics coincide with previously introduced semantics for logic programs with monotone c-atoms, and they extend the original answer set semantics of normal logic programs. We also study some properties of answer sets of programs with c-atoms, and relate our definitions to several semantics for logic programs with aggregates presented in the literature.|Tran Cao Son,Enrico Pontelli,Phan Huy Tu","66358|AAAI|2008|Loop Formulas for Logic Programs with Arbitrary Constraint Atoms|We formulate loop formulas for logic programs with arbitrary constraint atoms, for the semantics based on conditional satisfaction. This provides a method for answer set computation by computing models of completion. One particular attractive candidate for the latter task is pseudo-boolean constraint solvers. To strengthen this connection, we show examples of compact encoding of aggregates and global constraints by pseudo-boolean constraints.|Jia-Huai You,Guohua Liu"],["66114|AAAI|2007|Temporal and Information Flow Based Event Detection from Social Text Streams|Recently, social text streams (e.g., blogs, web forums, and emails) have become ubiquitous with the evolution of the web. In some sense, social text streams are sensors of the real world. Often, it is desirable to extract real world events from the social text streams. However, existing event detection research mainly focused only on the stream properties of social text streams but ignored the contextual, temporal, and social information embedded in the streams. In this paper, we propose to detect events from social text streams by exploring the content as well as the temporal, and social dimensions. We define the term event as the information flow between a group of social actors on a specific topic over a certain time period. We represent social text streams as multi-graphs, where each node represents a social actor and each edge represents the information flow between two actors. The content and temporal associations within the flow of information are embedded in the corresponding edge. Events are detected by combining text based clustering, temporal segmentation, and information flow-based graph cuts of the dual graph of the social networks. Experiments conducted with the Enron email dataset and the political blog dataset from Dailykos show the proposed event detection approach outperforms the other alternatives.|Qiankun Zhao,Prasenjit Mitra,Bi Chen","65550|AAAI|2005|Activity Recognition from Accelerometer Data|Activity recognition fits within the bigger framework of context awareness. In this paper, we report on our efforts to recognize user activity from accelerometer data. Activity recognition is formulated as a classification problem. Performance of base-level classifiers and meta-level classifiers is compared. Plurality Voting is found to perform consistently well across different settings.|Nishkam Ravi,Nikhil Dandekar,Preetham Mysore,Michael L. Littman","66385|AAAI|2008|Automatic Extraction of Data Points and Text Blocks from -Dimensional Plots in Digital Documents|Two dimensional plots (-D) in digital documents on the web are an important source of information that is largely under-utilized. In this paper, we outline how data and text can be extracted automatically from these -D plots, thus eliminating a time consuming manual process. Our information extraction algorithm identifies the axes of the figures, extracts text blocks like axes-labels and legends and identifies data points in the figure. It also extracts the units appearing in the axes labels and segments the legends to identify the different lines in the legend, the different symbols and their associated text explanations. Our algorithm also performs the challenging task of separating out overlapping text and data points effectively. Our experiments indicate that these techniques are computationally efficient and provide acceptable accuracy.|Saurabh Kataria,William Browuer,Prasenjit Mitra,C. Lee Giles","66269|AAAI|2007|Relation Extraction from Wikipedia Using Subtree Mining|The exponential growth and reliability of Wikipedia have made it a promising data source for intelligent systems. The first challenge of Wikipedia is to make the encyclopedia machine-processable. In this study, we address the problem of extracting relations among entities from Wikipedia's English articles, which in turn can serve for intelligent systems to satisfy users' information needs. Our proposed method first anchors the appearance of entities in Wikipedia articles using some heuristic rules that are supported by their encyclopedic style. Therefore, it uses neither the Named Entity Recognizer (NER) nor the Coreference Resolution tool, which are sources of errors for relation extraction. It then classifies the relationships among entity pairs using SVM with features extracted from the web structure and subtrees mined from the syntactic structure of text. The innovations behind our work are the following a) our method makes use of Wikipedia characteristics for entity allocation and entity classification, which are essential for relation extraction b) our algorithm extracts a core tree, which accurately reflects a relationship between a given entity pair, and subsequently identifies key features with respect to the relationship from the core tree. We demonstrate the effectiveness of our approach through evaluation of manually annotated data from actual Wikipedia articles.|Dat P. T. Nguyen,Yutaka Matsuo,Mitsuru Ishizuka","65096|AAAI|1987|Information Retrieval from Never-Ending Stories|The System for Conceptual Information Summarization, Organization, and Retrieval (SCISOR) is a research system that consists of a set of programs to parse short newspaper texts in the domain of corporate takeovers and finance. The conceptual information extracted from these stories may then be accessed through a natural language interface. Events in the world of corporate takeovers unfold slowly over time. As a result of this, the input to SCISOR consists of multiple short articles, most of which add a new piece of information to an ongoing story. This motivates a natural language, knowledge-based approach to information retrieval, as traditional methods of document retrieval are inappropriate for retrieving multiple short articles describing events that take place over time. A natural language, knowledge-based approach facilitates obtaining both concise answers to straightforward questions and summaries or updates of the events that take place. The predictable events that take place in the domain make expectation-driven, partial parsing feasible.|Lisa F. Rau","65216|AAAI|2004|Methods for Domain-Independent Information Extraction from the Web An Experimental Comparison|Our KNOWITALL system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an autonomous, domain-independent, and scalable manner. In its first major run, KNOWITALL extracted over , facts with high precision, but suggested a challenge How can we improve KNOWITALL's recall and extraction rate without sacrificing precision This paper presents three distinct ways to address this challenge and evaluates their performance. Rule Learning learns domain-specific extraction rules. Subclass Extraction automatically identifies sub-classes in order to boost recall. List Extraction locates lists of class instances, learns a \"wrapper\" for each list, and extracts elements of each list. Since each method bootstraps from KNOWITALL's domain-independent methods, no hand-labeled training examples are required. Experiments show the relative coverage of each method and demonstrate their synergy. In concert, our methods gave KNOWITALL a -fold to -fold increase in recall, while maintaining high precision, and discovered , cities missing from the Tipster Gazetteer.|Oren Etzioni,Michael J. Cafarella,Doug Downey,Ana-Maria Popescu,Tal Shaked,Stephen Soderland,Daniel S. Weld,Alexander Yates","66062|AAAI|2007|Mining Web Query Hierarchies from Clickthrough Data|In this paper, we propose to mine query hierarchies from clickthrough data, which is within the larger area of automatic acquisition of knowledge from the Web. When a user submits a query to a search engine and clicks on the returned Web pages, the user's understanding of the query as well as its relation to the Web pages is encoded in the clickthrough data. With millions of queries being submitted to search engines every day, it is both important and beneficial to mine the knowledge hidden in the queries and their intended Web pages. We can use this information in various ways, such as providing query suggestions and organizing the queries. In this paper, we plan to exploit the knowledge hidden in clickthrough logs by constructing query hierarchies, which can reflect the relationship among queries. Our proposed method consists of two stages generating candidate queries and determining \"generalizationspecialization\" relatinns between these queries in a hierarchy. We test our method on some labeled data sets and illustrate the effectiveness of our proposed solution empirically.|Dou Shen,Min Qin,Weizhu Chen,Qiang Yang,Zheng Chen","66512|AAAI|2008|Mining Translations of Web Queries from Web Click-through Data|Query translation for Cross-Lingual Information Retrieval (CLIR) has gained increasing attention in the research area. Previous work mainly used machine translation systems, bilingual dictionaries, or web corpora to perform query translation. However, most of these approaches require either expensive language resources or complex language models, and cannot achieve timely translation for new queries. In this paper, we propose a novel solution to automatically acquire query translation pairs from the knowledge hidden in the click-through data, that are represented by the URL a user clicks after submitting a query to a search engine. Our proposed solution consists of two stages identitying bilingual URL pair patterns in the click-through data and matching query translation pairs based on user click behavior. Experimental results on a real dataset show that our method not only generates existing query translation pairs with high precision, but also generates many timely query translation pairs that could not be obtained by previous methods. A comparative study between our system and two commercial online translation systems shows the advantage of our proposed method.|Rong Hu,Weizhu Chen,Jian Hu,Yansheng Lu,Zheng Chen,Qiang Yang","66532|AAAI|2008|Text Categorization with Knowledge Transfer from Heterogeneous Data Sources|Multi-category classification of short dialogues is a common task performed by humans. When assigning a question to an expert, a customer service operator tries to classify the customer query into one of N different classes for which experts are available. Similarly, questions on the web (for example questions at Yahoo Answers) can be automatically forwarded to a restricted group of people with a specific expertise. Typical questions are short and assume background world knowledge for correct classification. With exponentially increasing amount of knowledge available, with distinct properties (labeled vs unlabeled, structured vs unstructured), no single knowledge-transfer algorithm such as transfer learning, multi-task learning or selftaught learning can be applied universally. In this work we show that bag-of-words classifiers performs poorly on noisy short conversational text snippets. We present an algorithm for leveraging heterogeneous data sources and algorithms with significant improvements over any single algorithm, rivaling human performance. Using different algorithms for each knowledge source we use mutual information to aggressively prune features. With heterogeneous data sources including Wikipedia, Open Directory Project (ODP), and Yahoo Answers, we show .% and .% correct classification on Google Answers corpus and Switchboard corpus using only  featuresclass. This reflects a huge improvement over bag of words approaches and -% error reduction over previously published state of art (Gabrilovich et. al. ).|Rakesh Gupta,Lev-Arie Ratinov","66616|AAAI|2010|Commonsense Knowledge Mining from the Web|In this article, two learning classifier systems based on evolutionary techniques are described to classify remote sensing images. Usually, these images contain voluminous, complex, and sometimes erroneous and noisy data. The first approach implements ICU, an evolutionary rule discovery system, generating simple and robust rules. The second approach applies the real-valued accuracy-based classification system XCSR. The two algorithms are detailed and validated on hyperspectral data.|Chi-Hsin Yu,Hsin-Hsi Chen"],["66108|AAAI|2007|Making the Difference in Semantic Web Service Composition|Automation of Web service composition is one of the most interesting challenges facing the Semantic Web today. In this paper we propose a mean of performing automated Web service composition by exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction. The key idea is that the matchmaking enables, at run time, finding semantic compatibilities among independently defined Web service descriptions. To this end, our approach extends existing methods in order to explain misconnections between Web services. From this we generate Web service compositions that realize the goal, satisfying and optimizing the semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach.|Freddy L√©cu√©,Alexandre Delteil","66019|AAAI|2007|Web Service Composition as Planning Revisited In Between Background Theories and Initial State Uncertainty|Thanks to recent advances, AI Planning has become the underlying technique for several applications. Amongst these, a prominent one is automated Web Service Composition (WSC). One important issue in this context has been hardly addressed so far WSC requires dealing with background ontologies. The support for those is severely limited in current planning tools. We introduce a planning formalism that faithfully represents WSC. We show that, unsurprisingly, planning in such a formalism is very hard. We then identify an interesting special case that covers many relevant WSC scenarios, and where the semantics are simpler and easier to deal with. This opens the way to the development of effective support tools for WSC. Furthermore, we show that if one additionally limits the amount and form of outputs that can be generated, then the set of possible states becomes static, and can be modelled in terms of a standard notion of initial state uncertainty. For this, effective tools exist these can realize scalable WSC with powerful background ontologies. In an initial experiment, we show how scaling WSC instances are comfortably solved by a tool incorporating modern planning heuristics.|J√∂rg Hoffmann,Piergiorgio Bertoli,Marco Pistore","65633|AAAI|2006|SEMAPLAN Combining Planning with Semantic Matching to Achieve Web Service Composition|In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers.|Rama Akkiraju,Biplav Srivastava,Anca-Andreea Ivan,Richard Goodwin,Tanveer Fathima Syeda-Mahmood","65756|AAAI|2006|Using Semantic Web Technologies for Policy Management on the Web|With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.|Lalana Kagal,Tim Berners-Lee,Dan Connolly,Daniel J. Weitzner","65625|AAAI|2006|A Platform to Evaluate the Technology for Service Discovery in the Semantic Web|Since the description of the Semantic Web paradigm in , technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.|C√©cile Aberg,Johan Aberg,Patrick Lambrix,Nahid Shahmehri","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","66048|AAAI|2007|A Planning Approach for Message-Oriented Semantic Web Service Composition|In this paper, we consider the problem of composing a set of web services, where the requirements are specified in terms of the input and output messages of the composite workflow. We propose a semantic model of messages using RDF graphs that encode OWL ABox assertions. We also propose a model of web service operations where the input message requirements and output message characteristics are modeled using RDF graph patterns. We formulate the message-oriented semantic web service composition problem and show how it can be translated into a planning problem. There are, however, significant challenges in scalably doing planning in this domain, especially since DL reasoning may be performed to check if an operation can be given a certain input message. We propose a two-phase planning algorithm that incorporates DLP reasoning and evaluate the performance of this planning algorithm.|Zhen Liu,Anand Ranganathan,Anton Riabov","65845|AAAI|2006|An Investigation into the Feasibility of the Semantic Web|We investigate the challenges that must be addressed for the Semantic Web to become a feasible enterprise. Specifically we focus on the query answering capability of the Semantic Web. We put forward that two key challenges we face are heterogeneity and scalability. We propose a flexible and decentralized framework for addressing the heterogeneity problem and demonstrate that sufficient reasoning is possible over a large dataset by taking advantage of database technologies and making some tradeoff decisions. As a proof of concept, we collect a significant portion of the available Semantic Web data use our framework to resolve some heterogeneity and reason over the data as one big knowledge base. In addition to demonstrating the feasibility of a \"real\" Semantic Web, our experiments have provided us with some interesting insights into how it is evolving and the type of queries that can be answered.|Zhengxiang Pan,Abir Qasem,Jeff Heflin","66517|AAAI|2008|Efficient Learning of Action Schemas and Web-Service Descriptions|This work addresses the problem of efficiently learning action schemas using a bounded number of samples (interactions with the environment). We consider schemas in two languages--traditional STRIPS, and a new language STRIPS+WS that extends STRIPS to allow for the creation of new objects when an action is executed. This modification allows STRIPS+WS to model web services and can be used to describe web-service composition (planning) problems. We show that general STRIPS operators cannot be efficiently learned through raw experience, though restricting the size of action preconditions yields a positive result. We then show that efficient learning is possible without this restriction if an agent has access to a \"teacher\" that can provide solution traces on demand. We adapt this learning algorithm to efficiently learn web-service descriptions in STRIPS+WS.|Thomas J. Walsh,Michael L. Littman","65813|AAAI|2006|Spinning Multiple Social Networks for Semantic Web|Social networks are important for the Semantic Web. Several means can be used to obtain social networks using social networking services, aggregating Friend-of-a-Friend (FOAF) documents, mining text information on the Web or in e-mail messages, and observing face-to-face communication using sensors. Integrating multiple social networks is a key issue for further utilization of social networks in the Semantic Web. This paper describes our attempt to extract, analyze and integrate multiple social networks from the same community user-registered knows networks, web-mined collaborator networks, and face-to-face meets networks. We operated a social network-based community support system called Polyphonet at the th, th and th Annual Conferences of the Japan Society of Artificial Intelligence (JSAI, JSAI, and JSAI) and at The International Conference on Ubiquitous Computing (UbiComp ). Multiple social networks were obtained and analyzed. We discuss the integration of multiple networks based on the analyses.|Yutaka Matsuo,Masahiro Hamasaki,Yoshiyuki Nakamura,Takuichi Nishimura,K√¥iti Hasida,Hideaki Takeda,Junichiro Mori,Danushka Bollegala,Mitsuru Ishizuka"],["66206|AAAI|2007|Predicate Projection in a Bimodal Spatial Reasoning System|Spatial reasoning is a fundamental aspect of intelligent behavior, which cognitive architectures must address in a problem-independent way. Bimodal systems, employing both qualitative and quantitative representations of spatial information, are efficient and psychologically plausible means for spatial reasoning. Any such system must employ a translation from the qualitative level to the quantitative, where new objects (images) are created through the process of predicate projection. This translation has received little scrutiny. We examine this issue in the context of a bimodal spatial reasoning system integrated with a cognitive architecture (Soar). As part of this system, we define an expressive language for predicate projection that supports general and flexible image creation. We demonstrate this system on multiple spatial reasoning problems in the ORTS real-time strategy game environment.|Samuel Wintermute,John E. Laird","66006|AAAI|2007|Spatial Representation and Reasoning for Human-Robot Collaboration|How should a robot represent and reason about spatial information when it needs to collaborate effectively with a human The form of spatial representation that is useful for robot navigation may not be useful in higher-level reasoning or working with humans as a team member. To explore this question, we have extended previous work on how children and robots learn to play hide and seek to a human-robot team covertly approaching a moving target. We used the cognitive modeling system, ACT-R, with an added spatial module to support the robot's spatial reasoning. The robot interacted with a team member through voice, gestures, and movement during the team's covert approach of a moving target. This paper describes the new robotic system and its integration of metric, symbolic, and cognitive layers of spatial representation and reasoning for its individual and team behavior.|William G. Kennedy,Magdalena D. Bugajska,Matthew Marge,William Adams,Benjamin R. Fransen,Dennis Perzanowski,Alan C. Schultz,J. Gregory Trafton","65894|AAAI|2006|Optimizing Similarity Assessment in Case-Based Reasoning|The definition of accurate similarity measures is a key issue of every Case-Based Reasoning application. Although some approaches to optimize similarity measures automatically have already been applied, these approaches are not suited for all CBR application domains. On the one hand, they are restricted to classification tasks. On the other hand, they only allow optimization of feature weights. We propose a novel learning approach which addresses both problems, i.e. it is suited for most CBR application domains beyond simple classification and it enables learning of more sophisticated similarity measures.|Armin Stahl,Thomas Gabel","65380|AAAI|2005|A Learning and Reasoning System for Intelligence Analysis|This paper presents a personal cognitive assistant, called Disciple-LTA, that can acquire expertise in intelligence analysis directly from intelligence analysts, can train new analysts, and can help analysts find solutions to complex problems through mixed-initiative reasoning, making possible the synergistic integration of a human's experience and creativity with an automated agent's knowledge and speed, and facilitating the collaboration with complementary experts and their agents.|Mihai Boicu,Gheorghe Tecuci,Cindy Ayers,Dorin Marcu,Cristina Boicu,Marcel Barbulescu,Bogdan Stanescu,William Wagner,Vu Le,Denitsa Apostolova,Adrian Ciubotariu","65259|AAAI|2004|An Explainable Artificial Intelligence System for Small-unit Tactical Behavior|As the artificial intelligence (AI) systems in military simulations and computer games become more complex, their actions become increasingly difficult for users to understand. Expert systems for medical diagnosis have addressed this challenge though the addition of explanation generation systems that explain a system's internal processes. This paper describes the AI architecture and associated explanation capability used by Full Spectrum Command, a training system developed for the U.S. Army by commercial game developers and academic researchers.|Michael van Lent,William Fisher,Michael Mancuso","65496|AAAI|2005|Complexity-Guided Case Discovery for Case Based Reasoning|The distribution of cases in the case base is critical to the performance of a Case Based Reasoning system. The case author is given little support in the positioning of new cases during the development stage of a case base. In this paper we argue that classification boundaries represent important regions of the problem space. They are used to identify locations where new cases should be acquired. We introduce two complexity-guided algorithms which use a local complexity measure and boundary identification techniques to actively discover cases close to boundaries. The ability of these algorithms to discover new cases that significantly improve the accuracy of case bases is demonstrated on five public domain classification datasets.|Stewart Massie,Susan Craw,Nirmalie Wiratunga","65978|AAAI|2007|Explanation Support for the Case-Based Reasoning Tool myCBR|Case-Based Reasoning, in short, is the process of solving new problems based on solutions of similar past problems, much like humans solve many problems. myCBR, an extension of the ontology editor Protg, provides such similarity-based retrieval functionality. Moreover, the user is supported in modelling appropriate similarity measures by forward and backward explanations.|Daniel Bahls,Thomas Roth-Berghofer","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II","66460|AAAI|2008|Knowledge-Based Spatial Reasoning for Scene Generation from Text Descriptions|This system translates basic English descriptions of a wide range of objects in a simplistic zoo environment into plausible, three-dimensional, interactive visualizations of their positions, orientations, and dimensions. It combines a semantic network and contextually sensitive knowledge base as representations for explicit and implicit spatial knowledge, respectively. Its linguistic aspects address underspecification, vagueness, uncertainty, and context with respect to intrinsic, extrinsic, and deictic frames of spatial reference. The underlying, commonsense reasoning fomlalism is probability-based geometric fields that are solved through constraint satisfaction. The architecture serves as an extensible test-and-evaluation framework for a multitude of linguistic and artificial-intelligence investigations.|Dan Tappan","65725|AAAI|2006|Bounded Treewidth as a Key to Tractability of Knowledge Representation and Reasoning|Several forms of reasoning in AI - like abduction, closed world reasoning, circumscription, and disjunctive logic programming - are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae or programs is bounded by some constant. Clearly, these theoretical tractability results as such do not immediately yield feasible algorithms. However, we have recently established a new method based on monadic datalog which allowed us to design an efficient algorithm for a related problem in the database area. In this work, we exploit the monadic datalog approach to construct new algorithms for logic-based abduction.|Georg Gottlob,Reinhard Pichler,Fang Wei"],["65969|AAAI|2007|The Marchitecture A Cognitive Architecture for a Robot Baby|The Marchitecture is a cognitive architecture for autonomous development of representations. The goals of The Marchitecture are domain independence, operating in the absence of knowledge engineering, learning an ontology of parameterized relational concepts, and elegance of design. To this end, The Marchitecture integrates classification, parsing, reasoning, and explanation. The Marchitecture assumes an ample amount of raw data to develop its representations, and it is therefore appropriate for long lived agents.|Marc Pickett,Tim Oates","65669|AAAI|2006|Multimodal Cognitive Architecture Making Perception More Central to Intelligent Behavior|I propose that the notion of cognitive state be broadened from the current predicate-symbolic, Language-of-Thought framework to a multi-modal one, where perception and kinesthetic modalities participate in thinking. In contrast to the roles assigned to perception and motor activities as modules external to central cognition in the currently dominant theories in AI and Cognitive Science, in the proposed approach, central cognition incorporates parts of the perceptual machinery. I motivate and describe the proposal schematically, and describe the implementation of a bi-modal version in which a diagrammatic representation component is added to the cognitive state. The proposal explains our rich multimodal internal experience, and can be a key step in the realization of embodied agents. The proposed multimodal cognitive state can significantly enhance the agent's problem solving.|B. Chandrasekaran","66374|AAAI|2008|Achieving Far Transfer in an Integrated Cognitive Architecture|Transfer is the ability to employ knowledge acquired in one task to improve performance in another. We study transfer in the context of the ICARUS cognitive architecture, which supplies diverse capabilities for execution, inference, planning, and learning. We report on an extension to ICARUS called representation mapping that transfers structured skills and concepts between disparate tasks that may not even be expressed with the same symbol set. We show that representation mapping is naturally integrated into ICARUS' cognitive processing loop, resulting in a system that addresses a qualitatively new class of problems by considering the relevance of past experience to current goals.|Daniel G. Shapiro,Tolga K√∂nik,Paul O'Rorke","66187|AAAI|2007|An Intelligent System for Chinese Calligraphy|Our work links Chinese calligraphy to computer science through an integrated intelligence approach. We first extract strokes of existent calligraphy using a semi-automatic, two-phase mechanism the first phase tries to do the best possible extraction using a combination of algorithmic techniques the second phase presents an intelligent user interface to allow the user to provide input to the extraction process for the difficult cases such as those in highly random, cursive, or distorted styles. Having derived a parametric representation of calligraphy, we employ a supervised learning based method to explore the space of visually pleasing calligraphy. A numeric grading method for judging the beauty of calligraphy is then applied to the space. We integrate such a grading unit into an existent constraint-based reasoning system for calligraphy generation, which results in a significant enhancement in terms of visual quality in the automatically generated calligraphic characters. Finally, we construct an intelligent calligraphy tutoring system making use of the above. This work represents our first step towards understanding the human process of appreciating beauty through modeling the process with an integration of available AI techniques. More results and supplementary materials are provided at httpwww.cs.hku.hksonghuacalligraphy.|Songhua Xu,Hao Jiang,Francis Chi-Moon Lau,Yunhe Pan","65602|AAAI|2005|A Learning Architecture for Automating the Intelligent Environment|Developing technologies and systems for perception and perspicacious automated control of home and workplace environments is a challenging problem. We present a complete agent architecture for learning to automate the intelligent environment and discuss the development, deployment, and techniques utilized in our working intelligent environments. Empirical evaluation of our approach has proven its effectiveness at reducing inhabitant interactions by .%.|G. Michael Youngblood,Diane J. Cook,Lawrence B. Holder","65699|AAAI|2006|LOCATE Intelligent Systems Demonstration Adapting Help to the Cognitive Styles of Users|LOCATE is workspace layout design software that also serves as a testbed for developing and refining principles of adaptive aiding. This demonstration illustrates LOCATE's ability to determine user cognitive styles and provide help matched to those styles. Users are assessed along a Wholist-Analytic dimension and a Verbal-Imagery-Kinesthetic \"trimension\" and that information is stored in a User Model maintained by LOCATE. Help options provided to users for selecting alternative forms of help permit the system to track those selections and allow for system adaptation to the user's preferred style of help.|Jack L. Edwards,Greg Scott","66141|AAAI|2007|An Architecture for Adaptive Algorithmic Hybrids|We describe a cognitive architecture for creating more robust intelligent systems by executing hybrids of algorithms based on different computational formalisms. The architecture is motivated by the belief that () most existing computational methods often exhibit some of the characteristics desired of intelligent systems at the cost of other desired characteristics and () a system exhibiting robust intelligence can be designed by implementing hybrids of these computational methods. The main obstacle to this approach is that the various relevant computational methods are based on data structures and algorithms that are very difficult to integrate into one system. We describe a new method of executing hybrids of algorithms using the focus of attention of multiple modules. This approach has been embodied in the Polyscheme cognitive architecture. Systems based on Polyscheme can integrate reactive robotic controllers, logical and probabilistic inference algorithms, frame-based formalisms and sensor-processing algorithms into one system. Existing applications involve human-robot interaction, heterogeneous information retrieval and natural language understanding. Systems built using Polyscheme demonstrate that algorithmic hybrids implemented using a focus of attention can () exhibit more characteristics of intelligence than individual computational methods alone and () deal with problems that have formerly been beyond the reach of synthetic computational intelligence.|Nicholas L. Cassimatis,Magdalena D. Bugajska,Scott Dugas,Arthi Murugesan,Paul Bello","65013|AAAI|1987|An Architecture for Intelligent Task Automation|This report discusses the Martin Marietta Intelligent Task Automation Project (ITA). The purpose of the ITA project is to integrate Artificial Intelligence (AI) task planning. path planning. vision, and robotics technologies into a system designed to autonomously perform manufacturing tasks in dynamic or unstructured environments. The application domain chosen for primary demonstrations is dimensional measurement of an F- bulkhead. The overall goal is to be able to perform the inspection an order of magnitude faster than the current manual method. which takes about  hours for about  inspection points. The project was conducted in two phases. Phase I. completed in December . demonstrated the readiness of the technologies in each of the areas making up the ITA system. Phase II. which was mostly complete in June . demonstrated that the technologies can be integrated into a working system and that the system can be transferred to other applications. The architecture of the ITA system is discussed with an emphasis on the AI components making up the system. The strengths and weaknesses of the architecture and AI techniques applied are discussed.|Jeffrey M. Becker,Fred L. Garrett","66160|AAAI|2007|Extending Cognitive Architecture with Episodic Memory|In this paper, we explore the hypothesis that episodic memory is a critical component for cognitive architectures that support general intelligence. Episodic memory overlaps with case-based reasoning (CBR) and can be seen as a task-independent, architectural approach to CBR. We define the design space for episodic memory systems and the criteria any implementation must meet to be useful in a cognitive architecture. We present an implementation and demonstrate how episodic memory, combined with other components of a cognitive architecture, supports a wealth of cognitive capabilities that are difficult to attain without it.|Andrew Nuxoll,John E. Laird","65777|AAAI|2006|A Unified Cognitive Architecture for Physical Agents|In this paper we describe ICARUS, a cognitive architecture for physical agents that integrates ideas from a number of traditions, but that has been especially influenced by results from cognitive psychology. We review ICARUS' commitments to memories and representations, then present its basic processes for performance and learning. We illustrate the architecture's behavior on a task from in-city driving that requires interaction among its various components. In addition, we discuss ICARUS' consistency with qualitative findings about the nature of human cognition. In closing, we consider the framework's relation to other cognitive architectures that have been proposed in the literature.|Pat Langley,Dongkyu Choi"],["66248|AAAI|2007|Automatic Algorithm Configuration Based on Local Search|The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variablevalue ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at httpwww.cs.ubc.calabsbetaProjectsParamILS.|Frank Hutter,Holger H. Hoos,Thomas St√ºtzle","65806|AAAI|2006|Memory Intensive Branch-and-Bound Search for Graphical Models|ANDOR search spaces have recently been introduced as a unifying paradigm for advanced algorithmic schemes for graphical models. The main virtue of this representation is its sensitivity to the structure of the model, which can translate into exponential time savings for search algorithms. ANDOR Branch-and-Bound (AOBB) is a new algorithm that explores the ANDOR search tree for solving optimization tasks in graphical models. In this paper we extend the algorithm to explore an ANDOR search graph by equipping it with a context-based adaptive caching scheme similar to good and no-good recording. The efficiency of the new graph search algorithm is demonstrated empirically on various benchmarks, including the very challenging ones that arise in genetic linkage analysis.|Radu Marinescu 0002,Rina Dechter","65429|AAAI|2005|Fast Planning in Domains with Derived Predicates An Approach Based on Rule-Action Graphs and Local Search|The ability to express \"derived predicates\" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of \"Rule-Action Graphs\", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina,Paolo Toninelli","66102|AAAI|2007|Best-First ANDOR Search for Graphical Models|The paper presents and evaluates the power of best-first search over ANDOR search spaces in graphical models. The main virtue of the ANDOR representation is its sensitivity to the structure of the graphical model, which can translate into significant time savings. Indeed, in recent years depth-first ANDOR Branch-and-Bound algorithms were shown to be very effective when exploring such search spaces, especially when using caching. Since best-first strategies are known to be superior to depth-first when memory is utilized, exploring the best-first control strategy is called for. In this paper we introduce two classes of best-first ANDOR search algorithms those that explore a context-minimal ANDOR search graph and use static variable orderings, and those that use dynamic variable orderings but explore an ANDOR search tree. The superiority of the best-first search approach is demonstrated empirically on various real-world benchmarks.|Radu Marinescu 0002,Rina Dechter","66080|AAAI|2007|Search Space Reduction and Russian Doll Search|In a constraint optimization problem (COP), many feasible valuations lead to the same objective value. This often means a huge search space and poor performance in the propagation between the objective and problem variables. In this paper, we propose a different modeling and search strategy which focuses on the cost function. We show that by constructing a dual model on the objective variables, we can get strong propagalion between the objective variables and the problem variables which allows search on the objective variables. We explain why and when searching on the objective variables can lead to large gains. We present a new Russian Doll Search algorithm, ORDS, which works on objective variables with dynamic variable ordering. Finally, we demonstrate using the hard Still-Life optimization problem the benefits of changing to the objective function model and ORDS.|Kenil C. K. Cheng,Roland H. C. Yap","66287|AAAI|2008|Fast Planning by Search in Domain Transition Graph|Recent advances in classical planning have used the SAS+ formalism, and several effective heuristics have been developed based on the SAS+ formalism. Comparing to the traditional STRIPSADL formalism, SAS+ is capable of capturing vital information such as domain transition structures and causal dependencies. In this paper, we propose a new SAS+ based incomplete planning approach. Instead of using SAS+ to derive heuristics within a heuristic search planner, we directly search in domain transition graphs (DTGs) and causal graphs (CGs) derived from the SAS+ formalism. The new method is efficient because the SAS+ representation is often much more compact than STRIPS. The CGs and DTGs provide rich information of domain structures that can effectively guide the search towards solutions. Experimental results show strong performance of the proposed planner on recent international planning competition domains.|Yixin Chen,Ruoyun Huang,Weixiong Zhang","66118|AAAI|2007|Best-First Search for Treewidth|Finding the exact treewidth of a graph is central to many operations in a variety of areas, including probabilistic reasoning and constraint satisfaction. Treewidth can be found by searching over the space of vertex elimination orders. This search space differs from those where best-first search is typically applied, because a solution path is evaluated by its maximum edge cost instead of the sum of its edge costs. We show how to make best-first search admissible on max-cost problem spaces. We also employ breadth-first heuristic search to reduce the memory requirement while still eliminating all duplicate nodes in the search space. Our empirical results show that our algorithms find the exact treewidth an order of magnitude faster than the previous state-of-the-art algorithm on hard benchmark graphs.|P. Alex Dow,Richard E. Korf","66603|AAAI|2010|A Fast Heuristic Search Algorithm for Finding the Longest Common Subsequence of Multiple Strings|Inference of high-level context is becoming crucial in development of context-aware applications. An example is social context inference - i.e., deriving social relations based upon the user's daily communication with other people. The efficiency of this mechanism mainly depends on the method(s) used to draw inferences based on existing evidence and sample information, such as a training data. Our approach uses rule-based data mining, Bayesian network inference, and user feedback to compute the probabilities of another user being in the specific social relationship with a user whose daily communication is logged by a mobile phone. In addition, a privacy mechanism is required to ensure the user's personal integrity and privacy when sharing this user's sensitive context data. Therefore, the derived social relations are used to define a user's policies for context access control, which grant the restricted context information scope depending on the user's current context. Finally, we propose a distributed architecture capable of managing this context information based upon these context access policies.|Qingguo Wang,Mian Pan,Yi Shang,Dmitry Korkin","66782|AAAI|2010|Search Space Reduction Using Swamp Hierarchies|We describe a method to extract content text from diverse Web pages by using the HTML document's text-to-tag ratio rather than specific HTML cues that may not be constant across various Web pages. We describe how to compute the text-to-tag ratio on a line-by-line basis and then cluster the results into content and non-content areas. With this approach we then show surprisingly high levels of recall for all levels of precision, and a large space savings.|Nir Pochter,Aviv Zohar,Jeffrey S. Rosenschein,Ariel Felner","66640|AAAI|2010|Fast Local Search Algorithm for Weighted Feedback Arc Set in Tournaments|In this paper, we present a new representation of sports video abstract-music sports-video (MSV), which provides exciting sports content accompanied with high quality background music for audiences and is available for high-quality audio-visual entertainment. We also propose a system generating MSV from user-provided sports video and music automatically. Firstly, the given sports video is segmented into a series of story units. Then all the story units are ordered by the predefined exciting degree (ED) and some high ED or user preferred story units are selected for MSV generation. Secondly, the ED of the given music is estimated by energy analysis on music beat. Thirdly, the selected story units are matched with music by their ED corresponding. Finally, the output MSV is rendered by connecting the selected exciting story units with appropriate transition effects, accompanied with the music. Experiments show encouraging results|Fedor V. Fomin,Daniel Lokshtanov,Venkatesh Raman,Saket Saurabh"],["66037|AAAI|2007|Revenue Monotonicity in Combinatorial Auctions|Intuitively, one might expect that a seller's revenue from an auction weakly increases as the number of bidders grows, as this increases competition. However, it is known that for combinatorial auctions that use the VCG mechanism, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this problem can occur under other dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of \"revenue monotonicity\" are not limited to mechanisms that achieve efficient allocations. Instead, they can occur under any dominant-strategy direct mechanism that sets prices using critical values, and that always chooses an allocation that cannot be augmented to make some bidder better off, while making none worse off.|Baharak Rastegari,Anne Condon,Kevin Leyton-Brown","65195|AAAI|2004|Eliciting Bid Taker Non-price Preferences in Combinatorial Auctions|Recent algorithms provide powerful solutions to the problem of determining cost-minimizing (or revenue-maximizing) allocations of items in combinatorial auctions. However, in many settings, criteria other than cost (e.g., the number of winners, the delivery date of items, etc.) are also relevant in judging the quality of an allocation. Furthermore, the bid taker is usually uncertain about her preferences regarding tradeoffs between cost and nonprice features. We describe new methods that allow the bid taker to determine (approximately) optimal allocations despite this. These methods rely on the notion of minimax regret to guide the elicitation of preferences from the bid taker and to measure the quality of an allocation in the presence of utility function uncertainty. Computational experiments demonstrate the practicality of minimax computation and the efficacy of our elicitation techniques.|Craig Boutilier,Tuomas Sandholm,Rob Shields","65264|AAAI|2004|Methods for Boosting Revenue in Combinatorial Auctions|We study the recognized open problem of designing revenue-maximizing combinatorial auctions. It is unsolved even for two bidders and two items for sale. Rather than pursuing the pure economic approach of attempting to characterize the optimal auction, we explore techniques for automatically modifying existing mechanisms in a way that increase expected revenue. We introduce a general family of auctions, based on bidder weighting and allocation boosting, which we call virtual valuations combinatorial auctions (VVCA). All auctions in the family are based on the Vickrey-Clarke-Groves (VCG) mechanism, executed on virtual valuations that are linear transformations of the bidders' real valuations. The restriction to linear transformations is motivated by incentive compatibility. The auction family is parameterized by the coefficients in the linear transformations. The problem of designing a high revenue mechanism is therefore reduced to search in the parameter space of VVCA. We analyze the complexity of the search for the optimal such mechanism and conclude that the search problem is computationally hard. Despite that, optimal parameters for VVCA can be found at least in settings with few items and bidders (the experiments show that VVCA yield a substantial increase in revenue over the traditionally used VCG). In larger auctions locally optimal parameters, which still yield an improvement over VCG, can be found.|Anton Likhodedov,Tuomas Sandholm","65483|AAAI|2005|Approximating Revenue-Maximizing Combinatorial Auctions|Designing revenue-maximizing combinatorial auctions (CAs) is a recognized open problem in mechanism design. It is unsolved even for two bidders and two items for sale. Rather than attempting to characterize the optimal auction, we focus on designing approximations (suboptimal auction mechanisms which yield high revenue). Our approximations belong to the family of virtual valuations combinatorial auctions (VVCA). VVCA is a Vickrey-Clarke-Groves (VCG) mechanism run on virtual valuations that are linear transformations of the bidders' real valuations. We pursue two approaches to constructing approximately optimal CAs. The first is to construct a VVCA with worst-case and average-case performance guarantees. We give a logarithmic approximation auction for basic important special cases of the problem ) limited supply of items on sale with additive valuations and ) unlimited supply. The second approach is to search the parameter space of VVCAs in order to obtain high-revenue mechanisms for the general problem. We introduce a series of increasingly sophisticated algorithms that use economic insights to guide the search and thus reduce the computational complexity. Our experiments demonstrate that in many cases these algorithms perform almost as well as the optimal VVCA, yield a substantial increase in revenue over the VCG mechanism and drastically outperform the straightforward algorithms in run-time.|Anton Likhodedov,Tuomas Sandholm","66850|AAAI|2010|A Lower Bound on the Size of Decomposable Negation Normal Form|As the Internet increases explosively, the QoS support in e-business server is becoming more and more important. However, the implementation of a fine-grained QoS control model in the e-business servers is a challenging task. In this paper, we propose a WorkManager-based QoS scheduling framework (WMQ) for e-business servers. This framework hides the complexity of QoS scheme implementations and provides a flexible architecture supporting fine-grained QoS control. A prototype of our framework has been implemented within the open-source server Tomcat. The evaluation of this prototype shows that our approach allows Tomcat to effectively meet the QoS requirement of the applications by fine-grained QoS control. Our experience also shows that the WMQ framework can efficiently simplify and reduce the implementation effort involved in developing QoS-enabled e-business servers.|Thammanit Pipatsrisawat,Adnan Darwiche","65557|AAAI|2005|Mixed-Integer Programming Methods for Finding Nash Equilibria|We present, to our knowledge, the first mixed integer program (MIP) formulations for finding Nash equilibria in games (specifically, two-player normal form games). We study different design dimensions of search algorithms that are based on those formulations. Our MIP Nash algorithm outperforms Lemke-Howson but not Porter-Nudelman-Shoham (PNS) on GAMUT data. We argue why experiments should also be conducted on games with equilibria with medium-sized supports only, and present a methodology for generating such games. On such games MIP Nash drastically outperforms PNS but not Lemke-Howson. Certain MIP Nash formulations also yield anytime algorithms for -equilibrium. with provable bounds. Another advantage of MIP Nash is that it can be used to find an optimal equilibrium (according to various objectives). The prior algorithms can be extended to that setting, but they are orders of magnitude slower.|Tuomas Sandholm,Andrew Gilpin,Vincent Conitzer","66504|AAAI|2008|Within-problem Learning for Efficient Lower Bound Computation in Max-SAT Solving|This paper focuses on improving branch-and-bound Max-SAT solvers by speeding up the lower bound computation. We notice that the existing propagation-based computing methods and the resolution-based computing methods, which have been studied intensively, both suffer from several drawbacks. In order to overcome these drawbacks, we propose a new method with a nice property that guarantees the increment of lower bounds. The new method exploits within-problem learning techniques. More specifically, at each branch point in the search-tree, the current node is enabled to inherit inconsistencies from its parent and learn information about effectiveness of the lower bound computing procedure from previous nodes. Furthermore, after branching on a new variable, the inconsistencies may shrink by applying unit propagation to them, and such process increases the probability of getting better lower bounds. We graft the new techniques into maxsatz and the experimental results demonstrate that the new solver outperforms the best state-of-the-art solvers on a wide range of instances including random and structured ones.|Han Lin,Kaile Su,Chu Min Li","65208|AAAI|2004|Combinatorial Auctions with Structured Item Graphs|Combinatorial auctions (CAs) are important mechanisms for allocating interrelated items. Unfortunately, winner determination is NP-complete unless there is special structure. We study the setting where there is a graph (with some desired property), with the items as vertices, and every bid bids on a connected set of items. Two computational problems arise ) clearing the auction when given the item graph, and ) constructing an item graph (if one exists) with the desired property.  was previously solved for the case of a tree or a cycle, and  for the case of a line graph or a cycle. We generalize the first result by showing that given an item graph with bounded treewidth, the clearing problem can be solved in polynomial time (and every CA instance has some treewidth the complexity is exponential in only that parameter). We then give an algorithm for constructing an item tree (treewidth ) if such a tree exists, thus closing a recognized open problem. We show why this algorithm does not work for treewidth greater than , but leave open whether item graphs of (say) treewidth  can be constructed in polynomial time. We show that finding the item graph with the fewest edges is NP-complete (even when a graph of treewidth  exists). Finally, we study how the results change if a bid is allowed to have more than one connected component. Even for line graphs, we show that clearing is hard even with  components, and constructing the line graph is hard even with .|Vincent Conitzer,Jonathan Derryberry,Tuomas Sandholm","65305|AAAI|2004|Study of Lower Bound Functions for MAX--SAT|Recently. several lower bound functions are proposed for solving the MAX--SAT problem optimally in a branch-and-bound algorithm. These lower bounds improve significantly the performance of these algorithms. Based on the study of these lower bound functions, we propose a new, linear-time lower bound function. We show that the new lower bound function is admissible and it is consistently and substantially better than other known lower bound functions. The result of this study is a high-performance implementation of an exact algorithm for MAX--SAT which outperforms any implementation of the same class.|Haiou Shen,Hantao Zhang","65402|AAAI|2005|Combinatorial Auctions with wise Dependent Valuations|We analyze the computational and communication complexity of combinatorial auctions from a new perspective the degree of interdependency between the items for sale in the bidders' preferences. Denoting by Gk the class of valuations displaying up to k-wise dependencies, we consider the hierarchy G  G  ...  Gm, where m is the number of items for sale. We show that the minimum non-trivial degree of interdependency (-wise dependency) is sufficient to render NP-hard the problem of computing the optimal allocation (but we also exhibit a restricted class of such valuations for which computing the optimal allocation is easy). On the other hand, bidders' preferences can be communicated efficiently (i.e., exchanging a polynomial amount of information) as long as the interdependencies between items are limited to sets of cardinality up to k, where k is an arbitrary constant. The amount of communication required to transmit the bidders' preferences becomes super-polynomial (under the assumption that only value queries are allowed) when interdependencies occur between sets of cardinality g(m), where g(m) is an arbitrary function such that g(m)   as m  . We also consider approximate elicitation, in which the auctioneer learns, asking polynomially many value queries, an approximation of the bidders' actual preferences.|Vincent Conitzer,Tuomas Sandholm,Paolo Santi"],["66195|AAAI|2007|The Virtual Solar-Terrestrial Observatory A Deployed Semantic Web Application Case Study for Scientific Research|The Virtual Solar-Terrestrial Observatory is a production semantic web data framework providing access to observational datasets from fields spanning upper atmospheric terrestrial physics to solar physics. The observatory allows virtual access to a highly distributed and heterogeneous set of data that appears as if all resources are organized, stored and retrievedused in a common way. The end-user community comprises scientists, students, data providers numbering over  out of an estimated community of . We present details on the case study, our technological approach including the semantic web languages, tools and infrastructure deployed, benefits of AI technology to the application, and our present evaluation after the initial nine months of use.|Deborah L. McGuinness,Peter Fox,Luca Cinquini,Patrick West,Jose Garcia,James L. Benedict,Don Middleton","65150|AAAI|1988|Perceptron Trees A Case Study In Hybrid Concept Representations|THE PAPER PRESENTS A CASE STUDY IN EXAMINING THE BIAS OF TWO PARTICULAR FORMALISMS DECISION TREES AND LINEAR THRESHOLD UNITS. THE IMMEDIATE RESULT IS A NEW HYBRID PRESENTATION, CALLED A ''PERCEPTRON TREE'', AND AN ASSOCIATED LEARNING ALGORITHM CALLED THE ''PERCEPTRON TREE ERROR CORRECTION PROCEDURE''. THE LONGER TERM RESULT IS A MODEL FOR EXPLORING ISSUES RELATED TO UNDERSTANDING REPRESENTATIONAL BIAS AND CONSTRUCTING OTHER USEFUL HYBRID REPRESENTATIONS.|Paul E. Utgoff","65865|AAAI|2006|Machine Translation for Manufacturing A Case Study at Ford Motor Company|Machine Translation was one of the first applications of Artificial Intelligence technology that was deployed to solve real-world problems. Since the early s, researchers have been building and utilizing computer systems that can translate from one language to another without extensive human intervention. In the late s, Ford Vehicle Operations began working with Systran Software Inc to adapt and customize their Machine Translation (MT) technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch and Portuguese. The use of Machine Translation (MT) was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. Our MT system has already translated over  million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation (Iwanska & Shapiro ) and natural language processing (Gazdar & Mellish ), can improve the accuracy of Machine Translation in a dynamic environment such as auto manufacturing.|Nestor Rychtyckyj","66605|AAAI|2010|Integrating a Closed World Planner with an Open World Robot A Case Study|BitTorrent (BT) is one of the most popular Peer-to- Peer (PP) protocols for delivering media files in the Internet today. Although BT is quite efficient for sharing and downloading files by using PP swarming technique, the users have to download almost the whole media file before playing it. This is determined by the Rarest-Block-Download-First strategy of standard BT implementations, which is designed for fast delivery of files in the systems but not for streaming application. In this paper, we present LiveBT, a new protocol which supports video-on- demand streaming service and is totally compatible to the current BitTorrent protocol. LiveBT enables users to play hot movies shared in the BT systems smoothly just after - minutes of buffering time. We also develop the prototype of LiveBT and test the performance through the real BT download tasks of media files. By comparing our prototype with some popular BT clients claiming to support view-as- download service such as Bitcomet, we find that LiveBT spends a much shorter buffering time to play and achieves quite smooth playback performance.|Kartik Talamadupula,J. Benton,Paul W. Schermerhorn,Subbarao Kambhampati,Matthias Scheutz","66404|AAAI|2008|Linking Social Networks on the Web with FOAF A Semantic Web Case Study|One of the core goals of the Semantic Web is to store data in distributed locations, and use ontologies and reasoning to aggregate it. Social networking is a large movement on the web, and social networking data using the Friend of a Friend (FOAF) vocabulary makes up a significant portion of all data on the Semantic Web. Many traditional web-based social networks share their members' information in FOAF format. While this is by far the largest source of FOAF online, there is no information about whether the social network models from each network overlap to create a larger unified social network, or whether they are simply isolated components. If there are intersections, it is evidence that Semantic Web representations and technologies are being used to create interesting, useful data models. In this paper, we present a study of the intersection of FOAF data found in many online social networks. Using the semantics of the FOAF ontology and applying Semantic Web reasoning techniques, we show that a significant percentage of profiles can be merged from multiple networks. We present results on how this affects network structure and what it says about the success of the Semantic Web.|Jennifer Golbeck,Matthew Rothstein","66574|AAAI|2008|A Case Study of AI Application on Language Instruction CSIEC|CSIEC (Computer Simulation in Educational Communication), is not only an intelligent web-based human-computer dialogue system with natural language for English instruction, but also a learning assessment system for learners and teachers. Its multiple functions including grammar gap filling exercises, talk show, free chatting and chatting on a given topic, can satisfy the various needs from the students with different backgrounds and learning abilities. After the brief explanation of the motivation and the survey of the related works, we illustrate the system structure, and describe its pedagogical functions with the underlying AI techniques in details such as NLP and human-computer interaction. We summarize the free Internet usage from a six months period and its integration into English classes in universities and middle schools. The evaluation findings show that the chatting function has been improved and frequently utilized by the users, and the application of the CSIEC system on English instruction can motivate the learners to practice English and enhance their learning process. Finally, we draw some conclusions for the future improvement.|Jiyou Jia","65496|AAAI|2005|Complexity-Guided Case Discovery for Case Based Reasoning|The distribution of cases in the case base is critical to the performance of a Case Based Reasoning system. The case author is given little support in the positioning of new cases during the development stage of a case base. In this paper we argue that classification boundaries represent important regions of the problem space. They are used to identify locations where new cases should be acquired. We introduce two complexity-guided algorithms which use a local complexity measure and boundary identification techniques to actively discover cases close to boundaries. The ability of these algorithms to discover new cases that significantly improve the accuracy of case bases is demonstrated on five public domain classification datasets.|Stewart Massie,Susan Craw,Nirmalie Wiratunga","66394|AAAI|2008|A Case Study on the Critical Role of Geometric Regularity in Machine Learning|An important feature of many problem domains in machine learning is their geometry. For example, adjacency relationships, symmetries, and Cartesian coordinates are essential to any complete description of board games, visual recognition, or vehicle control. Yet many approaches to learning ignore such information in their representations, instead inputting flat parameter vectors with no indication of how those parameters are situated geometrically. This paper argues that such geometric information is critical to the ability of any machine learning approach to effectively generalize even a small shift in the configuration of the task in space from what was experienced in training can go wholly unrecognized unless the algorithm is able to learn the regularities in decision-making across the problem geometry. To demonstrate the importance of learning from geometry, three variants of the same evolutionary learning algorithm (NeuroEvolution of Augmenting Topologies), whose representations vary in their capacity to encode geometry, are compared in checkers. The result is that the variant that can learn geometric regularities produces a significantly more general solution. The conclusion is that it is important to enable machine learning to detect and thereby learn from the geometry of its problems.|Jason Gauci,Kenneth O. Stanley","65303|AAAI|2004|Useful Roles of Emotions in Artificial Agents A Case Study from Artificial Life|In this paper, we discuss the role of emotions in AI and possible ways to determine their utility for the design of artificial agents. We propose a research methodology for determining the utility of emotional control and apply it to the study of autonomous agents that compete for resources in an artificial life environment. The results show that the emotional control can improve performance in some circumstances.|Matthias Scheutz","66046|AAAI|2007|KSU Willie in Semantic Vision Challenge|The KSU Willie entry in the Semantic Vision Challenge will use a variety of classifiers, some standard classifiers and some newly developed classifiers, to learn the classification of images downloaded from the web. KSU Willie will use those classifiers to identify objects in the environment.|David Gustafson,Aaron Chavez,Michael Marlen,Andrew King,Alejandro Alliana,Ondrej Linda"],["66824|AAAI|2010|A General Game Description Language for Incomplete Information Games|A novel improved linear discriminant analysis (ILDA) method is presented. Comparing with LDA, under the condition of d  c -, d and c are the dimensionality of feature subspace and the number of classes respectively, ILDA uniformly preserves the class distances of classpairs by rearranging the contribution of each class-pair to the generalized between-class scatter matrix after whitening within-class scatter matrix. Experiment results based on simulating data and measured radar data both show that, under the condition of d  c -, the features extracted by ILDA are more efficient for multi-class classification than those extracted by LDA.|Michael Thielscher","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","65720|AAAI|2006|On the Update of Description Logic Ontologies at the Instance Level|We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.|Giuseppe De Giacomo,Maurizio Lenzerini,Antonella Poggi,Riccardo Rosati","65791|AAAI|2006|A Modular Action Description Language|\"Toy worlds\" involving actions, such as the blocks world and the Missionaries and Cannibals puzzle, are often used by researchers in the areas of commonsense reasoning and planning to illustrate and test their ideas. We would like to create a datahase of general-purpose knowledge about actions that encodes common features of many action domains of this kind. in the same way as abstract algebra and topology represent common features of specific number systems. This paper is a report on the first stage of this project--the design of an action description language in which this database will be written The new language is an extension of the action language C+. Its main distinctive feature is the possibility of referring to other action descriptions in the definition of a new action domain.|Vladimir Lifschitz,Wanwan Ren","65006|AAAI|1986|A Logical-Form and Knowledge-Base Design for Natural Language Generation|This paper presents a technique for interpreting output demands by a natural language sentence generator in a formally transparent and efficient way. These demands are stated in a logical language. A network knowledge base organizes the concepts of the application domain into categories known to the generator. The logical expressions are interpreted by the generator using the knowledge base and a restricted, but efficient, hybrid knowledge representation system. This design has been used to allow the NIGEL generator to interpret statements in a first-order predicate calculus using the NIKL and KL-TWO knowledge representation systems. The success of this experiment has led to plans for the inclusion of this design in both the evolving Penman natural language generator and the Janus natural language interface.|Norman K. Sondheimer,Bernhard Nebel","66129|AAAI|2007|Complexity Boundaries for Horn Description Logics|Horn description logics (Horn-DLs) have recently started to attract attention due to the fact that their (worst-case) data complexities are in general lower than their overall (i.e. combined) complexities, which makes them attractive for reasoning with large ABoxes. However, the natural question whether Horn-DLs also provide advantages for TBox reasoning has hardly been addressed so far. In this paper, we therefore provide a thorough and comprehensive analysis of the combined complexities of Horn-DLs. While the combined complexity for many Horn-DLs turns out to be the same as for their non-Horn counterparts, we identify subboolean DLs where Hornness simplifies reasoning.|Markus Kr√∂tzsch,Sebastian Rudolph,Pascal Hitzler","65505|AAAI|2005|Knowledge Integration for Description Logics|Description logic reasoners are able to detect incoherences (such as logical inconsistency and concept unsatisfiability) in knowledge bases, but provide little support for resolving them. We propose to recast techniques for propositional inconsistency management into the description logic setting. We show that the additional structure afforded by description logic statements can be used to refine these techniques. Our focus in this paper is on the formal semantics for such techniques, although we do provide high-level decision procedures for the knowledge integration strategies discussed.|Thomas Andreas Meyer,Kevin Lee,Richard Booth","65842|AAAI|2006|Characterizing Data Complexity for Conjunctive Query Answering in Expressive Description Logics|Description Logics (DLs) are the formal foundations of the standard web ontology languages OWL-DL and OWL-Lite. In the Semantic Web and other domains, ontologies are increasingly seen also as a mechanism to access and query data repositories. This novel context poses an original combination of challenges that has not been addressed before (i) sufficient expressive power of the DL to capture common data modeling constructs (ii) well established and flexible query mechanisms such as Conjunctive Queries (CQs) (iii) optimization of inference techniques with respect to data size, which typically dominates the size of ontologies. This calls for investigating data complexity of query answering in expressive DLs. While the complexity of DLs has been studied extensively, data complexity has been characterized only for answering atomic queries, and was still open for answering CQs in expressive DLs. We tackle this issue and prove a tight CONP upper bound for the problem in SHIQ, as long as no transitive roles occur in the query. We thus establish that for a whole range of DLs from AL to SHIQ, answering CQs with no transitive roles has CONP-complete data complexity. We obtain our result by a novel tableaux-based algorithm for checking query entailment, inspired by the one in , but which manages the technical challenges of simultaneous inverse roles and number restrictions (which leads to a DL lacking the finite model property).|Magdalena Ortiz,Diego Calvanese,Thomas Eiter","66176|AAAI|2007|A Modular Action Description Language for Protocol Composition|Protocols are modular abstractions that capture patterns of interaction among agents. The compelling vision behind protocols is to enable creating customized interactions by refining and composing existing protocols. Realizing this vision presupposes () maintaining repositories of protocols and () refining and composing selected protocols. To this end, this paper synthesizes recent advances on protocols and on the knowledge representation of actions. This paper presents MAD-P, a modular action description language tailored for protocols. MAD-P enables building an aggregation hierarchy of protocols via composition. This paper demonstrates the value of such compositions via a simplified, but realistic, business scenario.|Nirmit Desai,Munindar P. Singh","66183|AAAI|2007|Integrating Natural Language Knowledge Representation and Reasoning and Analogical Processing to Learn by Reading|Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read.|Kenneth D. Forbus,Christopher Riesbeck,Lawrence Birnbaum,Kevin Livingston,Abhishek Sharma,Leo C. Ureel II"]]}}