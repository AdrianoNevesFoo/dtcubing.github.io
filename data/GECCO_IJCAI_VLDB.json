{"abstract":{"entropy":6.637609974026952,"topics":["genetic algorithm, algorithm, evolutionary algorithm, genetic programming, problem, particle swarm, solving problem, present algorithm, constraint satisfaction, optimization problem, optimization algorithm, optimization, evolutionary computation, search algorithm, algorithm problem, constraint problem, heuristic search, estimation distribution, neural networks, search","data, data base, natural language, machine learning, data management, data system, database, data model, learning system, expert system, play role, learning classifier, database system, management system, relational database, large data, data stream, classifier system, system, web search","artificial intelligence, real world, mobile robot, agents, research area, autonomous agents, pattern recognition, robot, time series, vision system, research intelligence, resources allocation, control system, planning plan, artificial system, multi-agent system, recent research, describes system, research artificial, research","knowledge representation, present approach, theorem proving, logic, approach based, programming language, reasoning, model, description logic, model based, logic programming, present model, theorem prover, natural language, present framework, describes approach, program, knowledge, temporal reasoning, qualitative reasoning","consider problem, markov decision, solution problem, ant colony, bayesian networks, markov processes, problem networks, spanning tree, bayesian algorithm, study problem, traveling salesman, problem optimal, partially observable, algorithm problem, markov mdps, paper problem, decision processes, processes mdps, decision problem, problem graph","evolutionary algorithm, search algorithm, multi-objective evolutionary, multi-objective optimization, local search, evolutionary problem, evolutionary optimization, evolutionary search, differential evolution, local algorithm, algorithm applied, multi-objective algorithm, evolutionary eas, applied problem, fitness function, reinforcement learning, evolution strategies, evolutionary, optimization emo, evolutionary applied","relational data, relational database, xml data, query data, xml documents, search engine, query processing, queries data, web engine, database query, query, web pages, relational xml, xml, efficient xml, query optimization, queries, database, xml language, data points","data base, data management, data system, data, play role, large data, data stream, applications data, data information, large number, problem data, web search, data model, management system, algorithm data, base management, database management, data sources, processing data, used data","vision system, pattern recognition, computer system, recognition objects, recognition system, objects, computer program, computer vision, image objects, moving objects, computer, system scene, behavior system, simultaneous localization, problem computer, software system, software engineering, localization mapping, system objects, line drawing","artificial intelligence, mobile robot, research area, control system, artificial system, research, recent research, research intelligence, recent years, development system, research artificial, becoming increasingly, neural networks, robot, sensor networks, research system, system robot, artificial agents, control robot, autonomous robot","temporal reasoning, qualitative reasoning, nonmonotonic logic, dimensionality reduction, belief revision, spatial reasoning, modal logic, boolean satisfiability, first-order logic, nonmonotonic reasoning, general purpose, default logic, model, temporal constraint, formalism reasoning, default reasoning, spatial temporal, complexity reasoning, problem given, logic formalism","theorem proving, theorem prover, present framework, general framework, situation calculus, program, logic program, present methodology, computer program, present program, predicate calculus, theorem, model presented, system described, program system, effects actions, horn clause, system presented, present called, present overview"],"ranking":[["16250|IJCAI|2005|A Scalable Method for Multiagent Constraint Optimization|We present in this paper a new, complete method for distributed constraint optimization, based on dynamic programming. It is a utility propagation method, inspired by the sum-product algorithm, which is correct only for tree-shaped constraint networks. In this paper, we show how to extend that algorithm to arbitrary topologies using a pseudotree arrangement of the problem graph. Our algorithm requires a linear number of messages, whose maximal size depends on the induced width along the particular pseudotree chosen. We compare our algorithm with backtracking algorithms, and present experimental results. For some problem types we report orders of magnitude fewer messages, and the ability to deal with arbitrarily large problems. Our algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well.|Adrian Petcu,Boi Faltings","57629|GECCO|2006|GRASP - evolution for constraint satisfaction problems|There are several evolutionary approaches for solving random binary Constraint Satisfaction Problems (CSPs). In most of these strategies we find a complex use of information regarding the problem at hand. Here we present a hybrid Evolutionary Algorithm that outperforms previous approaches in terms of effectiveness and compares well in terms ofefficiency. Our algorithm is conceptual and simple, featuring a GRASP-like (GRASP stands for Greedy Randomized Adaptive Search Procedure) mechanism for genotype-to-phenotype mapping, and without considering any specific knowledge of the problem. Therefore, we provide a simple algorithm that harnesses generality while boosting performance.|Manuel Cebrián,Iván Dotú","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Suárez,Manuel Valenzuela-Rendón,Hugo Terashima-Marín,Eduardo Uresti-Charre","58515|GECCO|2008|Parameter-less evolutionary search|The paper presents the parameter-less implementation of an evolutionary-based search. It does not need any predefined control parameters values, which are usually used for genetic algorithms and similar techniques. Efficiency of the proposed algorithm was evaluated by CEC benchmark functions and a real-world product optimization problem.|Gregor Papa","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","58554|GECCO|2008|An evolutionary approach for competency-based curriculum sequencing|The process of creating e-learning contents using reusable learning objects (LOs) can be broken down in two sub-processes LOs finding and LO sequencing. Sequencing is usually performed by instructors, who create courses targeting generic profiles rather than personalized materials. This paper proposes an evolutionary approach to automate this latter problem while, simultaneously, encourages reusability and interoperability by promoting standards employment. A model that enables automated curriculum sequencing is proposed. By means of interoperable competency records and LO metadata, the sequencing problem is turned into a constraint satisfaction problem. Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) agents are designed, built and tested in real and simulated scenarios. Results show both approaches succeed in all test cases, and that they handle reasonably computational complexity inherent to this problem, but PSO approach outperforms GA.|Luis de Marcos,José-Javier Martínez,José Antonio Gutiérrez,Roberto Barchino,José María Gutiérrez","57046|GECCO|2003|Pruning Neural Networks with Distribution Estimation Algorithms|This paper describes the application of four evolutionary algorithms to the pruning of neural networks used in classification problems. Besides of a simple genetic algorithm (GA), the paper considers three distribution estimation algorithms (DEAs) a compact GA, an extended compact GA, and the Bayesian Optimization Algorithm. The objective is to determine if the DEAs present advantages over the simple GA in terms of accuracy or speed in this problem. The experiments considered a feedforward neural network trained with standard backpropagation and  public-domain and artificial data sets. In most cases, the pruned networks seemed to have better or equal accuracy than the original fully-connected networks. We found few differences in the accuracy of the networks pruned by the four EAs, but found large differences in the execution time. The results suggest that a simple GA with a small population might be the best algorithm for pruning networks on the data sets we tested.|Erick Cantú-Paz","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule","58942|GECCO|2010|A robust estimation of distribution algorithm for power electronic circuits design|The automated synthesis and optimization of power electronic circuits (PECs) is a significant and challenging task in the field of power electronics. Traditional methods such as the gradient-based methods, the hill-climbing techniques and the genetic algorithms (GA), are either prone to local optima or not efficient enough to find highly accurate solutions for this problem. To better optimize the design of PECs, this paper presents an extended histogram-based estimation of distribution algorithm with an adaptive refinement process (EDAa-r). In the EDAa-r, the histogram-based estimation of distribution algorithm is used to roughly locate the global optimum, while the adaptive refinement process is used to improve the accuracy of solutions. The adaptive refinement process, with its search radius adjusted adaptively during the evolution, is executed to search the surrounding region of the best-so-far solution in every generation. To maintain the diversity, a historic learning strategy is used in constructing the probabilistic model and a mutation strategy is hybridized in the sampling operation. The proposed EDAa-r has been successfully used to optimize the design of a buck regulator. Experimental results show that compared with the GA and the particle swarm optimization (PSO), the EDAa-r can obtain much better mean solution quality and is less likely to be trapped into local optima.|Jinghui Zhong,Jun Zhang"],["80357|VLDB|2004|Linear Road A Stream Data Management Benchmark|This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora  (out of Brandeis University, Brown University and MIT) and STREAM  (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses \"variable tolling\" , ,  an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of  on streaming data applications.|Arvind Arasu,Mitch Cherniack,Eduardo F. Galvez,David Maier,Anurag Maskey,Esther Ryvkina,Michael Stonebraker,Richard Tibbetts","79955|VLDB|1979|Spatial Management of Data Abstract|Spatial Data Management is a technique for organizing and retrieving information by positioning it in a Graphical Data Space (GDS). This Graphical Data Space is viewed through a color raster scan display which enables users to traverse the GDS surface or zoom into the image to obtain greater detail. In contrast to conventional database management systems - in which users access data by asking questions in a formal query language, a Spatial Data Management System (SDMS) presents the information graphically in a form which seems to encourage browsing and to require less prior knowledge of the contents and organization of the database. This paper presents an overview of the SDMS concept and describes its implementation in a prototype system for retrieving information from both a symbolic database management system and from an optical videodisk.|Christopher F. Herot","80048|VLDB|1981|A Performance Evaluation of Data Base Machine Architectures Invited Paper|The rapid advances in the development of low-cost computer hardware have led to many proposals for the use of this hardware to improve the performance of database management systems. Usually the design proposals are quite vague about the performance of the system with respect to a given data management application. In this paper we develop an analytical model of the performance of a conventional database management system and four generic database machine architectures. This model is then used to compare the performance of each type of machine with a conventional DBMS. We demonstrate that no one type of database machine is best for executing all types of queries. We also show that for several classes of queries certain database machine designs which have been proposed are actually slower than a DBMS on a conventional processor.|David J. DeWitt,Paula B. Hawthorn","80225|VLDB|2002|ProTDB Probabilistic Data in XML|Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.|Andrew Nierman,H. V. Jagadish","80061|VLDB|1981|DSIS - A Database System with Interrelational Semantics|DSIS is an experimental multi-user database system that supports an entity-oriented data model with relational and interrelational semantics. It maintains a sharp distinction between the user workspace and the shared database. Transactions are executed by a simulated network of stream processors.|Y. Edmund Lien,Jonathan E. Shopiro,Shalom Tsur","79893|VLDB|1977|Psychological Issues in Data Base Management|In order to maximize total data base system effectiveness, it is becoming increasingly important to attend to psychological issues. This paper identifies issues in data base design and management where psychological considerations are likely to play a significant role. Specific studies that address some of these issues are reviewed. Next, a number of areas of basic psychological research that are likely to be of use to data base system designers are indicated. Finally, general recommendations and considerations for data base interfaces are given.|John C. Thomas","15177|IJCAI|1995|Advances of the DBLearn System for Knowledge Discovery in Large Databases|A prototyped data mining system, DBLearn, was developed in Simon Fraser Univ., which integrates machine learning methodologies with database technologies and efficiently and effectively extracts characteristic and discriminant rules from relational databases. Further developments, of DBLearn lead to a new generation data mining system DBMiner, with the following features () mining new kinds of rules from large databases, including multiple-level association rules, classification rules, cluster description rules, etc., () automatic generation and refinement of concept hierarchies, () high level SQL-like and graphical data mining interfaces, and () clientserver architecture and performance improvements for large applications. The major features of the system are demonstrated with experiments in a research grant information database.|Jiawei Han,Yongjian Fu,Simon Tang","80034|VLDB|1981|Derived Relations A Unified Mechanism for Views Snapshots and Distributed Data|In a relational system, a database is composed of base relations, views, and snapshots. We show that this traditional approach can be extended to different classes of derived relations, and we propose a unified data definition mechanism for centralized and distributed databases. Our mechanism, called DEREL, can be used to define base relations and to derive different classes of views, snapshots, partitioned and replicated data. DEREL is intended to be part of a general purpose distributed relational database management system.|Michel E. Adiba","14063|IJCAI|1983|ACE An Expert System for Telephone Cable Maintenance|ACE, a system for Automated Cable Expertise, is a Knowledge-Based Expert System designed to provide troubleshooting reports and management analyses for telephone cable maintenance. Design decisions faced during the construction of ACE were guided by recent successes in expert systems technology, most notably RXCN, the Digital Equipment Corporation VAX configuration program. ACE departs from \"standard\" expert system architectures in its use of a conventional data base management system as its primary source of information. Its primary sources of knowledge are the users of the database system and primers on maintenance analysis strategies.|Gregg T. Vesonder,Salvatore J. Stolfo,John E. Zielinski,Frederick D. Miller,David H. Copp","79844|VLDB|1977|The Role Concept in Data Models|A new data model is described which permits the representation of the different roles which a real world entity may play. This data model is an extension of the network model, as used in I-D-S and its derivative, the CODASYL database system. The concepts of item and set are retained. The record concept has been refined to clarify the old record concept and introduce the new role-segment concept. The record represents the existence of an entity of the real world while the role-segment represents the existence of one of the entity's roles. A person and a corporation are examples of an entity, while a stockholder and a customer are examples of a role that either the person or corporation can assume. A role-segment occurrence serves to group and name the properties concerning the existence of one role. This paper shows that the record and role segment concepts can be integrated into the required data description and data manipulation language. The meta entity types of the role data model are contrasted with those of older data models for the data occurrence domain. The ambigous use of meta types by the older models is thus shown. This ambiguity appears to prohibit those older data models from serving as the basis of a conceptual schema where data transformation support of richer data models is required. The meta entity types of the role data model are identified, described and related to the real world, data occurrence, conceptual schema, and data description domains.|Charles W. Bachman,Manilal Daya"],["16713|IJCAI|2007|Learning and Multiagent Reasoning for Autonomous Agents|One goal of Artificial Intelligence is to enable the creation of robust, fully autonomous agents that can coexist with us in the real world. Such agents will need to be able to learn, both in order to correct and circumvent their inevitable imperfections, and to keep up with a dynamically changing world. They will also need to be able to interact with one another, whether they share common goals, they pursue independent goals, or their goals are in direct conflict. This paper presents current research directions in machine learning, multiagent reasoning, and robotics, and advocates their unification within concrete application domains. Ideally, new theoretical results in each separate area will inform practical implementations while innovations from concrete multiagent applications will drive new theoretical pursuits, and together these synergistic research approaches will lead us towards the goal of fully autonomous agents.|Peter Stone","13685|IJCAI|1977|Speech Understanding and AIAI and Speech Understanding|This panel will review research in speech understanding (SU) and in artificial intelligence (AI) from two perspectives the contributions that AI has made to SU -- the resources in AI that have been used in the development of SU systems. the contributions that SU has made to AI - the results of the SU program that have affected or arc likely to affect futuie AI research. Four topics are identified for major consideration Multiple sources of Knowledge which are required how should they be oiganized, System control how to manage the complex inter actions involved. language understanding comparisons of text and speech input. Organization of research -creating complex, multisource, knowledge-based systems.|Donald E. Walker,Lee D. Erman,Allen Newell,Nils J. Nilsson,William H. Paxton,Terry Winograd,William A. Woods","13689|IJCAI|1977|Planning in the World of the Air Traffic Controller|An enroute air traffic control (ATC) simulation has provided the basis for research into the marriage of discrete simulation and artificial intelligence techniques. A program which simulates, using real world data, the movement of aircraft in an ATC environment forms a robot's world model. Using a production system to respond to events in the simulated world, the robot is able to look ahead and form a plan of instructions which guarantees safe, expedient aircraft transit. A distinction is made between the real world, where pilots can make mistakes, change their minds, etc., and an idealized plan-ahead world which the robot uses the over-all simulation alternates between updating the real world and planning in the idealized one to investigate the robot's ability to plan in the face of uncertainty.|Robert B. Wesson","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","14892|IJCAI|1991|Massively Parallel Artificial Intelligence|Massively Parallel Artificial Intelligence is a new and growing area of AI research, enabled by the emergence of massively parallel machines. It is a new paradigm in AI research. A high degree of parallelism not only affects computing performance, but also triggers drastic change in the approach toward building intelligent systems memory-based reasoning and parallel marker-passing are examples of new and redefined approaches. These new approaches, fostered by massively parallel machines, offer a golden opportunity for AI in challenging the vastness and irregularities of real - world data that are encountered when a system accesses and processes Very Large Data Bases and Knowledge Bases. This article describes the current status of massively parallel artificial intelligence research and positions of each panelist.|Hiroaki Kitano,James A. Hendler,Tetsuya Higuchi,Dan I. Moldovan,David L. Waltz","14178|IJCAI|1985|Building a Bridge Between AI and Robotics|About fifteen years ago, the hand eye system was one of the exciting research topics in artificial intelligence. Several AI groups attempted to develop intelligent robots by combining computer vision with computer controlled manipulator. However, after the success of early prototypes, research efforts have been splitted into general intelligence research and real world oriented robotics research. Currently, the author feels there exists a significant gap between AI and robotics in spite of the necessity of communication and integration. Thus, without building a bridge over the gap, AI will lose a fertile research field that needs real-time real-world intelligence, and robotics will never acquire intelligence even if it works skillfully. In this paper, I would like to encourage AI community to promote more efforts on real world robotics, with the discussions about key points for the study. This paper also introduces current steps of our robotics research that attempt to connect perception with action as intelligently as possible.|Hirochika Inoue","13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson","14891|IJCAI|1991|Commitment and Effectiveness of Situated Agents|Recent research in real-time Artificial Intelligence has focussed upon the design of situated agents and, in particular, how to achieve effective and robust behaviour with limited computational resources. A range of architectures and design principles has been proposed to solve this problem. This has led to the development of simulated worlds that can serve as testbeds in which the effectiveness of different agents can be evaluated. We report here an experimental program that aimed to investigate how commitment to goals contributes to effective behaviour and to compare the properties of different strategies for reacting to change. Our results demonstrate the feasibility of developing systems for empirical measurement of agent performance that are stable, sensitive, and capable of revealing the effect of \"high-level\" agent characteristics such as commitment.|David Kinny,Michael P. Georgeff","13398|IJCAI|1973|Planning Considerations for a Roving Robot with Arm|The Jet Propulsion Laboratory is engaged in a robot research program. The program is aimed at the development and demonstration of technology required to integrate a variety of robntic functions (locomotion, manipulation, sensing and perception, decision making, and man-robot interaction) into a working robot unit operating in a real world environment and dealing with both man-made and natural objects. This paper briefly describes the hardware and software system architecture of the robot breadboard and summarizes the developments to date. The content of the paper is focused on the unique planning considerations involved in incorporating a manipulator as part of an autonomous robot system. In particular, the effects of system architecture, arm trajectory calculations, and arm dynamics and control are discussed in the context of planning arm motion in complex and changing sensory and workspace environments.|Richard A. Lewis,Antal K. Bejczy","14284|IJCAI|1985|A Robot Planning Structure Using Production Rules|Robot plan generation is a field which engendered the development of AI languages and rule-based expert systems. Utilization of these latter concepts permits a flexible formalism for robot planning research. We present a robot plan-generation architecture and its application to a real-world mobile robot system. The system undergoes tests through its utilization in the IIILARE robot project (Ciralt, et al., ). Though the article concentrates on planning, execution monitoring and error recovery are discussed. The system includes models of its synergistic environment as well SR of its sensors and effectors (i.e. operators). Its rules embody both planning specific and domair specific knowledge. The system gains generality and adaptiveness through the use of planning variables which provide constraints to the plan generation system. It is implemented in an efficient compiled Production System language (PSL).|Ralph P. Sobek"],["15866|IJCAI|2003|A Resolution Theorem for Algebraic Domains|W. C. Rounds and G.-Q. Zhang have recently proposed to study a form of resolution on algebraic domains Rounds and Zhang, . This framework allows reasoning with knowledge which is hierarchically structured and forms a (suitable) domain, more precisely, a coherent algebraic cpo as studied in domain theory. In this paper, we give conditions under which a resolution theorem -- in a form underlying resolution-based logic programming systems -- can be obtained. The investigations bear potential for engineering new knowledge representation and reasoning systems on a firm domain-theoretic background.|Pascal Hitzler","16227|IJCAI|2005|Temporal Context Representation and Reasoning|This paper demonstrates how a model for temporal context reasoning can be implemented. The approach is to detect temporally related events in natural language text and convert the events into an enriched logical representation. Reasoning is provided by a first order logic theorem prover adapted to text. Results show that temporal context reasoning boosts the performance of a Question Answering system.|Dan I. Moldovan,Christine Clark,Sanda M. Harabagiu","14929|IJCAI|1991|Representing Diagnostic Knowledge for Probabilistic Horn Abduction|This paper presents a simple logical framework for abduction, with probabilities associated with hypotheses. The language is an extension to pure Prolog, and it has straight-forward implementations using branch and bound search with either logic-programming technology or ATMS technology. The main focus of this paper is arguing for a form of representational adequacy of this very simple system for diagnostic reasoning. It is shown how it can represent model-based knowledge, with and without faults, and with and without nonintermittency assumptions. It is also shown how this representation can represent any probabilistic knowledge representable in a Bayesian belief network.|David Poole","15005|IJCAI|1993|Proving Theorems in a Multi-Source Environment|This paper describes a logic for reasoning in a multi-source environment and a theorem prover for this logic. We assume the existence of several sources of information (dataknowledge bases), each of them providing information. The main problem dealt with here is the problem of the consistency of the information  even if each separate source is consistent, the global set of information may be inconsistent. In our approach, we assume that the different sources are totally ordered, according to their reliability. This order is then used in order to avoid inconsistency. The logic we define for reasoning in this case is based on a classical logic augmented with pseudo-modalities. Its semantic is first detailed. Then a sound and complete axiomatic is given. Finally, a theorem prover is specified at the meta-level. We prove that it is correct with regard to the logic. We then implement it in a PROLOG-like language.|Laurence Cholvy","15474|IJCAI|1999|Stable Model Checking Made Easy|Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (V-free) logic programs liecause stable model checking - deciding whether a given model is a stable model of a propositional DLP program is co-NP-complete, while it is polynomial for normal logic programs. This paper proposes a new transformation M(P) which reduces stable model checking to UNSAT - i.e., to deciding whether a given CNF formula is unsatisfiable. Thus, the stability of a model AI for a program P can be verified by calling a Satisfiability Checker on the CNF formula M(P). The transformation is parsimonious and efficiently computable, as it runs in logarithmic space. Moreover, the size of the generated CNF formula never exceeds the size of the input. The proposed approach to stable model checking lias been implemented in a DLP system, and a number of experiments and benchmarks have been run.|Christoph Koch,Nicola Leone","15438|IJCAI|1999|Programming Resource-Bounded Deliberative Agents|This paper is concerned with providing a common framework for both the logical specification and execution of agents. While numerous high-level agent theories have been proposed in order to model agents, such as theories of intention, these often have little formal connection to practical agentbased systems. On the other hand, many of the agent-based programming languages used for implementing real agents lack firm logical semantics. Our approach is to define a logical framework in which agents can be specified, and then show how such specifications can be directly executed in order to implement the agent's behaviour. We here extend this approach to capture an important aspect of practical agents, namely their resource-bounded nature. We present a logic in which resource-boundedness can be specified, and then consider how specifications within this logic can be directly executed. The mechanism we use to capture finite resources is to replace the standard modal logic previously used to represent an agent's beliefs, with a multi-context representation of belief, thus providing tight control over the agent's reasoning capabilities where necessary. This logical framework provides the basis for the specification and execution of agents comprising dynamic (temporal) activity, deliberation concerning goals, and resource-bounded reasoning.|Michael Fisher,Chiara Ghidini","15662|IJCAI|2001|A-System Problem Solving through Abduction|This paper presents a new system, called the A- System, performing abductive reasoning within the framework of Abductive Logic Programming. It is based on a hybrid computational model that implements the abductive search in terms of two tightly coupled processes a reduction process of the highlevel logical representation to a lower-level constraint store and a lower-level constraint solving process. A set of initial \"proof of principle\" experiments demonstrate the versatility of the approach stemming from its declarative representation of problems and the good underlying computational behaviour of the system. The approach offers a general methodology of declarative problem solving in AI where an incremental and modular refinement of the high-level representation with extra domain knowledge can improve and scale the computational performance of the framework.|Antonis C. Kakas,Bert Van Nuffelen,Marc Denecker","15375|IJCAI|1997|PRISM A Language for Symbolic-Statistical Modeling|We present an overview of symbolic-statistical modeling language PRISM whose programs are not only a probabilistic extension of logic programs but also able to learn from examples with the help of the EM learning algorithm. As a knowledge representation language appropriate for probabilistic reasoning, it can describe various types of symbolic-statistical modeling formalism known but unrelated so far in a single framework. We show by examples, together with learning results, that most popular probabilistic modeling formalisms, the hidden Markov model and Bayesian networks, are described by PRISM programs.|Taisuke Sato,Yoshitaka Kameya","15000|IJCAI|1993|An Abductive Framework for General Logic Programs and other Nonmonotonic Systems|We present an abductive semantics for general propositional logic programs which defines the meaning of a logic program in terms of its extensions. This approach extends the stable model semantics for normal logic programs in a natural way. The new semantics is equivalent to stable semantics for a logic program P whenever P is normal and has a stable model. The abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner. Our approach is based on an idea recently proposed by Konolige for causal reasoning. Instead of maximizing the set of hypotheses alone we maximize the union of the hypotheses, along with possible hypotheses that are excused or refuted by the theory.|Gerhard Brewka,Kurt Konolige","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"],["15126|IJCAI|1995|Exploiting Structure in Policy Construction|Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods.|Craig Boutilier,Richard Dearden,Moisés Goldszmidt","16747|IJCAI|2007|Context-Driven Predictions|Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.|Marc G. Bellemare,Doina Precup","16313|IJCAI|2005|Temporal-Difference Networks with History|Temporal-difference (TD) networks are a formalism for expressing and learning grounded world knowledge in a predictive form Sutton and Tanner, . However, not all partially observable Markov decision processes can be efficiently learned with TD networks. In this paper, we extend TD networks by allowing the network-update process (answer network) to depend on the recent history of previous actions and observations rather than only on the most recent action and observation. We show that this extension enables the solution of a larger class of problems than can be solved by the original TD networks or by history-based methods alone. In addition, we apply TD networks to a problem that, while still simple, is significantly larger than has previously been considered. We show that history-extended TD networks can learn much of the common-sense knowledge of an egocentric gridworld domain with a single bit of perception.|Brian Tanner,Richard S. Sutton","16248|IJCAI|2005|Algebraic Markov Decision Processes|In this paper, we provide an algebraic approach to Markov Decision Processes (MDPs), which allows a unified treatment of MDPs and includes many existing models (quantitative or qualitative) as particular cases. In algebraic MDPs, rewards are expressed in a semiring structure, uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure, and preferences over policies are represented by Generalized Expected Utility. We recast the problem of finding an optimal policy at a finite horizon as an algebraic path problem in a decision rule graph where arcs are valued by functions, which justifies the use of the Jacobi algorithm to solve algebraic Bellman equations. In order to show the potential of this general approach, we exhibit new variations of MDPs, admitting complete or partial preference structures, as well as probabilistic or possibilistic representation of uncertainty.|Patrice Perny,Olivier Spanjaard,Paul Weng","16409|IJCAI|2007|Topological Value Iteration Algorithm for Markov Decision Processes|Value Iteration is an inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, LAO, LRTDP and HDP are state-of-theart ones. All of these use reachability analysis and heuristics to avoid some unnecessary backups. However, none of these approaches fully exploit the graphical features of the MDPs or use these features to yield the best backup sequence of the state space. We introduce an algorithm named Topological Value Iteration (TVI) that can circumvent the problem of unnecessary backups by detecting the structure of MDPs and backing up states based on topological sequences. We prove that the backup sequence TVI applies is optimal. Our experimental results show that TVI outperforms VI, LAO, LRTDP and HDP on our benchmark MDPs.|Peng Dai,Judy Goldsmith","15718|IJCAI|2001|Complexity of Probabilistic Planning under Average Rewards|A general and expressive model of sequential decision making under uncertainty is provided by the Markov decision processes (MDPs) framework. Complex applications with very large state spaces are best modelled implicitly (instead of explicitly by enumerating the state space), for example as precondition-effect operators, the representation used in AI planning. This kind of representations are very powerful, and they make the construction of policiesplans computationally very complex. In many applications, average rewards over unit time is the relevant rationality criterion, as opposed to the more widely used discounted reward criterion, and for providing a solid basis for the development of efficient planning algorithms, the computational complexity of the decision problems related to average rewards has to be analyzed. We investigate the complexity of the policyplan existence problem for MDPs under the average reward criterion, with MDPs represented in terms of conditional probabilistic precondition-effect operators. We consider policies with and without memory, and with different degrees of sensingobservability. The unrestricted policy existence problem for the partially observable cases was earlier known to be undecidable. The results place the remaining computational problems to the complexity classes EXP and NEXP (deterministic and nondeterministic exponential time.)|Jussi Rintanen","15475|IJCAI|1999|Computing Factored Value Functions for Policies in Structured MDPs|Many large Markov decision processes (MDPs) can be represented compactly using a structured representation such as a dynamic Bayesian network. Unfortunately, the compact representation does not help standard MDP algorithms, because the value function for the MDP does not retain the structure of the process description. We argue that in many such MDPs, structure is approximately retained. That is, the value functions are nearly additive closely approximated by a linear function over factors associated with small subsets of problem features. Based on this idea, we present a convergent, approximate value determination algorithm for structured MDPs. The algorithm maintains an additive value function, alternating dynamic programming steps with steps that project the result back into the restricted space of additive functions. We show that both the dynamic programming and the projection steps can be computed efficiently, despite the fact that the number of states is exponential in the number of state variables.|Daphne Koller,Ronald Parr","15972|IJCAI|2003|Modular self-organization for a long-living autonomous agent|The aim of this paper is to provide a sound framework for addressing a difficult problem the automatic construction of an autonomous agent's modular architecture. We briefly present two apparently uncorrelated frameworks Autonomous planning through Markov Decision Processes and Kernel Clustering. Our fundamental idea is that the former addresses autonomy whereas the latter allows to tackle self-organizing issues. Relying on both frameworks, we show that modular self-organization can be formalized as a clustering problem in the space of MDPs. We derive a modular self-organizing algorithm in which an autonomous agent learns to efficiently spread n planning problems over m initially blank modules with m  n.|Bruno Scherrer","15407|IJCAI|1999|Bounding the Suboptimality of Reusing Subproblem|We are interested in the problem of determining a course of action to achieve a desired objective in a non-deterministic environment. Markov decision processes (MDPs) provide a framework for representing this action selection problem, and there are a number of algorithms that learn optimal policies within this formulation. This framework has also been used to study state space abstraction, problem decomposition, and policy reuse. These techniques sacrifice optimality of their solution for improved learning speed. In this paper we examine the suboptimality of reusing policies that are solutions to subproblems. This is done within a restricted class of MDPs, namely those where non-zero reward is received only upon reaching a goal state. We introduce the definition of a subproblem within this class and provide motivation for how reuse of subproblem solutions can speed up learning. The contribution of this paper is the derivation of a tight bound on the loss in optimality from this reuse. We examine a bound that is based on Bellman error, which applies to all MDPs, but is not tight enough to be useful. We contribute our own theoretical result that gives an empirically tight bound on this suboptimality.|Michael H. Bowling,Manuela M. Veloso","15259|IJCAI|1995|Approximating Optimal Policies for Partially Observable Stochastic Domains|The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases.|Ronald Parr,Stuart J. Russell"],["58180|GECCO|2007|Alternative techniques to solve hard multi-objective optimization problems|In this paper, we propose the combination of different optimization techniques in order to solve \"hard\" two- and three-objective optimization problems at a relatively low computational cost. First, we use the -constraint method in order to obtain a few points over (or very near of) the true Pareto front, and then we use an approach based on rough sets to spread these solutions, so that the entire Pareto front can be covered. The constrained single-objective optimizer required by the -constraint method, is the cultured differential evolution, which is an efficient approach for approximating the global optimum of a problem with a low number of fitness function evaluations. The proposed approach is validated using several difficult multi-objective test problems, and our results are compared with respect to a multi-objective evolutionary algorithm representative of the state-of-the-art in the area the NSGA-II.|Ricardo Landa Becerra,Carlos A. Coello Coello,Alfredo García Hernández-Díaz,Rafael Caballero,Julián Molina Luque","57919|GECCO|2007|Parallel skeleton for multi-objective optimization|Many real-world problems are based on the optimization of more than one objective function. This work presents a tool for the resolution of multi-objective optimization problems based on the cooperation of a set of algorithms. The invested time in the resolution is decreased by means of a parallel implementation of an evolutionary team algorithm. This model keeps the advantages of heterogeneous island models but also allows to assign more computational resources to the algorithms with better expectations. The elitist scheme applied aims to improve the results obtained with single executions of independent evolutionary algorithms. The user solves the problem without the need of knowing the internal operation details of the used evolutionary algorithms. The computational results obtained on a cluster of PCs for some tests available in the literature are presented.|Coromoto León,Gara Miranda,Carlos Segura","57699|GECCO|2006|Rotated test problems for assessing the performance of multi-objective optimization algorithms|This paper presents four rotatable multi-objective test problems that are designed for testing EMO (Evolutionary Multi-objective Optimization) algorithms on their ability in dealing with parameter interactions. Such problems can be solved efficiently only through simultaneous improvements to each decision variable. Evaluation of EMO algorithms with respect to this class of problem has relevance to real-world problems, which are seldom separable. However, many EMO test problems do not have this characteristic. The proposed set of test problems in this paper is intended to address this important requirement. The design principles of these test problems and a description of each new test problem are presented. Experimental results on these problems using a Differential Evolution Multi-objective Optimization algorithm are presented and contrasted with the Non-dominated Sorting Genetic Algorithm II (NSGA-II).|Antony W. Iorio,Xiaodong Li","57112|GECCO|2003|HEMO A Sustainable Multi-objective Evolutionary Optimization Framework|The capability of multi-objective evolutionary algorithms (MOEAs) to handle premature convergence is critically important when applied to real-world problems. Their highly multi-modal and discrete search space often makes the required performance out of reach to current MOEAs. Examining the fundamental cause of premature convergence in evolutionary search has led to proposing of a generic framework, named Hierarchical Fair Competition (HFC), for robust and sustainable evolutionary search. Here an HFC-based Hierarchical Evolutionary Multi-objective Optimization framework (HEMO) is proposed, which is characterized by its simultaneous maintenance of individuals of all degrees of evolution in hierarchically organized repositories, by its continuous inflow of random individuals at the base repository, by its intrinsic hierarchical elitism and hyper-grid-based density estimation. Two experiments demonstrate its search robustness and its capability to provide sustainable evolutionary search for difficult multi-modal problems. HEMO makes it possible to do reliable multi-objective search without risk of premature convergence. The paradigmatic transition of HEMO to handle premature convergence is that instead of trying to escape local optima from converged high fitness populations, it tries to maintain the opportunity for new optima to emerge from the bottom up as enabled by its hierarchical organization of individuals of different fitnesses.|Jianjun Hu,Kisung Seo,Zhun Fan,Ronald C. Rosenberg,Erik D. Goodman","58358|GECCO|2008|AMGA an archive-based micro genetic algorithm for multi-objective optimization|In this paper, we propose a new evolutionary algorithm for multi-objective optimization. The proposed algorithm benefits from the existing literature and borrows several concepts from existing multi-objective optimization algorithms. The proposed algorithm employs a new kind of selection procedure which benefits from the search history of the algorithm and attempts to minimize the number of function evaluations required to achieve the desired convergence. The proposed algorithm works with a very small population size and maintains an archive of best and diverse solutions obtained so as to report a large number of non-dominated solutions at the end of the simulation. Improved formulation for some of the existing diversity preservation techniques is also proposed. Certain implementation aspects that facilitate better performance of the algorithm are discussed. Comprehensive benchmarking and comparison of the proposed algorithm with some of the state-of-the-art multi-objective evolutionary algorithms demonstrate the improved search capability of the proposed algorithm.|Santosh Tiwari,Patrick Koch,Georges Fadel,Kalyanmoy Deb","58195|GECCO|2007|ICSPEA evolutionary five-axis milling path optimisation|ICSPEA is a novel multi-objective evolutionary algorithm which integrates aspects from the powerful variation operators of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and the well proven multi-objective Strength Pareto Evaluation Scheme of the SPEA . The CMA-ES has already shown excellent performance on various kinds of complex single-objective problems. The evaluation scheme of the SPEA  selects individuals with respect to their current position in the objective space using a scalar index in order to form proper Pareto front approximations. These indices can be used by the CMA-part of ICSPEA for learning and guiding the search towards better Pareto front approximations. The ICSPEA is applied to complex benchmark functions such as an extended n-dimensional Schaffer's function or Quagliarella's problem. The results show that the CMA operator allows ICSPEA to find the Pareto set of the generalised Schaffer's function faster than SPEA . Furthermore, this concept is tested on the complex real-world application of the multi-objective optimization of five-axis milling NC-paths. An application of ICSPEA to the milling-path optimisation problem yielded efficient sets of five-axis NC-paths.|Jörn Mehnen,Rajkumar Roy,Petra Kersting,Tobias Wagner","58701|GECCO|2009|Hybrid evolutionary algorithms for large scale continuous problems|Evolutionary Algorithms (EAs) are powerful metaheuristics that can be applied to almost any optimization problem. However, different Evolutionary Algorithms own different search capabilities that make them more suitable for one or another optimization problem. Furthermore, the combination of several EAs can boost the performance of individual approaches. In this paper we try to exploit the benefits of the combination of several evolutionary approaches by means of a Hybrid Evolutionary Algorithm, where the participation of each individual algorithm on the overall process is dynamically adjusted through its execution. Two different strategies to perform this adjustment are proposed one with a constant global population size and another one with variable global population size. Experimental results demonstrate that Hybrid Evolutionary Algorithms outperform the individual ones and that the dynamic strategy with variable population size obtains better results on most of the proposed functions.|Antonio LaTorre,José María Peña,Santiago Muelas,Manuel Zaforas","57225|GECCO|2003|Is a Self-Adaptive Pareto Approach Beneficial for Controlling Embodied Virtual Robots|A self-adaptive Pareto Evolutionary Multi-objective Optimization (EMO) algorithm is proposed for evolving controllers for a virtually embodied robot. The main contribution of the self-adaptive Pareto approach is its ability to produce controllers with different locomotion capabilities in a single run, therefore reducing the evolutionary computational cost significantly. The aim of this paper is to verify this hypothesis.|Jason Teo,Hussein A. Abbass","58947|GECCO|2010|A multi-objective meta-model assisted memetic algorithm with non gradient-based local search|In this paper, we present an approach in which a local search mechanism is coupled to a multi-objective evolutionary algorithm. The local search mechanism is assisted by a meta-model based on support vector machines. Such a mechanism consists of two phases the first one involves the use of an aggregating function which is defined by different weighted vectors. For the (scalar) optimization task involved, we adopt a non-gradient mathematical programming technique the Hooke-Jeeves method. The second phase computes new solutions departing from those obtained in the first phase. The local search engine generates a set of solutions which are used in the evolutionary process of our algorithm. The preliminary results indicate that our proposed approach is quite promising.|Saúl Zapotecas Martínez,Carlos A. Coello Coello","57685|GECCO|2006|A new proposal for multi-objective optimization using differential evolution and rough sets theory|This paper presents a new multi-objective evolutionary algorithm (MOEA) based on differential evolution and rough sets theory. The proposed approach adopts an external archive in order to retain the nondominated solutions found during the evolutionary process. Additionally, the approach also incorporates the concept of pa-dominance to get a good distribution of the solutions retained. The main idea of the approach is to use differential evolution (DE) as our main search engine, trying to translate its good convergence properties exhibited in single-objective optimization to the multi-objective case. Rough sets theory is adopted in a second stage of the search in order to improve the spread of the nondominated solutions that have been found so far. Our hybrid approach is validated using standard test functions and metrics commonly adopted in the specialized literature. Our results are compared with respect to the NSGA-II, which is a MOEA representative of the state-of-the-art in the area.|Alfredo García Hernández-Díaz,Luis V. Santana-Quintero,Carlos A. Coello Coello,Rafael Caballero,Julián Molina Luque"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80274|VLDB|2003|Temporal Slicing in the Evaluation of XML Queries|As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, XQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a XQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.|Dengfeng Gao,Richard T. Snodgrass","80151|VLDB|2000|Efficiently Publishing Relational Data as XML Documents|XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquoouter union plan&rdquo to generate the content of an XML document.|Jayavel Shanmugasundaram,Eugene J. Shekita,Rimon Barr,Michael J. Carey,Bruce G. Lindsay,Hamid Pirahesh,Berthold Reinwald","80281|VLDB|2003|XISSR XML Indexing and Storage System using RDBMS|We demonstrate the XISSR system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.|Philip J. Harding,Quanzhong Li,Bongki Moon","80236|VLDB|2002|XMark A Benchmark for XML Data Management|While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.|Albrecht Schmidt 0002,Florian Waas,Martin L. Kersten,Michael J. Carey,Ioana Manolescu,Ralph Busse","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80772|VLDB|2007|Unifying Data and Domain Knowledge Using Virtual Views|The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB  PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.|Lipyeow Lim,Haixun Wang,Min Wang","80315|VLDB|2003|XML Schemas in Oracle XML DB|The WC XML Scheme language is becomimg increasingly popular for expressing the data model for XML documents. It is a powerful language that incorporates both strutural and datatype modeling features. There are many benefits to storing XML Schema compliant data in a database system, including better queryability, optimied updates and stronger validation. However, the fidelity of the XML document cannot be sacrificed. Thus, the fundamental problem facing database implementers is how can XML Schemes be mapped to relational (and object-relational) database without losing schema semantics or data-fidelity In this paper, we present the Oracle XML DB solution for a flexible mapping of XML Schemas to object-relational database. It preserves document fidelity, including ordering, namespaces, comments, processing instructions etc., and handles all the XML Schema semantics including cyclic definitions, dervations (extension and restriction), and wildcards. We also discuss various query and update optimiations that involve rewriting XPath operations to directly operate on the underlying relational data.|Ravi Murthy,Sandeepan Banerjee","80435|VLDB|2004|Indexing XML Data Stored in a Relational Database|As XML usage grows for both data-centric and document-centric applications, introducing native support for XML data in relational databases brings significant benefits. It provides a more mature platform for the XML data model and serves as the basis for interoperability between relational and XML data. Whereas query processing on XML data shredded into one or more relational tables is well understood, it provides limited support for the XML data model. XML data can be persisted as a byte sequence (BLOB) in columns of tables to support the XML model more faithfully. This introduces new challenges for query processing such as the ability to index the XML blob for good query performance. This paper reports novel techniques for indexing XML data in the upcoming version of Microsoft SQL ServerTM, and how it ties into the relational framework for query processing.|Shankar Pal,Istvan Cseri,Gideon Schaller,Oliver Seeliger,Leo Giakoumakis,Vasili Vasili Zolotov","80419|VLDB|2004|Query Rewrite for XML in Oracle XML DB|Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQLXML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQLXML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery  over XML.|Muralidhar Krishnaprasad,Zhen Hua Liu,Anand Manikutty,James W. Warner,Vikas Arora,Susan Kotsovolos"],["79799|VLDB|1975|An Experiment in Dedicated Data Management|There is growing concern among users of data base management systems regarding the cost and performance of such systems. As is typical of highly complex, generalized software, data management systems can be expensive to run, particularly in terms of processing time. A possible solution to this problem is to offload the data management functions into an inexpensive, dedicated minicomputer, called a dedicated data management computer, or DDM. This \"backend\" approach to the problem has been investigated by Bell Telephone Laboratories , using a large DBTG-based system. Their results indicated that such an approach is feasible, though no performance figures have been published. In an effort to obtain more detailed data on the performance implications of a DDM system, two experiments were performed by the Sperry Corporate Research Center. The first experiment was designed to obtain estimates of the percentage of CP time which could be offloaded from the host to a DDM. The second experiment, a prototype implementation of a DDM configuration using a standard minicomputer, was designed to demonstrate the credibility of such a system.|H. C. Heacox,E. S. Cosloy,J. B. Cohen","79839|VLDB|1976|Structure and Redundancy in the Conceptual Schema in the Administration of Very Large Data Bases|This presentation explains the role of the conceptual schema in securing management control over very large data bases and in providing data independence for users of the data. The ccnceptual schema can also be used to describe the information model of an enterprise. The conceptual schema is presented as a tool of data base administration. This presentation addresses structure and redundancy in the conceptual schema. Two philosophies exist concerninq permissible mappings between an external schema and the conceptual schema. One, to provide maximum derivability of information from the data, assumes an unstructured, non-redundant conceptual schema, third normal conceptual record sets, and permits arbitrarily structured external records to be defined in terms of combinations of conceptual records. The other, to provide dynamic data independence during attribute migration and control over the information that can be derived from the data, assumes a highly structured, highly redundant conceptual schema, third normal,...,unnormalized conceptual record sets, and requires external records to be subsets of predefined conceptual records. A complete data base management system should provide both capabilities, and enforce constraints when each may be used.|Herbert S. Meltzer","79809|VLDB|1975|Hierarchical Performance Analysis Models for Data Base Systems|This paper presents a comprehensive set of hierarchically organized synthetic models developed for the performance evaluation of data base management systems. The first set of algebraic models for data base management system itself contains a program behavior model, a retrieval model, a logical data base model, a physical data base model, and a data processing model. These models are intended to clarify logical and physical data base structures and essential operations in the data processing on them. Another set of models for performance analysis contains a macroscopic program behavior model, a storage model, a processor model, a user behavior model, and an interactive model. In this set this paper is particularly concerned with the first  models. The macroscopic program behavior model is based on the discussion of data locality and allows us to estimate the frequency of data page loading expected on a given application program and a data base. The storage model enables us to estimate the traverse time of data page loading. Finally the processor model allows us to evaluate the data retrieval processing time of data manipulation commands of a given data base structure, a data base management system and a set of application programs under the multiprogramming environment. These models are to be applied to the optimization of the application programs or the data base structure in order to obtain a higher data retrieval performance.|Isao Miyamoto","79894|VLDB|1977|Multi-Level Structures of the DBTG Data Model for an Achievement of Physical Data Independence|Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.|Tetsuo Toh,Seiichi Kawazu,Kenji Suzuki","80761|VLDB|2007|Computer Science  A New World of Data Management|Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $ billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 's data under DBMS management reached % of the world's data. The six-fold growth of non-relational data in the period -- will reduce that number to well below %. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science . will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science .. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of  billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think Semantic web services will constitute a programming model of Computer Science .. How does data management fit into this semantically rich environment Computer Science . offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.|Michael L. Brodie","79893|VLDB|1977|Psychological Issues in Data Base Management|In order to maximize total data base system effectiveness, it is becoming increasingly important to attend to psychological issues. This paper identifies issues in data base design and management where psychological considerations are likely to play a significant role. Specific studies that address some of these issues are reviewed. Next, a number of areas of basic psychological research that are likely to be of use to data base system designers are indicated. Finally, general recommendations and considerations for data base interfaces are given.|John C. Thomas","79796|VLDB|1975|Keyword Access to a Mass Storage Device at the Record Level|A general software package was built to access individual data records stored on an IBM- photo-digital mass storage device with a current on-line capacity of  billion characters (bytes) and an infinitely extensible off-line capacity. An existing data base management system was used to maintain the pointers to the data on the mass storage device and to store the controls for the data driven interactive code. Existing data dictionaries used for sequentially processing the data bases mere stored in the DBMS and used to display individual data items within the retrieved records. Data retrieved from the mass storage device is displayed interactively or sent for further processing to various report generation and statistics packages. The system provides dial-up terminal retrieval capability for exceedingly large socio-economic and demographic data bases used by national and regional planning agencies of the federal government. It was developed with three man-months of effort.|Fredric C. Gey,Marilyn M. Mantei","79950|VLDB|1979|Data Base Management Systems Security and INGRES|The problem of providing a secure implementation of a data base management system is examined, and a kernel based architectural approach is developed. The design is then successfully applied to the existing data management system Ingres. It is concluded that very highly secure data base management systems are feasible.|Deborah Downs,Gerald J. Popek","79783|VLDB|1975|Using Large Data Bases for Interactive Problem Solving|Interactive problem solving involves user-machine dialogues to solve unstructured problems, such as selecting marketing strategies and planning mass transit routes. A characteristic of most interactive problem solving systems is that they operate on data bases that are special purpose subsets derived from a large data base. The problem solving system must include code for interfacing with this data base, or the data must be converted to the formats required by the problem solving system. Effective interactive problem solving requires a data management system which provides flexibility in accessing a large data base and fast performance for the problem solving system. This paper describes a method, called data extraction, for interfacing large data bases with interactive problem solving systems. Figure  shows the basic components in the interface. A large data base management system is used to maintain and protect a set of data files. Data extraction is used to aggregate and subset from this set to provide information for problem solving. In addition to an IO interface, data extraction provides interactive data description and presentation functions. For more details on the functional requirements of the data extraction components see .|Eric D. Carlson","13966|IJCAI|1983|Intelligent Assistants for Knowledge and Information Resources Management|Work in Artificial Intelligence on knowledge based technology has produced a variety of practical applications while data management systems are used in almost every commercial enterprise of any significant size. Despite the advances in both data base and knowledge base technologies there is still a large gap between the capabilities of these tools and the \"paperwork and information explosion\" that faces modern society. Concern with this information access problem in the United States has lead to the creation of the new field of Information Resources Management and to Federal laws and executive orders aimed at reducing paperwork and promoting more effective management of information. In this paper we outline the nature of this information management problem and describe certain AI and database technology that may help to close this information access gap. We introduce the notion of \"Intelligent Assistants\" for Information Resources Management(IRM) and describe how work on current approaches to \"Knowledge Management\" may lead to the technology necessary for the eventual creation of such intelligent assistants.|Charles Kellogg"],["13585|IJCAI|1977|Recognition and Depth Perception of Objects in Real World Scenes|Automatons and other robotic applications which are designed to move around and interact with their physical environment need a computer vision system for recognizing and understanding the spatial relationships of objects in real world scenes. The perceptual system must be able to identify salient objects in a scene, develop an understanding of their spatial relationships, and maintain continuity from one view to the next as either the objects or the system's camera moves through the scene. Outlined here and described in more detail in Douglass, , is a system which has been implemented in SIMULA and tested on hand coded outdoor scenes of simple subjects such as houses and automobiles. It uses a recognition cone, a segmentation algorithm for dividing a scene into similar regions and a routine for constructing a three dimensional world model. Visual inference routines interpret perspective, shadows, highlights, occlusions, shading and texture gradients, and monocular motion parallax. Other visual knowledge is added with long term models and short term object representations. The final program will be tested on color photographs of outdoor scenes using as input a series of views of the same scene from different angles which approximates what an automaton would \"see\" as it moves down a street.|R. J. Douglass","16301|IJCAI|2005|SVM-based Obstacles Recognition for Road Vehicle Applications|This paper describes an obstacle Recognition System based on SVM and vision. The basic components of the detected objects are first located in the image and then combined with a SVM-based classifier. A distributed learning approach is proposed in order to better deal with objects variability, illumination conditions, partial occlusions and rotations. A large database containing thousands of object examples extracted from real road images has been created for learning purposes. We present and discuss the results achieved up to date.|Miguel ?ngel Sotelo,Jesús Nuevo,David Fernández,I. Parra,Luis Miguel Bergasa,Manuel Ocaña,Ramón Flores","13296|IJCAI|1969|MIKROKOSMS and Robots|This paper presents a computer program that simulates situations (called \"MIKROKOSMS\") in which severl entities (called \"organisms\") wander around in an environment that includes one another plus other, apparently simpler, entities (called \"objects\"). The program has a rather simple set of \"laws\" of objects, which can be thought of as the laws of physics of the MIKROKOSM. It also has other specifications for organisms - their input and output functions (called \"perception\" and \"response\"), their reward functions (called \"motivation\" or \"needs\"), and their mechanisms for building up internal memories (called \"learning\" and \"hypothesis formation\") that will'help them to recognize objects in the future, and respond appropriately to them (for example, in order to maximize expected rewards). This program lays bare the processes that are needed to handle interactions among simulated organisms and objects, including the learning of hypotheses that will guide future action. The present program has only the simplest of pattern recognition, hypothesis formation and need-satisfaction capabilities. Its purpose is to make clear and concrete how such things can be interrelated in a complete system.|Leonard Uhr,Manfred Kochen","13268|IJCAI|1969|Machine Perception and Description of Pictorial Data|This investigation of machine processing of pictorial data is based on the premise that people can recognize visual objects and describe them well enough so that other individuals can recognize the object from the description. Given a system of linguistic communication between a person and a digital computer, and given that the computer possesses adequate perceptual machinery, many currently refractory problems in pictorial data processing would be open to solution. This paper describes a computer system which can perceive a limited class of graphical objects, create linguistic descriptions for the objects, and classify objects by comparison with a reference set of descriptions as might be produced in normal human communication.|Martin A. Fischler","13346|IJCAI|1971|One System for Simulation of Pattern Recognition Algorithms|A general purpose system for a computer simulation of pattern recognition algorithms is proposed that consists of a universal scanner for optical images input to the computer, a display for visual output, and software. A characteristic feature of the system is a blok structure of the hard-ware and software. The system allows carrying out most of the data processing necessary for simulating the algorithms of recognizing the optical images of objects of various physical natures.|V. I. Rybak,Georgy L. Gimel'farb,E. F. Kushner","14417|IJCAI|1987|Color Algorithms for a General Vision System|We show that an intelligent approach to color can be used to significantly improve the capabilities of a vision system. In previous work, we adopted general physical models which describe how objects interact with light. These models are far more general than those typically used in computer vision. In this report, we use our physical models to derive powerful algorithms for extracting invariant properties of objects from images. The first algorithm is used for the generic classification of objects according to material. The second algorithm provides a solution to the color constancy problem. These algorithms have been implemented and produce correct results on real images. Some examples of experimental results are presented.|Glenn Healey,Thomas O. Binford","14207|IJCAI|1985|Visual Recognition from Spatial Correspondence and Perceptual Organization|Depth reconstruction from the two-dimensional image plays an important role in certain visual tasks and has been a major focus of computer vision research. However, in this paper we argue that most instances of recognition in human and machine vision can best be performed without the preliminary reconstruction of depth. Three other mechanisms are described that can be used to bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization can be used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Secondly, evidential reasoning can be used to combine evidence from these groupings and other sources of information to reduce the size of the search-space during model-based matching. Finally, a process of spatial correspondence can be used to bring the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. These methods have been combined in an experimental computer vision system named SCERPO. This system has demonstrated the use of these methods for the recognition of objects from unknown viewpoints in single gray-scale images.|David G. Lowe","16576|IJCAI|2007|Peripheral-Foveal Vision for Real-time Object Recognition and Tracking in Video|Human object recognition in a physical -d environment is still far superior to that of any robotic vision system. We believe that one reason (out of many) for this--one that has not heretofore been significantly exploited in the artificial vision literature--is that humans use a fovea to fixate on, or near an object, thus obtaining a very high resolution image of the object and rendering it easy to recognize. In this paper, we present a novel method for identifying and tracking objects in multiresolution digital video of partially cluttered environments. Our method is motivated by biological vision systems and uses a learned \"attentive\" interest map on a low resolution data stream to direct a high resolution \"fovea.\" Objects that are recognized in the fovea can then be tracked using peripheral vision. Because object recognition is run only on a small foveal image, our system achieves performance in real-time object recognition and tracking that is well beyond simpler systems.|Stephen Gould,Joakim Arfvidsson,Adrian Kaehler,Benjamin Sapp,Marius Messner,Gary R. Bradski,Paul Baumstarck,Sukwon Chung,Andrew Y. Ng","58559|GECCO|2009|Tracking multiple objects in non-stationary video|One of the key problems in computer vision and pattern recognition is tracking. Multiple objects, occlusion, and tracking moving objects using a moving camera are some of the challenges that one may face in developing an effective approach for tracking. While there are numerous algorithms and approaches to the tracking problem with their own shortcomings, a less-studied approach considers swarm intelligence. Swarm intelligence algorithms are often suited for optimization problems, but require advancements for tracking objects in video. This paper presents an improved algorithm based on Bacterial Foraging Optimization in order to track multiple objects in real-time video exposed to full and partial occlusion, using video from both fixed and moving cameras. A comparison with various algorithms is provided.|Hoang Nguyen,Bir Bhanu","13562|IJCAI|1977|Image Segmentation Technique for Locating Automotive Parts on Belt Conveyors|A simple, model free computer vision program to determine the locations of non-overlapping parts on belt conveyors is described. This program illustrates a simple and effective procedure for segmenting objects from background in instances where simple thresholding of a gray-level image does not suffice. The procedure consists of a unique sequence of standard image enhancement processes. The resultant image exhibits silhouettes of the objects, which contain sufficient information for locating those parts whose orientation can be determined without observation of internal features. The technique has been implemented on a large research computer, as well as a mini-computer coupled to a prototype belt conveyor-robot arm part transfer system. The technique has been validated for a large variety of parts and belt surfaces. It can meet production rates and has the potential for actual production use.|M. L. Baird"],["13685|IJCAI|1977|Speech Understanding and AIAI and Speech Understanding|This panel will review research in speech understanding (SU) and in artificial intelligence (AI) from two perspectives the contributions that AI has made to SU -- the resources in AI that have been used in the development of SU systems. the contributions that SU has made to AI - the results of the SU program that have affected or arc likely to affect futuie AI research. Four topics are identified for major consideration Multiple sources of Knowledge which are required how should they be oiganized, System control how to manage the complex inter actions involved. language understanding comparisons of text and speech input. Organization of research -creating complex, multisource, knowledge-based systems.|Donald E. Walker,Lee D. Erman,Allen Newell,Nils J. Nilsson,William H. Paxton,Terry Winograd,William A. Woods","14815|IJCAI|1991|Integration of Neural Networks and Expert Systems for Process Fault Diagnosis|The main thrust of this research is the development of an artificial intelligence (AI) system to be used as an operators' aid in the diagnosis of faults in large-scale chemical process plants. The operator advisory system involves the integration of two fundamentally different AI techniques expert systems and neural networks. A diagnostic strategy based on the hierarchical use of neural networks is used as a first level filter to diagnose faults commonly encountered in chemical process plants. Once the faults are localized within the process by the neural networks, the deep knowledge expert system analyzes the results, and either confirms the diagnosis or offers alternative solutions. The model-based expert system contains information of the plant's structure and function within its object-oriented knowledge base. The diagnostic strategy can handle novel or previously unencountered faults, noisy process sensor measurements, and multiple faults. The operator advisory system is demonstrated using a multi-column distillation plant as a case study.|Warren R. Becraft,Peter L. Lee,Robert B. Newell","13689|IJCAI|1977|Planning in the World of the Air Traffic Controller|An enroute air traffic control (ATC) simulation has provided the basis for research into the marriage of discrete simulation and artificial intelligence techniques. A program which simulates, using real world data, the movement of aircraft in an ATC environment forms a robot's world model. Using a production system to respond to events in the simulated world, the robot is able to look ahead and form a plan of instructions which guarantees safe, expedient aircraft transit. A distinction is made between the real world, where pilots can make mistakes, change their minds, etc., and an idealized plan-ahead world which the robot uses the over-all simulation alternates between updating the real world and planning in the idealized one to investigate the robot's ability to plan in the face of uncertainty.|Robert B. Wesson","14635|IJCAI|1989|Neuroplanners and Their Application to EyesHeadNeck Coordination|We review a neuroplanner architecture for use in constructing subcognitive controllers and new application that uses it. These controllers have wo important properties () the ability to learn the topology of three continuous spaces a steering space, a control space, and an observation space, and () the ability to integrate the three spaces so that initial and goal steering conditions can suggest a sequence of control states that lead the controlled system to the goal in the presence of obstacles. The result is a rudimentary planner or guidance system that can be used for such subcognitive tasks as robot manipulator control, headeye coordination, and task sequencing. In this paper, we consider the second domain. The term neuroplanner is intended to convey the impression that the planner is implemented neurally and is more rudimentary than the conventional symbolic planners typical of artificial intelligence research.|Daryl H. Graf,Wilf R. LaLonde","14633|IJCAI|1989|Decision-Making in an Embedded Reasoning System|The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.|Michael P. Georgeff,François Felix Ingrand","13731|IJCAI|1981|MARK I Robot|This film shows an autonomus robot in action. the goal of the NRL research effert is to establish a technical base for an autonomous under-water robot. It is expected that such a robot would be able to do only simple taske, but would be able to do only simple tasks, but would be able to do them one or two orders of magnitude cheaper than other methods. (The economy is mainly because it is not necessary for ship to standby on the surface). The robot vehicle would be progrenuned at a base and proceed to its assigned dentination on the floor of the ocean. It would then carry out its work autonomously, without even a communications link. This project is concerned with software nather than hardware. The Mark I Robot is a system designed to operute in the laborutory as a research. It is a simplified robot a first out at providing the minimal intelligence necessary for an autonomous underwater robot. The robot performed as exjected, sucessfully carrying out the task it was assigned.|John K. Dixon,Susan A. Bouchard,William G. Kennedy,James R. Slagle","58382|GECCO|2008|Context-dependent predictions and cognitive arm control with XCSF|While John Holland has always envisioned learning classifier systems (LCSs) as cognitive systems, most work on LCSs has focused on classification, datamining, and function approximation. In this paper, we show that the XCSF classifier system can be very suitably modified to control a robot system with redundant degrees of freedom, such as a robot arm. Inspired by recent research insights that suggest that sensorimotor codes are nearly ubiquitous in the brain and an essential ingredient for cognition in general, the XCSF system is modified to learn classifiers that encode piecewise linear sensorimotor structures, which are conditioned on prediction-relevant contextual input. In the investigated robot arm problem, we show that XCSF partitions the (contextual) posture space of the arm in such a way that accurate hand movements can be predicted given particular motor commands. Furthermore, we show that the inversion of the sensorimotor predictive structures enables accurate goal-directed closed-loop control of arm reaching movements. Besides the robot arm application, we also investigate performance of the modified XCSF system on a set of artificial functions. All results point out that XCSF is a useful tool to evolve problem space partitions that are maximally effective for the encoding of sensorimotor dependencies. A final discussion elaborates on the relation of the taken approach to actual brain structures and cognitive psychology theories of learning and behavior.|Martin V. Butz,Oliver Herbort","13280|IJCAI|1969|A Mobile Automaton An Application of Artificial Intelligence Techniques|A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS- computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.|Nils J. Nilsson","13398|IJCAI|1973|Planning Considerations for a Roving Robot with Arm|The Jet Propulsion Laboratory is engaged in a robot research program. The program is aimed at the development and demonstration of technology required to integrate a variety of robntic functions (locomotion, manipulation, sensing and perception, decision making, and man-robot interaction) into a working robot unit operating in a real world environment and dealing with both man-made and natural objects. This paper briefly describes the hardware and software system architecture of the robot breadboard and summarizes the developments to date. The content of the paper is focused on the unique planning considerations involved in incorporating a manipulator as part of an autonomous robot system. In particular, the effects of system architecture, arm trajectory calculations, and arm dynamics and control are discussed in the context of planning arm motion in complex and changing sensory and workspace environments.|Richard A. Lewis,Antal K. Bejczy","14702|IJCAI|1989|Robot Navigation|Robot navigation is a problem that encompasses most of the major areas of AI research. Machine architecture, search, knowledge acquisition and representation, planning, scheduling, reaction, perception, and of course robotics, all can play integral roles in a mobile robot navigation system. For this reason, a large part of the AI community is now interested (and has been since STRIPS was used to guide the Shakey robot) in mobile robot navigation.|David P. Miller"],["14211|IJCAI|1985|Two Results on Default Logic|We focus on default logic, a formalism introduced by Reiter to model default reasoning. The paper consists of two parts. In the first one a translation method of non-normal defaults into the normal ones is given. Although not generally valid, this translation seems to work for a wide class of defaults. In the second part a semantics for normal default theories is given and the completeness theorem is proved.|Witold Lukaszewicz","16416|IJCAI|2007|A Logic Program Characterization of Causal Theories|Nonmonotonic causal logic, invented by McCain and Turner, is a formalism well suited for representing knowledge about actions, and the definite fragment of that formalism has been implemented in the reasoning and planning system called CCalc. A  theorem due to McCain shows howto translate definite causal theories into logic programming under the answer set semantics, and thus opens the possibility of using answer set programming for the implementation of such theories. In this paper we propose a generalization of McCain's theorem that extends it in two directions. First, it is applicable to arbitrary causal theories, not only definite. Second, it covers causal theories of a more general kind, which can describe non-Boolean fluents.|Paolo Ferraris","14960|IJCAI|1991|Modal Interpretations of Default Logic|In the paper we study a new and natural modal interpretation of defaults. We show that under this interpretation there are whole families of modal nonmonotonic logics that accurately represent default reasoning. One of these logics is used in a definition of possible-worlds semantics for default logic. This semantics yields a characterization of default extensions similar to the characterization of stable expansions by means of autoepistemic interpretation. We also show that the disjunctive information can easily be handled if disjunction is represented by means of modal disjunctive defaults -- modal formulas that we use in our interpretation. Our results indicate that there is no single modal logic for describing default reasoning. On the contrary, there exist whole ranges of modal logics, each of which can be used in the embedding as a \"host\" logic.|Miroslaw Truszczynski","15122|IJCAI|1995|On Bimodal Nonmonotonic Logics and Their Unimodal and Nonmodal Equivalents|We study a bimodal nonmonotonic logic MBNF suggested in Lifschitz,  as a generalization of a number of nonmonotonic formalisms We show first that it is equivalent to a certain nonmodal system involving rules of a special kind Next, it is shown that the latter admits a modal representation that uses only one modal opera tor the operator of belief Moreover, under this translation the models of MBNF correspond to expansions of the associated modal nonmonotonic logic Finally we show that, as far as such models are concerned, MBNF is redunhle to nonmodal default consequence relations from Bochman,  These results have general consequences concerning relationship between different formalizations of nonmonotonic reasoning.|Alexander Bochman","15332|IJCAI|1997|Reasoning about Action in Polynomial Time|Although many formalisms for reasoning about action exist, surprisingly few approaches have taken computational complexity into consideration. The contributions of this paper are the following a temporal logic with a restriction for which deciding satisfiability is tractable, a tractable extension for reasoning about action, and NP-completeness results for the unrestricted problems. Many interesting reasoning problems can be modelled, involving nondeterminism, concurrency and memory of actions. The reasoning process is proved to be sound and complete.|Thomas Drakengren,Marcus Bjäreland","15425|IJCAI|1999|Preferred Arguments are Harder to Compute than Stable Extension|Based on an abstract framework for nonmonotonic reasoning, Bondarenko et at. have extended the logic programming semantics of admissible and preferred arguments to other nonmonotonic formalisms such as circumscription, autoepisternic logic and default logic. Although the new semantics have been tacitly assumed to mitigate the computational problems of nonmonotonic reasoning under the standard semantics of stable extensions, it seems questionable whether they improve the worst-case behaviour. As a matter of fact, we show that credulous reasoning under the new semantics in propositional logic programming and prepositional default logic has the same computational complexity as under the standard semantics. Furthermore, sceptical reasoning under the admissibility semantics is easier - since it is trivialised to monotonic reasoning. Finally, sceptical reasoning under the preferability semantics is harder than under the standard semantics.|Yannis Dimopoulos,Bernhard Nebel,Francesca Toni","15474|IJCAI|1999|Stable Model Checking Made Easy|Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (V-free) logic programs liecause stable model checking - deciding whether a given model is a stable model of a propositional DLP program is co-NP-complete, while it is polynomial for normal logic programs. This paper proposes a new transformation M(P) which reduces stable model checking to UNSAT - i.e., to deciding whether a given CNF formula is unsatisfiable. Thus, the stability of a model AI for a program P can be verified by calling a Satisfiability Checker on the CNF formula M(P). The transformation is parsimonious and efficiently computable, as it runs in logarithmic space. Moreover, the size of the generated CNF formula never exceeds the size of the input. The proposed approach to stable model checking lias been implemented in a DLP system, and a number of experiments and benchmarks have been run.|Christoph Koch,Nicola Leone","16752|IJCAI|2007|A Description Logic of Change|We combine the modal logic S with the description logic (DL) ALCQI. The resulting multi-dimensional DL SALCQI supports reasoning about change by allowing to express that concepts and roles change over time. It cannot, however, discriminate between changes in the past and in the future. Our main technical result is that satisfiability of SALCQI concepts with respect to general TBoxes (including GCIs) is decidable and -EXPTIME-complete. In contrast, reasoning in temporal DLs that are able to discriminate between past and future is inherently undecidable. We argue that our logic is sufficient for reasoning about temporal conceptual models with time-stamping constraints.|Alessandro Artale,Carsten Lutz,David Toman","14413|IJCAI|1987|Non-Standard Semantics for the Method of Temporal Arguments|An expressive first-order temporal logic using the method of temporal arguments is developed and provided with a non-standard semantics that explicates its underlying temporal structure. Objections from the AI literature against the adequacy of temporal argument theories are answered in the course of discussing representational issues for the developed logic. This logic \"accords a special status to time,\" distinguishes the temporal features of event occurrences from ordinary facts, and supports changing ontologies. We conclude that the method of temporal arguments remains a viable candidate for temporal reasoning in AI.|Brian A. Haugh","14371|IJCAI|1987|A Logic for Representing Default and Prototypical Properties|The area of default reasoning may be considered as consisting of two separate but closely related subareas. The first deals with representing default information, and addresses issues such as reasoning about default statements or determining the consistency of a set of defaults. The second addresses reasoning about the default properties of an individual, given a set of default statements. Most extant work in default reasoning has concentrated on the second area. This paper addresses the first area only, as an initial investigation into the area of default reasoning. The approach is based on adding a \"variable conditional\" operator to first-order logic. This operator is used to express prototypical, rather than strict, relations between entities and properties. A possible worlds semantics is provided for the logic along with a proof theory soundness and completeness results are also provided. It is argued that the variable conditional in the logic captures common intuitions concerning defaults and prototypical properties.|James P. Delgrande"],["13752|IJCAI|1981|The Conceptual Calculus For Automatic Program Understanding|The CAN system for automatic understanding of LISP programs is presented. When applied on a LISP program, CAN needs no assertion about what the program is intended to do. CAN's general task is to associate e meaning to a program, from which it is able to diagnose semantic properties (unused portions of code, undefinitions, underused programs, classes Of data for which evaluation of an expression never terminates), and to exhibit deep semantic properties. CAN's underlying theory is the conceptual calculus, based on the clausal form of predicate logic. It involves two main ideas  - an Induction principle specially adapted to program understanding (as opposed to program verification). - An extension of unificetion to equation solving.|Daniel Goossens","13369|IJCAI|1973|A Man-Machine Theorem-Proving System|This paper describes a man-machine theorem proving system at the University of Texas (Austin) which has been used to prove a few theorems in general topology. The theorem (or subgoal) being proved is presented on the scope in its natural form so that the user can easily comprehend it and, by a series of interactive commands, can help with the proof when he desires. A feature called DETAIL is employed which allows the human to interact only when needed and only to the extent necessary for the proof. The program is built around a modified form of IMPLY, a natural-deduction-like theorem proving technique which has been described earlier. A few examples of proofs are given.|W. W. Bledsoe,Peter Bruell","14368|IJCAI|1987|Logic Program Derivation for a Class of First Order Logic Relations|Logic programming has been an attempt to bridge the gap betwen specification and programming language and thus to simplify the software development process. Even though the only difference between a specification and a program in a logic programming framework is that of efficiency, there is still some conceptual distance to be covered between a naive, intuitively correct specification and an efficiently executable version of it And even though some mechanical tools have been developed to assist in covering this distance, no fully automatic system for this purpose is yet known. In this paper vt present a general class of first-order logic relations, which is a subset of the extended Horn clause subset of logic, for which we give mechanical means for deriving Horn logic programs, which are guaranteed to be correct and complete with respect to the initial specifications.|George Dayantis","13436|IJCAI|1973|Understanding Without Proofs|The paper describes the analysis part of a running analysis and generation program for natural language. The system is entirely oriented to matching meaningful patterns onto fragmented paragraph length input. Its core Is a choice system based on what I call \"semantic density\". The system is contrasted with () syntax oriented linguistic approaches and () theorem proving approaches to the understanding problem. It is argued by means of examples that the present system is not only more workable, but more intuitively acceptable, at least as an understander for the purpose of translation, than deduction-based systems.|Yorick Wilks","13570|IJCAI|1977|A Lemma Driven Automatic Theorem Prover for Recursive Function Theory|We describe work in progress on an automatic theorem prover for recursive function theory that we intend to apply in the analysis (including verification and transformation) of useful computer programs. The mathematical theory of our theorem prover is extendible by the user and serves as a logical basis of program specification (analogous to, say, the predicate calculus). The theorem prover permits no interaction once given a goal, but many aspects of its behavior are influenced by previously proved results. Thus, its performance on difficult theorems can be radically improved by having it first prove relevant lemmas. We describe several ways that the theorem prover employs such lemmas. Among the interesting theorems proved are the correctness of a simple optimizing compiler for expressions and the correctnessof a \"big number\" addition algorithm.|Robert S. Boyer,J. Strother Moore","13297|IJCAI|1969|PROW A Step Toward Automatic Program Writing|This paper Describes a program, called \"PROW\", which writes programs PROW accepts the specification of the program in the language of predicate calculus, decides the algorithm for the program and then produces a LISP program which is an implementation of the algorithm. Since the construction of the algorithm is obtained by formal theorem-proving techniques, the programs that PROW writes are free from logical errors and do not have to be debugged The user of PROW can make PROW write programs in languages other than LISP by modifying the part of PROW that translates an algorithm to a LISP program. Thus PROW can be modified to write programs in any language In the end of this paper, it is shown that PROW can also be used as a question-answering program|Richard J. Waldinger,Richard C. T. Lee","13326|IJCAI|1971|A Paradigm for Reasoning by Analogy|A paradigm enabling heuristic problem solving programs to exploit an analogy between a current unsolved problem and a similar but previously solved problem to simplify its search for a solution is outlined. It is developed in detail for a first-order resolution logic theorem prover. Descriptions of the paradigm, implemented LISP programs, and preliminary experimental results are presented. This is believed to be the first system that develops analogical information and exploits it so that a problem-solving program can speed its search.|Rob Kling","14293|IJCAI|1985|Taxonomic Reasoning|In formalizing knowledge for common sense reasoning, one often needs to partition some domain. An instance of this from the Blocks World is the statement \"All blocks are either held, on the table, or on another block.\" Although we can write this axiom in predicate calculus or in clause form for input to a theorem prover, such representations are highly space inefficient. In this paper we present a generalized clause form that allows for the compact representation of arbitrary partitions, along with a set of corresponding inference rules. Additionally, a theorem prover implementing these rules is described that demonstrates their utility with certain kinds of common sense rule bases.|Josh D. Tenenberg","13644|IJCAI|1977|Program Inference from Traces using Multiple Knowledge Sources|This paper presents an overview of a framework for the synthesis of high-level program descriptions from traces and example pairs in an automatic programming system. The framework is described in terms of a methodology and a rule base for generating control and data structure specifications for the program to be synthesized, in a format suitable for transformation into program code in a given target language.|J. V. Phillips","15090|IJCAI|1993|A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases|The problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed. These are called historical time and belief time respectively. Historical time denotes the time for which information models realityBelief time denotes the time lor which a belief is held (by an agent or a knowledge base). We formalize an appropriate theory of time using logic as a meta-language. We then present a metalogic program derived from this theory through foldunfold transformations. The metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently. The metalogic program is directly implementable as a Prolog program and hence the need for a more complex theorem prover is obviated. The approach is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent.|Suryanarayana M. Sripada"]]},"title":{"entropy":5.736483726423881,"topics":["data base, for system, natural language, for data, the system, data, and data, and system, language for, for and, database system, data management, database, framework for, system, for database, data system, the data, the language, management system","genetic programming, using genetic, neural networks, genetic for, using, learning for, model for, genetic algorithm, for networks, algorithm for, using algorithm, using programming, evolutionary for, programming for, artificial intelligence, method for, networks, for using, differential evolution, time series","genetic algorithm, algorithm for, the problem, for problem, particle swarm, for the, the, for optimization, the algorithm, solving problem, the and, evolutionary algorithm, algorithm, algorithm with, search for, and algorithm, algorithm problem, constraint satisfaction, swarm optimization, particle optimization","and, reasoning about, for and, with and, for planning, the and, reasoning and, and its, for reasoning, reasoning, theorem proving, reasoning with, planning, and learning, and action, reinforcement learning, and application, planning and, planning with, knowledge representation","for system, the system, and system, database system, system, for database, data system, large data, management system, large base, expert system, architecture for, for large, and database, base system, for distributed, distributed system, the database, database, for and","efficient for, framework for, query for, for xml, query processing, for processing, for queries, query and, for and, query database, tool for, queries, query, efficient and, over data, xml, and processing, extracting from, data mining, xml data","learning for, neural networks, for networks, method for, learning, algorithm for, networks, for robot, using networks, approach for, neural for, sense disambiguation, the learning, algorithm networks, mobile robot, reinforcement learning, learning algorithm, and networks, word sense, bayesian networks","for using, strategies for, differential evolution, for selection, using algorithm, evolution for, classifier system, evolution strategies, evolution and, using and, building block, the evolution, system using, using evolution, feature for, selection and, feature selection, the using, for classification, using","for the, the and, the problem, the, the algorithm, ant colony, case study, the effects, the using, the use, analysis the, study the, the genetic, the search, the case, the evolution, the performance, the with, the programming, the dynamic","genetic algorithm, algorithm for, genetic for, the algorithm, algorithm with, and algorithm, evolutionary algorithm, the genetic, algorithm problem, genetic programming, genetic with, using algorithm, new for, and genetic, hybrid algorithm, crossover for, for graph, new the, genetic problem, performance algorithm","reinforcement learning, with and, theorem proving, and learning, with, learning with, and image, object recognition, for with, through and, and object, and application, mobile robot, with application, and domains, for domains, theorem for, and robot, planning robot, learning for","the and, and, for and, and its, between and, logic and, with and, reasoning and, the calculus, and recognition, and their, situation calculus, and application, and action, using and, belief and, its application, model and, temporal reasoning, belief revision"],"ranking":[["13687|IJCAI|1977|Writing a Natural Language Data Base System|We present a model for processing English requests for information from a relational data base. The model has as its main steps (a) locating semantic constituents of a request (b) matching these constituents against larger templates called concept case frames (c) filling in the concept case frame using information from the user's request, from the dialogue context and from the user's responses to questions posed by the system and (d) generating a formal data base query using the collected information. Methods are suggested for constructing the components of such a natural language processing system for an arbitrary relational data base. The model has been applied to a large data base of aircraft flight and maintenance data to generate a system called PLANES examples are drawn from this system.|David L. Waltz,Bradley A. Goodman","80723|VLDB|2006|Crimson A Data Management System to Support Evaluating Phylogenetic Tree Reconstruction Algorithms|Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a \"gold standard\" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.|Yifeng Zheng,Stephen Fisher,Shirley Cohen,Sheng Guo,Junhyong Kim,Susan B. Davidson","80037|VLDB|1981|The Implementation of GERM An Entity-Relationship Data Base Management System|The implementation of GERM, a data base management system based on the Entity-Relationship model, is presented. GERM is an end-user system providing complete facilities for interactive data base management, including retrieval, browsing, update, definition and reporting. The system has a modular design with the functional components arranged hierarchically. This paper discusses two specific areas, utility components and data access components. The data access components are layered, each approaching data access at a different level of abstraction. An example retrieval request is used to demonstrate how the components interact.|R. L. Benneworth,C. D. Bishop,C. J. M. Turnbull,W. D. Holman,F. M. Monette","79854|VLDB|1977|A Kernel Design for a Secure Data Base Management System|The need for reliable protection facilities, which allow controlled sharing of data in multi-user data base management systems, is steadily growing. This paper first discusses concepts relevant to such protection facilities, including data security, object grenularity, data incependence, and software certification. Those system characteristics required for reliable security and suitable functiorality are listec. The facilities which an operating system must provide in support of a such date base management system also are outlined. A kernel based data base management system architecture is then presented which supports value independent security and allows various grains of protection down to the size of comains in relations. It is shown that the proposed structure can substantially improve the reliability of protection in data bases.|Deborah Downs,Gerald J. Popek","80556|VLDB|2005|The Integrated Microbial Genomes IMG System A Case Study in Biological Data Management|Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.|Victor M. Markowitz,Frank Korzeniewski,Krishna Palaniappan,Ernest Szeto,Natalia Ivanova,Nikos Kyrpides","79853|VLDB|1977|A Processing Interface for Multiple External Schema Access to a Data Base Management System|A front-end software interface to a hierarchical data base management system is described. The interface validates the consistency of proposed external schema structures with a given main schema structure and provides a run-time mapping processor that translates data manipulation operations defined in the context of an external schema to operations on a data base disciplined by the main schema.|Alfred G. Dale,C. V. Yurkanan","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","13602|IJCAI|1977|ROBOT A High Performance Natural Language Data Base Query System|The field of natural language data base query has seen several successful research systems that have been able to process large subsets of the English language. The ROBOT system is a high performance natural language processor that extends the techniques developed in these earlier systems, in an attempt to provide a usable natural language query medium for non-technical users. ROBOT is already installed in four real, world environments, working in five application areas. Analysis of log files indicate that between -% of end user requests are successfully processed. The system successfully deals with both lexical and structural ambiguity, inter- and intrasentential pronomilization and sentence fragments. The resource requirements of ROBOT are well within the limits of medium to large computer systems. This paper describes the system from an information flow point of view. The goal is to obviate the creation of a semantic store purely for the natural language parser, since a different structure would be required for each domain of discourse. Instead the data base itself is used as the primary knowledge structure within the system. In this way the parser can be interfaced to other data bases with only minimal effort.|Larry R. Harris","79863|VLDB|1977|User-Oriented Data Base Query with the ROBOT Natural Language Query System|Now that large data bases of valuable information are in existence, we must face the problem of putting this information in the hands of the end user. This is not a task to be taken lightly, since it is a very difficult problem due to the mismatch between the user's conceptualization of the data and its actual structure. By analyzing this problem in some detail we argue that this mismatch must be consciously resolved in a single process, and that the use of natural language instead of an artificial query language is the only means by which this can be done. The technical feasibility of this approach is demonstrated by the ROBOT natural language query processor. Actual dialogs and end user experiences are used to show the resulting increase in end user orientation. The impact of natural language processing on the various DBMS architectures is also discussed.|Larry R. Harris","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber"],["57966|GECCO|2007|Discovering structures in gene regulatory networks using genetic programming and particle swarms|In this paper, we describe a Genetic Programming and Particle Swarm Hybrid algorithm for Gene Network discovery.|Xinye Cai,Stephen Welch,Praveen Koduru,Sanjoy Das","57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","58865|GECCO|2009|On the evolution of neural networks for pairwise classification using gene expression programming|Neural networks are a common choice for solving classification problems, but require experimental adjustments of the topology, weights and thresholds to be effective. Success has been seen in the development of neural networks with evolutionary algorithms, making the extension of this work to classification problems a logical step. This paper presents the first known use of the Gene Expression Programming-based GEP-NN algorithm to design neural networks for classification purposes. The system uses pairwise decomposition to produce a series of binary classifiers for a given multi-class problem, with the results of the classifier set being combined by majority vote.|Stephen Johns,Marcus V. dos Santos","15740|IJCAI|2001|Neural Logic Network Learning using Genetic Programming|Neural Logic Network or Neulonet is a hybrid of neural network expert systems. Its strength lies in its ability to learn and to represent human logic in decision making using component net rules. The technique originally employed in neulonet learning is backpropagation. However, the resulting weight adjustments will lead to a loss in the logic of the net rules. A new technique is now developed that allows the neulonet to learn by composing net rules using genetic programming. This paper presents experimental results to demonstrate this new and exciting capability in capturing human decision logic from examples. Comparisons will also be made between the use of net rules, and the use of standard boolean logic of negation, disjunction and conjunction in evolutionary computation.|Chew Lim Tan,Henry Wai Kit Chia","57769|GECCO|2006|Inference of genetic networks using S-system information criteria for model selection|In this paper we present an evolutionary approach for inferring the structure and dynamics in gene circuits from observed expression kinetics. For representing the regulatory interactions in a genetic network the decoupled S-system formalism has been used. We proposed an Information Criteria based fitness evaluation for model selection instead of the traditional Mean Squared Error (MSE) based fitness evaluation. A hill climbing local search method has been incorporated in our evolutionary algorithm for attaining the skeletal architecture which is most frequently observed in biological networks. Using small and medium-scale artificial networks we verified the implementation. The reconstruction method identified the correct network topology and predicted the kinetic parameters with high accuracy.|Nasimul Noman,Hitoshi Iba","14705|IJCAI|1989|Training Feedforward Neural Networks Using Genetic Algorithms|Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both ) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and ) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it.|David J. Montana,Lawrence Davis","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","58992|GECCO|2010|Improved forecasting of time series data of real system using genetic programming|A study is made to improve short term forecasting of time series data of real system using Genetic Programming (GP) under the framework of time delayed embedding technique. GP based approach is used to make analytical model of time series data of real system using embedded vectors that help reconstruct the phase space. The map equations, involving non-linear symbolic expressions in the form of binary trees comprising of time delayed components in the immediate past, are first obtained by carrying out single-step GP fit for the training data set and usually they are found to give good fitness as well as single-step predictions. However while forecasting the time series based on multi-step predictions in the out-of-sample region in an iterative manner, these solutions often show rapid deterioration as we dynamically forward the solution in future time. With a view to improve on this limitation, it is shown that if the multi-step aspect is incorporated while making the GP fit itself, the corresponding GP solutions give multi-step predictions that are improved to a fairly good extent for around those many multi-steps as incorporated during the multi-step GP fit. Two different methods for multi-step fit are introduced, and the corresponding prediction results are presented. The modified method is shown to make better forecast for out-of-sample multi-step predictions for the time series of a real system, namely Electroencephelograph (EEG) signals.|Dilip P. Ahalpara","58830|GECCO|2009|Evolution development and learning using self-modifying cartesian genetic programming|Self-Modifying Cartesian Genetic Programming (SMCGP) is a form of genetic programming that integrates developmental (self-modifying) features as a genotype-phenotype mapping. This paper asks Is it possible to evolve a learning algorithm using SMCGP|Simon Harding,Julian Francis Miller,Wolfgang Banzhaf","57139|GECCO|2003|Circuit Bipartitioning Using Genetic Algorithm|In this paper, we propose a hybrid genetic algorithm for partitioning a VLSI circuit graph into two disjoint graphs of minimum cut size. The algorithm includes a local optimization heuristic which is a modification of Fiduccia-Matheses algorithm. Using well-known benchmarks (including ACMSIGDA benchmarks), the combination of genetic algorithm and the local heuristic outperformed hMetis , a representative circuit partitioning algorithm.|Jong-Pil Kim,Byung Ro Moon"],["57864|GECCO|2006|A GA-ACO-local search hybrid algorithm for solving quadratic assignment problem|In recent decades, many meta-heuristics, including genetic algorithm (GA), ant colony optimization (ACO) and various local search (LS) procedures have been developed for solving a variety of NP-hard combinatorial optimization problems. Depending on the complexity of the optimization problem, a meta-heuristic method that may have proven to be successful in the past might not work as well. Hence it is becoming a common practice to hybridize meta-heuristics and local heuristics with the aim of improving the overall performance. In this paper, we propose a novel adaptive GA-ACO-LS hybrid algorithm for solving quadratic assignment problem (QAP). Empirical study on a diverse set of QAP benchmark problems shows that the proposed adaptive GA-ACO-LS converges to good solutions efficiently. The results obtained were compared to the recent state-of-the-art algorithm for QAP, and our algorithm showed obvious improvement.|Yiliang Xu,Meng-Hiot Lim,Yew-Soon Ong,Jing Tang","58756|GECCO|2009|Cheating for problem solving a genetic algorithm with social interactions|We propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population. Our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms. In biological populations, i.e. animals, even human beings and microorganisms, social interactions often affect the fitness of individuals. It is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum, thus avoiding a fast convergence of the population. We model the social interactions according to Game Theory. The population is, therefore, composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models (prisoner's dilemma, chicken game, and others). Our results on Knapsack problems show, for some game models, a significant performance improvement as compared to a standard genetic algorithm.|Rafael Lahoz-Beltra,Gabriela Ochoa,Uwe Aickelin","58185|GECCO|2007|On the relativity in the assessment of blind optimization algorithms and the problem-algorithm coevolution|Considering as an optimization problem the one of knowing what is hard for a blind optimization algorithm, the usefulness of absolute algorithm-independent hardness measures is called into question, establishing as a working hypothesis the relativity in the assessment of blind search. The results of the implementation of an incremental coevolutionary algorithm for coevolving populations of tunings of a simple genetic algorithm and simulated annealing, random search and -bit problems are presented, showing how these results are related to two popular views of hardness for genetic search deception and rugged fitness landscapes.|Carlos D. Toledo-Suárez,Manuel Valenzuela-Rendón,Hugo Terashima-Marín,Eduardo Uresti-Charre","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","58224|GECCO|2007|EcoPS a particle swarm algorithm to model group-foraging|Recent work has introduced a simulation model of ecological processes in terms of a very simple Particle Swarm algorithm. This abstract model produced qualitatively realistic behaviours, but do these results hold up in a model constrained by more plausible biological assumptions The objective of this paper is to answer this question.|Cecilia Di Chio,Riccardo Poli,Paolo Di Chio","57923|GECCO|2007|Hybrid quantum particle swarm optimization algorithm for combinatorial optimization problem|In this paper, a framework of hybrid PSO is proposed by reasonablycombining the Q-bit evolutionary search of quantum PSO and binary bit evolutionary search of genetic PSO.|Jiahai Wang,Yalan Zhou","57976|GECCO|2007|A discrete particle swarm optimization algorithm for the generalized traveling salesman problem|Dividing the set of nodes into clusters in the well-known traveling salesman problem results in the generalized traveling salesman problem which seeking a tour with minimum cost passing through only a single node from each cluster. In this paper, a discrete particle swarm optimization is presented to solve the problem on a set of benchmark instances. The discrete particle swarm optimization algorithm exploits the basic features of its continuous counterpart. It is also hybridized with a local search, variable neighborhood descend algorithm, to further improve the solution quality. In addition, some speed-up methods for greedy node insertions are presented. The discrete particle swarm optimization algorithm is tested on a set of benchmark instances with symmetric distances up to  nodes from the literature. Computational results show that the discrete particle optimization algorithm is very promising to solve the generalized traveling salesman problem.|Mehmet Fatih Tasgetiren,Ponnuthurai N. Suganthan,Quan-Qe Pan","57972|GECCO|2007|Kernel based automatic clustering using modified particle swarm optimization algorithm|This paper introduces a method for clustering complex and linearly non-separable datasets, without any prior knowledge of the number of naturally occurring clusters. The proposed method is based on an improved variant of the Particle Swarm Optimization (PSO) algorithm. In addition, it employs a kernel-induced similarity measure instead of the conventional sum-of-squares distance. Use of the kernel function makes it possible to cluster data that is linearly non-separable in the original input space into homogeneous groups in a transformed high-dimensional feature space. Computer simulations have been undertaken with a test bench of five synthetic and three real life datasets, in order to compare the performance of the proposed method with a few state-of-the-art clustering algorithms. The results reflect the superiority of the proposed algorithm in terms of accuracy, convergence speed and robustness.|Ajith Abraham,Swagatam Das,Amit Konar","58491|GECCO|2008|Convergence behavior of the fully informed particle swarm optimization algorithm|The fully informed particle swarm optimization algorithm (FIPS) is very sensitive to changes in the population topology. The velocity update rule used in FIPS considers all the neighbors of a particle to update its velocity instead of just the best one as it is done in most variants. It has been argued that this rule induces a random behavior of the particle swarm when a fully connected topology is used. This argument could explain the often observed poor performance of the algorithm under that circumstance. In this paper we study experimentally the convergence behavior of the particles in FIPS when using topologies with different levels of connectivity. We show that the particles tend to search a region whose size decreases as the connectivity of the population topology increases. We therefore put forward the idea that spatial convergence, and not a random behavior, is the cause of the poor performance of FIPS with a fully connected topology. The practical implications of this result are explored.|Marco Antonio Montes de Oca,Thomas Stützle","58759|GECCO|2009|The singly-linked ring topology for the particle swarm optimization algorithm|This paper introduces a new neighborhood structure for Particle Swarm Optimization, called Singly-Linked Ring. The approach proposes a neighborhood whose members share the information at a different rate. The objective is to avoid the premature convergence of the flock and stagnation into local optimal. The approach is applied at a set of global optimization problems commonly used in the literature. The singly-linked structure is compared against the state-of-the-art neighborhoods structures. The proposal is easy to implement, and its results and its convergence performance are better than other structures.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce"],["17077|IJCAI|2009|Reasoning with Knowledge Action and Time in Dynamic and Uncertain Domains|We propose a new framework for reasoning about knowledge, action and time for domains that include actions with non-deterministic and context-dependent effects. The axiomatization is based on the Event Calculus and combines the expressiveness of possible worlds semantics with the efficiency of approaches that dispense the use of the accessibility relation. The framework is proved logically sound and, when restricted to deterministic domains, is also logically complete. To prove correctness of the approach, we construct a knowledge theory based on a branching version of the Event Calculus and study their correlation.|Theodore Patkos,Dimitris Plexousakis","16132|IJCAI|2005|Integrating Planning and Temporal Reasoning for Domains with Durations and Time Windows|The treatment of exogenous events in planning is practically important in many domains. In this paper we focus on planning with exogenous events that happen at known times, and affect the plan actions by imposing that the execution of certain plan actions must be during some time windows. When actions have durations, handling such constraints adds an extra difficulty to planning, which we address by integrating temporal reasoning into planning. We propose a new approach to planning in domains with durations and time windows, combining graph-based planning and disjunctive constraint-based temporal reasoning. Our techniques are implemented in a planner that took part in the th International Planning Competition showing very good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina","15536|IJCAI|1999|Temporal Planning with Mutual Exclusion Reasoning|Many planning domains require a richer notion of time in which actions can overlap and have different durations. The key to fast performance in classical planners (e.g., Graphplan, IPP, and Blackbox) has been the use of a disjunctive representation with powerful mutual exclusion reasoning. This paper presents TGP, a new algorithm for temporal planning. TGP operates by incrementally expanding a compact planning graph representation that handles actions of differing duration. The key to TGP performance is tight mutual exclusion reasoning which is based on an expressive language for bounding mutexes and includes mutexes between actions and propositions. Our experiments demonstrate that mutual exclusion reasoning remains valuable in a rich temporal setting.|David E. Smith,Daniel S. Weld","15332|IJCAI|1997|Reasoning about Action in Polynomial Time|Although many formalisms for reasoning about action exist, surprisingly few approaches have taken computational complexity into consideration. The contributions of this paper are the following a temporal logic with a restriction for which deciding satisfiability is tractable, a tractable extension for reasoning about action, and NP-completeness results for the unrestricted problems. Many interesting reasoning problems can be modelled, involving nondeterminism, concurrency and memory of actions. The reasoning process is proved to be sound and complete.|Thomas Drakengren,Marcus Bjäreland","14681|IJCAI|1989|Visual Reasoning in Geometry Theorem Proving|We study the role of visual reasoning as a computationally feasible heuristic tool in geometry problem solving. We use an algebraic notation to represent geometric objects and to manipulate them. We show that this representation captures powerful heuristics for proving geometry theorems, and that it allows a systematic manipulation of geometric features in a manner similar to what may occur in human visual reasoning.|Michelle Y. Kim","13637|IJCAI|1977|Reasoning About Knowledge and Action|This paper discusses the problems of representing and reasoning with information about knowledge and action. The first section discusses the importance of having systems that understand the concept of knowledge, and how knowledge is related to action. Section  points out some of the special problems that are involved in reasoning about knowledge, and section S presents a logic of knowledge based on the idea of possible worlds. Section  integrates this with a logic of actions and gives an example of reasoning in the combined system. Section  makes some concluding comments.|Robert C. Moore","14806|IJCAI|1991|Reasoning about Student Knowledge and Reasoning|A basic feature of Intelligent Tutoring Systems (ITS) is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process. In this paper we present a general (first order logic) framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answers. This represantation makes it possible to describe the behaviour of well known ITSs and to provide a direct implementation in a logic programming language. Moreover, we point out several improvements that can be easily achieved by exploiting the features of a declarative approach. In particular, we address the representation and use of the knowledge that the system knows not to be possessed by the student.|Luigia Carlucci Aiello,Maria Cialdea,Daniele Nardi","14340|IJCAI|1987|Learning Strategies by Reasoning about Rules|One of the major 'weaknesses of current automated reasoning systems is that they lack the ability to control inference in a sophisticated, context-directed fashion. General strategies such as the set-of-support strategy are useful, but have proven inadequate for many individual problems. A strategy component is needed that possesses knowledge about many particular domains and problems. Such a body of knowledge would require a prohibitive amount of time to construct by hand. This leads us to consider means of automatically acquiring control knowledge from example proofs. One particular means of learning is explanation-based learning. This paper analyzes the basis of explanations -- finding weakest preconditions that enable a particular rule to fire -- to derive a representation within which explanations can be extracted from examples, generalized and used to guide the actions of a problem-solving system.|D. Paul Benjamin","14926|IJCAI|1991|Planning Robot Control Parameter Values with Qualitative Reasoning|A qualitative reasoning planner for determining robot control parameters to drive manipulation actions has been developed, integrated into a telerobot system, and demonstrated for a match striking task. The planner consists of a qualitative reasoner and a numerical execution history which interact to jointly direct and narrow the search for reliable numerical control parameter values. The planner algorithm, implementation, and an execution example are described. The relationship to previous qualitative reasoning work is also discussed.|Stephen F. Peters,Shigeoki Hirai,Toru Omata,Tomomasa Sato","15429|IJCAI|1999|Expressive Reasoning about Action in Nondeterministic Polynomial Time|The rapid development of efficient heuristics for deciding satisfiability for propositional logic motivates thorough investigations of the usability of NP-complete problems in general. In this paper we introduce a logic of action and change which is expressive in the sense that it can represent most propositional benchmark examples in the literature, and some new examples involving parallel composition of actions, and actions that may or may not be executed. We prove that satisfiability of a scenario in this logic is NP-complete, and that it subsumes an NP-complete logic (which in turn includes a nontrivial polynomial-time fragment) previously introduced by Drakengren and Bjareland.|Thomas Drakengren,Marcus Bjäreland"],["79815|VLDB|1975|Stanfords Generalized Database System|SPIRES (STANFORD PUBLIC INFORMATION RETRIEVAL SYSTEM) is a generalized, on-line database management system developed by Stanford University. Since  it has supported a diverse group of users who design and maintain their own databases. This paper presents an overview of not only the conceptual model of SPIRES, but also some of the underlying implementation techniques The discussion emphasizes the role of metadata schemae that describe the physical or logical mapping of databases are themselves records in the databank and are entered interactively. The database structure is defined in terms of forest sets (databases), forests (record sets), and trees (records). Three aspects of SPIRES schemae are reviewed a predicate which determines the membership of a record in a particular record set, forest-valued passing functions on trees, and files, which are forest subsets accessible to different user groups.|J. R. Schroeder,W. C. Kiefer,Richard L. Guertin,W. J. Berman","80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","79985|VLDB|1979|Decentralized Authorization in a Database System|Decentralization of the security functions of the database administrator is needed in large or complex databases for efficiency and simplicity, independently of whether the database itself is centralized or is distributed throughout a network. In this paper, a model of authorization for decentralized administration is developed. A key concept of the model is the use of data classes, sets of database objects, as the unit of delegation of authorization functions. Mechanisms for the delegation of administration and its revocation are presented. A possible scheme for the distribution of authorization-related data throughout a distributed processing network is described.|Christopher Wood,Eduardo B. Fernández","79800|VLDB|1975|Hardware and System Architecture for a Very Large Database|This paper describes practical experience in structuring a large amount of data (about  reels of -track  binary code decimal magnetic tapes of  census data). The hardware and software systems were given. The goal was to structure the database for online devices and to make the retrieval as efficient as possible. This paper explains only the database architecture finally chosen.|Robert Healey,Bradford Heckman","80061|VLDB|1981|DSIS - A Database System with Interrelational Semantics|DSIS is an experimental multi-user database system that supports an entity-oriented data model with relational and interrelational semantics. It maintains a sharp distinction between the user workspace and the shared database. Transactions are executed by a simulated network of stream processors.|Y. Edmund Lien,Jonathan E. Shopiro,Shalom Tsur","80091|VLDB|1985|Database Design Tools An Expert System Approach|In this paper, we report on the implementation of SECSI, an expert system for database design written in Prolog. Starting from an application description given with either a subset of the natural language, or a formal language, or a graphical interface, the system generates a specific semantic network portraying the application. Then, using a set of design rules, it completes and simplifies the semantic network up to reach flat normalized relations. All the design is interactively done with the end-user. The system is evolutive in the sense that it also offers an interactive interface which allows the database design expert to modify or add design rules.|Mokrane Bouzeghoub,Georges Gardarin,Elisabeth Métais","79877|VLDB|1977|An Architectural Extension for a Large Database System Incorporating a Processor for Disk Search|It is proposed to use a processor in each direct storage device of a conventional computer system to improve the performance of a large data management system. These processors are used to search the physical records for the information desired. The effect of these processors on the critical performance parameters of the system (throughput of memory, channel, DASD, and CPU), and on the overall performance, is evaluated. It is shown that the proposed architecture solves certain performance problems associated with the storage subsystem and the CPU under heavy transaction traffic.|T. Lang,E. Nahouraii,K. Kasuga,Eduardo B. Fernández","79986|VLDB|1979|Query Processing in a Relational Database Management System|In this paper the various tactics for query processing in INGRESS are empirically evaluated on a test bed of sample queries.|Karel Youssefi,Eugene Wong","80456|VLDB|2004|BilVideo Video Database Management System|A prototype video database management system, which we call BilVideo, is presented. BilVideo provides an integrated support for queries on spatio-temporal, semantic and low-level features (color, shape, and texture) on video data. BilVideo does not target a specific application, and thus, it can be used to support any application with video data. An example application, news archives search system, is presented with some sample queries.|\u2013zgür Ulusoy,Ugur Güdükbay,Mehmet Emin Dönderler,Ediz Saykol,Cemil Alper","79887|VLDB|1977|Architecture of the SOFIS Data Base Management System|SOFIS is a software system intended for processing of statistical data in national statistical information systems. The data base management system (DBMS) of SOFIS has been designed to meet two objectives to process efficiently separate voluminous files resulting from statistical surveys and at the same time to provide the possibility of complex exploitation of data bases consisting Of these files, i.e. the materialisation of relations among data, up to the network structure of these relations. In this paper, we present the architecture of SOFIS DBMS. It is essentially a file management system with a relational interface, equipped with a data dictionarydirectory system. The modular structure of SOFIS DBMS permits, as indicated in this paper, its gradual implementation in the statistical information system.|Anton Scheber"],["80710|VLDB|2006|Efficient Secure Query Evaluation over Encrypted XML Databases|Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.|Hui Wang,Laks V. S. Lakshmanan","80267|VLDB|2003|Query Processing for High-Volume XML Message Brokering|XML filtering solutions developed to date have focused on the matching of documents to large numbers of queries but have not addressed the customization of output needed for emerging distributed information infrastructures. Support for such customization can significantly increase the complexity of the filtering process. In this paper, we show how to leverage an efficient, shared path matching engine to extract the specific XML elements needed to generate customized output in an XML Message Broker. We compare three different approaches that differ in the degree to which they exploit the shared path matching engine. We also present techniques to optimize the post-processing of the path matching engine output, and to enable the sharing of such processing across queries. We evaluate these techniques with a detailed performance study of our implementation.|Yanlei Diao,Michael J. Franklin","80791|VLDB|2007|Randomized Algorithms for Data Reconciliation in Wide Area Aggregate Query Processing|Many aspects of the data integration problem have been considered in the literature how to match schemas across different data sources, how to decide when different records refer to the same entity, how to efficiently perform the required entity resolution in a batch fashion, and so on. However, what has largely been ignored is a way to efficiently deploy these existing methods in a realistic, distributed enterprise integration environment. The straightforward use of existing methods often requires that all data be shipped to a coordinator for cleaning, which is often unacceptable. We develop a set of randomized algorithms that allow efficient application of existing entity resolution methods to the answering of aggregate queries over data that have been distributed across multiple sites. Using our methods, it is possible to efficiently generate aggregate query results that account for duplicate and inconsistent values scattered across a federated system.|Fei Xu,Chris Jermaine","80004|VLDB|1980|Knowledge-Based Query Processing|Contemporary database query processing systems base their actions principally on \"syntactic\" considerations, and seek only the most efficacious way of answering a query as originally formulated. An alternative approach seeks to use knowledge of the semantics of the database's application to transform the original query into an alternative form, possibly quite different in its expression, but which is both equivalent to the original (in terms of the set of records from the database that it qualifies) and more efficient to process, given the existing file structures and access methods. The architecture of a system supporting such knowledge-based \"semantic\" transformations has been developed. It addresses such issues as the kinds of knowledge that should be included in the knowledge base and how it should be expressed, the kinds of transformations that can exploit this knowledge to improve query processing, and the way in which the system as a whole can be organized in the presence of large and intricate knowledge bases and a multiplicity of possible transformation types. This latter structure is based on a multi-processing model, in which each possible transformation is treated as a process, whose priority is assigned by a scheduler embodying a variety of heuristics. The principal contribution of the work is the establishment of a conceptual framework for this type of query optimization and the design of an architecture that can grow with the development of additional transformation techniques.|Michael Hammer,Stanley B. Zdonik","80360|VLDB|2004|A Framework for Using Materialized XPath Views in XML Query Processing|XML languages, such as XQuery, XSLT and SQLXML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.|Andrey Balmin,Fatma \u2013zcan,Kevin S. Beyer,Roberta Cochrane,Hamid Pirahesh","80622|VLDB|2006|Query Processing in the AquaLogic Data Services Platform|BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.|Vinayak R. Borkar,Michael J. Carey,Dmitry Lychagin,Till Westmann,Daniel Engovatov,Nicola Onose","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80278|VLDB|2003|Mixed Mode XML Query Processing|Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.|Alan Halverson,Josef Burger,Leonidas Galanis,Ameet Kini,Rajasekar Krishnamurthy,Ajith Nagaraja Rao,Feng Tian,Stratis Viglas,Yuan Wang,Jeffrey F. Naughton,David J. DeWitt","80344|VLDB|2003|Efficient Mining of XML Query Patterns for Caching|As XML becomes ubiquitous, the efficient retrieval of XML data becomes critical. Research to improve query response time has been largely concentrated on indexing paths, and optimizing XML queries. An orthogonal approach is to discover frequent XML query patterns and cache their results to improve the performance of XML management systems. In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.|Liang Huai Yang,Mong-Li Lee,Wynne Hsu","80293|VLDB|2003|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.|Christoph Koch"],["15381|IJCAI|1997|An Effective Learning Method for Max-Min Neural Networks|Max and min operations have interesting properties that facilitate the exchange of information between the symbolic and real-valued domains. As such, neural networks that employ max-min activation functions have been a subject of interest in recent years. Since max-min functions are not strictly differentiate, we propose a mathematically sound learning method based on using Fourier convergence analysis of side-derivatives to derive a gradient descent technique for max-min error functions. This method is applied to a \"typical\" fuzzy-neural network model employing max-rnin activation functions. We show how this network can be trained to perform function approximation its performance was found to be better than that of a conventional feedforward neural network.|Loo-Nin Teow,Kia-Fock Loe","16427|IJCAI|2007|Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning|TD-FALCON is a self-organizing neural network that incorporates Temporal Difference (TD) methods for reinforcement learning. Despite the advantages of fast and stable learning, TD-FALCON still relies on an iterative process to evaluate each available action in a decision cycle. To remove this deficiency, this paper presents a direct code access procedure whereby TD-FALCON conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values. Our comparative experiments show that TD-FALCON with direct code access produces comparable performance with the original TD-FALCON while improving significantly in computation efficiency and network complexity.|Ah-Hwee Tan","17031|IJCAI|2009|Learning Conditional Preference Networks with Queries|We investigate the problem of eliciting CP-nets in the well-known model of exact learning with equivalence and membership queries. The goal is to identify a preference ordering with a binary-valued CP-net by guiding the user through a sequence of queries. Each example is a dominance test on some pair of outcomes. In this setting, we show that acyclic CP-nets are not learnable with equivalence queries alone, while they are learnable with the help of membership queries if the supplied examples are restricted to swaps. A similar property holds for tree CP-nets with arbitrary examples. In fact, membership queries allow us to provide attribute-efficient algorithms for which the query complexity is only logarithmic in the number of attributes. Such results highlight the utility of this model for eliciting CP-nets in large multi-attribute domains.|Frédéric Koriche,Bruno Zanuttini","16489|IJCAI|2007|A Theoretical Framework for Learning Bayesian Networks with Parameter Inequality Constraints|The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. Domain knowledge can come in many forms. For example, expert knowledge about the relevance of variables relative to a certain problem can help perform better feature selection. Domain knowledge about the conditional independence relationships among variables can help learning of the Bayesian Network structure. This paper considers a different type of domain knowledge for constraining parameter estimates when learning Bayesian Networks. In particular, we consider domain knowledge that comes in the form of inequality constraints among subsets of parameters in a Bayesian Network with known structure. These parameter constraints are incorporated into learning procedures for Bayesian Networks, by formulating this task as a constrained optimization problem. The main contribution of this paper is the derivation of closed form Maximum Likelihood parameter estimators in the above setting.|Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao","15228|IJCAI|1995|Combining the Predictions of Multiple Classifiers Using Competitive Learning to Initialize Neural Networks|The primary goal of inductive learning is to generalize well - that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combinations of networks initialized the traditional way.|Richard Maclin,Jude W. Shavlik","14535|IJCAI|1987|Learning Phonetic Features Using Connectionist Networks|A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectraltemporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. A simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated % of  other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items.|Raymond L. Watrous,Lokendra Shastri","15744|IJCAI|2001|Active Learning for Structure in Bayesian Networks|The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper uses active learning to determine the experiments that are most informative towards uncovering the underlying structure. We formalize the causal learning task as that of learning the structure of a causal Bayesian network. We consider an active learner that is allowed to conduct experiments, where it intervenes in the domain by setting the values of certain variables. We provide a theoretical framework for the active learning problem, and an algorithm that actively chooses the experiments to perform based on the model learned so far. Experimental results show that active learning can substantially reduce the number of observations required to determine the structure of a domain.|Simon Tong,Daphne Koller","16667|IJCAI|2007|Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri|Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data andor do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks' links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.|George Tsatsaronis,Michalis Vazirgiannis,Ion Androutsopoulos","58635|GECCO|2009|Learning regulation functions of metabolic systems by artificial neural networks|Metabolic P systems, also called MP systems, are discrete dynamical systems which proved to be effective for modeling biological systems. Their dynamics is generated by means of a metabolic algorithm based on \"flux regulation functions\". A significant problem related to the generation of MP models from experimental data concerns the synthesis of these functions. In this paper we introduce a new approach to the synthesis of MP fluxes relying on neural networks as universal function approximators, and on evolutionary algorithms as learning techniques. This methodology is successfully tested in the case study of mitotic oscillator in early amphibian embryos.|Alberto Castellini,Vincenzo Manca","57074|GECCO|2003|Building a GA from Design Principles for Learning Bayesian Networks|Recent developments in GA theory have given rise to a number of design principles that serve to guide the construction of selecto-recombinative GAs from which good performance can be expected. In this paper, we demonstrate their application to the design of a GA for a well-known hard problem in machine learning the construction of a Bayesian network from data. We show that the resulting GA is able to efficiently and reliably find good solutions. Comparisons against state-of-the-art learning algorithms, moreover, are favorable.|Steven van Dijk,Dirk Thierens,Linda C. van der Gaag"],["57281|GECCO|2005|Optimization with constraints using a cultured differential evolution approach|In this paper we propose a cultural algorithm, where different knowledge sources modify the variation operator of a differential evolution algorithm. Differential evolution is used as a basis for the population, variation and selection processes. The experiments performed show that the cultured differential evolution is able to reduce the number of fitness function evaluations needed to obtain a good aproximation of the optimum value in constrained real-parameter optimization. Comparisons are provided with respect to three techniques that are representative of the state-of-the-art in the area.|Ricardo Landa Becerra,Carlos A. Coello Coello","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,Jörn Mehnen,Thomas Michelitsch","57462|GECCO|2005|Inference of gene regulatory networks using s-system and differential evolution|In this work we present an improved evolutionary method for inferring S-system model of genetic networks from the time series data of gene expression. We employed Differential Evolution (DE) for optimizing the network parameters to capture the dynamics in gene expression data. In a preliminary investigation we ascertain the suitability of DE for a multimodal and strongly non-linear problem like gene network estimation. An extension of the fitness function for attaining the sparse structure of biological networks has been proposed. For estimating the parameter values more accurately an enhancement of the optimization procedure has been also suggested. The effectiveness of the proposed method was justified performing experiments on a genetic network using different numbers of artificially created time series data.|Nasimul Noman,Hitoshi Iba","57985|GECCO|2007|Differential evolution and non-separability using selective pressure to focus search|Recent results show that the Differential Evolution algorithm has significant difficulty on functions that are not linearly separable. On such functions, the algorithm must rely primarily on its differential mutation procedure which, unlike its recombination strategy, is rotationally invariant. We conjecture that this mutation strategy lacks sufficient selective pressure when appointing parent and donor vectors to have satisfactory exploitative power on non-separable functions. We find that imposing pressure in the form of rank-based differential mutation results in a significant improvement of exploitation on rotated benchmarks.|Andrew M. Sutton,Monte Lunacek,L. Darrell Whitley","57728|GECCO|2006|Mixed-integer optimization of coronary vessel image analysis using evolution strategies|In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.|Rui Li,Michael Emmerich,Jeroen Eggermont,Ernst G. P. Bovenkamp","59109|GECCO|2010|Mixed-integer evolution strategy using multiobjective selection applied to warehouse design optimization|This paper reports about the application of a new variant of multiobjective Mixed-Integer Evolution Strategy to a warehouse design optimization problem. The algorithm is able to deal with real-valued, integer, and discrete nominal input variables in a multiobjective problem, and utilizes a multiobjective selection procedure based on either crowding distance or hypervolume contribution (also called S metric). The warehouse design optimization problem investigated in this study is represented by a warehouse simulator (provided by our collaboration partner) which calculates four warehouse performance measures total handling time, crate fill rate, order latency, and investment cost. Two of those are treated as objectives, while the other two are represented as constraints. As demonstrated by the results, the algorithm generates solutions which cover the whole Pareto front, as opposed to the expert-generated solutions. Moreover, the algorithm is able to find solutions which improve on the expert-generated solutions with respect to both objectives.|Edgar Reehuis,Thomas Bäck","57945|GECCO|2007|An experimental analysis of evolution strategies and particle swarm optimisers using design of experiments|The success of evolutionary algorithms (EAs) depends crucially on finding suitable parameter settings. Doing this by hand is a very time consuming job without the guarantee to finally find satisfactory parameters. Of course, there exist various kinds of parameter control techniques, but not for parameter tuning. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience. In this paper we analyse evolution strategies (ES) and particle swarm optimisation (PSO) with and without optimal parameters gathered with DoE. Reasonable improvements have been observed for the two ES variants.|Oliver Kramer,Bartek Gloger,Andreas Goebels","57399|GECCO|2005|Efficient differential evolution using speciation for multimodal function optimization|In this paper differential evolution is extended by using the notion of speciation for solving multimodal optimization problems. The proposed species-based DE (SDE) is able to locate multiple global optima simultaneously through adaptive formation of multiple species (or subpopulations) in an DE population at each iteration step. Each species functions as an DE by itself. Successive local improvements through species formation can eventually transform into global improvements in identifying multiple global optima. In this study the performance of SDE is compared with another recently proposed DE variant CrowdingDE. The computational complexity of SDE, the effect of population size and species radius on SDE are investigated. SDE is found to be more computationally efficient than CrowdingDE over a number of benchmark multimodal test functions.|Xiaodong Li","58961|GECCO|2010|Fast parallelization of differential evolution algorithm using MapReduce|MapReduce is a promising programming model for developing distributed applications due to its superb simplicity, scalability and fault tolerance. This paper demonstrates how to apply MapReduce and the open source Hadoop framework for a quick and easy parallelization of the Differential Evolution algorithm. Instead of parallelizing the whole evolution process, our simple solution is to only apply the MR model to the fitness evaluation part, which usually consumes most of the running time. Two alternative approaches are investigated, i.e., population based and data based. Experimental results reveal that even though the population based approach is a better way, the extra cost of Hadoop DFS IO operations and system bookkeeping overhead significantly reduces the benefits of parallelism.|Chi Zhou"],["14517|IJCAI|1987|CYPRESS-Soar A Case Study in Search and Learning in Algorithm Design|This paper describes a partial reimplementation of Doug Smith's CYPRESS algorithm design system within the Soar problem-solving architecture. The system, CYPRESS-SOAR, reproduces most of CYPRESS' behavior in the synthesis of three divide-and-conquer sorting algorithms from formal specifications. CYPRESS-Soar is based on heuristic search of problem spaces, and uses search to compensate for missing knowledge in some instances. CYPRESS-Soar also learns as it designs algorithms, exhibiting significant transfer of learned knowledge, both within a single design run, and across designs of several different algorithms. These results were produced by reimplementing just the high-level synthesis control of CYPRESS, simulating the results of calls to CYPRESS deduction engine. Thus after only two months of effort, we had a surprisingly effective research vehicle for investigating the roles of search, knowledge, and learning in this domain.|David M. Steier","57798|GECCO|2006|The effect of crossover on the behavior of the GA in dynamic environments a case study using the shaky ladder hyperplane-defined functions|One argument as to why the hyperplane-defined functions (hdf's) are a good testbed for the genetic algorithm (GA) is that the hdf's are built in the same way that the GA works. In this paper we test that hypothesis in a new setting by exploring the GA on a subset of the hdf's which are dynamic---the shaky ladder hyperplane-defined functions (sl-hdf's). In doing so we gain insight into how the GA makes use of crossover during its traversal of the sl-hdf search space. We begin this paper by explaining the sl-hdf's. We then conduct a series of experiments with various crossover rates and various rates of environmental change. Our results show that the GA performs better with than without crossover in dynamic environments. Though these results have been shown on some static functions in the past, they are re-confirmed and expanded here for a new type of function (the hdf) and a new type of environment (dynamic environments). Moreover we show that crossover is even more beneficial in dynamic environments than it is in static environments. We discuss how these results can be used to develop a richer knowledge about the use of building blocks by the GA.|William Rand,Rick L. Riolo,John H. Holland","57488|GECCO|2005|The problem with a self-adaptative mutation rate in some environments a case study using the shaky ladder hyperplane-defined functions|Dynamic environments have periods of quiescence and periods of change. In periods of quiescence a Genetic Algorithm (GA) should (optimally) exploit good individuals while in periods of change the GA should (optimally) explore new solutions. Self-adaptation is a mechanism which allows individuals in the GA to choose their own mutation rate, and thus allows the GA to control when it explores new solutions or exploits old ones. We examine the use of this mechanism on a recently devised dynamic test suite, the Shaky Ladder Hyperplane-Defined Functions (sl-hdf's). This test suite can generate random problems with similar levels of difficulty and provides a platform allowing systematic controlled observations of the GA in dynamic environments. We show that in a variety of circumstances self-adaptation fails to allow the GA to perform better on this test suite than fixed mutation, even when the environment is static. We also show that mutation is beneficial throughout the run of a GA, and that seeding a population with known good genetic material is not always beneficial to the results. We provide explanations for these observations, with particular emphasis on comparing our results to other results  which have shown the GA to work in static environments. We conclude by giving suggestions as to how to change the simple GA to solve these problems.|William Rand,Rick L. Riolo","14656|IJCAI|1989|Using Generic Knowledge in Analysis of Aerial Scenes A Case Study|Our goal is to produce high-quality symbolic descriptions from aerial scenes. We have chosen to work in the domain of large commercial airport complexes. Such scenes have a variety of features such as the transportation network, building structures, and mobile objects. This paper concentrates on detection and description of the transportation network (runways and taxiways). We illustrate the complexities of this problem and how it can be solved by using geometrical context and generic airport domain knowledge.|Andres Huertas,William Cole,Ramakant Nevatia","57324|GECCO|2005|A case study of process facility optimization using discrete event simulation and genetic algorithm|Optimization problems such as resource allocation, job-shop scheduling, equipment utilization and process scheduling occur in a broad range of processing industries. This paper presents modeling, simulation and optimization of a port facility such that effective operational management is obtained. A GA base approach has been integrated with the port system model to optimize its operation. A case study of bulk material port handling systems is considered.|Keshav P. Dahal,Stuart Galloway,Graeme M. Burt,Jim R. McDonald,Ian Hopkins","13254|IJCAI|1969|Search for a Solution A Case Study|This paper describes a series of attempts at the solution of a conceptually tough problem, the Firing Squad Synchronization Problem. These attempts demonstrate an increasing reliance on man-machine symbiosis and decreasing reliance on powerful heurisics and preplanning. These attempts consist of a clerical checking program, and four attempts utilizing a basic backtracking program for searching the solution space. The first two attempts, Serial Definition of Productions and Symbolic Definition of Productions were non - interactiveentirely computer directed attempts at solution. The second two, Functional Planning and Constraint Satisfaction were man-machine symbiotic attempts designed to allow the human to control and direct the computer search of the solution space. The benefits of these symbiotic attempts and the problems encountered with them are discussed.|Robert Balzer","58859|GECCO|2009|Convergence analysis of UMDA with finite populations a case study on flat landscapes|This paper presents some new analytical results on the continuous Univariate Marginal Distribution Algorithm (UMDAC), which is a well known Estimation of Distribution Algorithm based on Gaussian distributions. As the extension of the current theoretical work built on the assumption of infinite populations, the convergence behavior of UMDAC with finite populations is formally analyzed. We show both analytically and experimentally that, on flat landscapes, the Gaussian model in UMDAC tends to collapse with high probability, which is an important fact that is not well understood before.|Bo Yuan,Marcus Gallagher","15649|IJCAI|2001|High Performance Reasoning with Very Large Knowledge Bases A Practical Case Study|We present an empirical analysis of optimization techniques devised to speed up the so-called TBox classification supported by description logic systems which have to deal with very large knowledge bases (e.g. containing more than , concept introduction axioms). These techniques are integrated into the RACE architecture which implements a TBox and ABox reasoner for the description logic ALCNHR+. The described techniques consist of adaptions of previously known as well as new optimization techniques for efficiently coping with these kinds of very large knowledge bases. The empirical results presented in this paper are based on experiences with an ontology for the Unified Medical Language System and demonstrate a considerable runtime improvement. They also indicate that appropriate description logic systems based on sound and complete algorithms can be particularly useful for very large knowledge bases.|Volker Haarslev,Ralf Möller","57986|GECCO|2007|The defined cliffs variant in dynamic environments a case study using the shaky ladder hyperplane-defined functions|The shaky ladder hyperplane-defined functions (sl-hdfs) are a test suite utilized for exploring the behavior of the genetic algorithm (GA) in dynamic environments. This test suite can generate arbitrary problems with similar levels of difficulty and it provides a platform for systematic controlled observations of the GA in dynamic environments. Previous work has found two factors that contribute to the GA's success on sl-hdfs () short initial building blocks and () significantly changing the reward structure during fitness landscape changes. Therefore a test function that combines these two features should facilitate even better GA performance. This has led to the construction of a new sl-hdf variant, \"Defined Cliffs,\" in which we combine short elementary building blocks with sharp transitions in the environment. We examine this variant with two different levels of dynamics, static and regularly changing, using four different metrics. The results show superior GA performance on the Defined Cliffs over all previous variants (Cliffs, Weight, and Smooth). Our observations and conclusions in this variant further the understanding of the GA in dynamic environments.|Abir Alharbi,William Rand,Rick L. Riolo","16777|IJCAI|2007|Recent Progress in Heuristic Search A Case Study of the Four-Peg Towers of Hanoi Problem|We integrate a number of new and recent advances in heuristic search, and apply them to the fourpeg Towers of Hanoi problem. These include frontier search, disk-based search, parallel processing, multiple, compressed, disjoint, and additive pattern database heuristics, and breadth-first heuristic search. New ideas include pattern database heuristics based on multiple goal states, a method to reduce coordination among multiple parallel threads, and a method for reducing the number of heuristic calculations. We perform the first complete breadth-first searches of the  and -disc fourpeg Towers of Hanoi problems, and extend the verification of \"presumed optimal solutions\" to this problem from  to  discs. Verification of the -disc problem is in progress.|Richard E. Korf,Ariel Felner"],["57206|GECCO|2003|A Hybrid Genetic Algorithm Based on Complete Graph Representation for the Sequential Ordering Problem|A hybrid genetic algorithm is proposed for the sequential ordering problem. It is known that the performance of a genetic algorithm depends on the survival environment and the reproducibility of building blocks. For decades, various chromosomal structures and crossover operators were proposed for the purpose. In this paper, we use Voronoi quantized crossover that adopts complete graph representation. It showed remarkable improvement in comparison with state-of-the-art genetic algorithms.|Dong-il Seo,Byung Ro Moon","57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57057|GECCO|2003|A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem|We propose a hybrid genetic algorithm for the hexagonal tortoise problem. We combined the genetic algorithm with an efficient local heuristic and aging mechanism. Another search heuristic which focuses on the space around existing solutions is also incorporated into the genetic algorithm. With the proposed algorithm, we could find the optimal solutions of up to a fairly large problem.|Heemahn Choe,Sung-Soon Choi,Byung Ro Moon","57073|GECCO|2003|The Spatially-Dispersed Genetic Algorithm|Spatially structured population models improve the performance of genetic algorithms by assisting the selection scheme in maintaining diversity. A significant concern with these systems is that they need to be carefully configured in order to operate at their optimum. Failure to do so can often result in performance that is significantly under that of an equivalent non-spatial implementation. This paper introduces a GA that uses a population structure that requires no additional configuration. Early experimentation with this paradigm indicates that it is able to improve the searching abilities of the genetic algorithm on some problem domains.|Grant Dick","58084|GECCO|2007|A genetic algorithm for coverage problems|This paper describes a genetic algorithm approach to coverage problems, that is, problems where the aim is to discover an example for each class in a given classification scheme.|Colin G. Johnson","57116|GECCO|2003|Designing A Hybrid Genetic Algorithm for the Linear Ordering Problem|The Linear Ordering Problem(LOP), which is a well-known NP-hard problem, has numerous applications in various fields. Using this problem as an example, we illustrate a general procedure of designing a hybrid genetic algorithm, which includes the selection of crossovermutation operators, accelerating the local search module and tuning the parameters. Experimental results show that our hybrid genetic algorithm outperforms all other existing exact and heuristic algorithms for this problem.|Gaofeng Huang,Andrew Lim","57756|GECCO|2006|A fast hybrid genetic algorithm for the quadratic assignment problem|Genetic algorithms (GAs) have recently become very popular by solving combinatorial optimization problems. In this paper, we propose an extension of the hybrid genetic algorithm for the well-known combinatorial optimization problem, the quadratic assignment problem (QAP). This extension is based on the \"fast hybrid genetic algorithm\" concept. An enhanced tabu search is used in the role of the fast local improvement of solutions, whereas a robust reconstruction (mutation) strategy is responsible for maintaining a high degree of the diversity within the population. We tested our algorithm on the instances from the QAP instance library QAPLIB. The results demonstrate promising performance of the proposed algorithm.|Alfonsas Misevicius","57237|GECCO|2003|The Virtual Gene Genetic Algorithm|This paper presents the virtual gene genetic algorithm (vgGA) which is a generalization of traditional genetic algorithms that use binary linear chromosomes. In the vgGA, traditional one point crossover and mutation are implemented as arithmetic functions over the integers or reals that the chromosome represents. This implementation allows the generalization to virtual chromosomes of alphabets of any cardinality. Also, the sites where crossover and mutation fall can be generalized in the vgGA to values that do not necessarily correspond to positions between bits or digits of another base, thus implementing generalized digits. Preliminary results that indicate that the vgGA outperforms a GA with binary linear chromosomes on integer and real valued problems where the underlying structure is not binary are presented.|Manuel Valenzuela-Rendón","57037|GECCO|2003|A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem|Recently proved successful for variants of the vehicle routing problem (VRP) involving time windows, genetic algorithms have not yet shown to compete or challenge current best search techniques in solving the classical capacitated VRP. In this paper, a hybrid genetic algorithm to address the capacitated vehicle routing problem is proposed. The basic scheme consists in concurrently evolving two populations of solutions to minimize total traveled distance using genetic operators combining variations of key concepts inspired from routing techniques and search strategies used for a time-variant of the problem to further provide search guidance while balancing intensification and diversification. Results from a computational experiment over common benchmark problems report the proposed approach to be very competitive with the best-known methods.|Jean Berger,Mohamed Barkaoui","59051|GECCO|2010|The K-bit-swap a new genetic algorithm operator|Genetic algorithms (GA) mostly commonly use three main operators selection, crossover and mutation, although many others have been proposed in the literature. This article introduces a new operator, k-bit-swap, which swaps bits between two strings without preserving the location of those bits, changing their order of bits in the string. It can be considered as a form of crossover. We investigate the effects of this operator and demonstrate that its use improves the speed and performance on several well-known problems.|Aram Ter-Sarkisov,Stephen R. Marsland,Barbara R. Holland"],["13979|IJCAI|1983|Model Structuring and Concept Recognition Two Aspects of Learning for a Mobile Robot|We present here a method for providing a mobile robot with learning capabilities. The method is based on a model of the environment with several hierarchical levels organized by degree of abstraction. The mathematical structuring tool used is the decomposition of a graph into its k-connected components (k and k). This structure allows the robot to improve navigation procedures and to recognize some concepts, such as a door, a room, or a corridor.|Jean-Paul Laumond","13591|IJCAI|1977|Robot Learning and Error Correction|We describe a learning paradigm designed to improve the performance of a robot in a partially unpredictable environment. The paradigm was suggested by phenomena observed in animal behavior and it models aspects of that behavior. (See FI for details.) An implementation as a working program is under way, intended for incorporation in the now operational JPL robot. Rieger's CSA system is the implementation language and includes a plan synthesizer (RI). A brief overview of the robot's existing system organization is given in Thompson's paper on robot navigation in these proceedings (TI).|Leonard Friedman","15798|IJCAI|2003|Simultaneous Adversarial Multi-Robot Learning|Multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. There has been a great deal of recent research on multiagent reinforcement learning in stochastic games, which is the intuitive extension of MDPs to multiple agents. This recent work, although general, has only been applied to small games with at most hundreds of states. On the other hand robot tasks have continuous, and often complex, state and action spaces. Robot learning tasks demand approximation and generalization techniques, which have only received extensive attention in single-agent learning. In this paper we introduce GraWoLF, a general-purpose, scalable, multiagent learning algorithm. It combines gradient-based policy learning techniques with the WoLF (\"Win or Learn Fast\") variable learning rate. We apply this algorithm to an adversarial multi-robot task with simultaneous learning. We show results of learning both in simulation and on the real robots. These results demonstrate that GraWoLF can learn successful policies, overcoming the many challenges in multi-robot learning.|Michael H. Bowling,Manuela M. Veloso","57159|GECCO|2003|Learning Features for Object Recognition|Features represent the characteristics of objects and selecting or synthesizing effective composite features are the key factors to the performance of object recognition. In this paper, we propose a co-evolutionary genetic programming (CGP) approach to learn composite features for object recognition. The motivation for using CGP is to overcome the limitations of human experts who consider only a small number of conventional combinations of primitive features during synthesis. On the other hand, CGP can try a very large number of unconventional combinations and these unconventional combinations may yield exceptionally good results in some cases. Our experimental results with real synthetic aperture radar (SAR) images show that CGP can learn good composite features. We show results to distinguish objects from clutter and to distinguish objects that belong to several classes.|Yingqiang Lin,Bir Bhanu","15679|IJCAI|2001|Mobile Robot Learning of Delayed Response Tasks through Event Extraction A Solution to the Road Sign Problem and Beyond|We show how event extraction can be used for handling delayed response tasks with arbitrary delay periods between the stimulus and the cue for response. Our approach is based on a number of information processing levels, where the lowest level works on raw time-stepped based sensory data. This data is classified using an unsupervised clustering mechanism. The second level works on this classified data, but still on the individual time-step basis. An event extraction mechanism detects and signals transitions between classes this forms the basis for the third level. As this level only is updated when events occur, it is independent of the time-scale of the lower level interaction. We also sketch how an event filtering mechanism could be constructed which discards irrelevant data from the event stream. Such a mechanism would output a fourth level representation which could be used for delayed response tasks where irrelevant, or distracting, events could occur during the delay.|Fredrik Linåker,Henrik Jacobsson","14748|IJCAI|1989|Learning Novel Domains Through Curiosity and Conjecture|This paper describes DIDO, a system we have developed to carry out exploratory learning of unfamiliar domains without assistance from an external teacher. The program incorporates novel approaches to experience generation and representation generation. The experience generator uses a heuristic based on Shannon's uncertainty function to find informative examples. The representation generator makes conjectures on the basis of small amounts of evidence and retracts them if they prove to be wrong or useless. A number of experiments arc described which demonstrate that the system can distribute its learning resources to steadily acquire a good representation of the whole of a domain, and that the system can readily acquire both disjunctive and conjunctive concepts even in the presence of noise.|Paul D. Scott,Shaul Markovitch","16402|IJCAI|2007|Color Learning on a Mobile Robot Towards Full Autonomy under Changing Illumination|A central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended period of time. It is commonly asserted that in order to do so, the agent must be able to learn to deal with unexpected environmental conditions. However an ability to learn is not sufficient. For true extended autonomy, an agent must also be able to recognize when to abandon its current model in favor of learning a new one and how to learn in its current situation. This paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. Past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a \"curriculum,\" an action sequence designed to facilitate learning. This paper introduces algorithms that enable a robot to i) devise its own curriculum and ii) recognize when the lighting conditions have changed sufficiently to warrant learning a new color map.|Mohan Sridharan,Peter Stone","57855|GECCO|2006|On-line evolutionary computation for reinforcement learning in stochastic domains|In reinforcement learning, an agent interacting with its environment strives to learn a policy that specifies, for each state it may encounter, what action to take. Evolutionary computation is one of the most promising approaches to reinforcement learning but its success is largely restricted to off-line scenarios. In on-line scenarios, an agent must strive to maximize the reward it accrues while it is learning. Temporal difference (TD) methods, another approach to reinforcement learning, naturally excel in on-line scenarios because they have selection mechanisms for balancing the need to search for better policies exploration) with the need to accrue maximal reward (exploitation). This paper presents a novel way to strike this balance in evolutionary methods by borrowing the selection mechanisms used by TD methods to choose individual actions and using them in evolution to choose policies for evaluation. Empirical results in the mountain car and server job scheduling domains demonstrate that these techniques can substantially improve evolution's on-line performance in stochastic domains.|Shimon Whiteson,Peter Stone","15752|IJCAI|2001|Reinforcement Learning in Distributed Domains Beyond Team Games|Using a distributed algorithm rather than a centralized one can be extremely beneficial in large search problems. In addition, the incorporation of machine learning techniques like Reinforcement Learning (RL) into search algorithms has often been found to improve their performance. In this article we investigate a search algorithm that combines these properties by employing RL in a distributed manner, essentially using the team game approach. We then present bi-utility search, which interleaves our distributed algorithm with (centralized) simulated annealing, by using the distributed algorithm to guide the exploration step of the simulated annealing. We investigate using these algorithms in the domain of minimizing the loss of importance-weighted communication data traversing a constellations of communication satellites. To do this we introduce the idea of running these algorithms \"on top\" of an underlying, learning-free routing algorithm. They do this by having the actions of the distributed learners be the introduction of virtual \"ghost\" traffic into the decision-making of the underlying routing algorithm, traffic that \"misleads\" the routing algorithm in a way that actually improves performance. We find that using our original distributed RL algorithm to set ghost traffic improves performance, and that bi-utility search -- a semi-distributed search algorithm that is widely applicable -- substantially outperforms both that distributed RL algorithm and (centralized) simulated annealing in our problem domain.|David Wolpert,Joseph Sill,Kagan Tumer","13710|IJCAI|1981|Learning of Sensory-Motor Schemas in a Mobile Robot|A learning system is described which was used to control a simple robot vehicle and to autonomously learn behaviour patterns. The system is loosely based on Becker's model of Intermediate Cognition.|Alan H. Bond,David H. Mott"],["15112|IJCAI|1995|Reasoning about Noisy Sensors in the Situation Calculus|Agents interacting with an incompletely known dynamic world need to be able to reason about the effects of their actions, and to gain further information about that world using sensors of some sort. Unfortunately, sensor information is inherently noisy, and in general serves only to increase the agent's degree of confidence in various propositions. Building on a general logical theory of action formalized in the situation calculus developed by Reiter and others, we propose a simple axiomatization of the effect on an agent's state of belief of taking a reading from a noisy sensor. By exploiting Reiter's solution to the frame problem, we automatically obtain that these sensor actions leave the rest of the world unaffected, and further, that non-sensor actions change the state of belief of the agent in appropriate ways.|Fahiem Bacchus,Joseph Y. Halpern,Hector J. Levesque","57253|GECCO|2003|A Specialized Island Model and Its Application in Multiobjective Optimization|This paper discusses a new model of parallel evolutionary algorithms (EAs) called the specialized island model (SIM) that can be used to generate a set of diverse non-dominated solutions to multiobjective optimization problems. This model is derived from the island model, in which an EA is divided into several subEAs that exchange individuals among them. In SIM, each subEA is responsible (i.e., specialized) for optimizing a subset of the objective functions in the original problem. The efficacy of SIM is demonstrated using a three-objective optimization problem. Seven scenarios of the model with a different number of subEAs, communication topology, and specialization are tested, and their results are compared. The results suggest that SIM effectively finds non-dominated solutions to multiobjective optimization problems.|Ningchuan Xiao,Marc P. Armstrong","15971|IJCAI|2003|Reasoning about the Interaction of Knowledge Time and Concurrent Actions in the Situation Calculus|A formal framework for specifying and developing agentsrobots must handle not only knowledge and sensing actions, but also time and concurrency. Researchers have extended the situation calculus to handle knowledge and sensing actions. Other researchers have addressed the issue of adding time and concurrent actions. Here both of these features are combined into a united logical theory of knowledge, sensing, time, and concurrency. The result preserves the solution to the frame problem of previous work, maintains the distinction between indexical and objective knowledge of time, and is capable of representing the various ways in which concurrency interacts with time and knowledge. Furthermore, a method based on regression is developed for solving the projection problem for theories specified in this version of the situation calculus.|Richard B. Scherl","15348|IJCAI|1997|Reasoning with Incomplete Initial Information and Nondeterminism in Situation Calculus|Situation Calculus is arguably the most widely studied and used formalism for reasoning about action and change. The main reason for its popularity is the ability to reason about different action sequences as explicit objects. In particular, planning can be formulated as an existence problem. This paper shows how these properties break down when incomplete information about the initial state and nondeterministic action effects are introduced, basically due to the fact that this incompleteness is not adequately manifested on the object level. A version of Situation Calculus is presented which adequately models the alternative ways the world can develop relative to a choice of actions.|Lars Karlsson","16452|IJCAI|2007|Progression of Situation Calculus Action Theories with Incomplete Information|In this paper, we propose a new progression mechanism for a restricted form of incomplete knowledge formulated as a basic action theory in the situation calculus. Specifically, we focus on functional fluents and deal directly with the possible values these fluents may have and how these values are affected by both physical and sensing actions. The method we propose is logically complete and can be calculated efficiently using database techniques under certain reasonable assumptions.|Stavros Vassos,Hector J. Levesque","15413|IJCAI|1999|An Inconsistency Tolerant Model for Belief Representation and Belief Revision|We propose a new model for representing and revising belief structures, which relies on a notion of partial language splitting and tolerates some amount of inconsistency while retaining classical logic. The model preserves an agent's ability to answer queries in a coherent way using Belnap's four-valued logic. Axioms analogous to the AGM axioms hold for this new model. The distinction between implicit and explicit beliefs is represented and psychologically plausible, computationally tractable procedures for query answering and belief base revision are obtained.|Samir Chopra,Rohit Parikh","14731|IJCAI|1989|Minimal Change and Maximal Coherence A Basis for Belief Revision and Reasoning about Actions|The study of belief revision and reasoning about actions have been two of the most active areas of research in AI. Both these areas involve reasoning about change. However very little work has been done in analyzing the principles common to both these areas. This paper presents a formal characterization of belief revision, based on the principles of minimal change and maximal coherence. This formal theory is then used to reason about actions. The resulting theory provides an elegant solution to the conceptual frame and ramification problems. It also facilitates reasoning in dynamic situations where the world changes during the execution of an action. The principles of minimal change and maximal coherence seem to unify belief revision and reasoning about actions and may form a fundamental core for reasoning about other dynamic processes that involve change.|Anand S. Rao,Norman Y. Foo","16348|IJCAI|2007|Decidable Reasoning in a Modified Situation Calculus|We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter's version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.|Yilan Gu,Mikhail Soutchanski","15337|IJCAI|1997|Reasoning about Concurrent Execution Prioritized Interrupts and Exogenous Actions in the Situation Calculus|As an alternative to planning, an approach to highlevel agent control based on concurrent program execution is considered. A formal definition in the situation calculus of such a programming language is presented and illustrated with a detailed example. The language includes facilities for prioritizing the concurrent execution, interrupting the execution when certain conditions become true, and dealing with exogenous actions. The language differs from other procedural formalisms for concurrency in that the initial state can be incompletely specified and the primitive actions can be user-defined by axioms in the situation calculus.|Giuseppe De Giacomo,Yves Lespérance,Hector J. Levesque","14784|IJCAI|1989|A Parallel Algorithm for Statistical Belief Refinement and its use in Causal Reasoning|This paper presents a new approach to efficient parallel computation of statistical inferences. This approach involves two heuristics, highest impact first and highest impact remaining, which control the speed of convergence and error estimation for an algorithm that iteratively refines degrees of belief. When applied to causal reasoning, this algorithm provides a performance solution to the qualification problem. This algorithm has been implemented and tested by a program called HITEST, which runs on parallel hardware.|Jay C. Weber"]]}}