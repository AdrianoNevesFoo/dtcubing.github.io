{"abstract":{"entropy":6.499079811080687,"topics":["based reasoning, xml data, queries data, case reasoning, based theory, query, web search, tasks text, represent data, xml documents, data streaming, time first, database query, processing data, given problem, reasoning research, large database, database data, multiple systems, given","data stream, data mining, management data, systems, learning classifier, classifier systems, learning, machine learning, support vector, learning systems, resolution process, growing interest, processing stream, xml repositories, vector machine, present learning, classifier xcs, support machine, applications data, learning approach","genetic algorithms, evolutionary algorithms, algorithms, genetic programming, particle swarm, present algorithms, optimization problem, evolutionary optimization, fitness function, optimization, particle pso, algorithms problem, algorithms search, algorithms optimization, present approach, estimation density, evolutionary computation, problem, local search, evolutionary problem","planning domain, recent years, combinatorial auctions, systems increasingly, temporal planning, probabilistic planning, important, algorithms planning, problem important, constraints problem, algorithms important, important genetic, algorithms constraints, constraints, become, mobile, communication, planning, deal, networks","information, study problem, reasoning research, study, research, current, understanding, useful, user, querying, processes, represent, intelligence, digital, theoretical, planning, efficient","xml data, data, queries data, tasks text, data streaming, processing data, queries, tasks, processing, used, provide, graph, model, target, technique, form, objects, long, way, landscape","growing interest, xml repositories, work, knowledge, available, reasoning, optimal, years, research, deployed, previous, made, describe, attention, number, best, developed","learning classifier, data stream, learning systems, classifier systems, processing stream, applications data, classifier xcs, learning networks, address problem, distributed data, applications emerging, applications, distributed, problem, networks, sources, consider, sequence, sensor, fundamental","genetic algorithms, genetic programming, present genetic, algorithms based, algorithms gas, genetic gas, genetic used, algorithms evolve, performance algorithms, genetic search, describe algorithms, genetic evolve, present programming, based, genetic population, describe genetic, describe programming, describe, genetic based, programming algorithms","particle swarm, particle pso, particle optimization, swarm optimization, estimation density, swarm pso, paper algorithms, estimation distribution, estimation algorithms, hybrid algorithms, present algorithms, optimization pso, paper optimization, distribution algorithms, swarm algorithms, algorithms optimization, novel particle, novel algorithms, novel swarm, introduce algorithms","problem important, important genetic, important, networks, representation, tools, computational, operator, first, number, time, function, sensor, size, automatically, significant, information, internet, crossover, applications","constraints problem, algorithms constraints, solving, constraints, logic, genetic, value, area, variables, algorithms, general, real, satisfiability, analyze, small, sat, dynamic, complex, programs, problem"],"ranking":[["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80508|VLDB|2005|Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases|With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.|Jost Enderle,Nicole Schneider,Thomas Seidl","80483|VLDB|2005|Content-Based Routing Different Plans for Different Data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.|Pedro Bizarro,Shivnath Babu,David J. DeWitt,Jennifer Widom","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","80468|VLDB|2005|ULoad Choosing the Right Storage for Your XML Application|A key factor for the outstanding success of database management systems is physical data independence queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query .|Andrei Arion,Véronique Benzaken,Ioana Manolescu,Ravi Vijay","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram","80550|VLDB|2005|CXHist  An On-line Classification-Based Histogram for XML String Selectivity Estimation|Query optimization in IBM's System RX, the first truly relational-XML hybrid data management system, requires accurate selectivity estimation of path-value pairs, i.e., the number of nodes in the XML tree reachable by a given path with the given text value. Previous techniques have been inadequate, because they have focused mainly on the tag-labeled paths (tree structure) of the XML data. For most real XML data, the number of distinct string values at the leaf nodes is orders of magnitude larger than the set of distinct rooted tag paths. Hence, the real challenge lies in accurate selectivity estimation of the string predicates on the leaf values reachable via a given path.In this paper, we present CXHist, a novel workload-aware histogram technique that provides accurate selectivity estimation on a broad class of XML string-based queries. CXHist builds a histogram in an on-line manner by grouping queries into buckets using their true selectivity obtained from query feedback. The set of queries associated with each bucket is summarized into feature distributions. These feature distributions mimic a Bayesian classifier that is used to route a query to its associated bucket during selectivity estimation. We show how CXHist can be used for two general types of path, string queries exact match queries and substring match queries. Experiments using a prototype show that CXHist provides accurate selectivity estimation for both exact match queries and substring match queries.|Lipyeow Lim,Min Wang,Jeffrey Scott Vitter"],["57472|GECCO|2005|Extraction of informative genes from microarray data|Identification of those genes that might anticipate the clinical behavior of different types of cancers is challenging due to availability of a smaller number of patient samples compared to huge number of genes, and the noisy nature of microarray data. After selection of some good genes based on signal-to-noise ratio, unsupervised learning like clustering and supervised learning like k-nearest neighbor (kNN) classifier are widely used in cancer researches to correlate the pathological behavior of cancers with the gene expression levels' differences in cancerous and normal tissues. By applying adaptive searches like Probabilistic Model Building Genetic Algorithm (PMBGA), it may be possible to get a smaller size gene subset that would classify patient samples more accurately than the above methods. In this paper, we propose a new PMBGA based method to extract informative genes from microarray data using Support Vector Machine (SVM) as a classifier. We apply our method to three microarray data sets and present the experimental results. Our method with SVM obtains encouraging results on those data sets as compared with the rank based method using kNN as a classifier.|Topon Kumar Paul,Hitoshi Iba","65477|AAAI|2005|Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts|Automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content. We aim at a summarization system that can replicate the quality of summaries created by humans. In this paper we investigate the machine learning method for extracting full sentences from documents based on the document semantic graph structure. In particular, we explore how the Support Vector Machines (SVM) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations. We apply two types of linguistic analysis () a simple part-of-speech tagging of noun phrases and verbs and () full logical form analysis which identifies Subject-Predicate-Object triples, and then build the semantic graphs. We train the SVM classifier to identify summary nodes and use these nodes to extract sentences. Experiments with the DUC  and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis. In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries.|Jure Leskovec,Natasa Milic-Frayling,Marko Grobelnik","65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","57264|GECCO|2005|Analysis of the initialization stage of a Pittsburgh approach learning classifier system|This paper is focused on studying the initialization stage of learning classifier systems (LCS) applying the Pittsburgh approach. It has a theoretical part where the covering probability of a random rule set is modelled and a practical part. The practical part has the objective of developing general initialization policies that have competent performance on a broad range of datasets. Two kinds of policies are tested () ways of tuning the initialization probability of the system and () smart initialization operators that create rules that are generalized versions of randomly sampled training instances. The results identify a subset of settings that are robust enough to be considered candidates to be the default initialization policy. These settings have competent performance compared to several alternative machine learning systems. Beside identifying the good policies, the experimentation made is also useful to give hints about what kind of initial solutions is the system able to process successfully to create well generalized solutions.|Jaume Bacardit","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","65427|AAAI|2005|A Hybrid GenerativeDiscriminative Approach to Semi-Supervised Classifier Design|Semi-supervised classifier design that simultaneously utilizes both labeled and unlabeled samples is a major research issue in machine learning. Existing semisupervised learning methods belong to either generative or discriminative approaches. This paper focuses on probabilistic semi-supervised classifier design and presents a hybrid approach to take advantage of the generative and discriminative approaches. Our formulation considers a generative model trained on labeled samples and a newly introduced bias correction model. Both models belong to the same model family. The proposed hybrid model is constructed by combining both generative and bias correction models based on the maximum entropy principle. The parameters of the bias correction model are estimated by using training data, and combination weights are estimated so that labeled samples are correctly classified. We use naive Bayes models as the generative models to apply the hybrid approach to text classification problems. In our experimental results on three text data sets, we confirmed that the proposed method significantly outperformed pure generative and discriminative methods when the classification performances of the both methods were comparable.|Akinori Fujino,Naonori Ueda,Kazumi Saito","65357|AAAI|2005|Robust Supervised Learning|Supervised machine learning techniques developed in the Probably Approximately Correct, Maximum A Posteriori, and Structural Risk Minimiziation frameworks typically make the assumption that the test data a learner is applied to is drawn from the same distribution as the training data. In various prominent applications of learning techniques, from robotics to medical diagnosis to process control, this assumption is violated. We consider a novel framework where a learner may influence the test distribution in a bounded way. From this framework, we derive an efficient algorithm that acts as a wrapper around a broad class of existing supervised learning algorithms while guarranteeing more robust behavior under changes in the input distribution.|J. Andrew Bagnell","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","57445|GECCO|2005|A first order logic classifier system|Motivated by the intention to increase the expressive power of learning classifier systems, we developed a new Xcs derivative, Fox-cs, where the classifier and observation languages are a subset of first order logic. We found that Fox-cs was viable at tasks in two relational task domains, poker and blocks world, which cannot be represented easily using traditional bit-string classifiers and inputs. We also found that for these tasks, the level of generality obtained by Fox-cs in the portion of population that produces optimal behaviour is consistent with Wilson's generality hypothesis.|Drew Mellor","57507|GECCO|2005|Event-driven learning classifier systems for online soccer games|This paper reports on the application of classifier systems to the acquisition of decision-making algorithms for agents in online soccer games. The objective of this research is to support changes in the video-game environment brought on by the Internet and to enable the provision of bug-free programs in a short period of time. To achieve real-time learning during a game, a bucket brigade algorithm is used to reinforce learning by classifiers and a technique for selecting learning targets according to event frequency is adopted. A hybrid system combining an existing strategy algorithm and a classifier system is also employed. In experiments that observed the outcome of , soccer games between this event-driven classifier system and a human-designed algorithm, the proposed system was found to be capable of learning effective decision-making algorithms in real time.|Yuji Sato,Ryutaro Kanno"],["57521|GECCO|2005|Breeding swarms a GAPSO hybrid|In this paper we propose a novel hybrid (GAPSO) algorithm, Breeding Swarms, combining the strengths of particle swarm optimization with genetic algorithms. The hybrid algorithm combines the standard velocity and position update rules of PSOs with the ideas of selection, crossover and mutation from GAs. We propose a new crossover operator, Velocity Propelled Averaged Crossover (VPAC), incorporating the PSO velocity vector. The VPAC crossover operator actively disperses the population preventing premature convergence. We compare the hybrid algorithm to both the standard GA and PSO models in evolving solutions to five standard function minimization problems. Results show the algorithm to be highly competitive, often outperforming both the GA and PSO.|Matthew Settles,Terence Soule","57501|GECCO|2005|Real-coded crossover as a role of kernel density estimation|This paper presents a kernel density estimation method by means of real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine the sampling strategy by means of a parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and the sampling strategy is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation.|Jun Sakuma,Shigenobu Kobayashi","57485|GECCO|2005|Understanding cooperative co-evolutionary dynamics via simple fitness landscapes|Cooperative co-evolution is often used to solve difficult optimization problems by means of problem decomposition. Its performance for such tasks can vary widely from good to disappointing. One of the reasons for this is that attempts to improve co-evolutionary performance using traditional EC analysis techniques often fail to provide the necessary insights into the dynamics of co-evolutionary systems, a key factor affecting performance. In this paper we use two simple fitness landscapes to illustrate the importance of taking a dynamical systems approach to analyzing co-evolutionary algorithms in order to understand them better and to improve their problem solving performance.|Elena Popovici,Kenneth A. De Jong","57429|GECCO|2005|The molecule evoluator an interactive evolutionary algorithm for designing drug molecules|To help chemists design new drugs, we created a tool that uses interactive evolution to design drug molecules, the \"Molecule Evoluator\". In contrast to most other evolutionary de novo design programs, the molecule representation and the set of mutations enable it to both search the chemical space of all drug like molecules extensively and to fine-tune molecular structures to the problem at hand. Additionally, we use interaction with the user as a fitness function, which is new in evolutionary algorithms in drug design. This interactivity allows the Molecule Evoluator to use the domain knowledge of the chemist to estimate the ease of synthesis and the biological activity of the compound. This knowledge can guide the optimization process and thereby improve its results. Chemists of our department using the Molecule Evoluator were able to find six novel and synthesizable druglike core structures, indicating that the Molecule Evoluator can be used as a tool to enhance the chemist's creativity.|Eric-Wubbo Lameijer,Adriaan P. IJzerman,Joost N. Kok","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57533|GECCO|2005|Alternative implementations of the Griewangk function|The well-known Griewangk function, used for evaluation of evolutionary algorithms, becomes easier as the number of dimensions grows. This paper suggests three alternative implementations that maintain function complexity for high-dimensional versions of the problem. Diagonal slices of the search landscape and local search are used to demonstrate and evaluate the difficulty of each function.|Artem Sokolov,L. Darrell Whitley,Monte Lunacek","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57461|GECCO|2005|Minimum spanning trees made easier via multi-objective optimization|Many real-world problems are multi-objective optimization problems and evolutionary algorithms are quite successful on such problems. Since the task is to compute or approximate the Pareto front, multi-objective optimization problems are considered as more difficult than single-objective problems. One should not forget that the fitness vector with respect to more than one objective contains more information that in principle can direct the search of evolutionary algorithms. Therefore, it is possible that a single-objective problem can be solved more efficiently via a generalized multi-objective model of the problem. That this is indeed the case is proved by investigating the computation of minimum spanning trees.|Frank Neumann,Ingo Wegener","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule","57483|GECCO|2005|Exploring extended particle swarms a genetic programming approach|Particle Swarm Optimisation (PSO) uses a population of particles that fly over the fitness landscape in search of an optimal solution. The particles are controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best point, while its momentum tries to keep it moving in its current direction.Previous research started exploring the possibility of evolving the force generating equations which control the particles through the use of genetic programming (GP).We independently verify the findings of the previous research and then extend it by considering additional meaningful ingredients for the PSO force-generating equations, such as global measures of dispersion and position of the swarm. We show that, on a range of problems, GP can automatically generate new PSO algorithms that outperform standard human-generated as well as some previously evolved ones.|Riccardo Poli,Cecilia Di Chio,William B. Langdon"],["65429|AAAI|2005|Fast Planning in Domains with Derived Predicates An Approach Based on Rule-Action Graphs and Local Search|The ability to express \"derived predicates\" in the formalization of a planning domain is both practically and theoretically important. In this paper, we propose an approach to planning with derived predicates where the search space consists of \"Rule-Action Graphs\", particular graphs of actions and rules representing derived predicates. We present some techniques for representing rules and reasoning with them, which are integrated into a method for planning through local search and rule-action graphs. We also propose some new heuristics for guiding the search, and some experimental results illustrating the performance of our approach. Our proposed techniques are implemented in a planner that took part in the fourth International Planning Competition showing good performance in many benchmark problems.|Alfonso Gerevini,Alessandro Saetti,Ivan Serina,Paolo Toninelli","65614|AAAI|2005|Exploiting the Structure of Hierarchical Plans in Temporal Constraint Propagation|Quantitative temporal constraints are an essential requirement for many planning domains. The HTN planning paradigm has proven to be better suited than other approaches to many applications. To date, however, efficiently integrating temporal reasoning with HTN planning has been little explored. This paper describes a means to exploit the structure of a HTN plan in performing temporal propagation on an associated Simple Temporal Network. By exploiting the natural restriction on permitted temporal constraints, the time complexity of propagation can be sharply reduced, while completeness of the inference is maintained. Empirical results indicate an order of magnitude improvement on real-world plans.|Neil Yorke-Smith","65426|AAAI|2005|Validating Plans in the Context of Processes and Exogenous Events|Complex planning domains push the boundaries of the expressive power of planning domain modelling languages. Recent extensions to the standard planning languages have included expressions for temporal, metric and resource structures. Other work has also considered how process models can be incorporated into domain models. In this paper we consider the problem of expressing and validating models containing events which are triggered as a consequence of the action of physical processes. We focus, primarily, on the validation of plans in the context of exogenous events, discussing the modelling, semantic and implementation issues that arise. Events impact not only on plans but on domain models as a whole and we also consider the problems that arise in considering the validation of event structures in domain models.|Maria Fox,Richard Howey,Derek Long","65485|AAAI|2005|Risk-Sensitive Planning with One-Switch Utility Functions Value Iteration|Decision-theoretic planning with nonlinear utility functions is important since decision makers are often risk-sensitive in high-stake planning situations. One-switch utility functions are an important class of nonlinear utility functions that can model decision makers whose decisions change with their wealth level. We study how to maximize the expected utility of a Markov decision problem for a given one-switch utility function, which is difficult since the resulting planning problem is not decomposable. We first study an approach that augments the states of the Markov decision problem with the wealth level. The properties of the resulting infinite Markov decision problem then allow us to generalize the standard risk-neutral version of value iteration from manipulating values to manipulating functions that map wealth levels to values. We use a probabilistic blocks-world example to demonstrate that the resulting risk-sensitive version of value iteration is practical.|Yaxin Liu,Sven Koenig","65404|AAAI|2005|Propositional Fragments for Knowledge Compilation and Quantified Boolean Formulae|Several propositional fragments have been considered so far as target languages for knowledge compilation and used for improving computational tasks from major AI areas (like inference, diagnosis and planning) among them are the (quite influential) ordered binary decision diagrams, prime implicates, prime implicants, \"formulae\" in decomposable negation normal form. On the other hand, the validity problem QBF for Quantified Boolean Formulae (QBF) has been acknowledged for the past few years as an important issue for AI, and many solvers have been designed for this purpose. In this paper, the complexity of restrictions of QBF obtained by imposing the matrix of the input QBF to belong to such propositional fragments is identified. Both tractability and intractability results (PSPACE-completeness) are obtained.|Sylvie Coste-Marquis,Daniel Le Berre,Florian Letombe,Pierre Marquis","65362|AAAI|2005|Using SAT and Logic Programming to Design Polynomial-Time Algorithms for Planning in Non-Deterministic Domains|We show that a Horn SAT and logic programming approach to obtain polynomial time algorithms for problem solving can be fruitfully applied to finding plans for various kinds of goals in a non-deterministic domain. We particularly focus on finding weak, strong, and strong cyclic plans for planning problems, as they are the most studied ones in the literature. We describe new algorithms for these problems and show how non-monotonic logic programming can be used to declaratively compute strong cyclic plans. As a further benefit, preferred plans among alternative candidate plans may be singled out this way. We give complexity results for weak. strong, and strong cyclic planning. Finally, we briefly discuss some of the kinds of goals in non-deterministic domains for which the approach in the paper can be used.|Chitta Baral,Thomas Eiter,Jicheng Zhao","65467|AAAI|2005|Heterogeneous Multirobot Coordination with Spatial and Temporal Constraints|Existing approaches to multirobot coordination separate scheduling and task allocation, but finding the optimal schedule with joint tasks and spatial constraints requires robots to simultaneously solve the scheduling, task allocation, and path planning problems. We present a formal description of the multirobot joint task allocation problem with heterogeneous capabilities and spatial constraints and an instantiation of the problem for the search and rescue domain. We introduce a novel declarative framework for modeling the problem as a mixed integer linear programming (MILP) problem and present a centralized anytime algorithm with error bounds. We demonstrate that our algorithm can outperform standard MILP solving techniques, greedy heuristics, and a market based approach which separates scheduling and task allocation.|Mary Koes,Illah R. Nourbakhsh,Katia P. Sycara","65483|AAAI|2005|Approximating Revenue-Maximizing Combinatorial Auctions|Designing revenue-maximizing combinatorial auctions (CAs) is a recognized open problem in mechanism design. It is unsolved even for two bidders and two items for sale. Rather than attempting to characterize the optimal auction, we focus on designing approximations (suboptimal auction mechanisms which yield high revenue). Our approximations belong to the family of virtual valuations combinatorial auctions (VVCA). VVCA is a Vickrey-Clarke-Groves (VCG) mechanism run on virtual valuations that are linear transformations of the bidders' real valuations. We pursue two approaches to constructing approximately optimal CAs. The first is to construct a VVCA with worst-case and average-case performance guarantees. We give a logarithmic approximation auction for basic important special cases of the problem ) limited supply of items on sale with additive valuations and ) unlimited supply. The second approach is to search the parameter space of VVCAs in order to obtain high-revenue mechanisms for the general problem. We introduce a series of increasingly sophisticated algorithms that use economic insights to guide the search and thus reduce the computational complexity. Our experiments demonstrate that in many cases these algorithms perform almost as well as the optimal VVCA, yield a substantial increase in revenue over the VCG mechanism and drastically outperform the straightforward algorithms in run-time.|Anton Likhodedov,Tuomas Sandholm","65457|AAAI|2005|The Deep Space Network Scheduling Problem|We describe the Deep Space Network's scheduling problem based on a user requirement language. The problem is difficult to encode by almost all existing planning and scheduling systems. We describe how it can be mapped into a system that supports metric resources, durative action, simple temporal network constraints, and task hierarchy among other language features. We also describe how we adapted a local search scheduler to generate schedules. However, we argue that the application will best serve the users if local search is combined with systematic search. We describe how an implemented systematic search can be effectively applied to rescheduling.|Bradley J. Clement,Mark D. Johnston","65482|AAAI|2005|Prottle A Probabilistic Temporal Planner|Planning with concurrent durative actions and probabilistic effects, or probabilistic temporal planning, is a relatively new area of research. The challenge is to replicate the success of modern temporal and probabilistic planners with domains that exhibit an interaction between time and uncertainty. We present a general framework for probabilistic temporal planning in which effects, the time at which they occur, and action durations are all probabilistic. This framework includes a search space that is designed for solving probabilistic temporal planning problems via heuristic search, an algorithm that has been tailored to work with it and an effective heuristic based on an extension of the planning graph data structure. Prottle is a planner that implements this framework, and can solve problems expressed in an extension of PDDL.|Iain Little,Douglas Aberdeen,Sylvie Thiébaux"],["65621|AAAI|2005|Goal-Directed Site-Independent Recommendations from Passive Observations|This paper introduces a novel method to find Web pages that satisfy the user's current information need. The method infers the user's need from the content of the pages the user has visited and the actions the user has applied to these pages. Unlike content-based systems that attempt to learn a user's long-tenn interests, our system learns user-independent patterns of behavior that identify the user's current information need, based on hisher current browsing session, then uses this information to suggest specific pages intended to address this need. Our system learns these behavior patterns from labeled data collected during a five-week user study, involving over one hundred participants working on their day-today tasks. We tested this learned model in a second phase of this same study, and found that this model can effectively identify the information needs of new users as they browse previously unseen pages, and that we can use this information to help them find relevant pages.|Tingshao Zhu,Russell Greiner,Gerald Häubl,Kevin Jewell,Robert Price","80484|VLDB|2005|Automatic Composition of Transition-based Semantic Web Services with Messaging|In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform (ii) their impact on the \"real world\" (modeled as a relational database) (iii) their transition-based behavior and (iv) the messages they can send and receive (fromto other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.|Daniela Berardi,Diego Calvanese,Giuseppe De Giacomo,Richard Hull,Massimo Mecella","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","65352|AAAI|2005|Tool Use for Autonomous Agents|The intelligent use of tools is a general and important human competence that AI research has not yet examined in depth. Other fields have studied the topic, however, with results we can compile into a broad characterization of habile (tool-using) agents. In this paper we give an overview of research on the use of physical tools, using this information to motivate the development of artificial habile agents. Specifically, we describe how research goals and methods in animal cognition overlap with those in artificial intelligence. We argue that analysis of activities of tool-using agents offers an informative way to evaluate intelligence.|Robert St. Amant,Alexander B. Wood","80497|VLDB|2005|An Efficient SQL-based RDF Querying Scheme|Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings ) They are difficult to integrate with SQL queries used in database applications, and ) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDFMATCH to query RDF data. The results of RDFMATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDFMATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDFMATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using  million RDF triples from UniProt protein and annotation data.|Eugene Inseok Chong,Souripriya Das,George Eadon,Jagannathan Srinivasan","65520|AAAI|2005|Interactive Knowledge Validation and Query Refinement in CBR|In most case-based reasoning (CBR) systems there has been little research done on validating new knowledge, specifically on how previous knowledge differs from current knowledge as a result of conceptual change. This paper proposes two methods that enable the domain expert, who is nonexpert in artificial intelligence (AI), to interactively supervise the knowledge validation process in a CBR system, and to enable dynamic updating of the system, to provide the best diagnostic questions. The first method is based on formal concept analysis which involves a graphical representation and comparison of the concepts, and a summary description highlighting the conceptual differences. We propose a dissimilarity metric for measuring the degree of variation between the previous and current concepts when a new case is added to the knowledge base. The second method involves determining unexpected classification-based association rules to form critical questions as the knowledge base gets updated.|Monica H. Ou,Geoff A. W. West,Mihai Lazarescu,Chris Clay","65374|AAAI|2005|Coordination and Adaptation in Impromptu Teams|Coordinating a team of autonomous agents is one of the major challenges in building effective multiagcnt systems. Many techniques have been devised for this problem. and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques. though. is that the team members are developed together as a whole. In many multi agent scenarios. this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills. models. strategies. and coordination mechanisms. and no external organization is imposed upon them. In particular. we propose two techniques. one adaptive and one predictive. for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.|Michael H. Bowling,Peter McCracken","65525|AAAI|2005|Redescription Mining Structure Theory and Algorithms|We introduce a new data mining problem--redescription mining--that unifies considerations of conceptual clustering, constructive induction, and logical formula discovery. Redescription mining begins with a collection of sets, views it as a propositional vocabulary, and identifies clusters of data that can be defined in at least two ways using this vocabulary. The primary contributions of this paper are conceptual and theoretical (i) we formally study the space of redescriptions underlying a dataset and characterize their intrinsic structure, (ii) we identify impossibility as well as strong possibility results about when mining redescriptions is feasible, (iii) we present several scenarios of how we can custom-build redescription mining solutions for various biases, and (iv) we outline how many problems studied in the larger machine learning community are really special cases of redescription mining. By highlighting its broad scope and relevance. we aim to establish the importance of redescription mining and make the case for a thrust in this new line of research.|Laxmi Parida,Naren Ramakrishnan","65371|AAAI|2005|Lazy Approximation for Solving Continuous Finite-Horizon MDPs|Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems. the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper, we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.|Lihong Li,Michael L. Littman","65531|AAAI|2005|A Framework for Bayesian Network Mapping|This research is motivated by the need to support inference across multiple intelligence systems involving uncertainty. Our objective is to develop a theoretical framework and related inference methods to map semantically similar variables between separate Bayesian networks in a principled way. The work is to be conducted in two steps. In the first step, we investigate the problem of formalizing the mapping between variables in two separate BNs with different semantics and distributions as pair-wise linkages. In the second step, we aim to justify the mapping between networks as a set of selected variable linkages, and then conduct inference along it.|Rong Pan,Yun Peng"],["80518|VLDB|2005|ConQuer A System for Efficient Querying Over Inconsistent Databases|Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.|Ariel Fuxman,Diego Fuxman,Renée J. Miller","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","80485|VLDB|2005|Efficient Evaluation of XQuery over Streaming Data|With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPathXQuery over data streams, we make the following three contributions. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to QizxOpen, Saxon, and Galax, our system ) is at least % faster on XMark queries with small datasets, ) is significantly faster on XMark queries with larger datasets, ) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and ) executes queries efficiently on large datasets when other systems often have memory overflows.|Xiaogang Li,Gagan Agrawal","80551|VLDB|2005|Revisiting Pipelined Parallelism in Multi-Join Query Processing|Multi-join queries are the core of any integration service that integrates data from multiple distributed data sources. Due to the large number of data sources and possibly high volumes of data, the evaluation of multi-join queries faces increasing scalability concerns. State-of-the-art parallel multi-join query processing commonly assume that the application of maximal pipelined parallelism leads to superior performance. In this paper, we instead illustrate that this assumption does not generally hold. We investigate how best to combine pipelined parallelism with alternate forms of parallelism to achieve an overall effective processing strategy. A segmented bushy processing strategy is proposed. Experimental studies are conducted on an actual software system over a cluster of high-performance PCs. The experimental results confirm that the proposed solution leads to about % improvement in terms of total processing time in comparison to existing state-of-the-art solutions.|Bin Liu,Elke A. Rundensteiner","65555|AAAI|2005|Learning Static Object Segmentation from Motion Segmentation|Dividing an image into its constituent objects can be a useful first step in many visual processing tasks, such as object classification or determining the arrangement of obstacles in an environment. Motion segmentation is a rich source of training data for learning to segment objects by their static image properties. Background subtraction can distinguish between moving objects and their surroundings, and the techniques of statistical machine learning can capture information about objects' shape, size. color, brightness, and texture properties. Presented with a new, static image, the trained model can infer the proper segmentation of the objects present in a scene. The algorithm presented in this work uses the techniques of Markov random field modeling and belief propagation inference, outperforms a standard segmentation algorithm on an object segmentation task, and outperforms a learned boundary detector at determining object boundaries on the test data.|Michael G. Ross,Leslie Pack Kaelbling","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla","80538|VLDB|2005|Bidirectional Expansion For Keyword Search on Graph Databases|Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the \"best\" answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree.In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search.|Varun Kacholia,Shashank Pandit,Soumen Chakrabarti,S. Sudarshan,Rushi Desai,Hrishikesh Karambelkar","80550|VLDB|2005|CXHist  An On-line Classification-Based Histogram for XML String Selectivity Estimation|Query optimization in IBM's System RX, the first truly relational-XML hybrid data management system, requires accurate selectivity estimation of path-value pairs, i.e., the number of nodes in the XML tree reachable by a given path with the given text value. Previous techniques have been inadequate, because they have focused mainly on the tag-labeled paths (tree structure) of the XML data. For most real XML data, the number of distinct string values at the leaf nodes is orders of magnitude larger than the set of distinct rooted tag paths. Hence, the real challenge lies in accurate selectivity estimation of the string predicates on the leaf values reachable via a given path.In this paper, we present CXHist, a novel workload-aware histogram technique that provides accurate selectivity estimation on a broad class of XML string-based queries. CXHist builds a histogram in an on-line manner by grouping queries into buckets using their true selectivity obtained from query feedback. The set of queries associated with each bucket is summarized into feature distributions. These feature distributions mimic a Bayesian classifier that is used to route a query to its associated bucket during selectivity estimation. We show how CXHist can be used for two general types of path, string queries exact match queries and substring match queries. Experiments using a prototype show that CXHist provides accurate selectivity estimation for both exact match queries and substring match queries.|Lipyeow Lim,Min Wang,Jeffrey Scott Vitter","80509|VLDB|2005|The TEXTURE Benchmark Measuring Performance of Text Queries on a Relational DBMS|We introduce a benchmark called TEXTURE (TEXT Under RElations) to measure the relative strengths and weaknesses of combining text processing with a relational workload in an RDBMS. While the well-known TREC benchmarks focus on quality, we focus on efficiency. TEXTURE is a micro-benchmark for query workloads, and considers two central text support issues that previous benchmarks did not () queries with relevance ranking, rather than those that just compute all answers, and () a richer mix of text and relational processing, reflecting the trend toward seamless integration. In developing this benchmark, we had to address the problem of generating large text collections that reflected the (performance) characteristics of a given \"seed\" collection this is essential for a controlled study of specific data characteristics and their effects on performance. In addition to presenting the benchmark, with performance numbers for three commercial DBMSs, we present and validate a synthetic generator for populating text fields.|Vuk Ercegovac,David J. DeWitt,Raghu Ramakrishnan","80598|VLDB|2005|Statistical Learning Techniques for Costing XML Queries|Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.|Ning Zhang 0002,Peter J. Haas,Vanja Josifovski,Guy M. Lohman,Chun Zhang"],["80552|VLDB|2005|A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries|Recent years have witnessed rapidly growing research attention on continuous query processing over streams , . A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem , , . Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.|Bin Liu,Yali Zhu,Mariana Jbantova,Bradley Momberger,Elke A. Rundensteiner","65469|AAAI|2005|Description Logic-Ground Knowledge Integration and Management|This abstract describes ongoing work in developing large-scale knowledge repositories. The project addresses three primary aspects of such systems integration of knowledge sources access and retrieval of stored knowledge scalable, effective repositories. Previous results have shown the effectiveness of description logic-based representations in integrating knowledge sources and the role of non-standard inferences in supporting repository reasoning tasks. Current efforts include developing general-purpose mechanisms for adapting reasoning algorithms for optimized inference under known domain structure and effective use of database technology as a large-scale knowledge base backend.|Joseph Kopena","65593|AAAI|2005|Inducing Hierarchical Process Models in Dynamic Domains|Research on inductive process modeling combines background knowledge with time-series data to construct explanatory models, but previous work has placed few constraints on search through the model space. We present an extended formalism that organizes process knowledge in a hierarchical manner, and we describe HIPM, a system that carries out constrained search for hierarchical process models. We report experiments that suggest this approach produces more accurate and plausible models with less effort. We conclude by discussing related research and directions for future work.|Ljupco Todorovski,Will Bridewell,Oren Shiran,Pat Langley","57577|GECCO|2005|Memory-based immigrants for genetic algorithms in dynamic environments|Investigating and enhancing the performance of genetic algorithms in dynamic environments have attracted a growing interest from the community of genetic algorithms in recent years. This trend reflects the fact that many real world problems are actually dynamic, which poses serious challenge to traditional genetic algorithms. Several approaches have been developed into genetic algorithms for dynamic optimization problems. Among these approches, random immigrants and memory schemes have shown to be beneficial in many dynamic problems. This paper proposes a hybrid memory and random immigrants scheme for genetic algorithms in dynamic environments. In the hybrid scheme, the best solution in memory is retrieved and acts as the base to create random immigrants to replace the worst individuals in the population. In this way, not only can diversity be maintained but it is done more efficiently to adapt the genetic algorithm to the changing environment. The experimental results based on a series of systematically constructed dynamic problems show that the proposed memory-based immigrants scheme efficiently improves the performance of genetic algorithms in dynamic environments.|Shengxiang Yang","65520|AAAI|2005|Interactive Knowledge Validation and Query Refinement in CBR|In most case-based reasoning (CBR) systems there has been little research done on validating new knowledge, specifically on how previous knowledge differs from current knowledge as a result of conceptual change. This paper proposes two methods that enable the domain expert, who is nonexpert in artificial intelligence (AI), to interactively supervise the knowledge validation process in a CBR system, and to enable dynamic updating of the system, to provide the best diagnostic questions. The first method is based on formal concept analysis which involves a graphical representation and comparison of the concepts, and a summary description highlighting the conceptual differences. We propose a dissimilarity metric for measuring the degree of variation between the previous and current concepts when a new case is added to the knowledge base. The second method involves determining unexpected classification-based association rules to form critical questions as the knowledge base gets updated.|Monica H. Ou,Geoff A. W. West,Mihai Lazarescu,Chris Clay","80545|VLDB|2005|FiST Scalable XML Document Filtering by Sequencing Twig Patterns|In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pub-sub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Pr&uumlfer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.|Joonho Kwon,Praveen Rao,Bongki Moon,Sukho Lee","80476|VLDB|2005|Personalizing XML Text Search in PimenT|A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.|Sihem Amer-Yahia,Irini Fundulaki,Prateek Jain,Laks V. S. Lakshmanan","57576|GECCO|2005|Population-based incremental learning with memory scheme for changing environments|In recent years there has been a growing interest in studying evolutionary algorithms for dynamic optimization problems due to its importance in real world applications. Several approaches have been developed, such as the memory scheme. This paper investigates the application of the memory scheme for population-based incremental learning (PBIL) algorithms, a class of evolutionary algorithms, for dynamic optimization problems. A PBIL-specific memory scheme is proposed to improve its adaptability in dynamic environments. In this memory scheme the working probability vector is stored together with the best sample it creates in the memory and is used to reactivate old environments when change occurs. Experimental study based on a series of dynamic environments shows the efficiency of the memory scheme for PBILs in dynamic environments. In this paper, the relationship between the memory scheme and the multi-population scheme for PBILs in dynamic environments is also investigated. The experimental results indicate a negative interaction of the multi-population scheme on the memory scheme for PBILs in the dynamic test environments.|Shengxiang Yang","65517|AAAI|2005|Recommender Systems Attack Types and Strategies|In the research to date, the performance of recommender systems has been extensively evaluated across various dimensions. Increasingly, the issue of robustness against malicious attack is receiving attention from the research community. In previous work, we have shown that knowledge of certain domain statistics is sufficient to allow successful attacks to be mounted against recommender systems. In this paper, we examine the extent of domain knowledge that is actually required and find that, even when little such knowledge is known, it remains possible to mount successful attacks.|Michael P. O'Mahony,Neil J. Hurley,Guenole C. M. Silvestre","80467|VLDB|2005|XML Full-Text Search Challenges and Opportunities|An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa , , , , , , , .|Sihem Amer-Yahia,Jayavel Shanmugasundaram"],["80579|VLDB|2005|Robust Real-time Query Processing with QStream|Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.|Sven Schmidt,Thomas Legler,Sebastian Schär,Wolfgang Lehner","80533|VLDB|2005|Customizable Parallel Execution of Scientific Stream Queries|Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.|Milena Ivanova,Tore Risch","57305|GECCO|2005|Extracted global structure makes local building block processing effective in XCS|Michigan-style learning classifier systems (LCSs), such as the accuracy-based XCS system, evolve distributed problem solutions represented by a population of rules. Recently, it was shown that decomposable problems may require effective processing of subsets of problem attributes, which cannot be generally assured with standard crossover operators. A number of competent crossover operators capable of effective identification and processing of arbitrary subsets of variables or string positions were proposed for genetic and evolutionary algorithms. This paper effectively introduces two competent crossover operators to XCS by incorporating techniques from competent genetic algorithms (GAs) the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of applying standard crossover operators, here a probabilistic model of the global population is built and sampled to generate offspring classifiers locally. Various offspring generation methods are introduced and evaluated. Results indicate that the performance of the proposed learning classifier systems XCSECGA and XCSBOA is similar to that of XCS with informed crossover operators that is given all information about problem structure on input and exploits this knowledge using problem-specific crossover operators.|Martin V. Butz,Martin Pelikan,Xavier Llorà,David E. Goldberg","65603|AAAI|2005|Unsupervised Activity Recognition Using Automatically Mined Common Sense|A fundamental difficulty in recognizing human activities is obtaining the labeled data needed to learn models of those activities. Given emerging sensor technology, however, it is possible to view activity data as a stream of natural language terms. Activity models are then mappings from such terms to activity names, and may be extracted from text corpora such as the web. We show that models so extracted are sufficient to automatically produce labeled segmentations of activity data with an accuracy of % over  activities, well above the .% baseline. The segmentation so obtained is sufficient to bootstrap learning, with accuracy of learned models increasing to %. To our knowledge, this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned, even for labeling.|Danny Wyatt,Matthai Philipose,Tanzeem Choudhury","57288|GECCO|2005|An abstraction agorithm for genetics-based reinforcement learning|Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect  is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.|William N. L. Browne,Dan Scott","80537|VLDB|2005|A Heartbeat Mechanism and Its Application in Gigascope|Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.|Theodore Johnson,S. Muthukrishnan,Vladislav Shkapenyuk,Oliver Spatscheck","80469|VLDB|2005|REED Robust Efficient Filtering and Event Detection in Sensor Networks|This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.|Daniel J. Abadi,Samuel Madden,Wolfgang Lindner","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","80559|VLDB|2005|Using Association Rules for Fraud Detection in Web Advertising Networks|Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.|Ahmed Metwally,Divyakant Agrawal,Amr El Abbadi","57553|GECCO|2005|Hyper-heuristics and classifier systems for solving D-regular cutting stock problems|This paper presents a method for combining concepts of Hyper-heuristics and Learning Classifier Systems for solving D Cutting Stock Problems. The idea behind Hyper-heuristics is to discover some combination of straightforward heuristics to solve a wide range of problems. To be worthwhile, such combination should outperform the single heuristics. In this paper, the Hyper-heuristic is formed using a XCS-type Learning Classifier System which learns a solution procedure when solving individual problems. The XCS evolves a behavior model which determines the possible actions (selection and placement heuristics) for given states of the problem. When tested with a collection of different problems, the method finds very competitive results for most of the cases. The testebed is composed of problems used in other similar studies in the literature. Some additional instances of the testbed were randomly generated.|Hugo Terashima-Marín,E. J. Flores-?lvarez,Peter Ross"],["57532|GECCO|2005|Unbiased tournament selection|Tournament selection is a popular form of selection which is commonly used with genetic algorithms, genetic programming and evolutionary programming. However, tournament selection introduces a sampling bias into the selection process. We review analytic results and present empirical evidence that shows this bias has a significant impact on search performance. We introduce two new forms of unbiased tournament selection that remove or reduce sampling bias in tournament selection.|Artem Sokolov,Darrell Whitley","57266|GECCO|2005|Goal-oriented preservation of essential genetic information by offspring selection|This contribution proposes an enhanced and generic selection model for Genetic Algorithms (GAs) and Genetic Programming (GP) which is able to preserve the alleles which are part of a high quality solution. Some selected aspects of these enhanced techniques are discussed exemplarily on the basis of standardized benchmark problems.|Michael Affenzeller,Stefan Wagner 0002,Stephan M. Winkler","57397|GECCO|2005|Parsing and translation of expressions by genetic programming|We have investigated the potential for using genetic programming to evolve compiler parsing and translation routines for processing arithmetic and logical expressions as they are used in a typical programming language. Parsing and translation are important and complex real-world problems for which evolved solutions must make use of a range of programming constructs. The exercise also tests the ability of genetic programming to evolve extensive and appropriate use of abstract data types - namely, stacks. Experimentation suggests that the evolution of such code is achievable, provided that program function and terminal sets are judiciously chosen.|David Jackson","57456|GECCO|2005|A comparison study between genetic algorithms and bayesian optimize algorithms by novel indices|Genetic Algorithms (GAs) are a search and optimization technique based on the mechanism of evolution. Recently, another sort of population-based optimization method called Estimation of Distribution Algorithms (EDAs) have been proposed to solve the GA's defects. Although several comparison studies between GAs and EDAs have been made, little is known about differences of statistical features between them. In this paper, we propose new statistical indices which are based on the concepts of crossover and mutation, used in GAs, to analyze the behavior of the population based optimization techniques. We also show simple results of comparison studies between GAs and the Bayesian Optimization Algorithm (BOA), a well-known Estimation of Distribution Algorithms (EDAs).|Naoki Mori,Masayuki Takeda,Keinosuke Matsumoto","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57385|GECCO|2005|Open-ended robust design of analog filters using genetic programming|Most existing research on robust design using evolutionary algorithms (EA) follows the paradigm of traditional robust design, in which parameters of a design solution are tuned to improve the robustness of the system. However, the topological structure of a system may set a limit on the possible robustness achievable through parameter tuning. This paper proposes a new robust design paradigm that exploits the open-ended topological synthesis capability of genetic programming to evolve more robust systems. As a case study, a methodology for automated synthesis of dynamic systems, based on genetic programming and bond graph modeling (GPBG), is applied to evolve robust low-pass and high-pass analog filters. Compared with a traditional robust design approach based on a state-of-the-art real-parameter genetic algorithm (GA), it is shown that open-ended topology search by genetic programming with a fitness criterion rewarding robustness can evolve more robust systems with respect to parameter perturbations than what was achieved through parameter tuning alone, for our test problems.|Jianjun Hu,Xiwei Zhong,Erik D. Goodman","57408|GECCO|2005|New topologies for genetic search space|We propose three distance measures for genetic search space. One is a distance measure in the population space that is useful for understanding the working mechanism of genetic algorithms. Another is a distance measure in the solution space for K-grouping problems. This can be used for normalization in crossover. The third is a level distance measure for genetic algorithms, which is useful for measuring problem difficulty with respect to genetic algorithms. We show that the proposed measures are metrics and the measures are efficiently computed.|Yong-Hyuk Kim,Byung Ro Moon","57270|GECCO|2005|On the practical genetic algorithms|This paper offers practical design-guidelines for developing efficient genetic algorithms (GAs) to successfully solve real-world problems. As an important design component, a practical population-sizing model is presented and verified.|Chang Wook Ahn,Sanghoun Oh,Rudrapatna S. Ramakrishna","57481|GECCO|2005|An investigation into using genetic programming as a means of inducing solutions to novice procedural programming problems|The study presented in this paper forms part of a larger initiative aimed at creating a generic architecture for the development of intelligent programming tutors (IPTs) in an attempt to reduce the costs associated with building IPTs. Thus, instead of requiring the lecturer to provide solution algorithms to the programming problems that students will be tested on by the system, the generic architecture will automatically generate the solutions to these problems. This paper reports on the results of an investigation conducted to test the hypothesis that genetic programming (GP) can be used for this purpose. The paper proposes a genetic programming system for the induction of solutions to arithmetic, character and string manipulation, conditional, iterative, nested iteration, and recursive problems. The paper analyses the results of applying the proposed system to  randomly chosen novice procedural programming problems. Extensions made to the proposed system based on this analysis, namely, the implementation of the iterative structure-based algorithm (ISBA), are discussed.|Nelishia Pillay","57435|GECCO|2005|The compact classifier system motivation analysis and first results|This paper presents an initial analysis of how maximally general and accurate rules can be evolved in a Pittsburgh-style classifier system. In order to be able to perform such analysis we introduce a simple bare-bones Pittsburgh classifier systems---the compact classifier system (CCS)---based on estimation of distribution algorithms. Using a common rule encoding scheme of Pittsburgh classifier systems, CCS maintains a dynamic set of probability vectors that compactly describe a rule set. The compact genetic algorithm is used to evolve each of the initially perturbed probability vectors which represents the rules. Results show how CCS is able to evolve in a compact, simple, and elegant manner rule sets composed by maximally general and accurate rules.|Xavier Llorà,Kumara Sastry,David E. Goldberg"],["57521|GECCO|2005|Breeding swarms a GAPSO hybrid|In this paper we propose a novel hybrid (GAPSO) algorithm, Breeding Swarms, combining the strengths of particle swarm optimization with genetic algorithms. The hybrid algorithm combines the standard velocity and position update rules of PSOs with the ideas of selection, crossover and mutation from GAs. We propose a new crossover operator, Velocity Propelled Averaged Crossover (VPAC), incorporating the PSO velocity vector. The VPAC crossover operator actively disperses the population preventing premature convergence. We compare the hybrid algorithm to both the standard GA and PSO models in evolving solutions to five standard function minimization problems. Results show the algorithm to be highly competitive, often outperforming both the GA and PSO.|Matthew Settles,Terence Soule","57331|GECCO|2005|An efficient evolutionary algorithm applied to the design of two-dimensional IIR filters|This paper presents an efficient technique of designing two-dimensional IIR digital filters using a new algorithm involving the tightly coupled synergism of particle swarm optimization and differential evolution. The design task is reformulated as a constrained minimization problem and is solved by our newly developed PSO-DV (Particle Swarm Optimizer with Differentially perturbed Velocity) algorithm. Numerical results are presented. The paper also demonstrates the superiority of the proposed design method by comparing it with two recently published filter design methods.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57489|GECCO|2005|An effective use of crowding distance in multiobjective particle swarm optimization|In this paper, we present an approach that extends the Particle Swarm Optimization (PSO) algorithm to handle multiobjective optimization problems by incorporating the mechanism of crowding distance computation into the algorithm of PSO, specifically on global best selection and in the deletion method of an external archive of nondominated solutions. The crowding distance mechanism together with a mutation operator maintains the diversity of nondominated solutions in the external archive. The performance of this approach is evaluated on test functions and metrics from literature. The results show that the proposed approach is highly competitive in converging towards the Pareto front and generates a well distributed set of nondominated solutions.|Carlo R. Raquel,Prospero C. Naval Jr.","57452|GECCO|2005|Bayesian optimization models for particle swarms|We explore the use of information models as a guide for the development of single objective optimization algorithms, giving particular attention to the use of Bayesian models in a PSO context. The use of an explicit information model as the basis for particle motion provides tools for designing successful algorithms. One such algorithm is developed and shown empirically to be effective. Its relationship to other popular PSO algorithms is explored and arguments are presented that those algorithms may be developed from the same model, potentially providing new tools for their analysis and tuning.|Christopher K. Monson,Kevin D. Seppi","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57329|GECCO|2005|Improving particle swarm optimization with differentially perturbed velocity|This paper introduces a novel scheme of improving the performance of particle swarm optimization (PSO) by a vector differential operator borrowed from differential evolution (DE). Performance comparisons of the proposed method are provided against (a) the original DE, (b) the canonical PSO, and (c) three recent, high-performance PSO-variants. The new algorithm is shown to be statistically significantly better on a seven-function test suite for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57330|GECCO|2005|Two improved differential evolution schemes for faster global search|Differential evolution (DE) is well known as a simple and efficient scheme for global optimization over continuous spaces. In this paper we present two new, improved variants of DE. Performance comparisons of the two proposed methods are provided against (a) the original DE, (b) the canonical particle swarm optimization (PSO), and (c) two PSO-variants. The new DE-variants are shown to be statistically significantly better on a seven-function test bed for the following performance measures solution quality, time to find the solution, frequency of finding the solution, and scalability.|Swagatam Das,Amit Konar,Uday Kumar Chakraborty","57520|GECCO|2005|Breeding swarms a new approach to recurrent neural network training|This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in  of  tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.|Matthew Settles,Paul Nathan,Terence Soule","57483|GECCO|2005|Exploring extended particle swarms a genetic programming approach|Particle Swarm Optimisation (PSO) uses a population of particles that fly over the fitness landscape in search of an optimal solution. The particles are controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best point, while its momentum tries to keep it moving in its current direction.Previous research started exploring the possibility of evolving the force generating equations which control the particles through the use of genetic programming (GP).We independently verify the findings of the previous research and then extend it by considering additional meaningful ingredients for the PSO force-generating equations, such as global measures of dispersion and position of the swarm. We show that, on a range of problems, GP can automatically generate new PSO algorithms that outperform standard human-generated as well as some previously evolved ones.|Riccardo Poli,Cecilia Di Chio,William B. Langdon"],["57582|GECCO|2005|Linkage learning overlapping building blocks and systematic strategy for scalable recombination|This paper aims at an important, but poorly studied area in genetic algorithm (GA) field How to design the crossover operator for problems with overlapping building blocks (BBs). To investigate this issue systematically, the relationship between an inaccurate linkage model and the convergence time of GA is studied. Specifically, the effect of the error of so-called false linkage is analogized to a lower exchange probability of uniform crossover. The derived qualitative convergence-time model is used to develop a scalable recombination strategy for problems with overlapping BBs. A set of problems with circularly overlapping BBs exemplify the recombination strategy.|Tian-Li Yu,Kumara Sastry,David E. Goldberg","80492|VLDB|2005|MDL Summarization with Holes|Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S &Theta H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H. We first characterize the problem by showing that solving the -d problem can be done in time linear to the size of hierarchy, but solving the -d problem is NP-hard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.|Shaofeng Bu,Laks V. S. Lakshmanan,Raymond T. Ng","57407|GECCO|2005|Parameterized versus generative representations in structural design an empirical comparison|Any computational approach to design, including the use of evolutionary algorithms, requires the transformation of the domain-specific knowledge into a formal design representation. This is a difficult and still not completely understood process. Its critical part is the choice of a type of design representation. The paper addresses this important issue by presenting and discussing results of a large number of design experiments in which parameterized and generative representations were used. Particularly, their computational and design related advantages and disadvantages were investigated and compared.Evolutionary design experiments reported in this paper considered two classes of structural design problems, including the design of a wind bracing system and the design of an entire structural system in a tall building. Parameterized and generative representations of the structural systems were introduced and their basic features discussed. The generative representations investigated in the paper were inspired by the processes of morphogenesis occurring in nature. Specifically, one-dimensional cellular automata were used to develop, or 'grow,' structural designs from the corresponding 'design embryos.'.The conducted research led to three major conclusions. First, generative representations based on cellular automata proved to scale well with the size of the considered design problems. Second, generative representations outperformed parameterized representations in minimizing weight of the structural systems in our problem domain by generating better designs and finding them faster. Finally, extensive experimental studies showed significant differences in optimal settings for evolutionary design experiments for the two representation types. The rate of mutation operator, the size of the parent population, and the type of the evolutionary algorithm were identified as the evolutionary parameters having the largest impact on the performance of evolutionary design processes in our problem domain.|Rafal Kicinger,Tomasz Arciszewski,Kenneth A. De Jong","57313|GECCO|2005|Improving EAX with restricted -opt|Edge Assembly Crossover (EAX) is by far the most successful crossover operator in solving the traveling salesman problem (TSP) with Genetic Algorithms (GAs). Various improvements have been proposed for EAX in GA. However, some of the improvements have to make compromises between performance and solution quality. In this work, we have combined several improvements proposed in the past, including heterogeneous pair selection (HpS), iterative child generation (ICG), and -opt. We also incorporate -opt into EAX, and restricted the -opt local searches to sub-tours in the intermediates generated by EAX.Our proposed method can improve the performance of EAX with decreased number of generations, error rates, and computation time. The applications of conventional -opt and our restricted -opt concurrently have additive effect on the performance gain, and this performance improvement is more obvious in larger problems. The proposed method also enhanced the solution quality of EAX. The significances of the restricted -opt and the conventional -opt in EAX were analyzed and discussed.|Chen-hsiung Chan,Sheng-An Lee,Cheng-Yan Kao,Huai-Kuang Tsai","57360|GECCO|2005|A statistical learning theory approach of bloat|Code bloat, the excessive increase of code size, is an important issue in Genetic Programming (GP). This paper proposes a theoretical analysis of code bloat in the framework of symbolic regression in GP, from the viewpoint of Statistical Learning Theory, a well grounded mathematical toolbox for Machine Learning. Two kinds of bloat must be distinguished in that context, depending whether the target function lies in the search space or not. Then, important mathematical results are proved using classical results from Statistical Learning. Namely, the Vapnik-Chervonenkis dimension of programs is computed, and further results from Statistical Learning allow to prove that a parsimonious fitness ensures Universal Consistency (the solution minimizing the empirical error does converge to the best possible error when the number of examples goes to infinity). However, it is proved that the standard method consisting in choosing a maximal program size depending on the number of examples might still result in programs of infinitely increasing size with their accuracy a more complicated modification of the fitness is proposed that theoretically avoids unnecessary bloat while nevertheless preserving the Universal Consistency.|Sylvain Gelly,Olivier Teytaud,Nicolas Bredeche,Marc Schoenauer","65363|AAAI|2005|Hybrid Possibilistic Networks|Possibilistic networks are important tools for dealing with uncertain pieces of information. For multiply-connected networks, it is well known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of standard propagation algorithm.|Salem Benferhat,Salma Smaoui","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","57515|GECCO|2005|Discovering biological motifs with genetic programming|Choosing the right representation for a problem is important. In this article we introduce a linear genetic programming approach for motif discovery in protein families, and we also present a thorough comparison between our approach and Koza-style genetic programming using ADFs. In a study of  protein families, we demonstrate that our algorithm, given equal processing resources and no prior knowledge in shaping of datasets, consistently generates motifs that are of significantly better quality than those we found by using trees as representation. For several of the studied protein families we evolve motifs comparable to those found in Prosite, a manually curated database of protein motifs.Our linear genome gave better results than Koza-style genetic programming for  of  families. The difference is statistically significant for  of the families at the % confidence level.|Rolv Seehuus,Amund Tveit,Ole Edsberg","57498|GECCO|2005|A genetic algorithm for unmanned aerial vehicle routing|Genetic Algorithms (GAs) can efficiently produce high quality results for hard combinatorial real world problems such as the Vehicle Routing Problem (VRP). Genetic Vehicle Representation (GVR), a recent approach to solving instances of the VRP with a GA, produces competitive or superior results to the standard benchmark problems. This work extends GVR research by presenting a more precise mathematical model of GVR than in previous works and a thorough comparison of GVR to Path Based Representation approaches. A suite of metrics that measures GVR's efficiency and effectiveness provides an adequate characterization of the jagged search landscape. A new variation of a crossover operator is introduced. A previously unmentioned insight about the convergence rate of the search is also noted that is especially important to the application of a priori and dynamic routing for swarms of Unmanned Aerial Vehicles (UAVs). Results indicate that the search is robust, and it exponentially drives toward high quality solutions in relatively short time. Consequently, a GA with GVR encoding is capable of providing a state-of-the-art engine for a UAV routing system or related application.|Matthew A. Russell,Gary B. Lamont","57443|GECCO|2005|A theoretical analysis of the HIFF problem|We present a theoretical analysis of Watson's Hierarchical-if-and-only-if (HIFF) problem using a variety of tools. These include schema theory and course graining, the concept of effective fitness, and statistical analysis. We first review the use of Stephen's exact schema equations and schema basis to compute the changes in population distributions over time. We then use the tools described above to solve for the limit distributions of the  and -bit HIFF problems, and show that these limit distributions are essentially one-dimensional. We also show that a combination of fitness and the number of break points (a rough measure of distance in crossover space) in a string can be used to almost completely explain the limit distribution in the -bit HIFF problem.|Nicholas Freitag McPhee,Ellery Fussell Crane"],["65553|AAAI|2005|SymChaff A Structure-Aware Satisfiability Solver|We present a novel low-overhead framework for encoding and utilizing structural symmetry in propositional satisfiability algorithms (SAT solvers). We use the notion of complete multi-class symmetry and demonstrate the efficacy of our technique through a solver SymChaff that achieves exponential speedup by using simple tags in the specification of problems from both theory and practice. Efficient implementations of DPLL-based SAT solvers are routinely used in areas as diverse as planning, scheduling, design automation, model checking, verification, testing, and algebra. A natural feature of many application domains is the presence of symmetry, such as that amongst all trucks at a certain location in logistics planning and all wires connecting two switch boxes in an FPGA circuit. Many of these problems turn out to have a concise description in many-sorted first order logic. This description can be easily specified by the problem designer and almost as easily inferred automatically. SymChaff, an extension of the popular SAT solver zChaff, uses information obtained from the \"sorts\" in the first order logic constraints to create symmetry sets that are used to partition variables into classes and to maintain and utilize symmetry information dynamically. Current approaches designed to handle symmetry include (A) symmetry breaking predicates (SBPs), (B) pseudo-Boolean solvers with implicit representation for counting, (C) modifications of DPLL that handle symmetry dynamically, and (D) techniques based on ZBDDs. SBPs are prohibitively many, often large, and expensive to compute for problems such as the ones we report experimental results for. Pseudo-Boolean solvers are provably exponentially slow in certain symmetric situations and their implicit counting representation is not always appropriate. Suggested modifications of DPLL either work on limited global symmetry and are difficult to extend, or involve expensive algebraic group computations. Finally, techniques based on ZBDDs often do not compare well even with ordinary DPLL-based solvers. Sym-Chaff addresses and overcomes most of these limitations.|Ashish Sabharwal","57584|GECCO|2005|MRI magnet design search space analysis EDAs and a real-world problem with significant dependencies|This paper introduces the design of superconductive magnet configurations in Magnetic Resonance Imaging (MRI) systems as a challenging real-world problem for Evolutionary Algorithms (EAs). Analysis of the problem structure is conducted using a general statistical method, which could be easily applied to other problems. The results suggest that the problem is highly multimodal and likely to present a significant challenge for many algorithms. Through a series of preliminary experiments, a continuous Estimation of Distribution Algorithm (EDA) is shown to be able to generate promising designs with a small computational effort. The importance of utilizing problem-specific knowledge and the ability of an algorithm to capture dependencies in solving complex real-world problems is also highlighted.|Bo Yuan,Marcus Gallagher,Stuart Crozier","65351|AAAI|2005|The Achilles Heel of QBF|In recent years we have seen significant progress in the area of Boolean satisfiability (SAT) solving and its applications. As a new challenge, the community is now moving to investigate whether similar advances can be made in the use of Quantified Boolean Formulas (QBF). QBF provides a natural framework for capturing problem solving and planning in multi-agent settings. However, contrarily to single-agent planning, which can be effectively formulated as SAT, we show that a QBF approach to planning in a multi-agent setting leads to significant unexpected computational difficulties. We identify as a key difficulty of the QBF approach the fact that QBF solvers often end up exploring a much larger search space than the natural search space of the original problem. This is in contrast to the experience with SAT approaches. We also show how one can alleviate these problems by introducing two special QBF formulations and a new QBF solution strategy. We present experiments that show the effectiveness of our approach in terms of a significant improvement in performance compared to earlier work in this area. Our work also provides a general methodology for formulating adversarial scenarios in QBF.|Carlos Ansótegui,Carla P. Gomes,Bart Selman","65612|AAAI|2005|A Unified Framework for Representing Logic Program Updates|As a promising formulation to represent and reason about agents' dynamic behavious, logic program updates have been considerably studied recently. While similarities and differences between various approaches were discussed and evaluated by researchers, there is a lack of method to represent different logic program update approaches under a common framework. In this paper, we continue our study on a general framework for logic program conflict solving based on notions of strong and weak forgettings (Zhang, Foo, & Wang ). We show that all major logic program update approaches can be transformed into our framework, under which each update approach becomes a specific conflict solving case with certain constraints. We also investigate related computational properties for these transformations.|Yan Zhang,Norman Y. Foo","57273|GECCO|2005|A genetic algorithm encoding for a class of cardinality constraints|A genetic algorithm encoding is proposed which is able to automatically satisfy a class of important cardinality constraints where the set of distinct values of the design variables must be a subset--of cardinality not exceeding a given value--of a larger set of available items.The solution of the practically important structural optimization problem where the set of distinct values of the design variables must be a small subset of a larger set of commercially available values is used as a test-bed. Very good results have been found in the numerical experiments performed using standard binary encoding and genetic operators.|Helio J. C. Barbosa,Afonso C. C. Lemonge","65558|AAAI|2005|Performing Bayesian Inference by Weighted Model Counting|Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman ). Solving such NP-complete tasks by \"compilation to SAT\" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. ) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether \"compilation to model-counting\" could be a practical technique for solving real-world P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.|Tian Sang,Paul Beame,Henry A. Kautz","65422|AAAI|2005|Efficient Maximization in Solving POMDPs|We present a simple, yet effective improvement to the dynamic programming algorithm for solving partially observable Markov decision processes. The technique targets the vector pruning operation during the maximization step, a key source of complexity in POMDP algorithms. We identify two types of structures in the belief space and exploit them to reduce significantly the number of constraints in the linear programs used for pruning. The benefits of the new technique are evaluated both analytically and experimentally, showing that it can lead to significant performance improvement. The results open up new research opportunities to enhance the performance and scalability of several POMDP algorithms.|Zhengzhu Feng,Shlomo Zilberstein","57476|GECCO|2005|Multiobjective shape optimization with constraints based on estimation distribution algorithms and correlated information|A new approach based on Estimation Distribution Algorithms for constrained multiobjective shape optimization is proposed in this article. Pareto dominance and feasibility rules are used to handle constraints. The algorithm uses feasible and infeasible individuals to estimate the probability distribution of evolving designs. Additionally, correlation among problem design variables is used to improve exploration. The design objectives are minimum weight and minimum nodal displacement. Also, the resulting structures must fulfill three design constraints a) maximum permissible Von Misses stress, b)connectedness of the structure elements, and c) small holes are not allowed in the structure. The finite element method is used to evaluate the objective functions and stress constraint.|Sergio Ivvan Valdez Peña,Salvador Botello Rionda,Arturo Hernández Aguirre","65544|AAAI|2005|Towards Learning Stochastic Logic Programs from Proof-Banks|Stochastic logic programs combine ideas from probabilistic grammars with the expressive power of definite clause logic as such they can be considered as an extension of probabilistic context-free grammars. Motivated by an analogy with learning tree-bank grammars, we study how to learn stochastic logic programs from proof-trees. Using proof-trees as examples imposes strong logical constraints on the structure of the target stochastic logic program. These constraints can be integrated in the least general generalization (lgg) operator, which is employed to traverse the search space. Our implementation employs a greedy search guided by the maximum likelihood principle and failure-adjusted maximization. We also report on a number of simple experiments that show the promise of the approach.|Luc De Raedt,Kristian Kersting,Sunna Torge","65565|AAAI|2005|Constraint-Based Entity Matching|Entity matching is the problem of deciding if two given mentions in the data, such as \"Helen Hunt\" and \"H. M. Hunt\", refer to the same real-world entity. Numerous solutions have been developed, but they have not considered in depth the problem of exploiting integrity constraints that frequently exist in the domains. Examples of such constraints include \"a mention with age two cannot match a mention with salary K\" and \"if two paper citations match, then their authors are likely to match in the same order\". In this paper we describe a probabilistic solution to entity matching that exploits such constraints to improve matching accuracy. At the heart of the solution is a generative model that takes into account the constraints during the generation process, and provides well-defined interpretations of the constraints. We describe a novel combination of EM and relaxation labeling algorithms that efficiently learns the model, thereby matching mentions in an unsupervised way, without the need for annotated training data. Experiments on several real-world domains show that our solution can exploit constraints to significantly improve matching accuracy, by -% F-, and that the solution scales up to large data sets.|Warren Shen,Xin Li,AnHai Doan"]]},"title":{"entropy":6.245548275041034,"topics":["evolutionary for, particle swarms, particle optimization, for networks, swarms optimization, for function, models for, dynamic environments, bayesian networks, planning for, based and, for scheduling, classifier system, for dynamic, neural networks, value function, function optimization, learning games, planning with, evolutionary optimization","and the, the problem, the, artificial immune, solving problem, for the, for problem, vector machines, search space, evolutionary the, the search, support machines, support vector, and search, and, combinatorial auction, and for, for search, evolutionary computation, evolution strategies","system for, for xml, framework for, negative selection, word disambiguation, word sense, sense disambiguation, for data, natural language, the web, distributed pomdps, system data, from data, for database, data stream, database system, for efficient, query for, for web, for complex","genetic algorithm, algorithm for, genetic for, genetic programming, using genetic, algorithm the, evolutionary algorithm, genetic the, using algorithm, and genetic, and programming, with genetic, using, with algorithm, optimization algorithm, for using, using programming, parallel genetic, and algorithm, optimization with","models for, for dynamic, based and, models and, strategy for, and dynamic, markov models, dynamic, method for, for based, and for, based, estimation, incremental, selection, measures, representation","for networks, bayesian networks, heuristic for, neural networks, training for, networks, networks training, for prediction, prediction, comparison, recurrent, local, global, time, domains, structure, continuous, modeling, algorithm, and","and strategies, and evolution, vector machines, support machines, support vector, combinatorial auction, strategies for, evolution strategies, evolution for, knowledge for, knowledge and, knowledge, towards","and for, and, and system, and the, the case, for results, the results, and results, logic, efficient, large, crossover, stochastic, time, versus, complexity","for xml, for database, the web, query for, for web, and xml, for over, for mapping, for search, database, query database, query, mapping, document, relational, xquery, classification, translation, engine, inference","word disambiguation, word sense, sense disambiguation, for efficient, via, efficient, pattern, querying, matching, with, mining","algorithm for, algorithm the, evolutionary algorithm, with algorithm, optimization algorithm, multiobjective optimization, and algorithm, based algorithm, evolutionary for, genetic algorithm, hybrid genetic, hybrid algorithm, for the, for routing, for multiple, networks algorithm, hybrid for, distribution algorithm, models for, genetic for","tree, information, decision, behavior, linear, structure, dimensional, gene, automated, for"],"ranking":[["57452|GECCO|2005|Bayesian optimization models for particle swarms|We explore the use of information models as a guide for the development of single objective optimization algorithms, giving particular attention to the use of Bayesian models in a PSO context. The use of an explicit information model as the basis for particle motion provides tools for designing successful algorithms. One such algorithm is developed and shown empirically to be effective. Its relationship to other popular PSO algorithms is explored and arguments are presented that those algorithms may be developed from the same model, potentially providing new tools for their analysis and tuning.|Christopher K. Monson,Kevin D. Seppi","57354|GECCO|2005|An artificial immune network for multimodal function optimization on dynamic environments|Multimodal optimization algorithms inspired by the immune system are generally characterized by a dynamic control of the population size and by diversity maintenance along the search. One of the most popular proposals is denoted opt-aiNet (artificial immune network for optimization) and is extended here to deal with time-varying fitness functions. Additional procedures are designed to improve the overall performance and the robustness of the immune-inspired approach, giving rise to a version for dynamic optimization, denoted dopt-aiNet. Firstly, challenging benchmark problems in static multimodal optimization are considered to validate the new proposal. No parameter adjustment is necessary to adapt the algorithm according to the peculiarities of each problem. In the sequence, dynamic environments are considered, and usual evaluation indices are adopted to assess the performance of dopt-aiNet and compare with alternative solution procedures available in the literature.|Fabrício Olivetti de França,Fernando J. Von Zuben,Leandro Nunes de Castro","57585|GECCO|2005|Constrained optimization via particle evolutionary swarm optimization algorithm PESO|We introduce the PESO (Particle Evolutionary Swarm Optimization) algorithm for solving single objective constrained optimization problems. PESO algorithm proposes two new perturbation operators \"c-perturbation\" and \"m-perturbation\". The goal of these operators is to fight premature convergence and poor diversity issues observed in Particle Swarm Optimization (PSO) implementations. Constraint handling is based on simple feasibility rules. PESO is compared with respect to a highly competitive technique representative of the state-of-the-art in the area using a well-known benchmark for evolutionary constrained optimization. PESO matches most results and outperforms other PSO algorithms.|Angel Eduardo Muñoz Zavala,Arturo Hernández Aguirre,Enrique Raúl Villa Diharce","57403|GECCO|2005|Dynamic-probabilistic particle swarms|The particle swarm algorithm is usually a dynamic process, where a point in the search space to be tested depends on the previous point and the direction of movement. The process can be decomposed, and probability distributions around a center can be used instead of the usual trajectory approach. A version that is both dynamic and Gaussian looks very promising.|James Kennedy","57468|GECCO|2005|A hardware pipeline for function optimization using genetic algorithms|Genetic Algorithms (GAs) are very commonly used as function optimizers, basically due to their search capability. A number of different serial and parallel versions of GA exist. In this paper, a pipelined version of the commonly used Genetic Algorithms and a corresponding hardware platform is described. The main idea of achieving pipelined execution of different operations of GA is to use a stochastic selection function which works with the fitness value of the candidate chromosome only. The modified algorithm is termed PLGA (Pipelined Genetic Algorithm). When executed in a CGA (Classical Genetic Algorithm) framework, the stochastic selection gives comparable performances with the roulette-wheel selection. In the pipelined hardware environment, PLGA will be much faster than the CGA. When executed on similar hardware platforms, PLGA may attain a maximum speedup of four over CGA. However, if CGA is executed in a uniprocessor system the speedup is much more. A comparison of PLGA against PGA (Parallel Genetic Algorithms) shows that PLGA may be even more effective than PGAs. A scheme for realizing the hardware pipeline is also presented. Since a general function evaluation unit is essential, a detailed description of one such unit is presented.|Malay Kumar Pakhira,Rajat K. De","57433|GECCO|2005|MeSwarm memetic particle swarm optimization|In this paper, a novel variant of particle swarm optimization (PSO), named memetic particle swarm optimization algorithm (MeSwarm), is proposed for tackling the overshooting problem in the motion behavior of PSO. The overshooting problem is a phenomenon in PSO due to the velocity update mechanism of PSO. While the overshooting problem occurs, particles may be led to wrong or opposite directions against the direction to the global optimum. As a result, MeSwarm integrates the standard PSO with the Solis and Wets local search strategy to avoid the overshooting problem and that is based on the recent probability of success to efficiently generate a new candidate solution around the current particle. Thus, six test functions and a real-world optimization problem, the flexible protein-ligand docking problem are used to validate the performance of MeSwarm. The experimental results indicate that MeSwarm outperforms the standard PSO and several evolutionary algorithms in terms of solution quality.|Bo-Fu Liu,Hung-Ming Chen,Jian-Hung Chen,Shiow-Fen Hwang,Shinn-Ying Ho","57276|GECCO|2005|Fitness-based neighbor selection for multimodal function optimization|We propose a selection scheme called Fitness-based Neighbor Selection (FNS) for multimodal optimization. The FNS is aimed for ill-scaled and locally multimodal domain, both found in real-world numerical optimization problem.In FNS, selection is applied to parent-child pair that most likely belong to the same attractor. We determine such pair with statistical comparison of the fitness values sampled from region between the pairs, instead of conventional Euclidean distance. In addition, the ranks of a parent among sampled values are used to determine if the parent is replaceable. These measurements makes the algorithm scale-invariant thus robust in ill-scaled domain.|Shin Ando,Shigenobu Kobayashi","57479|GECCO|2005|Evolutionary optimization of dynamic control problems accelerated by progressive step reduction|In this paper, we describe the use of an evolutionary algorithm (EA) to solve dynamic control optimization problems in engineering. In this class of problems, a set of control variables must be manipulated over time to optimize the outcome, which is obtained by solving a set of differential equations for the state variables. A new problem-specific technique, progressive step reduction (PSR), is shown to considerably improve the efficiency of the algorithm for this application. Factorial experimentation and rigorous statistical analysis are used to determine the effects of PSR and tune the parameters of the algorithm.|Q. Tuan Pham","57277|GECCO|2005|Adaptive isolation model using data clustering for multimodal function optimization|In this paper, we propose a GA model called Adaptive Isolation Model(AIM), for multimodal optimization. It uses a data clustering algorithm to detect clusters in GA population, which identifies the attractors in the fitness landscape. Then, subpopulations which makes-up the clusters are isolated and optimized independently. Meanwhile, the region of the isolated subpopulations in the original landscape are suppressed. The isolation increases comprehensiveness, i.e., the probability of finding weaker attractors, and the overall efficiency of multimodal search. The advantage of the AIM is that it does not require distance between the optima as a presumed parameter, as it is estimated from the variancecovariance matrix of the subpopulation.Further, AIM's behavior and efficiency is equivalent to basic GA in unimodal landscape, in terms of number of evaluation. Therefore, it is applied recursively to all subpopulations until they converge to a suboptima. This makes AIM suitable for locally-multimodal landscapes, which have closely located attractors that are difficult to distinguish in the initial run.The performance of AIM is evaluated in several benchmark problems and compared to iterated hill-climbing methods.|Shin Ando,Jun Sakuma,Shigenobu Kobayashi","57399|GECCO|2005|Efficient differential evolution using speciation for multimodal function optimization|In this paper differential evolution is extended by using the notion of speciation for solving multimodal optimization problems. The proposed species-based DE (SDE) is able to locate multiple global optima simultaneously through adaptive formation of multiple species (or subpopulations) in an DE population at each iteration step. Each species functions as an DE by itself. Successive local improvements through species formation can eventually transform into global improvements in identifying multiple global optima. In this study the performance of SDE is compared with another recently proposed DE variant CrowdingDE. The computational complexity of SDE, the effect of population size and species radius on SDE are investigated. SDE is found to be more computationally efficient than CrowdingDE over a number of benchmark multimodal test functions.|Xiaodong Li"],["65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","57436|GECCO|2005|Combating user fatigue in iGAs partial ordering support vector machines and synthetic fitness|One of the daunting challenges of interactive genetic algorithms (iGAs)---genetic algorithms in which fitness measure of a solution is provided by a human rather than by a fitness function, model, or computation---is user fatigue which leads to sub-optimal solutions. This paper proposes a method to combat user fatigue by augmenting user evaluations with a synthetic fitness function. The proposed method combines partial ordering concepts, notion of non-domination from multiobjective optimization, and support vector machines to synthesize a fitness model based on user evaluation. The proposed method is used in an iGA on a simple test problem and the results demonstrate that the method actively combats user fatigue by requiring -- times less user evaluation when compared to a simple iGA.|Xavier Llorà,Kumara Sastry,David E. Goldberg,Abhimanyu Gupta,Lalitha Lakshmi","57555|GECCO|2005|Using evolutionary computation methods to support analytical models for the evolution and maintenance of conditional strategies in |Biologists have developed models to explain why different environmentally induced morphs of the same organism exist over time. Such conditional strategies are a common form of adaptation to variable environments, whereby an environmental cue allows some individuals to respond to the cue and develop into a morph that is different from the morph of individuals that do not receive the cue. Recently, these efforts have resulted in two different analytical models that give somewhat different predictions. Here we apply evolutionary computation methods to test the two analytical models. The results bear a remarkable similarity to the results of one of the two analytical models. The paper that follows presents the details of a biological application involving snails and barnacles (that occur naturally in two different morphs), moving then to an explanation of two competing mathematical models of the application. Finally, the interdisciplinary paper, which coordinates three separate research projects of a biologist, a mathematician and a computer scientist, describes the evolutionary computation methods used to support one of the two competing analytical models.|Gloria Childress Townsend,Wade N. Hazel,Rick Smock","57480|GECCO|2005|Evolutionary strategies for multi-scale radial basis function kernels in support vector machines|In support vector machines (SVM), the kernel functions which compute dot product in feature space significantly affect the performance of classifiers. Each kernel function is suitable for some tasks. A universal kernel is not possible, and the kernel must be chosen for the tasks under consideration by hand. In order to obtain a flexible kernel function, a family of radial basis function (RBF) kernels is proposed. Multi-scale RBF kernels are combined by including weights. Then, the evolutionary strategies are used to adjust these weights and the widths of the RBF kernels. The proposed kernel is proved to be a Mercer's kernel. The experimental results show that the use of multi-scale RBF kernels result in better performance than that of a single Gaussian RBF on benchmarks.|Tanasanee Phienthrakul,Boonserm Kijsirikul","57584|GECCO|2005|MRI magnet design search space analysis EDAs and a real-world problem with significant dependencies|This paper introduces the design of superconductive magnet configurations in Magnetic Resonance Imaging (MRI) systems as a challenging real-world problem for Evolutionary Algorithms (EAs). Analysis of the problem structure is conducted using a general statistical method, which could be easily applied to other problems. The results suggest that the problem is highly multimodal and likely to present a significant challenge for many algorithms. Through a series of preliminary experiments, a continuous Estimation of Distribution Algorithm (EDA) is shown to be able to generate promising designs with a small computational effort. The importance of utilizing problem-specific knowledge and the ability of an algorithm to capture dependencies in solving complex real-world problems is also highlighted.|Bo Yuan,Marcus Gallagher,Stuart Crozier","80561|VLDB|2005|SVM in Oracle Database g Removing the Barriers to Widespread Adoption of Support Vector Machines|Contemporary commercial databases are placing an increased emphasis on analytic capabilities. Data mining technology has become crucial in enabling the analysis of large volumes of data. Modern data mining techniques have been shown to have high accuracy and good generalization to novel data. However, achieving results of good quality often requires high levels of user expertise. Support Vector Machines (SVM) is a powerful state-of-the-art data mining algorithm that can address problems not amenable to traditional statistical analysis. Nevertheless, its adoption remains limited due to methodological complexities, scalability challenges, and scarcity of production quality SVM implementations. This paper describes Oracle's implementation of SVM where the primary focus lies on ease of use and scalability while maintaining high performance accuracy. SVM is fully integrated within the Oracle database framework and thus can be easily leveraged in a variety of deployment scenarios.|Boriana L. Milenova,Joseph Yarmus,Marcos M. Campos","57546|GECCO|2005|Applying metaheuristic techniques to search the space of bidding strategies in combinatorial auctions|Many non-cooperative settings that could potentially be studied using game theory are characterized by having very large strategy spaces and payoffs that are costly to compute. Best response dynamics is a method of searching for pure-strategy equilibria in games that is attractive for its simplicity and scalability (relative to more analytical approaches). However, when the cost of determining the outcome of a particular set of joint strategies is high, it is impractical to compute the payoffs of all possible responses to the other players actions. Thus, we study metaheuristic approaches--genetic algorithms and tabu search in particular--to explore the strategy space. We configure the parameters of metaheuristics to adapt to the problem of finding the best response strategy and present how it can be helpful in finding Nash equilibria of combinatorial auctions which is an important solution concept in game theory.|Ashish Sureka,Peter R. Wurman","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","57333|GECCO|2005|Transition models as an incremental approach for problem solving in evolutionary algorithms|This paper proposes an incremental approach for building solutions using evolutionary computation. It presents a simple evolutionary model called a Transition model in which partial solutions are constructed that interact to provide larger solutions. An evolutionary process is used to merge these partial solutions into a full solution for the problem at hand. The paper provides a preliminary study on the evolutionary dynamics of this model as well as an empirical comparison with other evolutionary techniques on binary constraint satisfaction.|Anne Defaweux,Tom Lenaerts,Jano I. van Hemert,Johan Parent","57438|GECCO|2005|The enhanced evolutionary tabu search and its application to the quadratic assignment problem|We describe the Enhanced Evolutionary Tabu Search (EE-TS) local search technique. The EE-TS metaheuristic technique combines Reactive Tabu Search with evolutionary computing elements proven to work well in multimodal search spaces. An initial set of solutions is generated using a stochastic heuristic operator based on Restricted Candidate List. Reactive Tabu Search is augmented with selection and recombination operators that preserve common traits between solutions while maintaining a diverse set of good solutions. EE-TS performance is applied to the Quadratic Assignment Problem using problem instances from the QAPLIB. The results show that EE-TS compares favorably against other known techniques. In most cases, EE-TS was able to find the known optimal solutions in fewer iterations. We conclude by describing the main benefits and limitations of EE-TS.|John F. McLoughlin III,Walter Cedeño"],["65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","80496|VLDB|2005|MIX A Meta-data Indexing System for XML|We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.|SungRan Cho,Nick Koudas,Divesh Srivastava","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","57327|GECCO|2005|DXCS an XCS system for distributed data mining|XCS is a flexible system for data mining due to its ability to deal with environmental changes, learn online with little prior knowledge and evolve accurate and maximally general classifiers. In this paper, we propose DXCS which is an XCS-based distributed data mining system. A MDL metric is proposed to quantify and analyze network load, and study the balance between network load and classifier accuracy in the presence of noise. The DXCS system shows promising results.|Hai Huong Dam,Hussein A. Abbass,Chris Lokan","65393|AAAI|2005|Scaling Up Word Sense Disambiguation via Parallel Texts|A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL- English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL- English all-words task, and significantly outperforms the baseline of always choosing sense  of WordNet.|Yee Seng Chan,Hwee Tou Ng","65498|AAAI|2005|Searching for Common Sense Populating Cyc from the Web|The Cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information - what is known informally as \"common sense.\" Over the last twenty years, a sufficient core of common sense knowledge has been entered into Cyc to allow it to begin effectively and flexibly supporting its most important task increasing its own store of world knowledge. In this paper, we present initial work on a method of using a combination of Cyc and the World Wide Web, accessed via Google, to assist in entering knowledge into Cyc. The long-term goal is automating the process of building a consistent, formalized representation of the world in the Cyc knowledge base via machine learning. We present preliminary results of this work and describe how we expect the knowledge acquisition process to become more accurate, faster, and more automated in the future.|Cynthia Matuszek,Michael J. Witbrock,Robert C. Kahlert,John Cabral,David Schneider,Purvesh Shah,Douglas B. Lenat","65584|AAAI|2005|Using the GEMS System for Cancer Diagnosis and Biomarker Discovery from Microarray Gene Expression Data|We will demonstrate the GEMS system for automated development and evaluation of high-quality cancer diagnostic models and biomarker discovery from microarray gene expression data. The development of GEMS was informed by the results of an extensive algorithmic evaluation using  microarray datasets. The system was further evaluated in two cross-dataset applications and using  microarray datasets. The performance of models produced by GEMS is comparable or better than the results obtained by human analysts, and these models generalize well to independent samples in cross-dataset applications. The system is freely available for download from httpwww.gems-system.org for noncommercial use.|Alexander R. Statnikov,Ioannis Tsamardinos,Constantin F. Aliferis","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Kornél G. Markó,Stefan Schulz,Udo Hahn","80498|VLDB|2005|U-DBMS A Database System for Managing Constantly-Evolving Data|In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.|Reynold Cheng,Sarvjeet Singh,Sunil Prabhakar","80593|VLDB|2005|WmXML A System for Watermarking XML Data|As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.|Xuan Zhou,HweeHwa Pang,Kian-Lee Tan,Dhruv Mangla"],["57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","57269|GECCO|2005|Inexact pattern matching using genetic algorithm|A Genetic Algorithm for graphical pattern matching based on angle matching had been proposed. It has proven quite effective in matching simple patterns. However, the algorithm needs some modifications to enhance its accuracy on pattern matching when there are some differences between two patterns in terms of numbers of nodes, shapes and rotations. This paper presents the modifications, such as the introduction of node exemption, inexact matching between straight lines and curves in the patterns, and consideration of rotational degrees of the patterns. Each angle is also given with a weight to indicate the significant degree of the angle. A multi-objective function is used to reflect the similarity between two patterns. The experiments designed to evaluate the algorithm have shown very promising results. It is highly accurate on patterns matching with dissimilarities in shapes, numbers of nodes and rotational degrees.|Surapong Auwatanamongkol","57314|GECCO|2005|MDGA motif discovery using a genetic algorithm|Computationally identifying transcription factor binding sites in the promoter regions of genes is an important problem in computational biology and has been under intensive research for a decade. To predict the binding site locations efficiently, many algorithms that incorporate either approximate or heuristic techniques have been developed. However, the prediction accuracy is not satisfactory and binding site prediction thus remains a challenging problem. In this paper, we develop an approach that can be used to predict binding site motifs using a genetic algorithm. Based on the generic framework of a genetic algorithm, the approach explores the search space of all possible starting locations of the binding site motifs in different target sequences with a population that undergoes evolution. Individuals in the population compete to participate in the crossovers and mutations occur with a certain probability. Initial experiments demonstrated that our approach could achieve high prediction accuracy in a small amount of computation time. A promising advantage of our approach is the fact that the computation time does not explicitly depend on the length of target sequences and hence may not increase significantly when the target sequences become very long.|Dongsheng Che,Yinglei Song,Khaled Rasheed","57499|GECCO|2005|Multiobjective VLSI cell placement using distributed genetic algorithm|Genetic Algorithms have worked fairly well for the VLSI cell placement problem, albeit with significant run times. Two parallel models for GA are presented for VLSI cell placement where the objectives are optimizing power dissipation, timing performance and interconnect wirelength, while layout width is a constraint. A Master-Slave approach is mentioned wherein both fitness calculation and crossover mechanism are distributed among slaves. A Multi-Deme parallel GA is also presented in which each processor works independently on an allocated subpopulation followed by information exchange through migration of chromosomes. A pseudo-diversity approach is taken, wherein similar solutions with the same overall cost values are not permitted in the population at any given time. A series of experiments are performed on ISCAS- benchmarks to show the performance of the Multi-Deme approach.|Sadiq M. Sait,Mohammed Faheemuddin,Mahmood R. Minhas,Syed Sanaullah","57450|GECCO|2005|Evolution of a human-competitive quantum fourier transform algorithm using genetic programming|In this paper, we show how genetic programming (GP) can be used to evolve system-size-independent quantum algorithms, and present a human-competitive Quantum Fourier Transform (QFT) algorithm evolved by GP.|Paul Massey,John A. Clark,Susan Stepney","57428|GECCO|2005|Multiplex PCR primer design for gene family using genetic algorithm|The multiplex PCR experiment is to amplify multiple regions of a DNA sequence at the same time by using different primer pairs. Designing feasible primer pairs for multiplex PCR is a tedious task since there are too many constraints to be satisfied. In this paper, a new method for multiplex PCR primer design strategy using genetic algorithm is proposed. The proposed algorithm is able to find a set of suitable primer pairs more efficient and uses a MAP model to speed up the examination of the specificity constraint that is important for gene family sequences. The dry-dock experiment shows that the proposed algorithm finds several sets of primer pairs of gene family sequences for multiplex PCR that not only obey the design properties, but also have specificity.|Hong-Long Liang,Chungnan Lee,Jain-Shing Wu","57495|GECCO|2005|Mission planning for joint suppression of enemy air defenses using a genetic algorithm|In this paper we present a genetic algorithm applied to the problem of mission planning for Joint Suppression of Enemy Air Defenses (JSEAD) in support of air strike operations. The stochastic nature of JSEAD scenarios and the complexity of JSEAD operations and interactions make this an especially challenging problem within the military domain. JSEAD planners and analysts stand to benefit from any advances in tools that address this problem. While our interest in this subject is broad, in this paper we are specifically investigating methods for developing robust plans that include routes for JSEAD assets, target types, firing ranges, and take off time, subject to multiple objective functions that capture different aspects of mission performance. The multi-objective optimization is performed by the Dynamic Non-Dominated Sorting GA (DNSGA), a non-elitist variant of NSGA-II. The objective functions are evaluated using a stochastic agent-based JSEAD simulation, and we assess the quality of mission plans produced by the GA in a set of test scenarios. The results from these tests indicate that our approach has significant promise as a component of a JSEAD mission planning tool.|Jeffrey P. Ridder,Jason C. HandUber","57432|GECCO|2005|Primer design for multiplex PCR using a genetic algorithm|Multiplex Polymerase Chain Reaction (PCR) experiments are used for amplifying several segments of the target DNA simultaneously and thereby to conserve template DNA, reduce the experimental time, and minimize the experimental expense. The success of the experiment is dependent on primer design. However, this can be a dreary task as there are many constrains such as melting temperatures, primer length, GC content and complementarity that need to be optimized to obtain a good PCR product. Motivated by the lack of primer design tools for multiplex PCR genotypic assay, we propose a multiplex PCR primer design tool using a genetic algorithm, which is a stochastic approach based on the concept of biological evolution, biological genetics and genetic operations on chromosomes, to find an optimal selection of primer pairs for multiplex PCR experiments. The presented experimental results indicate that the proposed algorithm is capable of finding a series of primer pairs that obeies the design properties in the same tube.|Feng-Mao Lin,Hsien-Da Huang,Hsi-Yuan Huang,Jorng-Tzong Horng","57309|GECCO|2005|Optimization of passenger car design for the mitigation of pedestrian head injury using a genetic algorithm|The problem of pedestrian injury is a significant one throughout the world. In , there were  pedestrian fatalities in Europe and  in the US. Significant advances have been made by automotive safety researchers and vehicle manufacturers to address this issue with respect to the design of vehicles, but the complex nature of pedestrian accident scenarios has resulted in great difficulty when using traditional statistical methods. Specifically, problems have been encountered when attempting to study the effects of individual parameters of vehicle front-end geometry on pedestrian head injury. This paper attempts to demonstrate the feasibility of applying the field of evolutionary computation to the problem of pedestrian safety by using a simple genetic algorithm to optimize the centre-line geometry of a car's front-end for the reduction of pedestrian head and thoracic injury. The fitness of each design is assessed by creating a multi-body mathematical model of the vehicle front and simulating impacts with models of different sized pedestrians, and ranking according to the combined injury scores.|Emma Carter,Steve Ebdon,Clive Neal-Sturgess","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["65593|AAAI|2005|Inducing Hierarchical Process Models in Dynamic Domains|Research on inductive process modeling combines background knowledge with time-series data to construct explanatory models, but previous work has placed few constraints on search through the model space. We present an extended formalism that organizes process knowledge in a hierarchical manner, and we describe HIPM, a system that carries out constrained search for hierarchical process models. We report experiments that suggest this approach produces more accurate and plausible models with less effort. We conclude by discussing related research and directions for future work.|Ljupco Todorovski,Will Bridewell,Oren Shiran,Pat Langley","57343|GECCO|2005|Interactive estimation of agent-based financial markets models modularity and learning|Building upon the interactive inversion method introduced by Ashburn and Bonabeau (), we show how to dramatically improve the results by exploiting modularity and by letting the computer learn user preferences.|M. Ihsan Ecemis,Eric Bonabeau,Trent Ashburn","57577|GECCO|2005|Memory-based immigrants for genetic algorithms in dynamic environments|Investigating and enhancing the performance of genetic algorithms in dynamic environments have attracted a growing interest from the community of genetic algorithms in recent years. This trend reflects the fact that many real world problems are actually dynamic, which poses serious challenge to traditional genetic algorithms. Several approaches have been developed into genetic algorithms for dynamic optimization problems. Among these approches, random immigrants and memory schemes have shown to be beneficial in many dynamic problems. This paper proposes a hybrid memory and random immigrants scheme for genetic algorithms in dynamic environments. In the hybrid scheme, the best solution in memory is retrieved and acts as the base to create random immigrants to replace the worst individuals in the population. In this way, not only can diversity be maintained but it is done more efficiently to adapt the genetic algorithm to the changing environment. The experimental results based on a series of systematically constructed dynamic problems show that the proposed memory-based immigrants scheme efficiently improves the performance of genetic algorithms in dynamic environments.|Shengxiang Yang","65529|AAAI|2005|Function-Based Classification from D Data via Generic and Symbolic Models|We propose a novel scheme for function-based classification of objects in D images. The classification process calls for constructing a generic multi-level hierarchical description of object classes in terms of functional components. Functionality is derived from a large set of geometric attributes and relationships between object parts. Initially, the input range data describing each object instance is segmented, each object part is labeled as one of a few possible primitives, and each group of primitive parts is tagged by a functional symbol. Connections between primitive parts and functional parts at the same level in the hierarchy are labeled as well. Then, the generic multi-level hierarchical description of object classes is built using the functionalities of a number of object instances. During classification, a search through a finite graph using a probabilistic fitness measure is performed to find the best assignment of object parts to the functional structures of each class. An object is assigned to a class providing the highest fitness value. The scheme does not require a-priori knowledge about any class. We tested the proposed scheme on a database of about one thousand different D objects. The results show high accuracy in classification.|Michael Pechuk,Octavian Soldea,Ehud Rivlin","65462|AAAI|2005|Dynamic Regime Identification and Prediction Based on Observed Behavior in Electronic Marketplaces|We present a method for an autonomous agent to identify dominant market conditions, such as oversupply or scarcity. The characteristics of economic regimes are learned from historic data and used, together with real-time observable information, to identify the current market regime and to forecast market changes. The approach is validated with data from the Trading Agent Competition for Supply Chain Management.|Wolfgang Ketter","57292|GECCO|2005|Dynamic optimization of migration topology in internet-based distributed genetic algorithms|Distributed Genetic Algorithms (DGAs) designed for the Internet have to take its high communication cost into consideration. For island model GAs, the migration topology has a major impact on DGA performance. This paper describes and evaluates an adaptive migration topology optimizer that keeps the communication load low while maintaining high solution quality. Experiments on benchmark problems show that the optimized topology outperforms static or random topologies of the same degree of connectivity. The applicability of the method on real-world problems is demonstrated on a hard optimization problem in VLSI design.|Johan Berntsson,Maolin Tang","65424|AAAI|2005|Incremental Estimation of Discrete Hidden Markov Models Based on a New Backward Procedure|We address the problem of learning discrete hidden Markov models from very long sequences of observations. Incremental versions of the Baum-Welch algorithm that approximate the -values used in the backward procedure are commonly used for this problem, since their memory complexity is independent of the sequence length. We introduce an improved incremental Baum-Welch algorithm with a new backward procedure that approximates the -values based on a one-step lookahead in the training sequence. We justify the new approach analytically, and report empirical results that show it converges faster than previous incremental algorithms.|German Florez-Larrahondo,Susan Bridges,Eric A. Hansen","57575|GECCO|2005|Probabilistic distribution models for EDA-based GP|This paper proposes a novel technique for a program evolution based on probabilistic models. In the proposed method, two probabilistic distribution models with probabilistic dependencies between variables are used together. We empirically comfirm that our proposed method has higher search performance. Thereafter, we discuss the effectiveness of its distribution models.|Kohsuke Yanai,Hitoshi Iba","57587|GECCO|2005|Search-based mutation testing for models|The efficient and effective generation of test-data from high-level models is of crucial importance in advanced modern software engineering. Empirical studies have shown that mutation testing is highly effective. This paper describes how search-based automatic test-data generation methods can be used to find mutation adequate test-sets for MatlabSimulink models.|Yuan Zhan,John A. Clark","57491|GECCO|2005|Fractional dynamic fitness functions for GA-based circuit design|This paper proposes and analyses the performance of a Genetic Algorithm (GA) using two new concepts, namely a static fitness function including a discontinuity measure and a fractional-order dynamic fitness function. The GA is adopted for the synthesis of combinational logic circuits. In both cases, experiments reveal superior results in terms of speed and convergence to achieve a solution.|Cecília Reis,José António Tenreiro Machado,J. Boaventura Cunha"],["57434|GECCO|2005|A differential evolution based incremental training method for RBF networks|The Differential Evolution (DE) is a floating-point encoded evolutionary strategy for global optimization. It has been demonstrated to be an efficient, effective, and robust optimization method, especially for problems containing continuous variables. This paper concerns applying a DE-based algorithm to training Radial Basis Function (RBF) networks with variables including centres, weights, and widths of RBFs. The proposed algorithm consists of three steps the first step is the initial tuning, which focuses on searching for the center, weight, and width of a one-node RBF network, the second step is the local tuning, which optimizes the three variables of the one-node RBF network --- its centre, weight, and width, and the third step is the global tuning, which optimizes all the parameters of the whole network together. The second step and the third step both use the cycling scheme to find the parameters of RBF network. The Mean Square Error from the desired to actual outputs is applied as the objective function to be minimized. Training the networks is demonstrated by approximating a set of functions, using different strategies of DE. A comparison of the net performances with several approaches reported in the literature is given and shows the resulting network performs better in the tested functions. The results show that proposed method improves the compared approximation results.|Junhong Liu,Jouni Lampinen","65433|AAAI|2005|Extending Continuous Time Bayesian Networks|Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller  ), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.|Karthik Gopalratnam,Henry A. Kautz,Daniel S. Weld","80534|VLDB|2005|BATON A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.|H. V. Jagadish,Beng Chin Ooi,Quang Hieu Vu","57320|GECCO|2005|A pareto archive evolutionary strategy based radial basis function neural network training algorithm for failure rate prediction in overhead feeders|This paper outlines a radial basis function neural network approach to predict the failures in overhead distribution lines of power delivery systems. The RBF networks are trained using historical data. The network sizes and errors are simultaneously minimized using the Pareto Archive Evolutionary Strategy algorithm. Mutation of the network is carried out by invoking an orthogonal least square procedure. The performance of the proposed method was compared to a fuzzy inference approach and with multilayered perceptrons. The results suggest that this approach outperforms the other techniques for the prediction of failure rates.|Grant Cochenour,Jerad Simon,Sanjoy Das,Anil Pahwa,Surasish Nag","65576|AAAI|2005|Discriminative Training of Markov Logic Networks|Many machine learning applications require a combination of probability and first-order logic. Markov logic networks (MLNs) accomplish this by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Model parameters (i.e., clause weights) can be learned by maximizing the likelihood of a relational database, but this can be quite costly and lead to suboptimal results for any given prediction task. In this paper we propose a discriminative approach to training MLNs, one which optimizes the conditional likelihood of the query predicates given the evidence ones, rather than the joint likelihood of all predicates. We extend Collins's () voted perceptron algorithm for HMMs to MLNs by replacing the Viterbi algorithm with a weighted satisfiability solver. Experiments on entity resolution and link prediction tasks show the advantages of this approach compared to generative MLN training, as well as compared to purely probabilistic and purely logical approaches.|Parag Singla,Pedro Domingos","65591|AAAI|2005|Approximate Inference of Bayesian Networks through Edge Deletion|In this paper, we introduce two new algorithms for approximate inference of Bayesian networks that use edge deletion techniques. The first reduces a network to its maximal weight spanning tree using the Kullback-Leibler information divergence as edge weights, and then runs Pearl's algorithm on the resulting tree for linear-time inference. The second algorithm deletes edges from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound, thus placing a polynomial time bound on inference. When tested for efficiency, these two algorithms perform up to , times faster than exact techniques. See www.cis.ksu.edujasresearch.html for more information.|Julie Thornton","65363|AAAI|2005|Hybrid Possibilistic Networks|Possibilistic networks are important tools for dealing with uncertain pieces of information. For multiply-connected networks, it is well known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of standard propagation algorithm.|Salem Benferhat,Salma Smaoui","80471|VLDB|2005|Indexing Data-oriented Overlay Networks|The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.|Karl Aberer,Anwitaman Datta,Manfred Hauswirth,Roman Schmidt","57406|GECCO|2005|Compact genetic algorithm for active interval scheduling in hierarchical sensor networks|This paper introduces a novel scheduling problem called the active interval scheduling problem in hierarchical wireless sensor networks for long-term periodical monitoring applications. To improve the report sensitivity of the hierarchical wireless sensor networks, an efficient scheduling algorithm is desired. In this paper, we propose a compact genetic algorithm (CGA) to optimize the solution quality for sensor network maintenance. The experimental result shows that the proposed CGA brings better solutions in acceptable calculation time.|Ming-Hui Jin,Cheng-Yan Kao,Yu-Cheng Huang,D. Frank Hsu,Ren-Guey Lee,Chih-Kung Lee","80470|VLDB|2005|Semantic Overlay Networks|In a handful of years only, Peer-to-Peer (PP) systems have become an integral part of the Internet. After a few key successes related to music-sharing (e.g., Napster or Gnutella), they rapidly developed and are nowadays firmly established in various contexts, ranging from large-scale content distribution (Bit Torrent) to Internet telephony(Skype) or networking platforms (JXTA). The main idea behind PP is to leverage on the power of end-computers Instead of relying on central components (e.g., servers), services are powered by decentralized overlay architectures where end-computers connect to each other dynamically.|Karl Aberer,Philippe Cudré-Mauroux"],["65605|AAAI|2005|Unsupervised and Semi-Supervised Multi-Class Support Vector Machines|We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results.|Linli Xu,Dale Schuurmans","57555|GECCO|2005|Using evolutionary computation methods to support analytical models for the evolution and maintenance of conditional strategies in |Biologists have developed models to explain why different environmentally induced morphs of the same organism exist over time. Such conditional strategies are a common form of adaptation to variable environments, whereby an environmental cue allows some individuals to respond to the cue and develop into a morph that is different from the morph of individuals that do not receive the cue. Recently, these efforts have resulted in two different analytical models that give somewhat different predictions. Here we apply evolutionary computation methods to test the two analytical models. The results bear a remarkable similarity to the results of one of the two analytical models. The paper that follows presents the details of a biological application involving snails and barnacles (that occur naturally in two different morphs), moving then to an explanation of two competing mathematical models of the application. Finally, the interdisciplinary paper, which coordinates three separate research projects of a biologist, a mathematician and a computer scientist, describes the evolutionary computation methods used to support one of the two competing analytical models.|Gloria Childress Townsend,Wade N. Hazel,Rick Smock","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","57436|GECCO|2005|Combating user fatigue in iGAs partial ordering support vector machines and synthetic fitness|One of the daunting challenges of interactive genetic algorithms (iGAs)---genetic algorithms in which fitness measure of a solution is provided by a human rather than by a fitness function, model, or computation---is user fatigue which leads to sub-optimal solutions. This paper proposes a method to combat user fatigue by augmenting user evaluations with a synthetic fitness function. The proposed method combines partial ordering concepts, notion of non-domination from multiobjective optimization, and support vector machines to synthesize a fitness model based on user evaluation. The proposed method is used in an iGA on a simple test problem and the results demonstrate that the method actively combats user fatigue by requiring -- times less user evaluation when compared to a simple iGA.|Xavier Llorà,Kumara Sastry,David E. Goldberg,Abhimanyu Gupta,Lalitha Lakshmi","57480|GECCO|2005|Evolutionary strategies for multi-scale radial basis function kernels in support vector machines|In support vector machines (SVM), the kernel functions which compute dot product in feature space significantly affect the performance of classifiers. Each kernel function is suitable for some tasks. A universal kernel is not possible, and the kernel must be chosen for the tasks under consideration by hand. In order to obtain a flexible kernel function, a family of radial basis function (RBF) kernels is proposed. Multi-scale RBF kernels are combined by including weights. Then, the evolutionary strategies are used to adjust these weights and the widths of the RBF kernels. The proposed kernel is proved to be a Mercer's kernel. The experimental results show that the use of multi-scale RBF kernels result in better performance than that of a single Gaussian RBF on benchmarks.|Tanasanee Phienthrakul,Boonserm Kijsirikul","57513|GECCO|2005|Using predators and preys in evolution strategies|This poster presents an evolution strategy for single- and multi-objective optimization. The model uses the predator-prey approach from ecology to scale between both cases. Furthermore the main issue of adaptation working for single- and multi-objective problem-instances equally is discussed. Particular, the well proved self-adaptation mechanism for the mutation strengths in the single-objective case is adopted for the multi-objective one. This self-adaptation process is supported by a new strategy of competition between predators and preys. Six test functions are used to demonstrate the practicability of the model.|Karlheinz Schmitt,Jörn Mehnen,Thomas Michelitsch","80561|VLDB|2005|SVM in Oracle Database g Removing the Barriers to Widespread Adoption of Support Vector Machines|Contemporary commercial databases are placing an increased emphasis on analytic capabilities. Data mining technology has become crucial in enabling the analysis of large volumes of data. Modern data mining techniques have been shown to have high accuracy and good generalization to novel data. However, achieving results of good quality often requires high levels of user expertise. Support Vector Machines (SVM) is a powerful state-of-the-art data mining algorithm that can address problems not amenable to traditional statistical analysis. Nevertheless, its adoption remains limited due to methodological complexities, scalability challenges, and scarcity of production quality SVM implementations. This paper describes Oracle's implementation of SVM where the primary focus lies on ease of use and scalability while maintaining high performance accuracy. SVM is fully integrated within the Oracle database framework and thus can be easily leveraged in a variety of deployment scenarios.|Boriana L. Milenova,Joseph Yarmus,Marcos M. Campos","57550|GECCO|2005|Learned mutation strategies in genetic programming for evolution and adaptation of simulated snakebot|In this work we propose an approach of incorporating learned mutation strategies (LMS) in genetic programming (GP) employed for evolution and adaptation of locomotion gaits of simulated snake-like robot (Snakebot). In our approach the LMS are implemented via learned probabilistic context-sensitive grammar (LPCSG). The LPCSG is derived from the originally defined context-free grammar, which usually expresses the syntax of genetic programs in canonical GP. Applying LMS implies that the probabilities of applying each of particular production rules in LPCGS during the mutation depend on the context. These probabilities are learned from the aggregated reward values obtained from the parsed syntax of the evolved best-of-generation Snakebots. Empirically obtained results verify that LMS contributes to the improvement of computational effort of both (i) the evolution of the fastest possible locomotion gaits for various fitness conditions and (ii) the adaptation of these locomotion gaits to challenging environment and degraded mechanical abilities of Snakebot. In all of the cases considered in this study, the locomotion gaits, evolved and adapted employing GP with LMS feature higher velocity and are obtained faster than with canonical GP.|Ivan Tanev","65390|AAAI|2005|Learning Support Vector Machines from Distributed Data Sources|In this paper we address the problem of learning Support Vector Machine (SVM) classifiers from distributed data sources. We identify sufficient statistics for learning SVMs and present an algorithm that learns SVMs from distributed data by iteratively computing the set of sufficient statistics. We prove that our algorithm is exact with respect to its centralized counterpart and efficient in terms of time complexity.|Cornelia Caragea,Doina Caragea,Vasant Honavar","57525|GECCO|2005|Niching in evolution strategies|EAs have the tendency to converge quickly into a single solution. Niching methods, the extension of EAs to address this issue, have been investigated up to date mainly within the field of Genetic Algorithms (GAs). In our study we investigate the basis for niching methods within Evolution Strategies (ES), and propose the first ES niching method. Results show that this method can reliably find and maintain multiple niches even for high-dimensional problems.|Ofer M. Shir,Thomas Bäck"],["57418|GECCO|2005|Artificial immune system for solving generalized geometric problems a preliminary results|Generalized geometric programming (GGP) is an optimization method in which the objective function and constraints are nonconvex functions. Thus, a GGP problem includes multiple local optima in its solution space. When using conventional nonlinear programming methods to solve a GGP problem, local optimum may be found, or the procedure may be mathematically tedious. To find the global optimum of a GGP problem, a bio-immune-based approach is considered. This study presents an artificial immune system (AIS) including an operator to control the number of antigen-specific antibodies based on an idiotypic network hypothesis an editing operator of receptor with a Cauchy distributed random number, and a bone marrow operator used to generate diverse antibodies. The AIS method was tested with a set of published GGP problems, and their solutions were compared with the known global GGP solutions. The testing results show that the proposed approach potentially converges to the global solutions.|Jui-Yu Wu,Yun-Kung Chung","57337|GECCO|2005|Some theoretical results about the computation time of evolutionary algorithms|This paper focuses on the computation time of evolutionary algorithms. First, some exact expressions of the mean first hitting times of general evolutionary algorithms in finite search spaces are obtained theoretically by using the properties of Markov chain. Then, by introducing drift analysis and applying Dynkin's Formula, the general upper and lower bounds of the mean first hitting times of evolutionary algorithms are given rigorously under some mild conditions. These results obtained in this paper, and the analytic methods used in this paper, are widely valid for analyzing the computation time of evolutionary algorithms in any search space(finite or infinite)as long as some simple technique processes are introduced.|Lixin X. Ding,Jinghu Yu","80547|VLDB|2005|Early Hash Join A Configurable Algorithm for the Efficient and Early Production of Join Results|Minimizing both the response time to produce the first few thousand results and the overall execution time is important for interactive querying. Current join algorithms either minimize the execution time at the expense of response time or minimize response time by producing results early without optimizing the total time. We present a hash-based join algorithm, called early hash join, which can be dynamically configured at any point during join processing to tradeoff faster production of results for overall execution time. We demonstrate that varying how inputs are read has a major effect on these two factors and provide formulas that allow an optimizer to calculate the expected rate of join output and the number of IO operations performed using different input reading strategies. Experimental results show that early hash join performs significantly fewer IO operations and executes faster than other early join algorithms, especially for one-to-many joins. Its overall execution time is comparable to standard hybrid hash join, but its response time is an order of magnitude faster. Thus, early hash join can replace hybrid hash join in any situation where a fast initial response time is beneficial without the penalty in overall execution time exhibited by other early join algorithms.|Ramon Lawrence","65601|AAAI|2005|A Domain-Independent System for Case-Based Task Decomposition without Domain Theories|We propose using domain-independent task decomposition techniques for situations in which cases are the sole or the main source for domain knowledge. Our work is motivated by project planning domains, where hierarchical cases are readily available, but neither a planning domain theory nor case adaptation knowledge is available. We present DInCaD (Domain-Independent System for Case-Based Task Decomposition), a system that encompasses case retrieval, refinement, and reuse, following from the idea of reusing generalized cases to solve new problems. DInCaD consists of a case refinement procedure that reduces case over-generalization, and a similarity criterion that takes advantage of the refinement to improve case retrieval precision. We will analyze the properties of the system, and present an empirical evaluation.|Ke Xu,Héctor Muñoz-Avila","65496|AAAI|2005|Complexity-Guided Case Discovery for Case Based Reasoning|The distribution of cases in the case base is critical to the performance of a Case Based Reasoning system. The case author is given little support in the positioning of new cases during the development stage of a case base. In this paper we argue that classification boundaries represent important regions of the problem space. They are used to identify locations where new cases should be acquired. We introduce two complexity-guided algorithms which use a local complexity measure and boundary identification techniques to actively discover cases close to boundaries. The ability of these algorithms to discover new cases that significantly improve the accuracy of case bases is demonstrated on five public domain classification datasets.|Stewart Massie,Susan Craw,Nirmalie Wiratunga","65415|AAAI|2005|Strong and Uniform Equivalence in Answer-Set Programming Characterizations and Complexity Results for the Non-Ground Case|Recent research in nonmonotonic logic programming under the answer-set semantics studies different notions of equivalence. In particular, strong and uniform equivalence are proposed as useful tools for optimizing (parts of) a logic program. While previous research mainly addressed propositional (i.e., ground) programs, we deal here with the more general case of non-ground programs, and provide semantical characterizations capturing the essence of equivalence, generalizing the concepts of SE-models and UE-models, respectively, as originally introduced for propositional programs. We show that uniform equivalence is undecidable, and we give decidability results and precise complexity bounds for strong equivalence (thereby correcting a previous complexity bound for strong equivalence from the literature) as well as for uniform equivalence for finite vocabularies.|Thomas Eiter,Michael Fink,Hans Tompits,Stefan Woltran","57445|GECCO|2005|A first order logic classifier system|Motivated by the intention to increase the expressive power of learning classifier systems, we developed a new Xcs derivative, Fox-cs, where the classifier and observation languages are a subset of first order logic. We found that Fox-cs was viable at tasks in two relational task domains, poker and blocks world, which cannot be represented easily using traditional bit-string classifiers and inputs. We also found that for these tasks, the level of generality obtained by Fox-cs in the portion of population that produces optimal behaviour is consistent with Wilson's generality hypothesis.|Drew Mellor","57435|GECCO|2005|The compact classifier system motivation analysis and first results|This paper presents an initial analysis of how maximally general and accurate rules can be evolved in a Pittsburgh-style classifier system. In order to be able to perform such analysis we introduce a simple bare-bones Pittsburgh classifier systems---the compact classifier system (CCS)---based on estimation of distribution algorithms. Using a common rule encoding scheme of Pittsburgh classifier systems, CCS maintains a dynamic set of probability vectors that compactly describe a rule set. The compact genetic algorithm is used to evolve each of the initially perturbed probability vectors which represents the rules. Results show how CCS is able to evolve in a compact, simple, and elegant manner rule sets composed by maximally general and accurate rules.|Xavier Llorà,Kumara Sastry,David E. Goldberg","80567|VLDB|2005|Shuffling a Stacked Deck The Case for Partially Randomized Ranking of Search Engine Results|In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits andor in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively \"shut out,\" and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.|Sandeep Pandey,Sourashis Roy,Christopher Olston,Junghoo Cho,Soumen Chakrabarti","65544|AAAI|2005|Towards Learning Stochastic Logic Programs from Proof-Banks|Stochastic logic programs combine ideas from probabilistic grammars with the expressive power of definite clause logic as such they can be considered as an extension of probabilistic context-free grammars. Motivated by an analogy with learning tree-bank grammars, we study how to learn stochastic logic programs from proof-trees. Using proof-trees as examples imposes strong logical constraints on the structure of the target stochastic logic program. These constraints can be integrated in the least general generalization (lgg) operator, which is employed to traverse the search space. Our implementation employs a greedy search guided by the maximum likelihood principle and failure-adjusted maximization. We also report on a number of simple experiments that show the promise of the approach.|Luc De Raedt,Kristian Kersting,Sunna Torge"],["80566|VLDB|2005|XQuery Implementation in a Relational Database System|Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging WC recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging WC recommendation XPath . for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server . XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.|Shankar Pal,Istvan Cseri,Oliver Seeliger,Michael Rys,Gideon Schaller,Wei Yu,Dragan Tomic,Adrian Baras,Brandon Berg,Denis Churin,Eugene Kogan","80576|VLDB|2005|A Faceted Query Engine Applied to Archaeology|In this demonstration, we describe a system for storing and querying faceted hierarchies. We have developed a general faceted domain model and a query language for hierarchically classified data. We present here the use of our system on two real archaeological datasets containing thousands of artifacts. Our system is a sharable, evolvable resource that can provide global access to sizeable datasets in queriable format, and can serve as a valuable tool for data analysis and research in many application domains.|Kenneth A. Ross,Angel Janevski,Julia Stoyanovich","80554|VLDB|2005|Query Caching and View Selection for XML Databases|In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath queryview answerability, which allows us to reduce tree operations to string operations for matching a queryview pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.|Bhushan Mandhani,Dan Suciu","80583|VLDB|2005|Semantic Query Optimization for XQuery over XML Streams|We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.|Hong Su,Elke A. Rundensteiner,Murali Mani","80575|VLDB|2005|Analyzing Plan Diagrams of Database Query Optimizers|A \"plan diagram\" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.|Naveen Reddy,Jayant R. Haritsa","80585|VLDB|2005|An Efficient and Versatile Query Engine for TopX Search|This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in ) the need to consider scores for XML elements while aggregating them at the document level, ) the combination of vague content conditions with XML path conditions, ) the need to relax query conditions if too few results satisfy all conditions, and ) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.|Martin Theobald,Ralf Schenkel,Gerhard Weikum","80520|VLDB|2005|The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents|This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.|Jens Graupmann,Ralf Schenkel,Gerhard Weikum","80564|VLDB|2005|Native XML Support in DB Universal Database|The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB Universal Database&reg is enhanced with comprehensive native XML support. \"Native\" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQLXML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB a true hybrid database system which places equal weight on XML and relational data management.|Matthias Nicola,Bert Van der Linden","65484|AAAI|2005|Query Translation Disambiguation as Graph Partitioning|Resolving ambiguity in the process of query translation is crucial to cross-language information retrieval when only a bilingual dictionary is available. In this paper we propose a novel approach for query translation disambiguation, named \"spectral query translation model\". The proposed approach views the problem of query translation disambiguation as a graph partitioning problem. For a given query, a weighted graph is first created for all possible translations of query words based on the co-occurrence statistics of the translation words. The best translation of the query is then determined by the most strongly connected component within the graph. The proposed approach distinguishes from previous approaches in that the translations of all query words are estimated simultaneously. Furthermore, translation probabilities are introduced in the proposed approach to capture the uncertainty in translating queries. Empirical studies with TREC datasets have shown that the spectral query translation model achieves a relative % -% improvement in cross-language information retrieval, compared to other approaches that also exploit word co-occurrence statistics for query translation disambiguation.|Yi Liu,Rong Jin","80519|VLDB|2005|Database Change Notifications Primitives for Efficient Database Query Result Caching|Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier , . The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.|César A. Galindo-Legaria,Torsten Grabs,Christian Kleinerman,Florian Waas"],["80518|VLDB|2005|ConQuer A System for Efficient Querying Over Inconsistent Databases|Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.|Ariel Fuxman,Diego Fuxman,Renée J. Miller","80590|VLDB|2005|Mining Compressed Frequent-Pattern Sets|A major challenge in frequent-pattern mining is the sheer size of its mining results. In many cases, a high minsup threshold may discover only commonsense patterns but a low one may generate an explosive number of output patterns, which severely restricts its usage.In this paper, we study the problem of compressing frequent-pattern sets. Typically, frequent patterns can be clustered with a tightness measure &delta (called &delta-cluster), and a representative pattern can be selected for each cluster. Unfortunately, finding a minimum set of representative patterns is NP-Hard. We develop two greedy methods, RPglobal and RPlocal. The former has the guaranteed compression bound but higher computational complexity. The latter sacrifices the theoretical bounds but is far more efficient. Our performance study shows that the compression quality using RPlocal is very close to RPglobal, and both can reduce the number of closed frequent patterns by almost two orders of magnitude. Furthermore, RPlocal mines even faster than FPClose, a very fast closed frequent-pattern mining method. We also show that RPglobal and RPlocal can be combined together to balance the quality and efficiency.|Dong Xin,Jiawei Han,Xifeng Yan,Hong Cheng","65537|AAAI|2005|Word Sense Disambiguation with Semi-Supervised Learning|Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on  nouns of Senseval- (SE) English lexical sample task and SE English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy.|Thanh Phong Pham,Hwee Tou Ng,Wee Sun Lee","65393|AAAI|2005|Scaling Up Word Sense Disambiguation via Parallel Texts|A critical porblem faced by current supervised WSD systems is the lack or manually annotated training data. Tackling this data acquisition bottleneck is crucial, in order to build high-accuracy and wide-coverage WSD systems. In this paper, we show that the approach of automatically gathering training examples from parallel texts is scalable to a large set of nouns. We conducted evaluation on the nouns of SENSEVAL- English all-words task, using fine-grained sense scoring. Our evaluation shows that training on examples gathered from MB of parallel texts achieves accuracy comparable to the best system of SENSEVAL- English all-words task, and significantly outperforms the baseline of always choosing sense  of WordNet.|Yee Seng Chan,Hwee Tou Ng","65528|AAAI|2005|SenseRelate  TargetWord-A Generalized Framework for Word Sense Disambiguation|Many words in natural language have different meanings when used in different contexts. Sense Relate Target Word is a Perl package that disambiguates a target word in context by finding the sense that is most related to its neighbors according to a WordNet Similarity measure of relatedness.|Siddharth Patwardhan,Satanjeev Banerjee,Ted Pedersen","65498|AAAI|2005|Searching for Common Sense Populating Cyc from the Web|The Cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information - what is known informally as \"common sense.\" Over the last twenty years, a sufficient core of common sense knowledge has been entered into Cyc to allow it to begin effectively and flexibly supporting its most important task increasing its own store of world knowledge. In this paper, we present initial work on a method of using a combination of Cyc and the World Wide Web, accessed via Google, to assist in entering knowledge into Cyc. The long-term goal is automating the process of building a consistent, formalized representation of the world in the Cyc knowledge base via machine learning. We present preliminary results of this work and describe how we expect the knowledge acquisition process to become more accurate, faster, and more automated in the future.|Cynthia Matuszek,Michael J. Witbrock,Robert C. Kahlert,John Cabral,David Schneider,Purvesh Shah,Douglas B. Lenat","80497|VLDB|2005|An Efficient SQL-based RDF Querying Scheme|Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings ) They are difficult to integrate with SQL queries used in database applications, and ) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDFMATCH to query RDF data. The results of RDFMATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDFMATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDFMATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using  million RDF triples from UniProt protein and annotation data.|Eugene Inseok Chong,Souripriya Das,George Eadon,Jagannathan Srinivasan","65493|AAAI|2005|Unsupervised Multilingual Word Sense Disambiguation via an Interlingua|We present an unsupervised method for resolving word sense ambiguities in one language by using statistical evidence assembled from other languages. It is crucial for this approach that texts are mapped into a language-independent interlingual representation. We also show that the coverage and accuracy resulting from multilingual sources outperform analyses where only monolingual training data is taken into account.|Kornél G. Markó,Stefan Schulz,Udo Hahn","65484|AAAI|2005|Query Translation Disambiguation as Graph Partitioning|Resolving ambiguity in the process of query translation is crucial to cross-language information retrieval when only a bilingual dictionary is available. In this paper we propose a novel approach for query translation disambiguation, named \"spectral query translation model\". The proposed approach views the problem of query translation disambiguation as a graph partitioning problem. For a given query, a weighted graph is first created for all possible translations of query words based on the co-occurrence statistics of the translation words. The best translation of the query is then determined by the most strongly connected component within the graph. The proposed approach distinguishes from previous approaches in that the translations of all query words are estimated simultaneously. Furthermore, translation probabilities are introduced in the proposed approach to capture the uncertainty in translating queries. Empirical studies with TREC datasets have shown that the spectral query translation model achieves a relative % -% improvement in cross-language information retrieval, compared to other approaches that also exploit word co-occurrence statistics for query translation disambiguation.|Yi Liu,Rong Jin","80553|VLDB|2005|From Region Encoding To Extended Dewey On Efficient Processing of XML Twig Pattern Matching|Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.|Jiaheng Lu,Tok Wang Ling,Chee Yong Chan,Ting Chen"],["57265|GECCO|2005|Hybrid multiobjective genetic algorithm with a new adaptive local search process|This paper is concerned with a specific brand of evolutionary algorithms Memetic algorithms. A new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges. Two performance criteria were assessed the convergence of the achieved results towards the true Pareto fronts and their distribution.|Salem F. Adra,Ian Griffin,Peter J. Fleming","57417|GECCO|2005|Designing resilient networks using a hybrid genetic algorithm approach|As high-speed networks have proliferated across the globe, their topologies have become sparser due to the increased capacity of communication media and cost considerations. Reliability has been a traditional goal within network design optimization of sparse networks. This paper proposes a genetic approach that uses network resilience as a design criterion in order to ensure the integrity of network services in the event of component failures. Network resilience measures have been previously overlooked as a network design objective in an optimization framework because of their computational complexity - requiring estimation by simulation. This paper analyzes the effect of noise in the simulation estimator used to evaluate network resilience on the performance of the proposed optimization approach.|Abdullah Konak,Alice E. Smith","57306|GECCO|2005|A hybrid evolutionary algorithm for the p-median problem|A hybrid evolutionary algorithm (EA) for the p-median problem consist of two stages, each of which is a steady-state hybrid EA. These EAs encode selections of medians as subsets of the candidate sites, apply a recombination operator tailored to the problem, and select symbols in chromosomes to mutate based on an explicit collective memory (named virtual loser). They also apply a sequence of two or three local search procedures to each new solution. Tests e.g. on the benchmark problem instances of ORLIB returned results within .% of the best solutions known.|István Borgulya","57382|GECCO|2005|Genetic drift in univariate marginal distribution algorithm|Like Darwinian-type genetic algorithms, there also exists genetic drift in Univariate Marginal Distribution Algorithm (UMDA). Since the universal analysis of genetic drift in UMDA is very difficult, in this paper, we just approach a certain kind of problem (WOneMax Problem). For WOneMax Problem, The individual space in UMDA can be denoted as a full binary tree, and the selecting process in UMDA can be considered as a process of cutting branch. We employ this binary tree to calculate the probability change of each variable between two adjacent generations. Comparing this change with our experimental data, we find that when the population size is limited, there exists genetic drift in UMDA. In order to avoid genetic drift, we model the probability of each variable as a signal with noise, and then use smoothing filter to eliminate genetic drift. Numerical results show this method is effective.|Yi Hong,Qingsheng Ren,Jin Zeng","57425|GECCO|2005|A multi-objective genetic algorithm for robust design optimization|Real-world multi-objective engineering design optimization problems often have parameters with uncontrollable variations. The aim of solving such problems is to obtain solutions that in terms of objectives and feasibility are as good as possible and at the same time are least sensitive to the parameter variations. Such solutions are said to be robust optimum solutions. In order to investigate the trade-off between the performance and robustness of optimum solutions, we present a new Robust Multi-Objective Genetic Algorithm (RMOGA) that optimizes two objectives a fitness value and a robustness index. The fitness value serves as a measure of performance of design solutions with respect to multiple objectives and feasibility of the original optimization problem. The robustness index, which is based on a non-gradient based parameter sensitivity estimation approach, is a measure that quantitatively evaluates the robustness of design solutions. RMOGA does not require a presumed probability distribution of uncontrollable parameters and also does not utilize the gradient information of these parameters. Three distance metrics are used to obtain the robustness index and robust solutions. To illustrate its application, RMOGA is applied to two well-studied engineering design problems from the literature.|Mian Li,Shapour Azarm,Vikrant Aute","57474|GECCO|2005|A hybrid genetic algorithm with pattern search for finding heavy atoms in protein crystals|One approach for determining the molecular structure of proteins is a technique called iso-morphous replacement, in which crystallographers dope protein crystals with heavy atoms, such as mercury or platinum. By comparing measured amplitudes of diffracted x-rays through protein crystals with and without the heavy atoms, the locations of the heavy atoms can be estimated. Once the locations of the heavy atoms are known, the phases of the diffracted x-rays through the protein crystal can be estimated, which in turn enables the structure of the protein to be estimated. Unfortunately, the key step in this process is the estimation of the locations of the heavy atoms, and this is a multi-modal, non-linear inverse problem. We report results of a pilot study that show that a -stage hybrid algorithm, using a stochastic genetic algorithm for stage  followed by a deterministic pattern search algorithm for stage , can successfully locate up to  heavy atoms in computer simulated crystals using noise free data. We conclude that the method may be a viable approach for finding heavy atoms in protein crystals, and suggest ways in which the approach can be scaled up to larger problems.|Joshua L. Payne,Margaret J. Eppstein","57516|GECCO|2005|A multiple objective evolutionary algorithm for multiple sequence alignment|The problem of multiple sequence alignment is important for bioinformatics. This problem is widely studied and a popular tool to solve this problem is Clustal X. This work introduces a multiple objective evolutionary algorithm to improve solutions obtained from Clustal X. The proposed method is tested with the dataset from BAliBASE database.|Pasut Seeluangsawat,Prabhas Chongstitvatana","57416|GECCO|2005|A co-evolutionary hybrid algorithm for multi-objective optimization of gene regulatory network models|In this paper, the parameters of a genetic network for rice flowering time control have been estimated using a multi-objective genetic algorithm approach. We have modified the recently introduced concept of fuzzy dominance to hybridize the well-known Nelder Mead Simplex algorithm for better exploitation with a multi-objective genetic algorithm. A co-evolutionary approach is proposed to adapt the fuzzy dominance parameters. Additional changes to the previous approach have also been incorporated here for faster convergence, including elitism. Our results suggest that this hybrid algorithm performs significantly better than NSGA-II, a standard algorithm for multi-objective optimization.|Praveen Koduru,Sanjoy Das,Stephen Welch,Judith L. Roe,Zenaida P. Lopez-Dee","57498|GECCO|2005|A genetic algorithm for unmanned aerial vehicle routing|Genetic Algorithms (GAs) can efficiently produce high quality results for hard combinatorial real world problems such as the Vehicle Routing Problem (VRP). Genetic Vehicle Representation (GVR), a recent approach to solving instances of the VRP with a GA, produces competitive or superior results to the standard benchmark problems. This work extends GVR research by presenting a more precise mathematical model of GVR than in previous works and a thorough comparison of GVR to Path Based Representation approaches. A suite of metrics that measures GVR's efficiency and effectiveness provides an adequate characterization of the jagged search landscape. A new variation of a crossover operator is introduced. A previously unmentioned insight about the convergence rate of the search is also noted that is especially important to the application of a priori and dynamic routing for swarms of Unmanned Aerial Vehicles (UAVs). Results indicate that the search is robust, and it exponentially drives toward high quality solutions in relatively short time. Consequently, a GA with GVR encoding is capable of providing a state-of-the-art engine for a UAV routing system or related application.|Matthew A. Russell,Gary B. Lamont","57271|GECCO|2005|Genetic algorithm optimization of superresolution parameters|Superresolution is the process of producing a high resolution image from a collection of low resolution images. This process has potential application in a wide spectrum of fields in which navigation, surveillance, and observation are important, yet in which target images have limited resolution. There have been numerous methods proposed and developed to implement superresolution, each with its own advantages and limitations. However, there is no standard method or software for superresolution. In this paper a genetic algorithm solution for determining the registration and point spread function (PSF) parameters for superresolution is proposed and implemented, and a superresolved image is generated using genetic algorithm optimization of an existing superresolution method.|Barry Ahrens"],["65432|AAAI|2005|TEXTAL Automated Crystallographic Protein Structure Determination|This paper reports on TEXTAL, a deployed application that uses a variety of AI techniques to automate the process of determining the D structure of proteins by x-ray crystallography. The TEXTAL project was initiated in , and the application is currently deployed in three ways () a web-based interface called WebTex, operational since June  () as the automated model-building component of an integrated crystallography software called PHENIX, first released in July  () binary distributions, available since September . TEXTAL and its sub-components are currently being used by crystallographers around the world, both in the industry and in academia. TEXTAL saves up to weeks of effort typically required to determine the structure of one protein the system has proven to be particularly helpful when the quality of the data is poor, which is very often the case. Automated protein modeling systems like TEXTAL are critical to the structural genomics initiative, a worldwide effort to determine the D structure of all proteins in a high-throughput mode, thereby keeping up with the rapid growth of genomic sequence databases.|Kreshna Gopal,Tod D. Romo,Erik McKee,Kevin Childs,Lalji Kanbi,Reetal Pai,Jacob Smith,James C. Sacchettini,Thomas R. Ioerger","80534|VLDB|2005|BATON A Balanced Tree Structure for Peer-to-Peer Networks|We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.|H. V. Jagadish,Beng Chin Ooi,Quang Hieu Vu","57286|GECCO|2005|Using a genetic algorithm to evolve behavior in multi dimensional cellular automata emergence of behavior|Cellular automata are used in many fields to generate a global behavior with local rules. Finding the rules that display a desired behavior can be a hard task especially in real world problems. This paper proposes an improved approach to generate these transition rules for multi dimensional cellular automata using a genetic algorithm, thus giving a generic way to evolve global behavior with local rules, thereby mimicking nature. Three different problems are solved using multi dimensional topologies of cellular automata to show robustness, flexibility and potential. The results suggest that using multiple dimensions makes it easier to evolve desired behavior and that combining genetic algorithms with multi dimensional cellular automata is a very powerful way to evolve very diverse behavior and has great potential for real world problems.|Ron Breukelaar,Thomas Bäck","57512|GECCO|2005|Using gene deletion and gene duplication in evolution strategies|Self-adaptation of the mutation strengths is a powerful mechanism in evolution strategies (ES), but it can fail. As a consequence premature convergence or ending up in a local optimum in multi-modal fitness landscapes can occur. In this article a new approach controlling the process of self-adaptation is proposed. This approach combines the old ideas of gene deletion and gene duplication with the self-adaptation mechanism of the ES. Gene deletion and gene duplication is used to vary the number of independent mutation strengths. In order to demonstrate the practicability of the new approach several multi-modal test functions are used. Methods from statistical design of experiments and regression tree methods are applied to improve the performance of a specific heuristic-problem combination.|Karlheinz Schmitt","65395|AAAI|2005|Constrained Decision Diagrams|A general n-ary constraint is usually represented explicitly as a set of its solution tuples, which may need exponential space. In this paper, we introduce a new representation for general n-ary constraints called Constrained Decision Diagram (CDD). CDD generalizes BDD-style representations and the main feature is that it combines constraint reasoningconsistency techniques with a compact data structure. We present an application of CDD for recording all solutions of a conjunction of constraints. Instead of an explicit representation, we can implicitly encode the solutions by means of constraint propagation. Our experiments confirm the scalability and demonstrate that CDDs can drastically reduce the space needed over explicit and ZBDD representations.|Kenil C. K. Cheng,Roland H. C. Yap","57531|GECCO|2005|Evolving fuzzy decision tree structure that adapts in real-time|A fuzzy logic algorithm has been developed that automatically allocates electronic attack (EA) resources distributed over different platforms in real-time. The controller must be able to make decisions based on rules provided by experts. The fuzzy logic approach allows the direct incorporation of expertise. Genetic algorithm based optimization is conducted to determine the form of the membership functions for the fuzzy root concepts. The resource manager is made up of five parts, the isolated platform model, the multi-platform model, the fuzzy EA model, the fuzzy parameter selection tree and the fuzzy strategy tree. Automatic determination of fuzzy decision tree topology using a genetic program, an algorithm that creates other algorithms is discussed. A comparison to subtrees obtained using a genetic program and those constructed by hand from rules is made. Experiments designed to test various concepts in the expert system are discussed, including its ability to allow multiple platforms to self-organize.|James F. Smith III","65543|AAAI|2005|Enhanced Direct Linear Discriminant Analysis for Feature Extraction on High Dimensional Data|We present an enhanced direct linear discriminant analysis (EDLDA) solution to effectively and efficiently extract discriminatory features from high dimensional data. The EDLDA integrates two types of class-wise weighting terms in estimating the average within-class and between-class scatter matrices in order to relate the resulting Fisher criterion more closely to the minimization of classification error. Furthermore, the extracted discriminant features are weighted by mutual information between features and class labels. Experimental results on four biometric datasets demonstrate the promising performance of the proposed method.|A. Kai Qin,S. Y. M. Shi,Ponnuthurai N. Suganthan,Marco Loog","57497|GECCO|2005|Classification of human decision behavior finding modular decision rules with genetic algorithms|In search tasks, for example when individuals search for the best price of a product, individuals are confronted in sequential steps with different situations and they have to decide whether to continue or stop searching. The decision behavior of individuals in such search tasks is described by a search strategy.This paper presents a new approach of finding high-quality search strategies by using genetic algorithms (GAs). Only the structure of the search strategies and the basic building blocks (price thresholds and price patterns) that can be used for the search strategies are pre-specified. It is the purpose of the GA to construct search strategies that well describe human search behavior. The search strategies found by the GA are able to predict human behavior in search tasks better than traditional search strategies from the literature which are usually based on theoretical assumptions about human behavior in search tasks. Furthermore, the found search strategies are reasonable in the sense that they can be well interpreted, and generally that means they describe the search behavior of a larger group of individuals and allow some kind of categorization and classification.The results of this study open a new perspective for future research in developing behavioral strategies. Instead of deriving search strategies from theoretical assumptions about human behavior, researchers can directly analyze human behavior in search tasks and find appropriate and high-quality search strategies. These can be used for gaining new insights into the motivation behind human search and for developing new theoretical models about human search behavior.|Franz Rothlauf,Daniel Schunk,Jella Pfeiffer","57557|GECCO|2005|Improvements to penalty-based evolutionary algorithms for the multi-dimensional knapsack problem using a gene-based adaptive mutation approach|Knapsack problems are among the most common problems in literature tackled with evolutionary algorithms (EA). Their major advantage lies in the fact that they are relatively simple to implement while they allow generalizations for a wide range of real world problems. The multi-dimensional knapsack problem (MKP), which belongs to the class of NP-complete combinatorial optimization problems, is one of the variations of the knapsack problem. The MKP has a wide range of real world applications such as cargo loading, selecting projects to fund, budget management, cutting stock, etc. The MKP has been studied quite extensively in the EA community. Due to the constrained nature of the problem, constraint handling techniques gain great importance in the performance of the proposed EA approaches. In this study, the applicability of a generational EA that uses a penalty-based constraint handling technique and a gene locus based, asymmetric, adaptive mutation scheme is explored for the MKP. The effects of the parameters of the explored approach is determined through tests. Further experiments, using large MKP instances from commonly used benchmarks available through the Internet are performed. Comparison tables are given for the performance of the explored approach and other good performing EAs found in literature for the MKP. Results show that performance improves greatly when compared with other penalty-based techniques, but the explored approach is still not the best performer among all. However, unlike the explored technique, the EAs using the other constraint handling techniques require a great amount of extra computational effort and need heuristic information specific to the optimization problem. Based on these observations, and the fact that the performance difference between the explored scheme and the better performers is not too high, research on improving the explored approach is still in progress.|Sima Uyar,Gülsen Eryigit","57303|GECCO|2005|Information landscapes|We give a new interpretation to the concept of \"landscape\". This allows us to develop a new theoretical model to study search algorithms. Particularly, we are able to quantify the amount and quality of \"information\" embedded in a landscape and to predict the performance of a search algorithm over it. We conclude presenting empirical results for a simple genetic algorithm which strongly support this idea.|Yossi Borenstein,Riccardo Poli"]]}}